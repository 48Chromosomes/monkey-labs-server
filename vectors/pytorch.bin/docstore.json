[["0",{"pageContent":".. _torch_dynamo:\n\ntorch._dynamo\n--------------------------\n\n.. warning ::\n     This module is an early prototype and is subject to change.\n\n.. currentmodule:: torch._dynamo\n\n.. automodule:: torch._dynamo\n    :members:\n    :member-order: bysource","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/_dynamo.rst","loc":{"lines":{"from":1,"to":13}}}}],["1",{"pageContent":".. role:: hidden\n    :class: hidden-section\n.. currentmodule:: {{ module }}\n\n\n{{ name | underline}}\n\n.. autoclass:: {{ name }}\n    :inherited-members:\n    :members:\n\n.. autogenerated from source/_templates/autosummary/class.rst","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/_templates/autosummary/class.rst","loc":{"lines":{"from":1,"to":12}}}}],["2",{"pageContent":".. role:: hidden\n    :class: hidden-section\n.. currentmodule:: {{ module }}\n\n\n{{ name | underline}}\n\n.. autoclass:: {{ name }}\n    :members:\n\n\n..\n  autogenerated from source/_templates/classtemplate.rst\n  note it does not have :inherited-members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/_templates/classtemplate.rst","loc":{"lines":{"from":1,"to":14}}}}],["3",{"pageContent":".. currentmodule:: {{ module }}\n\n\n{{ name | underline}}\n\n.. autoclass:: {{ name }}\n    :members:\n    :exclude-members: MAXBIT, MAXDIM\n    :undoc-members:\n\n\n..\n  autogenerated from source/_templates/sobolengine.rst\n  note it has specific options","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/_templates/sobolengine.rst","loc":{"lines":{"from":1,"to":14}}}}],["4",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\nAutomatic Mixed Precision package - torch.amp\n=============================================\n\n.. Both modules below are missing doc entry. Adding them here for now.\n.. This does not add anything to the rendered page\n.. py:module:: torch.cpu\n.. py:module:: torch.cpu.amp\n.. py:module:: torch.cuda.amp\n\n.. automodule:: torch.amp\n.. currentmodule:: torch.amp\n\n:class:`torch.amp` provides convenience methods for mixed precision,\nwhere some operations use the ``torch.float32`` (``float``) datatype and other operations\nuse lower precision floating point datatype (``lower_precision_fp``): ``torch.float16`` (``half``) or ``torch.bfloat16``. Some ops, like linear layers and convolutions,\nare much faster in ``lower_precision_fp``. Other ops, like reductions, often require the dynamic\nrange of ``float32``.  Mixed precision tries to match each op to its appropriate datatype.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":1,"to":20}}}}],["5",{"pageContent":"Ordinarily, \"automatic mixed precision training\" with datatype of ``torch.float16`` uses :class:`torch.autocast` and\n:class:`torch.cuda.amp.GradScaler` together, as shown in the :ref:`CUDA Automatic Mixed Precision examples<amp-examples>`\nand `CUDA Automatic Mixed Precision recipe <https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html>`_.\nHowever, :class:`torch.autocast` and :class:`torch.cuda.amp.GradScaler` are modular, and may be used separately if desired.\nAs shown in the CPU example section of :class:`torch.autocast`, \"automatic mixed precision training/inference\" on CPU with\ndatatype of ``torch.bfloat16`` only uses :class:`torch.autocast`.\n\nFor CUDA and CPU, APIs are also provided separately:\n\n* ``torch.autocast(\"cuda\", args...)`` is equivalent to ``torch.cuda.amp.autocast(args...)``.\n* ``torch.autocast(\"cpu\", args...)`` is equivalent to ``torch.cpu.amp.autocast(args...)``. For CPU, only lower precision floating point datatype of ``torch.bfloat16`` is supported for now.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":22,"to":32}}}}],["6",{"pageContent":".. contents:: :local:\n\n.. _autocasting:\n\nAutocasting\n^^^^^^^^^^^\n.. currentmodule:: torch\n\n.. autoclass:: autocast\n    :members:\n\n.. currentmodule:: torch.cuda.amp\n\n.. autoclass:: autocast\n    :members:\n\n.. autofunction::  custom_fwd\n\n.. autofunction::  custom_bwd\n\n.. currentmodule:: torch.cpu.amp\n\n.. autoclass:: autocast\n    :members:\n\n.. _gradient-scaling:\n\nGradient Scaling\n^^^^^^^^^^^^^^^^\n\nIf the forward pass for a particular op has ``float16`` inputs, the backward pass for\nthat op will produce ``float16`` gradients.\nGradient values with small magnitudes may not be representable in ``float16``.\nThese values will flush to zero (\"underflow\"), so the update for the corresponding parameters will be lost.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":34,"to":67}}}}],["7",{"pageContent":"To prevent underflow, \"gradient scaling\" multiplies the network's loss(es) by a scale factor and\ninvokes a backward pass on the scaled loss(es).  Gradients flowing backward through the network are\nthen scaled by the same factor.  In other words, gradient values have a larger magnitude,\nso they don't flush to zero.\n\nEach parameter's gradient (``.grad`` attribute) should be unscaled before the optimizer\nupdates the parameters, so the scale factor does not interfere with the learning rate.\n\n.. currentmodule:: torch.cuda.amp\n\n.. autoclass:: GradScaler\n    :members:\n\n.. _autocast-op-reference:\n\nAutocast Op Reference\n^^^^^^^^^^^^^^^^^^^^^\n\n.. _autocast-eligibility:\n\nOp Eligibility\n--------------\nOps that run in ``float64`` or non-floating-point dtypes are not eligible, and will\nrun in these types whether or not autocast is enabled.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":69,"to":92}}}}],["8",{"pageContent":".. _autocast-eligibility:\n\nOp Eligibility\n--------------\nOps that run in ``float64`` or non-floating-point dtypes are not eligible, and will\nrun in these types whether or not autocast is enabled.\n\nOnly out-of-place ops and Tensor methods are eligible.\nIn-place variants and calls that explicitly supply an ``out=...`` Tensor\nare allowed in autocast-enabled regions, but won't go through autocasting.\nFor example, in an autocast-enabled region ``a.addmm(b, c)`` can autocast,\nbut ``a.addmm_(b, c)`` and ``a.addmm(b, c, out=d)`` cannot.\nFor best performance and stability, prefer out-of-place ops in autocast-enabled\nregions.\n\nOps called with an explicit ``dtype=...`` argument are not eligible,\nand will produce output that respects the ``dtype`` argument.\n\n.. _autocast-cuda-op-reference:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":92,"to":110}}}}],["9",{"pageContent":"Ops called with an explicit ``dtype=...`` argument are not eligible,\nand will produce output that respects the ``dtype`` argument.\n\n.. _autocast-cuda-op-reference:\n\nCUDA Op-Specific Behavior\n-------------------------\nThe following lists describe the behavior of eligible ops in autocast-enabled regions.\nThese ops always go through autocasting whether they are invoked as part of a :class:`torch.nn.Module`,\nas a function, or as a :class:`torch.Tensor` method. If functions are exposed in multiple namespaces,\nthey go through autocasting regardless of the namespace.\n\nOps not listed below do not go through autocasting.  They run in the type\ndefined by their inputs.  However, autocasting may still change the type\nin which unlisted ops run if they're downstream from autocasted ops.\n\nIf an op is unlisted, we assume it's numerically stable in ``float16``.\nIf you believe an unlisted op is numerically unstable in ``float16``,\nplease file an issue.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":110,"to":128}}}}],["10",{"pageContent":"If an op is unlisted, we assume it's numerically stable in ``float16``.\nIf you believe an unlisted op is numerically unstable in ``float16``,\nplease file an issue.\n\nCUDA Ops that can autocast to ``float16``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``__matmul__``,\n``addbmm``,\n``addmm``,\n``addmv``,\n``addr``,\n``baddbmm``,\n``bmm``,\n``chain_matmul``,\n``multi_dot``,\n``conv1d``,\n``conv2d``,\n``conv3d``,\n``conv_transpose1d``,\n``conv_transpose2d``,\n``conv_transpose3d``,\n``GRUCell``,\n``linear``,\n``LSTMCell``,\n``matmul``,\n``mm``,\n``mv``,\n``prelu``,\n``RNNCell``\n\nCUDA Ops that can autocast to ``float32``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":128,"to":160}}}}],["11",{"pageContent":"CUDA Ops that can autocast to ``float32``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``__pow__``,\n``__rdiv__``,\n``__rpow__``,\n``__rtruediv__``,\n``acos``,\n``asin``,\n``binary_cross_entropy_with_logits``,\n``cosh``,\n``cosine_embedding_loss``,\n``cdist``,\n``cosine_similarity``,\n``cross_entropy``,\n``cumprod``,\n``cumsum``,\n``dist``,\n``erfinv``,\n``exp``,\n``expm1``,\n``group_norm``,\n``hinge_embedding_loss``,\n``kl_div``,\n``l1_loss``,\n``layer_norm``,\n``log``,\n``log_softmax``,\n``log10``,\n``log1p``,\n``log2``,\n``margin_ranking_loss``,\n``mse_loss``,\n``multilabel_margin_loss``,\n``multi_margin_loss``,\n``nll_loss``,\n``norm``,\n``normalize``,\n``pdist``,\n``poisson_nll_loss``,\n``pow``,\n``prod``,\n``reciprocal``,\n``rsqrt``,\n``sinh``,\n``smooth_l1_loss``,\n``soft_margin_loss``,\n``softmax``,\n``softmin``,\n``softplus``,\n``sum``,\n``renorm``,\n``tan``,\n``triplet_margin_loss``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":160,"to":213}}}}],["12",{"pageContent":"CUDA Ops that promote to the widest input type\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThese ops don't require a particular dtype for stability, but take multiple inputs\nand require that the inputs' dtypes match.  If all of the inputs are\n``float16``, the op runs in ``float16``.  If any of the inputs is ``float32``,\nautocast casts all inputs to ``float32`` and runs the op in ``float32``.\n\n``addcdiv``,\n``addcmul``,\n``atan2``,\n``bilinear``,\n``cross``,\n``dot``,\n``grid_sample``,\n``index_put``,\n``scatter_add``,\n``tensordot``\n\nSome ops not listed here (e.g., binary ops like ``add``) natively promote\ninputs without autocasting's intervention.  If inputs are a mixture of ``float16``\nand ``float32``, these ops run in ``float32`` and produce ``float32`` output,\nregardless of whether autocast is enabled.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":215,"to":236}}}}],["13",{"pageContent":"Prefer ``binary_cross_entropy_with_logits`` over ``binary_cross_entropy``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThe backward passes of :func:`torch.nn.functional.binary_cross_entropy` (and :mod:`torch.nn.BCELoss`, which wraps it)\ncan produce gradients that aren't representable in ``float16``.  In autocast-enabled regions, the forward input\nmay be ``float16``, which means the backward gradient must be representable in ``float16`` (autocasting ``float16``\nforward inputs to ``float32`` doesn't help, because that cast must be reversed in backward).\nTherefore, ``binary_cross_entropy`` and ``BCELoss`` raise an error in autocast-enabled regions.\n\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using :func:`torch.nn.functional.binary_cross_entropy_with_logits`\nor :mod:`torch.nn.BCEWithLogitsLoss`.  ``binary_cross_entropy_with_logits`` and ``BCEWithLogits``\nare safe to autocast.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":238,"to":249}}}}],["14",{"pageContent":".. _autocast-cpu-op-reference:\n\nCPU Op-Specific Behavior\n------------------------\nThe following lists describe the behavior of eligible ops in autocast-enabled regions.\nThese ops always go through autocasting whether they are invoked as part of a :class:`torch.nn.Module`,\nas a function, or as a :class:`torch.Tensor` method. If functions are exposed in multiple namespaces,\nthey go through autocasting regardless of the namespace.\n\nOps not listed below do not go through autocasting.  They run in the type\ndefined by their inputs.  However, autocasting may still change the type\nin which unlisted ops run if they're downstream from autocasted ops.\n\nIf an op is unlisted, we assume it's numerically stable in ``bfloat16``.\nIf you believe an unlisted op is numerically unstable in ``bfloat16``,\nplease file an issue.\n\nCPU Ops that can autocast to ``bfloat16``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":251,"to":269}}}}],["15",{"pageContent":"CPU Ops that can autocast to ``bfloat16``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``conv1d``,\n``conv2d``,\n``conv3d``,\n``bmm``,\n``mm``,\n``baddbmm``,\n``addmm``,\n``addbmm``,\n``linear``,\n``matmul``,\n``_convolution``\n\nCPU Ops that can autocast to ``float32``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":269,"to":285}}}}],["16",{"pageContent":"``conv_transpose1d``,\n``conv_transpose2d``,\n``conv_transpose3d``,\n``avg_pool3d``,\n``binary_cross_entropy``,\n``grid_sampler``,\n``grid_sampler_2d``,\n``_grid_sampler_2d_cpu_fallback``,\n``grid_sampler_3d``,\n``polar``,\n``prod``,\n``quantile``,\n``nanquantile``,\n``stft``,\n``cdist``,\n``trace``,\n``view_as_complex``,\n``cholesky``,\n``cholesky_inverse``,\n``cholesky_solve``,\n``inverse``,\n``lu_solve``,\n``orgqr``,\n``inverse``,\n``ormqr``,\n``pinverse``,\n``max_pool3d``,\n``max_unpool2d``,\n``max_unpool3d``,\n``adaptive_avg_pool3d``,\n``reflection_pad1d``,\n``reflection_pad2d``,\n``replication_pad1d``,\n``replication_pad2d``,\n``replication_pad3d``,\n``mse_loss``,\n``ctc_loss``,\n``kl_div``,\n``multilabel_margin_loss``,\n``fft_fft``,\n``fft_ifft``,\n``fft_fft2``,\n``fft_ifft2``,\n``fft_fftn``,\n``fft_ifftn``,\n``fft_rfft``,\n``fft_irfft``,\n``fft_rfft2``,\n``fft_irfft2``,\n``fft_rfftn``,\n``fft_irfftn``,\n``fft_hfft``,\n``fft_ihfft``,\n``linalg_matrix_norm``,\n``linalg_cond``,\n``linalg_matrix_rank``,\n``linalg_solve``,\n``linalg_cholesky``,\n``linalg_svdvals``,\n``linalg_eigvals``,","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":287,"to":346}}}}],["17",{"pageContent":"``fft_rfftn``,\n``fft_irfftn``,\n``fft_hfft``,\n``fft_ihfft``,\n``linalg_matrix_norm``,\n``linalg_cond``,\n``linalg_matrix_rank``,\n``linalg_solve``,\n``linalg_cholesky``,\n``linalg_svdvals``,\n``linalg_eigvals``,\n``linalg_eigvalsh``,\n``linalg_inv``,\n``linalg_householder_product``,\n``linalg_tensorinv``,\n``linalg_tensorsolve``,\n``fake_quantize_per_tensor_affine``,\n``eig``,\n``geqrf``,\n``lstsq``,\n``_lu_with_info``,\n``qr``,\n``solve``,\n``svd``,\n``symeig``,\n``triangular_solve``,\n``fractional_max_pool2d``,\n``fractional_max_pool3d``,\n``adaptive_max_pool3d``,\n``multilabel_margin_loss_forward``,\n``linalg_qr``,\n``linalg_cholesky_ex``,\n``linalg_svd``,\n``linalg_eig``,\n``linalg_eigh``,\n``linalg_lstsq``,\n``linalg_inv_ex``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":346,"to":382}}}}],["18",{"pageContent":"CPU Ops that promote to the widest input type\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThese ops don't require a particular dtype for stability, but take multiple inputs\nand require that the inputs' dtypes match.  If all of the inputs are\n``bfloat16``, the op runs in ``bfloat16``.  If any of the inputs is ``float32``,\nautocast casts all inputs to ``float32`` and runs the op in ``float32``.\n\n``cat``,\n``stack``,\n``index_copy``\n\nSome ops not listed here (e.g., binary ops like ``add``) natively promote\ninputs without autocasting's intervention.  If inputs are a mixture of ``bfloat16``\nand ``float32``, these ops run in ``float32`` and produce ``float32`` output,\nregardless of whether autocast is enabled.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/amp.rst","loc":{"lines":{"from":384,"to":398}}}}],["19",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\nAutomatic differentiation package - torch.autograd\n==================================================\n\n.. automodule:: torch.autograd\n.. currentmodule:: torch.autograd\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    backward\n    grad\n\n.. _forward-mode-ad:\n\nForward-mode Automatic Differentiation\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. warning::\n    This API is in beta. Even though the function signatures are very unlikely to change, improved\n    operator coverage is planned before we consider this stable.\n\nPlease see the `forward-mode AD tutorial <https://pytorch.org/tutorials/intermediate/forward_ad_usage.html>`__\nfor detailed steps on how to use this API.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    forward_ad.dual_level\n    forward_ad.make_dual\n    forward_ad.unpack_dual\n\n.. _functional-api:\n\nFunctional higher level API\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":1,"to":40}}}}],["20",{"pageContent":"forward_ad.dual_level\n    forward_ad.make_dual\n    forward_ad.unpack_dual\n\n.. _functional-api:\n\nFunctional higher level API\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. warning::\n    This API is in beta. Even though the function signatures are very unlikely to change, major\n    improvements to performances are planned before we consider this stable.\n\nThis section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":40,"to":54}}}}],["21",{"pageContent":"This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.\n\nThis API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don't have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function ``f`` that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as ``f(input, constant, flag=flag)``\nyou can use it as ``functional.jacobian(lambda x: f(x, constant, flag=flag), input)``.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    functional.jacobian\n    functional.hessian\n    functional.vjp\n    functional.jvp\n    functional.vhp\n    functional.hvp\n\n.. _locally-disable-grad:\n\nLocally disabling gradient computation\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":54,"to":79}}}}],["22",{"pageContent":".. _locally-disable-grad:\n\nLocally disabling gradient computation\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSee :ref:`locally-disable-grad-doc` for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two. Also see :ref:`torch-rst-local-disable-grad`\nfor a list of functions that can be used to locally disable gradients.\n\n.. _default-grad-layouts:\n\nDefault gradient layouts\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nWhen a non-sparse ``param`` receives a non-sparse gradient during\n:func:`torch.autograd.backward` or :func:`torch.Tensor.backward`\n``param.grad`` is accumulated as follows.\n\nIf ``param.grad`` is initially ``None``:\n\n1. If ``param``'s memory is non-overlapping and dense, ``.grad`` is\n   created with strides matching ``param`` (thus matching ``param``'s\n   layout).\n2. Otherwise, ``.grad`` is created with rowmajor-contiguous strides.\n\nIf ``param`` already has a non-sparse ``.grad`` attribute:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":79,"to":105}}}}],["23",{"pageContent":"If ``param`` already has a non-sparse ``.grad`` attribute:\n\n3. If ``create_graph=False``, ``backward()`` accumulates into ``.grad``\n   in-place, which preserves its strides.\n4. If ``create_graph=True``, ``backward()`` replaces ``.grad`` with a\n   new tensor ``.grad + new grad``, which attempts (but does not guarantee)\n   matching the preexisting ``.grad``'s strides.\n\nThe default behavior (letting ``.grad``\\ s be ``None`` before the first\n``backward()``, such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to ``model.zero_grad()`` or ``optimizer.zero_grad()`` will not affect ``.grad``\nlayouts.\n\nIn fact, resetting all ``.grad``\\ s to ``None`` before each\naccumulation phase, e.g.::\n\n    for iterations...\n        ...\n        for param in model.parameters():\n            param.grad = None\n        loss.backward()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":105,"to":126}}}}],["24",{"pageContent":"for iterations...\n        ...\n        for param in model.parameters():\n            param.grad = None\n        loss.backward()\n\nsuch that they're recreated according to 1 or 2 every time,\nis a valid alternative to ``model.zero_grad()`` or ``optimizer.zero_grad()``\nthat may improve performance for some networks.\n\nManual gradient layouts\n-----------------------\n\nIf you need manual control over ``.grad``'s strides,\nassign ``param.grad =`` a zeroed tensor with desired strides\nbefore the first ``backward()``, and never reset it to ``None``.\n3 guarantees your layout is preserved as long as ``create_graph=False``.\n4 indicates your layout is *likely* preserved even if ``create_graph=True``.\n\nIn-place operations on Tensors\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":126,"to":146}}}}],["25",{"pageContent":"In-place operations on Tensors\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSupporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd's aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you're operating\nunder heavy memory pressure, you might never need to use them.\n\nIn-place correctness checks\n---------------------------\n\nAll :class:`Tensor` s keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you're using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.\n\nVariable (deprecated)\n^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":146,"to":166}}}}],["26",{"pageContent":"Variable (deprecated)\n^^^^^^^^^^^^^^^^^^^^^\n\n.. warning::\n    The Variable API has been deprecated: Variables are no longer necessary to\n    use autograd with tensors. Autograd automatically supports Tensors with\n    ``requires_grad`` set to ``True``. Below please find a quick guide on what\n    has changed:\n\n    - ``Variable(tensor)`` and ``Variable(tensor, requires_grad)`` still work as expected,\n      but they return Tensors instead of Variables.\n    - ``var.data`` is the same thing as ``tensor.data``.\n    - Methods such as ``var.backward(), var.detach(), var.register_hook()`` now work on tensors\n      with the same method names.\n\n    In addition, one can now create tensors with ``requires_grad=True`` using factory\n    methods such as :func:`torch.randn`, :func:`torch.zeros`, :func:`torch.ones`, and others\n    like the following:\n\n    ``autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)``\n\nTensor autograd functions\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.. autosummary::\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":166,"to":190}}}}],["27",{"pageContent":"``autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)``\n\nTensor autograd functions\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.. autosummary::\n    :nosignatures:\n\n   torch.Tensor.grad\n   torch.Tensor.requires_grad\n   torch.Tensor.is_leaf\n   torch.Tensor.backward\n   torch.Tensor.detach\n   torch.Tensor.detach_\n   torch.Tensor.register_hook\n   torch.Tensor.retain_grad\n\n:hidden:`Function`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. autoclass:: Function\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Function.forward\n    Function.backward\n    Function.jvp\n    Function.vmap\n\nContext method mixins\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nWhen creating a new :class:`Function`, the following methods are available to `ctx`.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    function.FunctionCtx.mark_dirty\n    function.FunctionCtx.mark_non_differentiable\n    function.FunctionCtx.save_for_backward\n    function.FunctionCtx.set_materialize_grads\n\n.. _grad-check:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":190,"to":233}}}}],["28",{"pageContent":"function.FunctionCtx.mark_dirty\n    function.FunctionCtx.mark_non_differentiable\n    function.FunctionCtx.save_for_backward\n    function.FunctionCtx.set_materialize_grads\n\n.. _grad-check:\n\nNumerical gradient checking\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    gradcheck\n    gradgradcheck\n\nProfiler\n^^^^^^^^\n\nAutograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are three modes\nimplemented at the moment - CPU-only using :class:`~torch.autograd.profiler.profile`.\nnvprof based (registers both CPU and GPU activity) using\n:class:`~torch.autograd.profiler.emit_nvtx`.\nand vtune profiler based using\n:class:`~torch.autograd.profiler.emit_itt`.\n\n.. autoclass:: torch.autograd.profiler.profile\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":233,"to":265}}}}],["29",{"pageContent":".. autoclass:: torch.autograd.profiler.profile\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    profiler.profile.export_chrome_trace\n    profiler.profile.key_averages\n    profiler.profile.self_cpu_time_total\n    profiler.profile.total_average\n\n.. autoclass:: torch.autograd.profiler.emit_nvtx\n.. autoclass:: torch.autograd.profiler.emit_itt\n\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    profiler.load_nvprof\n\nAnomaly detection\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. autoclass:: detect_anomaly\n\n.. autoclass:: set_detect_anomaly\n\n\nAutograd graph\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAutograd exposes methods that allow one to inspect the graph and interpose behavior during\nthe backward pass.\n\nThe ``grad_fn`` attribute of a :class:`torch.Tensor` holds a  :class:`torch.autograd.graph.Node`\nif the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is\nenabled and at least one of the inputs required gradients), or ``None`` otherwise.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":265,"to":301}}}}],["30",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    graph.Node.name\n    graph.Node.metadata\n    graph.Node.next_functions\n    graph.Node.register_hook\n    graph.Node.register_prehook\n\nSome operations need intermediary results to be saved during the forward pass\nin order to execute the backward pass.\nThese intermediary results are saved as attributes on the ``grad_fn`` and can be accessed.\nFor example::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":303,"to":316}}}}],["31",{"pageContent":">>> a = torch.tensor([0., 0., 0.], requires_grad=True)\n    >>> b = a.exp()\n    >>> print(isinstance(b.grad_fn, torch.autograd.graph.Node))\n    True\n    >>> print(dir(b.grad_fn))\n    ['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']\n    >>> print(torch.allclose(b.grad_fn._saved_result, b))\n    True","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":318,"to":325}}}}],["32",{"pageContent":"You can also define how these saved tensors should be packed / unpacked using hooks.\nA common application is to trade compute for memory by saving those intermediary results\nto disk or to CPU instead of leaving them on the GPU. This is especially useful if you\nnotice your model fits on GPU during evaluation, but not training.\nAlso see :ref:`saved-tensors-hooks-doc`.\n\n.. autoclass:: torch.autograd.graph.saved_tensors_hooks\n\n.. autoclass:: torch.autograd.graph.save_on_cpu\n\n.. autoclass:: torch.autograd.graph.disable_saved_tensors_hooks\n\n.. autoclass:: torch.autograd.graph.register_multi_grad_hook\n\n.. autoclass:: torch.autograd.graph.allow_mutation_on_saved_tensors","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/autograd.rst","loc":{"lines":{"from":327,"to":341}}}}],["33",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\ntorch.backends\n==============\n.. automodule:: torch.backends\n\n`torch.backends` controls the behavior of various backends that PyTorch supports.\n\nThese backends include:\n\n- ``torch.backends.cuda``\n- ``torch.backends.cudnn``\n- ``torch.backends.mps``\n- ``torch.backends.mkl``\n- ``torch.backends.mkldnn``\n- ``torch.backends.openmp``\n- ``torch.backends.opt_einsum``\n- ``torch.backends.xeon``\n\n\ntorch.backends.cuda\n^^^^^^^^^^^^^^^^^^^\n.. automodule:: torch.backends.cuda\n\n.. autofunction::  torch.backends.cuda.is_built\n\n.. attribute::  torch.backends.cuda.matmul.allow_tf32\n\n    A :class:`bool` that controls whether TensorFloat-32 tensor cores may be used in matrix\n    multiplications on Ampere or newer GPUs. See :ref:`tf32_on_ampere`.\n\n.. attribute::  torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction\n\n    A :class:`bool` that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/backends.rst","loc":{"lines":{"from":1,"to":35}}}}],["34",{"pageContent":"A :class:`bool` that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.\n\n.. attribute::  torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction\n\n    A :class:`bool` that controls whether reduced precision reductions are allowed with bf16 GEMMs.\n\n.. attribute::  torch.backends.cuda.cufft_plan_cache\n\n    ``cufft_plan_cache`` contains the cuFFT plan caches for each CUDA device.\n    Query a specific device `i`'s cache via `torch.backends.cuda.cufft_plan_cache[i]`.\n\n    .. attribute::  size\n\n        A readonly :class:`int` that shows the number of plans currently in a cuFFT plan cache.\n\n    .. attribute::  max_size\n\n        A :class:`int` that controls the capacity of a cuFFT plan cache.\n\n    .. method::  clear()\n\n        Clears a cuFFT plan cache.\n\n.. autofunction:: torch.backends.cuda.preferred_linalg_library\n\n.. autoclass:: torch.backends.cuda.SDPBackend\n\n.. autofunction:: torch.backends.cuda.flash_sdp_enabled","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/backends.rst","loc":{"lines":{"from":35,"to":62}}}}],["35",{"pageContent":"Clears a cuFFT plan cache.\n\n.. autofunction:: torch.backends.cuda.preferred_linalg_library\n\n.. autoclass:: torch.backends.cuda.SDPBackend\n\n.. autofunction:: torch.backends.cuda.flash_sdp_enabled\n\n.. autofunction:: torch.backends.cuda.enable_mem_efficient_sdp\n\n.. autofunction:: torch.backends.cuda.mem_efficient_sdp_enabled\n\n.. autofunction:: torch.backends.cuda.enable_flash_sdp\n\n.. autofunction:: torch.backends.cuda.math_sdp_enabled\n\n.. autofunction:: torch.backends.cuda.enable_math_sdp\n\n.. autofunction:: torch.backends.cuda.sdp_kernel\n\ntorch.backends.cudnn\n^^^^^^^^^^^^^^^^^^^^\n.. automodule:: torch.backends.cudnn\n\n.. autofunction:: torch.backends.cudnn.version\n\n.. autofunction:: torch.backends.cudnn.is_available\n\n.. attribute::  torch.backends.cudnn.enabled\n\n    A :class:`bool` that controls whether cuDNN is enabled.\n\n.. attribute::  torch.backends.cudnn.allow_tf32","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/backends.rst","loc":{"lines":{"from":62,"to":94}}}}],["36",{"pageContent":".. attribute::  torch.backends.cudnn.enabled\n\n    A :class:`bool` that controls whether cuDNN is enabled.\n\n.. attribute::  torch.backends.cudnn.allow_tf32\n\n    A :class:`bool` that controls where TensorFloat-32 tensor cores may be used in cuDNN\n    convolutions on Ampere or newer GPUs. See :ref:`tf32_on_ampere`.\n\n.. attribute::  torch.backends.cudnn.deterministic\n\n    A :class:`bool` that, if True, causes cuDNN to only use deterministic convolution algorithms.\n    See also :func:`torch.are_deterministic_algorithms_enabled` and\n    :func:`torch.use_deterministic_algorithms`.\n\n.. attribute::  torch.backends.cudnn.benchmark\n\n    A :class:`bool` that, if True, causes cuDNN to benchmark multiple convolution algorithms\n    and select the fastest.\n\n.. attribute::  torch.backends.cudnn.benchmark_limit","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/backends.rst","loc":{"lines":{"from":94,"to":114}}}}],["37",{"pageContent":"A :class:`bool` that, if True, causes cuDNN to benchmark multiple convolution algorithms\n    and select the fastest.\n\n.. attribute::  torch.backends.cudnn.benchmark_limit\n\n    A :class:`int` that specifies the maximum number of cuDNN convolution algorithms to try when\n    `torch.backends.cudnn.benchmark` is True. Set `benchmark_limit` to zero to try every\n    available algorithm. Note that this setting only affects convolutions dispatched via the\n    cuDNN v8 API.\n\n\ntorch.backends.mps\n^^^^^^^^^^^^^^^^^^\n.. automodule:: torch.backends.mps\n\n.. autofunction::  torch.backends.mps.is_available\n\n.. autofunction::  torch.backends.mps.is_built\n\n\ntorch.backends.mkl\n^^^^^^^^^^^^^^^^^^\n.. automodule:: torch.backends.mkl\n\n.. autofunction::  torch.backends.mkl.is_available\n\n.. autoclass::  torch.backends.mkl.verbose\n\n\ntorch.backends.mkldnn\n^^^^^^^^^^^^^^^^^^^^^\n.. automodule:: torch.backends.mkldnn\n\n.. autofunction::  torch.backends.mkldnn.is_available\n\n.. autoclass::  torch.backends.mkldnn.verbose","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/backends.rst","loc":{"lines":{"from":114,"to":149}}}}],["38",{"pageContent":"torch.backends.mkldnn\n^^^^^^^^^^^^^^^^^^^^^\n.. automodule:: torch.backends.mkldnn\n\n.. autofunction::  torch.backends.mkldnn.is_available\n\n.. autoclass::  torch.backends.mkldnn.verbose\n\n\ntorch.backends.openmp\n^^^^^^^^^^^^^^^^^^^^^\n.. automodule:: torch.backends.openmp\n\n.. autofunction::  torch.backends.openmp.is_available\n\n.. Docs for other backends need to be added here.\n.. Automodules are just here to ensure checks run but they don't actually\n.. add anything to the rendered page for now.\n.. py:module:: torch.backends.quantized\n.. py:module:: torch.backends.xnnpack\n\n\ntorch.backends.opt_einsum\n^^^^^^^^^^^^^^^^^^^^^^^^^\n.. automodule:: torch.backends.opt_einsum\n\n.. autofunction:: torch.backends.opt_einsum.is_available\n\n.. autofunction:: torch.backends.opt_einsum.get_opt_einsum\n\n.. attribute::  torch.backends.opt_einsum.enabled","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/backends.rst","loc":{"lines":{"from":149,"to":179}}}}],["39",{"pageContent":".. autofunction:: torch.backends.opt_einsum.is_available\n\n.. autofunction:: torch.backends.opt_einsum.get_opt_einsum\n\n.. attribute::  torch.backends.opt_einsum.enabled\n\n    A :class:``bool`` that controls whether opt_einsum is enabled (``True`` by default). If so,\n    torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)\n    if available to calculate an optimal path of contraction for faster performance.\n\n    If opt_einsum is not available, torch.einsum will fall back to the default contraction path\n    of left to right.\n\n.. attribute::  torch.backends.opt_einsum.strategy","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/backends.rst","loc":{"lines":{"from":179,"to":192}}}}],["40",{"pageContent":"If opt_einsum is not available, torch.einsum will fall back to the default contraction path\n    of left to right.\n\n.. attribute::  torch.backends.opt_einsum.strategy\n\n    A :class:``str`` that specifies which strategies to try when ``torch.backends.opt_einsum.enabled``\n    is ``True``. By default, torch.einsum will try the \"auto\" strategy, but the \"greedy\" and \"optimal\"\n    strategies are also supported. Note that the \"optimal\" strategy is factorial on the number of\n    inputs as it tries all possible paths. See more details in opt_einsum's docs\n    (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).\n\n\ntorch.backends.xeon\n^^^^^^^^^^^^^^^^^^^\n.. automodule:: torch.backends.xeon","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/backends.rst","loc":{"lines":{"from":192,"to":206}}}}],["41",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\nBenchmark Utils - torch.utils.benchmark\n==================================================\n\n.. automodule:: torch.utils.benchmark\n.. currentmodule:: torch.utils.benchmark\n\n.. autoclass:: Timer\n    :members:\n\n.. autoclass:: Measurement\n    :members:\n\n.. autoclass:: CallgrindStats\n    :members:\n\n.. autoclass:: FunctionCounts\n    :members:\n\n.. These are missing documentation. Adding them here until a better place\n.. is made in this file.\n.. py:module:: torch.utils.benchmark.examples\n.. py:module:: torch.utils.benchmark.op_fuzzers\n.. py:module:: torch.utils.benchmark.utils\n.. py:module:: torch.utils.benchmark.utils.valgrind_wrapper","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/benchmark_utils.rst","loc":{"lines":{"from":1,"to":27}}}}],["42",{"pageContent":"torch.utils.bottleneck\n======================\n\n.. automodule:: torch.utils.bottleneck\n.. currentmodule:: torch.utils.bottleneck\n\n`torch.utils.bottleneck` is a tool that can be used as an initial step for\ndebugging bottlenecks in your program. It summarizes runs of your script with\nthe Python profiler and PyTorch's autograd profiler.\n\nRun it on the command line with\n\n::\n\n    python -m torch.utils.bottleneck /path/to/source/script.py [args]\n\nwhere [args] are any number of arguments to `script.py`, or run\n``python -m torch.utils.bottleneck -h`` for more usage instructions.\n\n.. warning::\n    Because your script will be profiled, please ensure that it exits in a\n    finite amount of time.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/bottleneck.rst","loc":{"lines":{"from":1,"to":22}}}}],["43",{"pageContent":".. warning::\n    Because your script will be profiled, please ensure that it exits in a\n    finite amount of time.\n\n.. warning::\n    Due to the asynchronous nature of CUDA kernels, when running against\n    CUDA code, the cProfile output and CPU-mode autograd profilers may\n    not show correct timings: the reported CPU time reports the amount of time\n    used to launch the kernels but does not include the time the kernel\n    spent executing on a GPU unless the operation does a synchronize.\n    Ops that do synchronize appear to be extremely expensive under regular\n    CPU-mode profilers.\n    In these case where timings are incorrect, the CUDA-mode autograd profiler\n    may be helpful.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/bottleneck.rst","loc":{"lines":{"from":22,"to":35}}}}],["44",{"pageContent":".. note::\n    To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\n    look at, you should first check if your script is CPU-bound\n    (\"CPU total time is much greater than CUDA total time\").\n    If it is CPU-bound, looking at the results of the CPU-mode autograd\n    profiler will help. If on the other hand your script spends most of its\n    time executing on the GPU, then it makes sense to start\n    looking for responsible CUDA operators in the output of the CUDA-mode\n    autograd profiler.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/bottleneck.rst","loc":{"lines":{"from":37,"to":45}}}}],["45",{"pageContent":"Of course the reality is much more complicated and your script might not be\n    in one of those two extremes depending on the part of the model you're\n    evaluating. If the profiler outputs don't help, you could try looking at\n    the result of :func:`torch.autograd.profiler.emit_nvtx()` with ``nvprof``.\n    However, please take into account that the NVTX overhead is very high and\n    often gives a heavily skewed timeline. Similarly, ``Intel® VTune™ Profiler``\n    helps to analyze performance on Intel platforms further with\n    :func:`torch.autograd.profiler.emit_itt()`.\n\n.. warning::\n    If you are profiling CUDA code, the first profiler that ``bottleneck`` runs\n    (cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\n    in its time reporting. This should not matter if your bottlenecks result\n    in code much slower than the CUDA startup time.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/bottleneck.rst","loc":{"lines":{"from":47,"to":60}}}}],["46",{"pageContent":"For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor :func:`torch.autograd.profiler.profile()` for more information.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/bottleneck.rst","loc":{"lines":{"from":62,"to":64}}}}],["47",{"pageContent":"torch.utils.checkpoint\n======================\n\n.. note::\n    Checkpointing is implemented by rerunning a forward-pass segment for\n    each checkpointed segment during backward.  This can cause persistent\n    states like the RNG state to be advanced than they would without\n    checkpointing.  By default, checkpointing includes logic to juggle\n    the RNG state such that checkpointed passes making use of RNG\n    (through dropout for example) have deterministic output as\n    compared to non-checkpointed passes.  The logic to stash and restore\n    RNG states can incur a moderate performance hit depending on the runtime\n    of checkpointed operations.  If deterministic output compared to\n    non-checkpointed passes is not required, supply ``preserve_rng_state=False``\n    to ``checkpoint`` or ``checkpoint_sequential`` to omit stashing and\n    restoring the RNG state during each checkpoint.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/checkpoint.rst","loc":{"lines":{"from":1,"to":16}}}}],["48",{"pageContent":"The stashing logic saves and restores the RNG state for the current device\n    and the device of all cuda Tensor arguments to the ``run_fn``.\n    However, the logic has no way to anticipate if the user will move\n    Tensors to a new device within the ``run_fn`` itself.  Therefore, if you move\n    Tensors to a new device (\"new\" meaning not belonging to the set of\n    [current device + devices of Tensor arguments]) within ``run_fn``, deterministic\n    output compared to non-checkpointed passes is never guaranteed.\n\n.. currentmodule:: torch.utils.checkpoint\n.. autofunction:: checkpoint\n.. autofunction:: checkpoint_sequential","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/checkpoint.rst","loc":{"lines":{"from":18,"to":28}}}}],["49",{"pageContent":"PyTorch Governance | Build + CI\n===============================\n\nHow to Add a New Maintainer\n---------------------------\n\nFor the person to be a maintainer, a person needs to:\n\n* Land at least six commits to the related part of the PyTorch repository\n* At least one of these commits must be submitted in the last six months","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/build_ci_governance.rst","loc":{"lines":{"from":1,"to":10}}}}],["50",{"pageContent":"For the person to be a maintainer, a person needs to:\n\n* Land at least six commits to the related part of the PyTorch repository\n* At least one of these commits must be submitted in the last six months\n\nTo add a qualified person to the maintainers' list, please create\na PR that adds a person to the `persons of interests <https://pytorch.org/docs/master/community/persons_of_interest.html>`__ page and\n`merge_rules <https://github.com/pytorch/pytorch/blob/master/.github/merge_rules.yaml>`__ files. Current maintainers will cast their votes of\nsupport. Decision criteria for approving the PR:\n* Not earlier than two business days passed before merging (ensure the majority of the contributors have seen it)\n* PR has the correct label (`module: ci`)\n* There are no objections from the current maintainers\n* There are at least three net *thumbs up* from current maintainers (or all maintainers vote *thumbs up* when the module has less than 3 maintainers).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/build_ci_governance.rst","loc":{"lines":{"from":10,"to":22}}}}],["51",{"pageContent":"PyTorch Contribution Guide\n==========================\n\nPyTorch is a GPU-accelerated Python tensor computation package for\nbuilding deep neural networks using a tape-based autograd systems.\n\nContribution Process\n--------------------\n\nThe PyTorch organization is governed by :doc:`PyTorch\nGovernance <governance>` and the technical guide to contributing\ncan be found in `CONTRIBUTING.md <https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md>`_.\n\nThe PyTorch development process involves a healthy amount of open\ndiscussions between the core development team and the community.\n\nPyTorch operates similarly to most open source projects on GitHub.\nHowever, if you've never contributed to an open source project before,\nhere is the basic process.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":1,"to":19}}}}],["52",{"pageContent":"PyTorch operates similarly to most open source projects on GitHub.\nHowever, if you've never contributed to an open source project before,\nhere is the basic process.\n\n-  **Figure out what you're going to work on.** The majority of open\n   source contributions come from people scratching their own itches.\n   However, if you don't know what you want to work on, or are just\n   looking to get more acquainted with the project, here are some tips\n   for how to find appropriate tasks:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":19,"to":27}}}}],["53",{"pageContent":"-  Look through the `issue\n      tracker <https://github.com/pytorch/pytorch/issues/>`__ and see if\n      there are any issues you know how to fix. Issues that are\n      confirmed by other contributors tend to be better to investigate.\n      We also maintain some labels for issues that are likely to be\n      good for new people, e.g., **bootcamp** and **1hr**, although\n      these labels are less well maintained.\n   -  Join us on `dev discuss <https://dev-discuss.pytorch.org/>`_\n      and let us know you're interested in getting to\n      know PyTorch. We're very happy to help out researchers and\n      partners get up to speed with the codebase.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":29,"to":39}}}}],["54",{"pageContent":"-  **Figure out the scope of your change and reach out for design\n   comments on a GitHub issue if it's large.** The majority of pull\n   requests are small; in that case, no need to let us know about what\n   you want to do, just get cracking. But if the change is going to be\n   large, it's usually a good idea to get some design comments about it\n   first by `submitting an RFC <https://github.com/pytorch/rfcs/blob/master/README.md>`__.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":41,"to":46}}}}],["55",{"pageContent":"-  If you don't know how big a change is going to be, we can help you\n      figure it out! Just post about it on\n      `issues <https://github.com/pytorch/pytorch/issues/>`_ or\n      `dev discuss <https://dev-discuss.pytorch.org/>`_.\n   -  Some feature additions are very standardized; for example, lots of\n      people add new operators or optimizers to PyTorch. Design\n      discussion in these cases boils down mostly to, “Do we want this\n      operator/optimizer?” Giving evidence for its utility, e.g., usage\n      in peer reviewed papers, or existence in other frameworks, helps a\n      bit when making this case.\n\n      - **Adding operators / algorithms from recently-released research**\n        is generally not accepted unless there is overwhelming evidence that\n        this newly published work has ground-breaking results and will eventually\n        become a standard in the field. If you are not sure where your method falls,\n        open an issue first before implementing a PR.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":48,"to":63}}}}],["56",{"pageContent":"-  Core changes and refactors can be quite difficult to coordinate\n      since the pace of development on PyTorch master is quite fast.\n      Definitely reach out about fundamental or cross-cutting changes;\n      we can often give guidance about how to stage such changes into\n      more easily reviewable pieces.\n\n-  **Code it out!**\n\n   -  See the `CONTRIBUTING.md <https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md>`_ file for advice for working with PyTorch in a\n      technical form.\n\n-  **Open a pull request.**","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":65,"to":76}}}}],["57",{"pageContent":"-  If you are not ready for the pull request to be reviewed, create a draft\n      pull request first - you can later convert it to a full PR by pressing\n      \"Ready for review\" button. You can also prepend the title of the PR with\n      \"[WIP]\" (\"work in progress\") while it's still in draft. We will ignore\n      draft PRs when doing review passes. If you are working on a complex change,\n      it's good to start things off as a draft, because you will need to spend\n      time looking at CI results to see if things worked out or not.\n   -  Find an appropriate reviewer for your change. We have some folks\n      who regularly go through the PR queue and try to review\n      everything, but if you happen to know who the maintainer for a\n      given subsystem affected by your patch is, feel free to include\n      them directly on the pull request. You can learn more about\n      `Persons of Interest <https://pytorch.org/docs/master/community/persons_of_interest.html>`_\n      that could review your code.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":78,"to":91}}}}],["58",{"pageContent":"-  **Iterate on the pull request until it's accepted!**\n\n   -  We'll try our best to minimize the number of review round trips and\n      block PRs only when there are major issues. For the most common\n      issues in pull requests, take a look at `Common Mistakes <#common-mistakes-to-avoid>`__.\n   -  Once a pull request is accepted and CI is passing, there is\n      nothing else you need to do; we will merge the PR for you.\n\nGetting Started\n---------------\n\nProposing New Features\n~~~~~~~~~~~~~~~~~~~~~~\n\nNew feature ideas are best discussed on a specific issue. Please include\nas much information as you can, any accompanying data, and your proposed\nsolution. The PyTorch team and community frequently review new issues\nand comments where they think they can help. If you feel confident in\nyour solution, go ahead and implement it.\n\nReporting Issues\n~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":93,"to":114}}}}],["59",{"pageContent":"Reporting Issues\n~~~~~~~~~~~~~~~~\n\nIf you've identified an issue, first search through the `list of\nexisting issues <https://github.com/pytorch/pytorch/issues>`__ on the\nrepo. If you are unable to find a similar issue, then create a new one.\nSupply as much information you can to reproduce the problematic\nbehavior. Also, include any additional insights like the behavior you\nexpect.\n\nImplementing Features or Fixing Bugs\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you want to fix a specific issue, it's best to comment on the\nindividual issue with your intent. However, we do not lock or assign\nissues except in cases where we have worked with the developer before.\nIt's best to strike up a conversation on the issue and discuss your\nproposed solution. The PyTorch team can provide guidance that saves you\ntime.\n\nIssues that are labeled first-new-issue, low, or medium priority provide\nthe best entrance points and are great places to start.\n\nAdding Tutorials\n~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":114,"to":138}}}}],["60",{"pageContent":"Issues that are labeled first-new-issue, low, or medium priority provide\nthe best entrance points and are great places to start.\n\nAdding Tutorials\n~~~~~~~~~~~~~~~~\n\nA great deal of the tutorials on `pytorch.org <https://pytorch.org/>`__\ncome from the community itself and we welcome additional contributions.\nTo learn more about how to contribute a new tutorial you can learn more\nhere: `PyTorch.org Tutorial Contribution Guide on\nGitHub <https://github.com/pytorch/tutorials/#contributing>`__\n\nImproving Documentation & Tutorials\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe aim to produce high quality documentation and tutorials. On rare\noccasions that content includes typos or bugs. If you find something you\ncan fix, send us a pull request for consideration.\n\nTake a look at the `Documentation <#on-documentation>`__ section to learn how our system\nworks.\n\nParticipating in Online Discussions\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":138,"to":161}}}}],["61",{"pageContent":"Take a look at the `Documentation <#on-documentation>`__ section to learn how our system\nworks.\n\nParticipating in Online Discussions\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can find active discussions happening on the `PyTorch Discussion\nForums <https://discuss.pytorch.org/>`__  for users as well as the\n`PyTorch Dev Discussion Forums <https://dev-discuss.pytorch.org/>`__\nfor developers and maintainers.\n\nSubmitting Pull Requests to Fix Open Issues\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can view a list of all open issues\n`here <https://github.com/pytorch/pytorch/issues>`__. Commenting on an\nissue is a great way to get the attention of the team. From here you can\nshare your ideas and how you plan to resolve the issue.\n\nFor more challenging issues, the team will provide feedback and\ndirection for how to best solve the issue.\n\nIf you're not able to fix the issue yourself, commenting and sharing\nwhether you can reproduce the issue can help the team\nidentify problem areas.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":161,"to":185}}}}],["62",{"pageContent":"If you're not able to fix the issue yourself, commenting and sharing\nwhether you can reproduce the issue can help the team\nidentify problem areas.\n\nReviewing Open Pull Requests\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe appreciate your help reviewing and commenting on pull requests. Our\nteam strives to keep the number of open pull requests at a manageable\nsize, we respond quickly for more information if we need it, and we\nmerge PRs that we think are useful. However, due to the high level of\ninterest, additional eyes on the pull requests are always appreciated.\n\nImproving Code Readability\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nImproving code readability helps everyone. It is often better to submit a\nsmall number of pull requests that touch a few files versus a large pull\nrequest that touches many files. Starting a discussion in the PyTorch\nforum `here <https://discuss.pytorch.org/>`__ or on an issue related to\nyour improvement is the best way to get started.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":185,"to":205}}}}],["63",{"pageContent":"Adding Test Cases to Make the Codebase More Robust\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAdditional test coverage is appreciated.\n\nPromoting PyTorch\n~~~~~~~~~~~~~~~~~\n\nYour use of PyTorch in your projects, research papers, write ups, blogs,\nor general discussions around the internet helps to raise awareness for\nPyTorch and our growing community. Please reach out to\n`marketing@pytorch.org <mailto:marketing@pytorch.org>`__\nfor marketing support.\n\nTriaging Issues\n~~~~~~~~~~~~~~~\n\nIf you feel that an issue could benefit from a particular tag or level\nof complexity, comment on the issue and share your opinion. If you\nfeel an issue isn't categorized properly, comment and let the team know.\n\nAbout Open Source Development\n-----------------------------\n\nIf this is your first time contributing to an open source project, some\naspects of the development process may seem unusual to you.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":207,"to":232}}}}],["64",{"pageContent":"-  **There is no way to “claim” issues.** People often want to “claim”\n   an issue when they decide to work on it, to ensure that there isn't\n   wasted work when someone else ends up working on it. This doesn't\n   really work too well in open source, since someone may decide to work\n   on something, and end up not having time to do it. Feel free to give\n   information in an advisory fashion, but at the end of the day, we\n   will take running code and rough consensus to move forward quickly.\n-  **There is a high bar for new functionality.** Unlike\n   in a corporate environment, where the person who wrote code\n   implicitly “owns” it and can be expected to take care of it for the\n   code's lifetime, once a pull request is merged into an open\n   source project, it immediately becomes the collective responsibility\n   of all maintainers on the project. When we merge code, we are saying\n   that we, the maintainers, can review subsequent changes and","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":234,"to":247}}}}],["65",{"pageContent":"of all maintainers on the project. When we merge code, we are saying\n   that we, the maintainers, can review subsequent changes and\n   make a bugfix to the code. This naturally leads to a higher standard\n   of contribution.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":247,"to":250}}}}],["66",{"pageContent":"Common Mistakes To Avoid\n------------------------\n\n-  **Did you add tests?** (Or if the change is hard to test, did you\n   describe how you tested your change?)\n\n   -  We have a few motivations for why we ask for tests:\n\n      1. to help us tell if we break it later\n      2. to help us tell if the patch is correct in the first place\n         (yes, we did review it, but as Knuth says, “beware of the\n         following code, for I have not run it, merely proven it\n         correct”)\n\n   -  When is it OK not to add a test? Sometimes a change can't be\n      conveniently tested, or the change is so obviously correct (and\n      unlikely to be broken) that it's OK not to test it. On the\n      contrary, if a change seems likely (or is known to be likely)\n      to be accidentally broken, it's important to put in the time to\n      work out a testing strategy.\n\n-  **Is your PR too long?**","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":252,"to":273}}}}],["67",{"pageContent":"-  **Is your PR too long?**\n\n   -  It's easier for us to review and merge small PRs. The difficulty of\n      reviewing a PR scales nonlinearly with its size.\n   -  When is it OK to submit a large PR? It helps a lot if there was a\n      corresponding design discussion in an issue, with sign off from\n      the people who are going to review your diff. We can also help\n      give advice about how to split up a large change into individually\n      shippable parts. Similarly, it helps if there is a complete\n      description of the contents of the PR: it's easier to review code\n      if we know what's inside!","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":273,"to":283}}}}],["68",{"pageContent":"-  **Comments for subtle things?** In cases where the behavior of your code\n   is nuanced, please include extra comments and documentation to allow\n   us to better understand the intention of your code.\n-  **Did you add a hack?** Sometimes, the right answer is a hack. But\n   usually, we will have to discuss it.\n-  **Do you want to touch a very core component?** To prevent\n   major regressions, pull requests that touch core components receive\n   extra scrutiny. Make sure you've discussed your changes with the team\n   before undertaking major changes.\n-  **Want to add a new feature?** If you want to add new features,\n   comment your intention on the related issue. Our team tries to\n   comment on and provide feedback to the community. It's better to have\n   an open discussion with the team and the rest of the community before\n   building new features. This helps us stay aware of what you're\n   working on and increases the chance that it'll be merged.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":285,"to":299}}}}],["69",{"pageContent":"an open discussion with the team and the rest of the community before\n   building new features. This helps us stay aware of what you're\n   working on and increases the chance that it'll be merged.\n-  **Did you touch code unrelated to the PR?** To aid in code review,\n   please only include files in your pull request that are directly\n   related to your changes.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":299,"to":304}}}}],["70",{"pageContent":"Frequently Asked Questions\n--------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":306,"to":307}}}}],["71",{"pageContent":"-  **How can I contribute as a reviewer?** There is lots of value if\n   community developers reproduce issues, try out new functionality, or\n   otherwise help us identify or troubleshoot issues. Commenting on\n   tasks or pull requests with your environment details is helpful and\n   appreciated.\n-  **CI tests failed, what does it mean?** Maybe your PR is based\n   off a broken master? You can try to rebase your change on top\n   of the latest master. You can also see the current status of\n   master's CI at https://hud.pytorch.org/.\n-  **What are the most high risk changes?** Anything that touches build\n   configuration is a risky area. Please avoid changing these unless\n   you've had a discussion with the team beforehand.\n-  **Hey, a commit showed up on my branch, what's up with that?**\n   Sometimes another community member will provide a patch or fix to\n   your pull request or branch. This is often needed for getting CI tests\n   to pass.\n\nOn Documentation\n----------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":309,"to":327}}}}],["72",{"pageContent":"On Documentation\n----------------\n\nPython Docs\n~~~~~~~~~~~\n\nPyTorch documentation is generated from python source using\n`Sphinx <https://www.sphinx-doc.org/en/master/>`__. Generated HTML is\ncopied to the docs folder in the master branch of\n`pytorch.github.io <https://github.com/pytorch/pytorch.github.io/tree/master/docs>`__,\nand is served via GitHub pages.\n\n-  Site: https://pytorch.org/docs\n-  GitHub: https://github.com/pytorch/pytorch/tree/master/docs\n-  Served from:\n   `https://github.com/pytorch/pytorch.github.io/tree/master/doc <https://github.com/pytorch/pytorch.github.io/tree/master/docs>`__\n\nC++ Docs\n~~~~~~~~\n\nFor C++ code we use Doxygen to generate the content files. The C++ docs\nare built on a special server and the resulting files are copied to the\nhttps://github.com/pytorch/cppdocs repo, and are served from GitHub\npages.\n\n-  Site: https://pytorch.org/cppdocs\n-  GitHub: https://github.com/pytorch/pytorch/tree/master/docs/cpp\n-  Served from: https://github.com/pytorch/cppdocs","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":327,"to":354}}}}],["73",{"pageContent":"-  Site: https://pytorch.org/cppdocs\n-  GitHub: https://github.com/pytorch/pytorch/tree/master/docs/cpp\n-  Served from: https://github.com/pytorch/cppdocs\n\nTutorials\n---------\n\nPyTorch tutorials are documents used to help understand using PyTorch to\naccomplish specific tasks or to understand more holistic concepts.\nTutorials are built using\n`Sphinx-Gallery <https://sphinx-gallery.readthedocs.io/en/latest/index.html>`__\nfrom executable python source files, or from restructured-text (rst)\nfiles.\n\n-  Site: https://pytorch.org/tutorials\n-  GitHub: https://github.com/pytorch/tutorials\n\nTutorials Build Overview\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":354,"to":372}}}}],["74",{"pageContent":"-  Site: https://pytorch.org/tutorials\n-  GitHub: https://github.com/pytorch/tutorials\n\nTutorials Build Overview\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFor tutorials, `pull\nrequests <https://github.com/pytorch/tutorials/pulls>`__ trigger a\nrebuild of the entire site using CircleCI to test the effects of the\nchange. This build is sharded into 9 worker builds and takes around 40\nminutes total. At the same time, we do a Netlify build using *make\nhtml-noplot*, which builds the site without rendering the notebook\noutput into pages for quick review.\n\nAfter a PR is accepted, the site is rebuilt and deployed using GitHub\nActions.\n\nContributing a New Tutorial\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSee `PyTorch.org Tutorial Contribution\nGuide <https://github.com/pytorch/tutorials/#contributing>`__.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/contribution_guide.rst","loc":{"lines":{"from":372,"to":393}}}}],["75",{"pageContent":"PyTorch Design Philosophy\n=========================\n\nThis document is designed to help contributors and module maintainers\nunderstand the high-level design principles that have developed over\ntime in PyTorch. These are not meant to be hard-and-fast rules, but to\nserve as a guide to help trade off different concerns and to resolve\ndisagreements that may come up while developing PyTorch. For more\ninformation on contributing, module maintainership, and how to escalate a\ndisagreement to the Core Maintainers, please see `PyTorch\nGovernance <https://pytorch.org/docs/master/community/governance.html>`__.\n\nDesign Principles\n-----------------\n\nPrinciple 1: Usability over Performance\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis principle may be surprising! As one Hacker News poster wrote:\n*PyTorch is amazing! [...] Although I’m confused. How can a ML framework be\nnot obsessed with speed/performance?* See `Hacker News discussion on\nPyTorch <https://news.ycombinator.com/item?id=28066093>`__.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/design.rst","loc":{"lines":{"from":1,"to":22}}}}],["76",{"pageContent":"Soumith’s blog post on `Growing the PyTorch\nCommunity <https://soumith.ch/posts/2021/02/growing-opensource/?fbclid=IwAR1bvN_xZ8avGvu14ODJzS8Zp7jX1BOyfuGUf-zoRawpyL-s95Vjxf88W7s>`__\ngoes into this in some depth, but at a high-level:\n\n-  PyTorch’s primary goal is usability\n-  A secondary goal is to have *reasonable* performance\n\nWe believe the ability to maintain our flexibility to support\nresearchers who are building on top of our abstractions remains\ncritical. We can’t see what the future of what workloads will be, but we\nknow we want them to be built first on PyTorch and that requires\nflexibility.\n\nIn more concrete terms, we operate in a *usability-first* manner and try\nto avoid jumping to *restriction-first* regimes (for example, static shapes,\ngraph-mode only) without a clear-eyed view of the tradeoffs. Often there\nis a temptation to impose strict user restrictions upfront because it\ncan simplify implementation, but this comes with risks:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/design.rst","loc":{"lines":{"from":24,"to":41}}}}],["77",{"pageContent":"-  The performance may not be worth the user friction, either because\n   the performance benefit is not compelling enough or it only applies to\n   a relatively narrow set of subproblems.\n-  Even if the performance benefit is compelling, the restrictions can\n   fragment the ecosystem into different sets of limitations that can\n   quickly become incomprehensible to users.\n\nWe want users to be able to seamlessly move their PyTorch code to\ndifferent hardware and software platforms, to interoperate with\ndifferent libraries and frameworks, and to experience the full richness\nof the PyTorch user experience, not a least common denominator subset.\n\nPrinciple 2: Simple Over Easy\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nHere, we borrow from `The Zen of\nPython <https://peps.python.org/pep-0020/>`__:\n\n-  *Explicit is better than implicit*\n-  *Simple is better than complex*","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/design.rst","loc":{"lines":{"from":43,"to":62}}}}],["78",{"pageContent":"Here, we borrow from `The Zen of\nPython <https://peps.python.org/pep-0020/>`__:\n\n-  *Explicit is better than implicit*\n-  *Simple is better than complex*\n\nA more concise way of describing these two goals is `Simple Over\nEasy <https://www.infoq.com/presentations/Simple-Made-Easy/>`_. Let’s start with an example because *simple* and *easy* are\noften used interchangeably in everyday English. Consider how one may\nmodel `devices <https://pytorch.org/docs/master/tensor_attributes.html#torch.device>`__\nin PyTorch:\n\n-  **Simple / Explicit (to understand, debug):** every tensor is associated\n   with a device. The user explicitly specifies tensor device movement.\n   Operations that require cross-device movement result in an error.\n-  **Easy / Implicit (to use):** the user does not have to worry about\n   devices; the system figures out the globally optimal device\n   placement.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/design.rst","loc":{"lines":{"from":62,"to":79}}}}],["79",{"pageContent":"In this specific case, and as a general design philosophy, PyTorch\nfavors exposing simple and explicit building blocks rather than APIs\nthat are easy-to-use by practitioners. The simple version is immediately\nunderstandable and debuggable by a new PyTorch user: you get a clear\nerror if you call an operator requiring cross-device movement at the\npoint in the program where the operator is actually invoked. The easy\nsolution may let a new user move faster initially, but debugging such a\nsystem can be complex: How did the system make its determination? What\nis the API for plugging into such a system and how are objects\nrepresented in its IR?","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/design.rst","loc":{"lines":{"from":81,"to":90}}}}],["80",{"pageContent":"Some classic arguments in favor of this sort of design come from `A\nNote on Distributed\nComputation <https://dl.acm.org/doi/book/10.5555/974938>`__ (TLDR: Do not\nmodel resources with very different performance characteristics\nuniformly, the details will leak) and the `End-to-End\nPrinciple <http://web.mit.edu/Saltzer/www/publications/endtoend/endtoend.pdf>`__\n(TLDR: building smarts into the lower-layers of the stack can prevent\nbuilding performant features at higher layers in the stack, and often\ndoesn’t work anyway). For example, we could build operator-level or\nglobal device movement rules, but the precise choices aren’t obvious and\nbuilding an extensible mechanism has unavoidable complexity and latency\ncosts.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/design.rst","loc":{"lines":{"from":92,"to":103}}}}],["81",{"pageContent":"A caveat here is that this does not mean that higher-level “easy” APIs\nare not valuable; certainly there is a value in, for example,\nhigher-levels in the stack to support efficient tensor computations\nacross heterogeneous compute in a large cluster. Instead, what we mean\nis that focusing on simple lower-level building blocks helps inform the\neasy API while still maintaining a good experience when users need to\nleave the beaten path. It also allows space for innovation and the\ngrowth of more opinionated tools at a rate we cannot support in the\nPyTorch core library, but ultimately benefit from, as evidenced by\nour `rich ecosystem <https://pytorch.org/ecosystem/>`__. In other\nwords, not automating at the start allows us to potentially reach levels\nof good automation faster.\n\nPrinciple 3: Python First with Best In Class Language Interoperability\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis principle began as **Python First**:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/design.rst","loc":{"lines":{"from":105,"to":121}}}}],["82",{"pageContent":"Principle 3: Python First with Best In Class Language Interoperability\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis principle began as **Python First**:\n\n  PyTorch is not a Python binding into a monolithic C++ framework.\n  It is built to be deeply integrated into Python. You can use it\n  naturally like you would use `NumPy <https://www.numpy.org/>`__,\n  `SciPy <https://www.scipy.org/>`__, `scikit-learn <(https://scikit-learn.org/>`__,\n  or other Python libraries. You can write your new neural network\n  layers in Python itself, using your favorite libraries and use\n  packages such as `Cython <https://cython.org/>`__ and\n  `Numba <http://numba.pydata.org/>`__. Our goal is to not reinvent\n  the wheel where appropriate.\n\nOne thing PyTorch has needed to deal with over the years is Python\noverhead: we first rewrote the `autograd` engine in C++, then the majority\nof operator definitions, then developed TorchScript and the C++\nfrontend.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/design.rst","loc":{"lines":{"from":121,"to":139}}}}],["83",{"pageContent":"Still, working in Python provides easily the best experience for our\nusers: it is flexible, familiar, and perhaps most importantly, has a\nhuge ecosystem of scientific computing libraries and extensions\navailable for use. This fact motivates a few of our most recent\ncontributions, which attempt to hit a Pareto optimal point close to the\nPython usability end of the curve:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/design.rst","loc":{"lines":{"from":141,"to":146}}}}],["84",{"pageContent":"-  `TorchDynamo <https://dev-discuss.pytorch.org/t/torchdynamo-an-experiment-in-dynamic-python-bytecode-transformation/361>`__,\n   a Python frame evaluation tool capable of speeding up existing\n   eager-mode PyTorch programs with minimal user intervention.\n-  `torch_function <https://pytorch.org/docs/master/notes/extending.html#extending-torch>`__\n   and `torch_dispatch <https://dev-discuss.pytorch.org/t/what-and-why-is-torch-dispatch/557>`__\n   extension points, which have enabled Python-first functionality to be\n   built on-top of C++ internals, such as the `torch.fx\n   tracer <https://pytorch.org/docs/stable/fx.html>`__\n   and `functorch <https://github.com/pytorch/functorch>`__\n   respectively.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/design.rst","loc":{"lines":{"from":148,"to":157}}}}],["85",{"pageContent":"These design principles are not hard-and-fast rules, but hard won\nchoices and anchor how we built PyTorch to be the debuggable, hackable\nand flexible framework it is today. As we have more contributors and\nmaintainers, we look forward to applying these core principles with you\nacross our libraries and ecosystem. We are also open to evolving them as\nwe learn new things and the AI space evolves, as we know it will.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/design.rst","loc":{"lines":{"from":159,"to":164}}}}],["86",{"pageContent":"PyTorch Governance | Mechanics\n==============================\n\nSummary\n-------\n\nPyTorch adopts a technical governance structure that is hierarchical.\n\n* A community of **contributors** who file issues, make pull requests,\n  and contribute to the project.\n* A small set of **module maintainers** drive each module of the PyTorch\n  project.\n* They are overseen by **core maintainers**, who drive the\n  overall project direction.\n* The core maintainers have a **lead core maintainer**\n  who is the catch-all decision maker.\n\nAll maintainers are expected to have a strong bias towards\nPyTorch’s design philosophy.\n\nBeyond the maintainers, the community is encouraged to contribute,\nfile issues, make proposals, review pull requests and be present\nin the community. Given contributions and willingness to invest,\nanyone can be accepted as a maintainer and provided write access\nor ownership of parts of the codebase.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":1,"to":25}}}}],["87",{"pageContent":"Technical governance is strictly separated from business governance.\nSeparating technical from business governance ensures that there is\nno way for any person or company to “buy their way into” the\ntechnical guidance of the project. Additionally, membership in\nthe technical governance process is for **individuals**, not companies.\nThat is, there are no seats reserved for specific companies, and\nmembership is associated with the person rather than the company\nemploying that person.\n\nModule Maintainers\n------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":27,"to":37}}}}],["88",{"pageContent":"Modules are defined as GitHub repositories within the PyTorch org,\nor as directories within the core repository\n`pytorch/pytorch <https://github.com/pytorch/pytorch>`__.\nEach module will have its own maintainer group. Maintainer\ngroups are responsible for reviewing and approving commits,\nimproving design, and changing the scope of the module.\nEach maintainer group may adopt its own rules and procedures\nfor making decisions (majority vote being default). Module\nmaintainers have the right to dispute decisions made by other\nmodule maintainers -- especially if it affects them. When\ndisputes are made, the module maintainer group should\nprovide a reasonable and public explanation of the dispute,\nthe relevant arguments, and the resolution. In the exceptional\ncases where module maintainers cannot come to a conclusion\nthemselves, they will escalate to core maintainers for review.\nThe escalations are resolved by the core maintainers in\naccordance with their rules and procedures.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":39,"to":55}}}}],["89",{"pageContent":"Each maintainer group should publish publicly available\ncommunication for their module (a vision, rough roadmap,\ndesign docs, any disputes and dispute resolutions) so that\ncontributors and other interested parties understand the\nfuture direction of the project and can participate in discussion.\n\nResponsibilities of the maintainer includes:\n\n* Triaging high priority issues of the module\n* Triaging and reviewing and landing high priority pull requests of the module\n* Supporting public documentation related to the module\n* Running public developer meetings\n\nCore Maintainers\n----------------\n\nThe core maintainers are expected to have a deep understanding\nof the PyTorch code base and design philosophies. Their responsibilities\ninclude:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":57,"to":75}}}}],["90",{"pageContent":"Core Maintainers\n----------------\n\nThe core maintainers are expected to have a deep understanding\nof the PyTorch code base and design philosophies. Their responsibilities\ninclude:\n\n* Articulating a cohesive long-term vision for the project\n* Negotiating and resolving contentious issues in ways\n  acceptable to all parties involved\n* Receiving broad requests for changes from stakeholders of\n  PyTorch and evaluating / accepting them (small module-level\n  requests are handled by module maintainers)\n\nThe core maintainers as a group have the power to veto any\ndecision made at a Module maintainer level. The core\nmaintainers have power to resolve disputes as they see fit.\nThe core maintainers should publicly articulate their\ndecision-making, and give a clear reasoning for their\ndecisions, vetoes and dispute resolution.\n\nThe core maintainers are admins of the PyTorch GitHub Org\nand are listed in `Maintainers <https://pytorch.org/docs/stable/community/persons_of_interest.html>`__.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":75,"to":97}}}}],["91",{"pageContent":"The core maintainers are admins of the PyTorch GitHub Org\nand are listed in `Maintainers <https://pytorch.org/docs/stable/community/persons_of_interest.html>`__.\n\nLead Core Maintainer (BDFL)\n---------------------------\n\nThere may be decisions in which the core maintainers cannot\ncome to a consensus. To make such difficult decisions, the\ncore maintainers have an assigned and publicly declared Lead\nCore Maintainer amongst them, also commonly known in open-source\ngovernance models as a BDFL.\n\nThe Lead Core Maintainer should publicly articulate their\ndecision-making, and give a clear reasoning for their\ndecisions. The Lead Core Maintainer is also responsible for\nconfirming or removing core maintainers.\n\nNominating, Confirming and Removing Maintainers\n-----------------------------------------------\n\nThe Principles\n~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":97,"to":118}}}}],["92",{"pageContent":"Nominating, Confirming and Removing Maintainers\n-----------------------------------------------\n\nThe Principles\n~~~~~~~~~~~~~~\n\n* Membership in module maintainer groups is given to **individuals**\n  on **merit basis** after they demonstrated strong expertise of the\n  component through contributions, reviews and discussions and are\n  aligned with how the component fits in overall PyTorch direction.\n* For membership in the maintainer group the individual has to\n  demonstrate strong and continued alignment with the overall\n  PyTorch principles.\n* No term limits for module maintainers or core maintainers\n* Light criteria of moving module maintenance to ‘emeritus’\n  status if they don’t actively participate over long periods\n  of time. Each module maintainer group may define the inactive\n  period that’s appropriate for that module.\n* The membership is for an individual, not a company.\n\nThe Process for Nomination\n~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":118,"to":139}}}}],["93",{"pageContent":"The Process for Nomination\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n* Each module has its own process. Please contact module maintainers for more information.\n  However, if there is no process identified, you can file a request to the core\n  maintainers by submitting `this form <https://share.hsforms.com/1fh3SpHFMR2ihEBQ2orgN8A4tvhy>`__.\n  Core maintainers are meeting every three months.\n* If you are submitting a request to the core maintainers, the information in your request\n  must include the following items:\n\n  * The nominees depth and breadth of code, review and design\n    contributions on the module\n  * Testimonials (positive and negative) of the nominee’s interactions\n    with the maintainers, users, and the community\n  * General testimonials of support from the maintainers\n\n* The core maintainers then evaluate all information and make\n  a final decision to Confirm or Decline the nomination. The\n  decision of the core maintainers has to be articulated well\n  and would be public.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":139,"to":158}}}}],["94",{"pageContent":"The Process for Removal\n~~~~~~~~~~~~~~~~~~~~~~~\n\n* Similar to the process for nomination, anyone in the community\n  can nominate a person to be removed from a Module maintainer\n  position or a Core maintainer position.\n* A person can also self-nominate to be removed\n* The core maintainers (excluding persons with conflict of\n  interest) will request or put together more information around\n  the following:\n\n  * Their activity (or lack of) on the project\n  * Their changing thinking of the space, which results in\n    conflict with the overall direction of the project\n  * Other information that makes them unfit to be a maintainer,\n    such as Code of Conduct issues, their activity outside the\n    scope of the project that conflicts with the project’s values\n  * **Conflicts of interest**: filial or romantic relationships","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":160,"to":177}}}}],["95",{"pageContent":"* The core maintainers then evaluate all information and make\n  a final decision to Confirm or Decline the removal. The decision\n  of the core maintainers has to be articulated well and would be\n  public.\n\nNominating Core Maintainers\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n* Any core or module maintainer can nominate someone to become a\n  core maintainer\n* The lead maintainer (BDFL) is responsible for evaluating the\n  nomination.\n* The lead maintainer requests or puts together more information\n  around the strength of the candidate to be a core maintainer:\n\n  * Letters of support from other core and module maintainers\n  * General letters of support from stakeholders within the PyTorch\n    community\n  * Any new relevant information that is befitting for the candidacy\n\n* The lead maintainer evaluates all information and makes a final\n  decision to Confirm or Decline the nomination, with a clear public\n  articulation of their reasoning behind the decision.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":179,"to":201}}}}],["96",{"pageContent":"* The lead maintainer evaluates all information and makes a final\n  decision to Confirm or Decline the nomination, with a clear public\n  articulation of their reasoning behind the decision.\n\nRemoving the Lead Core Maintainer and Nominating a New Lead Core Maintainer\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n* A super-majority of core maintainers (75%) can choose to\n  remove the Lead Core Maintainer\n* After a removal of the Lead Core Maintainer or in unforeseen\n  circumstances (such as permanent unavailability of the Lead Core\n  Maintainer), the core maintainers follow a Ranked-Choice voting\n  method to elect a new Lead Core Maintainer.\n\nAdd, Remove, and Re-Scope Modules and Projects\n----------------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":201,"to":216}}}}],["97",{"pageContent":"Add, Remove, and Re-Scope Modules and Projects\n----------------------------------------------\n\nThe core maintainers together are responsible for taking\ndecisions on adding, removing and re-scoping new modules\nin the PyTorch org, either as new repositories in the\nPyTorch GitHub org, or as folders in the\n`pytorch/pytorch <https://github.com/pytorch/pytorch>`__\nrepository.\n\nThey invite proposals from members in the community\n(including themselves) for such changes.\nThe proposals are open-ended, but should have some basic\nground-work to make a convincing case to make change. The\nfollowing is an example approach to this process:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":216,"to":230}}}}],["98",{"pageContent":"#. Interview researchers / stakeholders, talk to community, gather issues;\n#. Read papers, attend conferences, build example pipelines based on experience;\n#. Create a state of the world - make sure this change is necessary,\n   for example adding a new project or module is worth the maintenance\n   cost; or removing a project or module will not remove too much value\n   from PyTorch;\n#. Create a proposal; the proposal covers the maintainership, development\n   and community plan once the proposal is approved.\n\nThe core maintainers take final decisions on the proposal, articulating\nthe reasoning behind the decision publicly.\n\n\nDecision Making\n---------------\n\nUncontroversial Changes\n~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":232,"to":249}}}}],["99",{"pageContent":"The core maintainers take final decisions on the proposal, articulating\nthe reasoning behind the decision publicly.\n\n\nDecision Making\n---------------\n\nUncontroversial Changes\n~~~~~~~~~~~~~~~~~~~~~~~\n\nPrimary work happens through issues and pull requests on\nGitHub. Maintainers should avoid pushing their changes directly to\nthe PyTorch repository, instead relying on pull requests. Approving a\npull request by a core or module maintainer allows it to be merged\nwithout further process. Core and module maintainers, as listed on\nthe `Maintainers <https://pytorch.org/docs/stable/community/persons_of_interest.html>`__\npage and within `CODEOWNERS <https://github.com/pytorch/pytorch/blob/master/CODEOWNERS>`__\nultimately approve these changes.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":249,"to":266}}}}],["100",{"pageContent":"Notifying relevant experts about an issue or a pull request\nis important. Reviews from experts in the given interest area are\nstrongly preferred, especially on pull request approvals. Failure to do\nso might end up with the change being reverted by the relevant expert.\n\nControversial Decision Process\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSubstantial changes in a given interest area require a GitHub issue to\nbe opened for discussion. This includes:\n\n-  Any semantic or syntactic change to the PyTorch framework or library.\n-  Backwards-incompatible changes to the Python or C++ API.\n-  Additions to the core framework or library, including substantial new\n   functionality within an existing library.\n-  Removal of core features or platform support\n\nCore and module maintainers ultimately approve these changes.\n\nGeneral Project Policies\n~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":268,"to":288}}}}],["101",{"pageContent":"Core and module maintainers ultimately approve these changes.\n\nGeneral Project Policies\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nPyTorch has been established as PyTorch a Series of LF Projects, LLC.\nPolicies applicable to PyTorch and participants in PyTorch, including\nguidelines on the usage of trademarks, are located at https://www.lfprojects.org/policies/.\n\nPyTorch participants acknowledge that the copyright in all new contributions\nwill be retained by the copyright holder as independent works of authorship\nand that no contributor or copyright holder will be required to assign copyrights\nto the project. Except as described below, all code contributions to the project\nmust be made using the 3-Clause-BSD License available here:\nhttps://opensource.org/licenses/BSD-3-Clause (the “Project License”).\nAll outbound code will be made available under the Project License.\nThe Maintainers may approve the use of an alternative open license or\nlicenses for inbound or outbound contributions on an exception basis.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":288,"to":305}}}}],["102",{"pageContent":"FAQ\n---\n\n**Q: What if I would like to own (or partly own) a part of the project\nsuch as a feature area or domain library, for example** `Linear Algebra <https://github.com/pytorch/pytorch/tree/master/torch/linalg>`__\n**or** `Torch Vision <https://github.com/pytorch/vision>`__ **?**\nThis is absolutely possible.\nThe first step is to start contributing to the existing project area and\nsupporting its health and success. In addition to this, you can\nmake a proposal through a GitHub issue for new functionality or changes\nto improve the project area.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":307,"to":317}}}}],["103",{"pageContent":"**Q: What if I am a company looking to use PyTorch internally for\ndevelopment, can I be granted or purchase a board seat to drive the\nproject direction?** No, the PyTorch project is strictly driven by the\na maintainer project philosophy and clearly separates technical\ngovernance from business governance. However, if you want to be\ninvolved in sponsorship and support, you can become involved in the\nPyTorch Foundation (PTF) and sponsorship through this. You can also\nhave individual engineers look to become maintainers, but this is\nnot guaranteed and is merit-based.\n\n**Q: Does the PyTorch project support grants or ways to support\nindependent developers using or contributing to the project?** No, not\nat this point. We are however looking at ways to better support the\ncommunity of independent developers around PyTorch. If you have\nsuggestions or inputs, please reach out on the PyTorch forums to\ndiscuss.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":319,"to":334}}}}],["104",{"pageContent":"**Q: How do I contribute code to the project?** If the change is\nrelatively minor, a pull request on GitHub can be opened up immediately\nfor review and merge by the project committers. For larger changes,\nplease open an issue to make a proposal to discuss prior. Please also\nsee the :doc:`PyTorch Contributor\nGuide <contribution_guide>` for contribution\nguidelines.\n\n**Q: Can I become a committer on the project?** Unfortunately, the\ncurrent commit process to PyTorch involves an interaction with Facebook\ninfrastructure that can only be triggered by Facebook employees. We are\nhowever looking at ways to expand the committer base to individuals\noutside of Facebook and will provide an update when the tooling exists\nto allow this.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":336,"to":349}}}}],["105",{"pageContent":"**Q: What if I would like to deliver a PyTorch tutorial at a conference\nor otherwise? Do I need to be 'officially' a committer to do this?** No,\nwe encourage community members to showcase their work wherever and\nwhenever they can. Please reach out to\n`marketing@pytorch.org <mailto:marketing@pytorch.org>`__\nfor marketing support.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/governance.rst","loc":{"lines":{"from":351,"to":356}}}}],["106",{"pageContent":"PyTorch Governance | Maintainers\n=========================================\n\nResponsibilities\n----------------\n\n* Triage and fix high priority issues assigned to the module or library\n* Triage, review, and land high priority pull requests assigned to the module or library\n* Answer module or library questions on `discuss.pytorch.org <https://discuss.pytorch.org/>`__\n  and `dev-discuss.pytorch.org <https://dev-discuss.pytorch.org/>`__\n* Maintain public user and development documentation\n* Run meetings and share minutes plus roadmap on a half or quarterly basis\n\nLead Core Maintainer (BDFL)\n---------------------------\n\n* Soumith Chintala (`soumith <https://github.com/soumith>`__)\n\nCore Maintainers\n-------------------\n\n-  Soumith Chintala (`soumith <https://github.com/soumith>`__)\n-  Edward Yang (`ezyang <https://github.com/ezyang>`__)\n-  Greg Chanan (`gchanan <https://github.com/gchanan>`__)\n-  Dmytro Dzhulgakov (`dzhulgakov <https://github.com/dzhulgakov>`__)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":1,"to":25}}}}],["107",{"pageContent":"Module-level maintainers\n------------------------\n\nNN APIs (torch.nn)\n~~~~~~~~~~~~~~~~~~\n\n-  Greg Chanan (`gchanan <https://github.com/gchanan>`__)\n-  Soumith Chintala (`soumith <https://github.com/soumith>`__)\n-  Joel Schlosser (`jbschlosser <https://github.com/jbschlosser>`__)\n-  Alban Desmaison (`albanD <https://github.com/albanD>`__)\n-  (emeritus) Sam Gross (`colesbury <https://github.com/colesbury>`__)\n-  (emeritus) Adam Paszke (`apaszke <https://github.com/apaszke>`__)\n\nOptimizers (torch.optim)\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  Alban Desmaison (`albanD <https://github.com/albanD>`__)\n-  Joel Schlosser (`jbschlosser <https://github.com/jbschlosser>`__)\n-  Soumith Chintala (`soumith <https://github.com/soumith>`__)\n-  (emeritus) Ilqar Ramazanli (`iramazanli <https://github.com/iramazanli>`__)\n-  (emeritus) Vincent Quenneville-Belair (`vincentqb <https://github.com/vincentqb>`__)\n\nAutograd (torch.autograd)\n~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":27,"to":50}}}}],["108",{"pageContent":"Autograd (torch.autograd)\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  Edward Yang (`ezyang <https://github.com/ezyang>`__)\n-  Alban Desmaison (`alband <https://github.com/alband>`__)\n-  Jeffrey Wan (`soulitzer <https://github.com/soulitzer>`__)\n-  (emeritus) Adam Paszke (`apaszke <https://github.com/apaszke>`__)\n\nCompilers (JIT / TorchScript / FX / TorchDynamo)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  Elias Ellison (`eellison <https://github.com/eellison>`__)\n-  Michael Suo (`suo <https://github.com/suo>`__)\n-  Yanan Cao (`gmagogsfm <https://github.com/gmagogsfm>`__)\n-  James Reed (`jamesr66a <https://github.com/jamesr66a>`__)\n-  Jason Ansel (`jansel <https://github.com/jansel>`__)\n-  (emeritus) Zach Devito (`zdevito <https://github.com/zdevito>`__)\n\n\nDistributions & RNG\n~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":50,"to":70}}}}],["109",{"pageContent":"Distributions & RNG\n~~~~~~~~~~~~~~~~~~~\n\n-  Fritz Obermeyer (`fritzo <https://github.com/fritzo>`__)\n-  Neeraj Pradhan (`neerajprad <https://github.com/neerajprad>`__)\n-  Alican Bozkurt (`alicanb <https://github.com/alicanb>`__)\n-  (emeritus) Vishwak Srinivasan (`vishwakftw <https://github.com/vishwakftw>`__)\n\nDistributed\n~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":70,"to":79}}}}],["110",{"pageContent":"Distributed\n~~~~~~~~~~~\n\n-  Shen Li (`mrshenli <https://github.com/mrshenli>`__)\n-  Pritam Damania (`pritamdamania87 <https://github.com/pritamdamania87>`__)\n-  Yanli Zhao (`zhaojuanmao <https://github.com/zhaojuanmao>`__)\n-  Rohan Varma (`rohan-varma <https://github.com/rohan-varma>`__)\n-  Wanchao Liang (`wanchaol <https://github.com/wanchaol>`__)\n-  Junjie Wang (`fduwjj <https://github.com/fduwjj>`__)\n-  Howard Huang (`H-Huang <https://github.com/H-Huang>`__)\n-  Tristan Rice (`d4l3k <https://github.com/d4l3k>`__)\n-  Alisson Azzolini (`aazzolini <https://github.com/aazzolini>`__)\n-  Ke Wen (`kwen2501 <https://github.com/kwen2501>`__)\n-  James Reed (`jamesr66a <https://github.com/jamesr66a>`__)\n-  Kiuk Chung (`kiukchung <https://github.com/kiukchung>`__)\n-  (emeritus) Pieter Noordhuis (`pietern <https://github.com/pietern>`__)\n-  (emeritus) Mingzhe Li (`mingzhe09088 <https://github.com/mingzhe09088>`__)\n-  (emeritus) Omkar Salpekar (`osalpekar <https://github.com/osalpekar>`__)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":79,"to":96}}}}],["111",{"pageContent":"Multiprocessing and DataLoaders\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  Vitaly Fedyunin (`VitalyFedyunin <https://github.com/VitalyFedyunin>`__)\n-  Simon Wang (`SsnL <https://github.com/SsnL>`__)\n-  (emeritus) Adam Paszke (`apaszke <https://github.com/apaszke>`__)\n\nLinear Algebra (torch.linalg)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  Mike Ruberry (`mruberry <https://github.com/mruberry>`__)\n-  Mario Lezcano (`lezcano <https://github.com/lezcano>`__)\n-  Ivan Yashchuk (`IvanYashchuk <https://github.com/IvanYashchuk>`__)\n-  (emeritus) Vishwak Srinivasan (`vishwakftw <https://github.com/vishwakftw>`__)\n\nSparse (torch.sparse)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  Pearu Peterson (`pearu <https://github.com/pearu>`__)\n-  Nikita Vedeneev (`nikitaved <https://github.com/nikitaved>`__)\n-  Ivan Yashchuk (`IvanYashchuk <https://github.com/IvanYashchuk>`__)\n-  Christian Puhrsch (`cpuhrsch <https://github.com/cpuhrsch>`__)\n-  Andrew James (`amjames <https://github.com/amjames>`__)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":98,"to":120}}}}],["112",{"pageContent":"NestedTensor (torch.nested)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  Alban Desmaison (`albanD <https://github.com/albanD>`__)\n-  Christian Puhrsch (`cpuhrsch <https://github.com/cpuhrsch>`__)\n-  Driss Guessous (`drisspg <https://github.com/drisspg>`__)\n-  Joel Schlosser (`jbschlosser <https://github.com/jbschlosser>`__)\n-  Mikayla Gawarecki (`mikaylagawarecki <https://github.com/mikaylagawarecki>`__)\n-  Natalia Gimelshein (`ngimel <https://github.com/ngimel>`__)\n\nMaskedTensor (torch.masked)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  Christian Puhrsch (`cpuhrsch <https://github.com/cpuhrsch>`__)\n-  (emeritus) George Qi (`george-qi <https://github.com/george-qi>`__)\n\nFast Fourier Transform (torch.fft)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  Mike Ruberry (`mruberry <https://github.com/mruberry>`__)\n-  Peter Bell (`peterbell10 <https://github.com/peterbell10>`__)\n\nCPU Performance / SIMD\n~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":122,"to":145}}}}],["113",{"pageContent":"-  Mike Ruberry (`mruberry <https://github.com/mruberry>`__)\n-  Peter Bell (`peterbell10 <https://github.com/peterbell10>`__)\n\nCPU Performance / SIMD\n~~~~~~~~~~~~~~~~~~~~~~\n\n-  Vitaly Fedyunin (`VitalyFedyunin <https://github.com/VitalyFedyunin>`__)\n-  Mingfei Ma (`mingfeima <https://github.com/mingfeima>`__)\n-  (emeritus) Xiaoqiang Zheng (`zheng-xq <https://github.com/zheng-xq>`__)\n-  (emeritus) Sam Gross (`colesbury <https://github.com/colesbury>`__)\n-  (emeritus) Christian Puhrsch (`cpuhrsch <https://github.com/cpuhrsch>`__)\n-  (emeritus) Ilia Cherniavskii (`ilia-cher <https://github.com/ilia-cher>`__)\n\nNVIDIA / CUDA\n~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":145,"to":159}}}}],["114",{"pageContent":"NVIDIA / CUDA\n~~~~~~~~~~~~~\n\n-  Natalia Gimelshein (`ngimel <https://github.com/ngimel>`__)\n-  Edward Yang (`ezyang <https://github.com/ezyang>`__)\n-  Piotr Bialecki (`ptrblck <https://github.com/ptrblck>`__)\n-  Christian Sarofeen (`csarofeen <https://github.com/csarofeen>`__)\n-  Andrew Tulloch (`ajtulloch <https://github.com/ajtulloch>`__)\n-  (emeritus) Xiaoqiang Zheng (`zheng-xq <https://github.com/zheng-xq>`__)\n\nNVFuser\n~~~~~~~\n\n-  Christian Sarofeen (`csarofeen <https://github.com/csarofeen>`__)\n-  Alex Jann (`jjsjann123 <https://github.com/jjsjann123>`__)\n-  Piotr Bialecki (`ptrblck <https://github.com/ptrblck>`__)\n-  Natalia Gimelshein (`ngimel <https://github.com/ngimel>`__)\n\nIntel / MKLDNN\n~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":159,"to":178}}}}],["115",{"pageContent":"Intel / MKLDNN\n~~~~~~~~~~~~~~\n\n-  Vitaly Fedyunin (`VitalyFedyunin <https://github.com/VitalyFedyunin>`__)\n-  Jianhui Li (`Jianhui-Li <https://github.com/Jianhui-Li>`__)\n-  Mingfei Ma (`mingfeima <https://github.com/mingfeima>`__)\n-  (emeritus) Junjie Bai (`bddppq <https://github.com/bddppq>`__)\n-  (emeritus) Yinghai Lu (`yinghai <https://github.com/yinghai>`__)\n\nAMD/ROCm/HIP\n~~~~~~~~~~~~\n\n-  Peng Sun (`sunway513 <https://github.com/sunway513>`__)\n-  Jithun Nair (`jithunnair-amd <https://github.com/jithunnair-amd>`__)\n-  Jeff Daily (`jeffdaily <https://github.com/jeffdaily>`__)\n-  (emeritus) Junjie Bai (`bddppq <https://github.com/bddppq>`__)\n\nBuild + CI\n~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":178,"to":196}}}}],["116",{"pageContent":"Build + CI\n~~~~~~~~~~\n\n-  Nikita Shulga (`malfet <https://github.com/malfet>`__)\n-  Eli Uriegas (`seemethere <https://github.com/seemethere>`__)\n-  Alban Desmaison (`alband <https://github.com/alband>`__)\n-  Mikey Dagitses (`dagitses <https://github.com/dagitses>`__)\n-  Omkar Salpekar (`osalpekar <https://github.com/osalpekar>`__)\n-  Zain Rizvi (`ZainRizvi <https://github.com/ZainRizvi>`__)\n-  Nirav Mehta (`mehtanirav <https://github.com/mehtanirav>`__)\n-  Andrey Talman (`atalman <https://github.com/atalman>`__)\n-  (emeritus) Zhuojie Zhou (`zhouzhuojie <https://github.com/zhouzhuojie>`__)\n-  (emeritus) Edward Yang (`ezyang <https://github.com/ezyang>`__)\n-  (emeritus) Karl Ostmo (`kostmo <https://github.com/kostmo>`__)\n\nPerformance Tools\n~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":196,"to":212}}}}],["117",{"pageContent":"Performance Tools\n~~~~~~~~~~~~~~~~~\n\n-  Adnan Aziz (`adnanaziz <https://github.com/adnanaziz>`__)\n-  CK Luk (`ckluk <https://github.com/ckluk>`__)\n-  Taylor Robie (`robieta <https://github.com/robieta>`__)\n-  Xu Zhao (`xuzhao9 <https://github.com/xuzhao9>`__)\n-  Geeta Chauhan (`chauhang <https://github.com/chauhang>`__)\n-  (emeritus) Victor Bittorf (`bitfort <https://github.com/bitfort>`__)\n-  (emeritus) Gisle Dankel (`gdankel <https://github.com/gdankel>`__)\n-  (emeritus) Natalia Gimelshein (`ngimel <https://github.com/ngimel>`__)\n-  (emeritus) Mingzhe Li (`mingzhe09088 <https://github.com/mingzhe09088>`__)\n\nC++ API\n~~~~~~~\n\n-  Joel Schlosser (`jbschlosser <https://github.com/jbschlosser>`__)\n-  (emeritus) Will Feng (`yf225 <https://github.com/yf225>`__)\n\nC10 utils and operator dispatch\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":212,"to":232}}}}],["118",{"pageContent":"-  Joel Schlosser (`jbschlosser <https://github.com/jbschlosser>`__)\n-  (emeritus) Will Feng (`yf225 <https://github.com/yf225>`__)\n\nC10 utils and operator dispatch\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  Brian Hirsh (`bdhirsh <https://github.com/bdhirsh>`__)\n-  Edward Yang (`ezyang <https://github.com/ezyang>`__)\n-  Dmytro Dzhulgakov (`dzhulgakov <https://github.com/dzhulgakov>`__)\n-  (emeritus) Sebastian Messmer (`smessmer <https://github.com/smessmer>`__)\n\nONNX exporter\n~~~~~~~~~~~~~\n-  Bowen Bao (`BowenBao <https://github.com/BowenBao>`__)\n-  Aaron Bockover (`abock <https://github.com/abock>`__)\n-  (emeritus) Gary Miguel (`garymm <https://github.com/garymm>`__)\n-  (emeritus) Lara Haidar (`lara-hdr <https://github.com/lara-hdr>`__)\n-  (emeritus) Lu Fang (`houseroad <https://github.com/houseroad>`__)\n-  (emeritus) Negin Raoof (`neginraoof <https://github.com/neginraoof>`__)\n-  (emeritus) Spandan Tiwari (`spandantiwari <https://github.com/spandantiwari>`__)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":232,"to":251}}}}],["119",{"pageContent":"Mobile / Edge\n~~~~~~~~~~~~~\n-  David Reiss (`dreiss <https://github.com/dreiss>`__)\n-  Raziel Guevara (`raziel <https://github.com/raziel>`__)\n-  Linbin Yu (`linbinyu <https://github.com/linbinyu>`__)\n-  Ivan Kobzarev (`IvanKobzarev <https://github.com/IvanKobzarev>`__)\n-  Tao Xu (`xta0 <https://github.com/xta0>`__)\n\nModel Compression & Optimization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n-  Vasiliy Kuznetsov (`vkuzo <https://github.com/vkuzo>`__)\n-  Jerry Zhang (`jerryzh168 <https://github.com/jerryzh168>`__)\n-  Zafar Takhirov (`z-a-f <https://github.com/z-a-f>`__)\n-  Supriya Rao (`supriyar <https://github.com/supriyar>`__)\n-  (emeritus) Raghuraman Krishnamoorthi (`raghuramank100 <https://github.com/raghuramank100>`__)\n\n\nWindows\n~~~~~~~\n\n-  Guoliang Hua (`nbcsm <https://github.com/nbcsm>`__)\n-  (emeritus) Teng Gao (`gaoteng-git <https://github.com/gaoteng-git>`__)\n-  (emeritus) Peter Johnson (`peterjc123 <https://github.com/peterjc123>`__)\n\nApple M1/MPS\n~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":253,"to":278}}}}],["120",{"pageContent":"Apple M1/MPS\n~~~~~~~~~~~~\n\n-  Alban Desmaison (`alband <https://github.com/alband>`__)\n-  Nikita Shulga (`malfet <https://github.com/malfet>`__)\n-  Kulin Seth (`kulinseth <https://github.com/kulinseth>`__)\n-  Ramin Azarmehr (`razarmehr <https://github.com/razarmehr>`__)\n\nPowerPC\n~~~~~~~\n\n-  Alfredo Mendoza (`avmgithub <https://github.com/avmgithub>`__)\n\nDocs / Tutorials\n~~~~~~~~~~~~~~~~\n\n- Svetlana Karslioglu (`svekars <https://github.com/svekars>`__)\n\nLibrary-level maintainers\n-------------------------\n\nXLA\n~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":278,"to":300}}}}],["121",{"pageContent":"Docs / Tutorials\n~~~~~~~~~~~~~~~~\n\n- Svetlana Karslioglu (`svekars <https://github.com/svekars>`__)\n\nLibrary-level maintainers\n-------------------------\n\nXLA\n~~~\n\n-  Jack Cao (`JackCaoG <https://github.com/JackCaoG>`__)\n-  Daniel Sohn (`jysohn23 <https://github.com/jysohn23>`__)\n-  Zach Cain (`zcain117 <https://github.com/zcain117>`__)\n-  Brian Hirsch (`bdhirsh <https://github.com/bdhirsh>`__)\n-  Gregory Chanan (`gchanan <https://github.com/gchanan>`__)\n-  (emeritus) Ailing Zhang (`ailzhang <https://github.com/ailzhang>`__)\n-  (emeritus) Davide Libenzi (`dlibenzi <https://github.com/dlibenzi>`__)\n-  (emeritus) Alex Suhan (`asuhan <https://github.com/asuhan>`__)\n\nTorchServe\n~~~~~~~~~~\n\n-  Geeta Chauhan (`chauhang <https://github.com/chauhang>`__)\n-  Manoj Rao (`mycpuorg <https://github.com/mycpuorg>`__)\n-  Vamshi Dantu (`vdantu <https://github.com/vdantu>`__)\n-  Dhanasekar Karuppasamy (`dhanainme <https://github.com/dhanainme>`__)\n\nTorchVision\n~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":300,"to":329}}}}],["122",{"pageContent":"TorchVision\n~~~~~~~~~~~\n\n-  Francisco Massa (`fmassa <https://github.com/fmassa>`__)\n-  Vasilis Vryniotis (`datumbox <https://github.com/datumbox>`__)\n-  Nicolas Hug (`NicolasHug <https://github.com/NicolasHug>`__)\n-  Yosua Michael Maranatha (`YosuaMichael <https://github.com/YosuaMichael>`__)\n-  Joao Gomes (`jdsgomes <https://github.com/jdsgomes>`__)\n-  Philip Meier (`pmeier <https://github.com/pmeier>`__)\n-  Victor Fomin (`vfdev-5 <https://github.com/vfdev-5>`__)\n\nTorchText\n~~~~~~~~~\n\n-  Nayef Ahmed (`Nayef211 <https://github.com/Nayef211>`__)\n-  (emeritus) Parmeet Singh Bhatia (`parmeet <https://github.com/parmeet>`__)\n-  (emeritus) Guanheng George Zhang (`zhangguanheng66 <https://github.com/zhangguanheng66>`__)\n-  (emeritus) Christian Puhrsch (`cpuhrsch <https://github.com/cpuhrsch>`__)\n\nTorchAudio\n~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":329,"to":349}}}}],["123",{"pageContent":"TorchAudio\n~~~~~~~~~~\n\n-  Moto Hira (`mthrok <https://github.com/mthrok>`__)\n-  Jeff Hwang (`hwangjeff <https://github.com/hwangjeff>`__)\n-  Caroline Chen (`carolineechen <https://github.com/carolineechen>`__)\n-  Xiaohui Zhang (`xiaohui-zhang <https://github.com/xiaohui-zhang>`__)\n-  Zhaoheng Ni (`nateanl <https://github.com/nateanl>`__)\n-  (emeritus) Christian Puhrsch (`cpuhrsch <https://github.com/cpuhrsch>`__)\n-  (emeritus) Vincent QB (`vincentqb <https://github.com/vincentqb>`__)\n\nTorchRec\n~~~~~~~~\n\n-  Dmytro Ivchenko (`divchenko <https://github.com/divchenko>`__)\n-  Colin Taylor (`colin2328 <https://github.com/colin2328>`__)\n\nTorchX\n~~~~~~\n\n-  Tristan Rice (`d4l3k <https://github.com/d4l3k>`__)\n-  Kiuk Chung (`kiukchung <https://github.com/kiukchung>`__)\n\nTorchData / TorchArrow\n~~~~~~~~~~~~~~~~~~~~~~\n-  Vitaly Fedyunin (`VitalyFedyunin <https://github.com/VitalyFedyunin>`__)\n-  Wenlei Xie (`wenleix <https://github.com/wenleix>`__)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/community/persons_of_interest.rst","loc":{"lines":{"from":349,"to":375}}}}],["124",{"pageContent":"Custom Backends\n===============\n\nOverview\n--------\n\n``torch.compile`` provides a straightforward method to enable users\nto define custom backends.\n\nA backend function has the contract\n``(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]) -> Callable``.\n\nBackend functions can be called by TorchDynamo, the graph tracing component of ``torch.compile``,\nafter tracing an FX graph and are\nexpected to return a compiled function that is equivalent to the traced FX graph.\nThe returned callable should have the same contract as the ``forward`` function of the original ``torch.fx.GraphModule``\npassed into the backend:\n``(*args: torch.Tensor) -> List[torch.Tensor]``.\n\nIn order for TorchDynamo to call your backend, pass your backend function as the ``backend`` kwarg in\n``torch.compile``. For example,\n\n.. code-block:: python\n\n    import torch\n\n    def my_custom_backend(gm, example_inputs):\n        return gm.forward\n\n    def f(...):\n        ...\n\n    f_opt = torch.compile(f, backend=my_custom_backend)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":1,"to":33}}}}],["125",{"pageContent":".. code-block:: python\n\n    import torch\n\n    def my_custom_backend(gm, example_inputs):\n        return gm.forward\n\n    def f(...):\n        ...\n\n    f_opt = torch.compile(f, backend=my_custom_backend)\n\n    @torch.compile(backend=my_custom_backend)\n    def g(...):\n        ...\n\nSee below for more examples.\n\nRegistering Custom Backends\n---------------------------\n\nYou can register your backend using the ``register_backend`` decorator, for example,\n\n.. code-block:: python\n\n    from torch._dynamo.optimizations import register_backend\n\n    @register_backend\n    def my_compiler(gm, example_inputs):\n        ...\n\nBesides the ``register_backend`` decorator, if your backend is in another python package, you could also register your\nbackend through entry points of python package, which provides a way for a package to register a plugin for another one.\n\n.. hint::\n\n    You can learn more about ``entry_points`` in the\n    `python packaging documentation <https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`__.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":33,"to":70}}}}],["126",{"pageContent":".. hint::\n\n    You can learn more about ``entry_points`` in the\n    `python packaging documentation <https://setuptools.pypa.io/en/latest/userguide/entry_point.html>`__.\n\nTo register your backend through ``entry_points``, you could add your backend function to the ``torch_dynamo_backends`` entry point group in the\n``setup.py`` file of your package like:\n\n.. code-block:: python\n\n    ...\n    setup(\n        ...\n        'torch_dynamo_backends': [\n            'my_compiler = your_module.submodule:my_compiler',\n        ]\n        ...\n    )","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":70,"to":87}}}}],["127",{"pageContent":".. code-block:: python\n\n    ...\n    setup(\n        ...\n        'torch_dynamo_backends': [\n            'my_compiler = your_module.submodule:my_compiler',\n        ]\n        ...\n    )\n\nPlease replace the ``my_compiler`` before ``=`` to the name of your backend's name and replace the part after ``=`` to\nthe module and function name of your backend function.\nThe entry point will be added to your python environment after the installation of the package.\nWhen you call ``torch.compile(model, backend=\"my_compiler\")``, PyTorch would first search the backend named ``my_compiler``\nthat has been registered with ``register_backend``. If not found, it will continue to search in all backends registered\nvia ``entry_points``.\n\nRegistration serves two purposes:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":87,"to":105}}}}],["128",{"pageContent":"Registration serves two purposes:\n\n* You can pass a string containing your backend function's name to ``torch.compile`` instead of the function itself,\n  for example, ``torch.compile(model, backend=\"my_compiler\")``.\n* It is required for use with the `minifier <https://pytorch.org/docs/master/compile/troubleshooting.html>`__. Any generated\n  code from the minifier must call your code that registers your backend function, typically through an ``import`` statement.\n\nCustom Backends after AOTAutograd\n---------------------------------\n\nIt is possible to define custom backends that are called by AOTAutograd rather than TorchDynamo.\nThis is useful for 2 main reasons:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":105,"to":116}}}}],["129",{"pageContent":"Custom Backends after AOTAutograd\n---------------------------------\n\nIt is possible to define custom backends that are called by AOTAutograd rather than TorchDynamo.\nThis is useful for 2 main reasons:\n\n* Users can define backends that support model training, as AOTAutograd can generate the backward graph for compilation.\n* AOTAutograd produces FX graphs consisting of `canonical Aten ops <https://pytorch.org/docs/master/ir.html#canonical-aten-ir>`__. As a result,\n  custom backends only need to support the canonical Aten opset, which is a significantly smaller opset than the entire torch/Aten opset.\n\nWrap your backend with\n``torch._dynamo.optimizations.training.aot_autograd`` and use ``torch.compile`` with the ``backend`` kwarg as before.\nBackend functions wrapped by ``aot_autograd`` should have the same contract as before.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":116,"to":128}}}}],["130",{"pageContent":"Backend functions are passed to ``aot_autograd`` through the ``fw_compiler`` (forward compiler)\nor ``bw_compiler`` (backward compiler) kwargs. If ``bw_compiler`` is not specified, the backward compile function\ndefaults to the forward compile function.\n\nOne caveat is that AOTAutograd requires compiled functions returned by backends to be \"boxed\". This can be done by wrapping\nthe compiled function with ``functorch.compile.make_boxed_func``.\n\nFor example,\n\n.. code-block:: python\n\n    from torch._dynamo.optimizations.training import aot_autograd\n    from functorch.compile import make_boxed_func\n\n    def my_compiler(gm, example_inputs):\n        return make_boxed_func(gm.forward)\n\n    my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n\n    model_opt = torch.compile(model, backend=my_backend)\n\nExamples\n--------\n\nDebugging Backend\n^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":130,"to":155}}}}],["131",{"pageContent":"my_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n\n    model_opt = torch.compile(model, backend=my_backend)\n\nExamples\n--------\n\nDebugging Backend\n^^^^^^^^^^^^^^^^^\n\nIf you want to better understand what is going on during a\ncompilation, you can create a custom compiler, which is referred to as\nbackend in this section, that will print pretty print the fx\n``GraphModule`` extracted from Dynamo’s bytecode analysis\nand return a ``forward()`` callable.\n\nFor example:\n\n.. code-block:: python\n\n   from typing import List\n   import torch\n   def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n       print(\"my_compiler() called with FX graph:\")\n       gm.graph.print_tabular()\n       return gm.forward  # return a python callable\n   @torch.compile(backend=my_compiler)\n   def fn(x, y):\n       a = torch.cos(x)\n       b = torch.sin(y)\n       return a + b\n   fn(torch.randn(10), torch.randn(10))\n\nRunning the above example produces the following output:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":155,"to":190}}}}],["132",{"pageContent":"Running the above example produces the following output:\n\n::\n\n   my_compiler() called with FX graph:\n   opcode         name    target                                                  args        kwargs\n   -------------  ------  ------------------------------------------------------  ----------  --------\n   placeholder    x       x                                                       ()          {}\n   placeholder    y       y                                                       ()          {}\n   call_function  cos     <built-in method cos of type object at 0x7f1a894649a8>  (x,)        {}\n   call_function  sin     <built-in method sin of type object at 0x7f1a894649a8>  (y,)        {}\n   call_function  add     <built-in function add>                                 (cos, sin)  {}\n   output         output  output                                                  ((add,),)   {}\n\nThis works for ``torch.nn.Module`` as well as shown below:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":190,"to":206}}}}],["133",{"pageContent":"This works for ``torch.nn.Module`` as well as shown below:\n\n.. code-block:: python\n\n   from typing import List\n   import torch\n   def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n       print(\"my_compiler() called with FX graph:\")\n       gm.graph.print_tabular()\n       return gm.forward  # return a python callable\n   class MockModule(torch.nn.Module):\n       def __init__(self):\n           super().__init__()\n           self.relu = torch.nn.ReLU()\n       def forward(self, x):\n           return self.relu(torch.cos(x))\n   mod = MockModule()\n   optimized_mod = torch.compile(mod, backend=my_compiler)\n   optimized_mod(torch.randn(10))\n\nLet’s take a look at one more example with control flow:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":206,"to":228}}}}],["134",{"pageContent":"Let’s take a look at one more example with control flow:\n\n.. code-block:: python\n\n   from typing import List\n   import torch\n   def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n       print(\"my_compiler() called with FX graph:\")\n       gm.graph.print_tabular()\n       return gm.forward  # return a python callable\n   @torch.compile(backend=my_compiler)\n   def toy_example(a, b):\n       x = a / (torch.abs(a) + 1)\n       if b.sum() < 0:\n           b = b * -1\n       return x * b\n   for _ in range(100):\n       toy_example(torch.randn(10), torch.randn(10))\n\nRunning this example produces the following output:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":228,"to":249}}}}],["135",{"pageContent":"my_compiler() called with FX graph:\n   opcode         name     target                                                  args              kwargs\n   -------------  -------  ------------------------------------------------------  ----------------  --------\n   placeholder    a        a                                                       ()                {}\n   placeholder    b        b                                                       ()                {}\n   call_function  abs_1    <built-in method abs of type object at 0x7f8d259298a0>  (a,)              {}\n   call_function  add      <built-in function add>                                 (abs_1, 1)        {}\n   call_function  truediv  <built-in function truediv>                             (a, add)          {}\n   call_method    sum_1    sum                                                     (b,)              {}\n   call_function  lt       <built-in function lt>                                  (sum_1, 0)        {}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":251,"to":260}}}}],["136",{"pageContent":"call_function  lt       <built-in function lt>                                  (sum_1, 0)        {}\n   output         output   output                                                  ((truediv, lt),)  {}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":260,"to":261}}}}],["137",{"pageContent":"my_compiler() called with FX graph:\n   opcode         name    target                   args         kwargs\n   -------------  ------  -----------------------  -----------  --------\n   placeholder    b       b                        ()           {}\n   placeholder    x       x                        ()           {}\n   call_function  mul     <built-in function mul>  (b, -1)      {}\n   call_function  mul_1   <built-in function mul>  (x, mul)     {}\n   output         output  output                   ((mul_1,),)  {}\n\n   my_compiler() called with FX graph:\n   opcode         name    target                   args       kwargs\n   -------------  ------  -----------------------  ---------  --------\n   placeholder    b       b                        ()         {}\n   placeholder    x       x                        ()         {}\n   call_function  mul     <built-in function mul>  (x, b)     {}\n   output         output  output                   ((mul,),)  {}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":263,"to":278}}}}],["138",{"pageContent":"The order of the last two graphs is nondeterministic depending\non which one is encountered first by the just-in-time compiler.\n\nSpeedy Backend\n^^^^^^^^^^^^^^\n\nIntegrating a custom backend that offers superior performance is also\neasy and we’ll integrate a real one\nwith `optimize_for_inference <https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html>`__:\n\n.. code-block:: python\n\n   def optimize_for_inference_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n       scripted = torch.jit.script(gm)\n       return torch.jit.optimize_for_inference(scripted)\n\nAnd then you should be able to optimize any existing code with:\n\n.. code-block:: python\n\n   @torch.compile(backend=optimize_for_inference_compiler)\n   def code_to_accelerate():\n       ...\n\nComposable Backends\n^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":280,"to":305}}}}],["139",{"pageContent":".. code-block:: python\n\n   @torch.compile(backend=optimize_for_inference_compiler)\n   def code_to_accelerate():\n       ...\n\nComposable Backends\n^^^^^^^^^^^^^^^^^^^\n\nTorchDynamo includes many backends, which can be found in\n`backends.py <https://github.com/pytorch/pytorch/blob/master/torch/_dynamo/optimizations/backends.py>`__\nor ``torch._dynamo.list_backends()``. You can combine these backends\ntogether with the following code:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":305,"to":319}}}}],["140",{"pageContent":".. code-block:: python\n\n   from torch._dynamo.optimizations import BACKENDS\n    def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n        try:\n            trt_compiled = BACKENDS[\"tensorrt\"](gm, example_inputs)\n            if trt_compiled is not None:\n                return trt_compiled\n        except Exception:\n            pass\n        # first backend failed, try something else...\n        try:\n            inductor_compiled = BACKENDS[\"inductor\"](gm, example_inputs)\n            if inductor_compiled is not None:\n                return inductor_compiled\n        except Exception:\n            pass\n        return gm.forward","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/custom-backends.rst","loc":{"lines":{"from":319,"to":336}}}}],["141",{"pageContent":"TorchDynamo Deeper Dive\n=======================\n**Author**: `Jason Ansel <https://github.com/jansel>`_\n\nWhat is a guard?\n----------------\n\nTorchDynamo operates just-in-time and specializes graphs based on\ndynamic properties. For example, the first graph above has the following\nguards:\n\n::\n\n   GUARDS:\n    - local 'a' TENSOR_MATCH\n    - local 'b' TENSOR_MATCH\n    - global 'torch' FUNCTION_MATCH\n\nIf any of those guards fail, the graph will be recaptured and\nrecompiled. The interesting guard type there is ``TENSOR_MATCH``, which\nchecks the following ``torch.Tensor`` properties:\n\n- Python class of the tensor (tensor subclassing, etc)\n- dtype\n- device\n- requires_grad\n- dispatch_key (with thread-local includes/excludes applied)\n- ndim\n- sizes\\* (optional)\n- strides\\* (optional)\n\nFor sizes/strides you can disable this specialization by setting the\nfollowing parameter:\n\n.. code-block:: python\n\n   torch._dynamo.config.dynamic_shapes = True","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/deep-dive.rst","loc":{"lines":{"from":1,"to":37}}}}],["142",{"pageContent":"For sizes/strides you can disable this specialization by setting the\nfollowing parameter:\n\n.. code-block:: python\n\n   torch._dynamo.config.dynamic_shapes = True\n\nThe full specialization mode allows the backend compiler to assume an\nentirely static graph. Unfortunately, most backends require this.\nOperators which return dynamic shapes will trigger a graph break when\nnot in dynamic shape mode.\n\nWhat is Dynamo doing?\n---------------------\n\nIf you want to understand better what TorchDynamo is doing, you can set:\n\n.. code-block:: python\n\n   import torch._dynamo.config\n   import logging\n\n   torch._dynamo.config.log_level = logging.INFO\n   torch._dynamo.config.output_code = True\n\nThis code triggers useful (but spammy) printouts.\n\nFor example, the printouts for the first graph in the ``toy_example``\nare:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/deep-dive.rst","loc":{"lines":{"from":37,"to":67}}}}],["143",{"pageContent":"__compiled_fn_0 <eval_with_key>.1\n   opcode         name     target                                                  args              kwargs\n   -------------  -------  ------------------------------------------------------  ----------------  --------\n   placeholder    a        a                                                       ()                {}\n   placeholder    b        b                                                       ()                {}\n   call_function  abs_1    <built-in method abs of type object at 0x7f9ca082f8a0>  (a,)              {}\n   call_function  add      <built-in function add>                                 (abs_1, 1)        {}\n   call_function  truediv  <built-in function truediv>                             (a, add)          {}\n   call_method    sum_1    sum                                                     (b,)              {}\n   call_function  lt       <built-in function lt>                                  (sum_1, 0)        {}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/deep-dive.rst","loc":{"lines":{"from":69,"to":78}}}}],["144",{"pageContent":"call_function  lt       <built-in function lt>                                  (sum_1, 0)        {}\n   output         output   output                                                  ((truediv, lt),)  {}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/deep-dive.rst","loc":{"lines":{"from":78,"to":79}}}}],["145",{"pageContent":"ORIGINAL BYTECODE toy_example example.py 9\n    10           0 LOAD_FAST                0 (a)\n                 2 LOAD_GLOBAL              0 (torch)\n                 4 LOAD_METHOD              1 (abs)\n                 6 LOAD_FAST                0 (a)\n                 8 CALL_METHOD              1\n                10 LOAD_CONST               1 (1)\n                12 BINARY_ADD\n                14 BINARY_TRUE_DIVIDE\n                16 STORE_FAST               2 (x)\n\n    11          18 LOAD_FAST                1 (b)\n                20 LOAD_METHOD              2 (sum)\n                22 CALL_METHOD              0\n                24 LOAD_CONST               2 (0)\n                26 COMPARE_OP               0 (<)\n                28 POP_JUMP_IF_FALSE       38\n\n    12          30 LOAD_FAST                1 (b)\n                32 LOAD_CONST               3 (-1)\n                34 BINARY_MULTIPLY\n                36 STORE_FAST               1 (b)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/deep-dive.rst","loc":{"lines":{"from":81,"to":102}}}}],["146",{"pageContent":"12          30 LOAD_FAST                1 (b)\n                32 LOAD_CONST               3 (-1)\n                34 BINARY_MULTIPLY\n                36 STORE_FAST               1 (b)\n\n    13     >>   38 LOAD_FAST                2 (x)\n                40 LOAD_FAST                1 (b)\n                42 BINARY_MULTIPLY\n                44 RETURN_VALUE","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/deep-dive.rst","loc":{"lines":{"from":102,"to":110}}}}],["147",{"pageContent":"MODIFIED BYTECODE\n     9           0 LOAD_GLOBAL              3 (__compiled_fn_0)\n                 2 LOAD_FAST                0 (a)\n                 4 LOAD_FAST                1 (b)\n                 6 CALL_FUNCTION            2\n                 8 UNPACK_SEQUENCE          2\n                10 STORE_FAST               2 (x)\n                12 POP_JUMP_IF_FALSE       24\n                14 LOAD_GLOBAL              4 (__resume_at_30_1)\n                16 LOAD_FAST                1 (b)\n                18 LOAD_FAST                2 (x)\n                20 CALL_FUNCTION            2\n                22 RETURN_VALUE\n           >>   24 LOAD_GLOBAL              5 (__resume_at_38_2)\n                26 LOAD_FAST                1 (b)\n                28 LOAD_FAST                2 (x)\n                30 CALL_FUNCTION            2\n                32 RETURN_VALUE\n\n   GUARDS:\n    - local 'a' TENSOR_MATCH\n    - local 'b' TENSOR_MATCH\n    - global 'torch' FUNCTION_MATCH","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/deep-dive.rst","loc":{"lines":{"from":112,"to":134}}}}],["148",{"pageContent":"GUARDS:\n    - local 'a' TENSOR_MATCH\n    - local 'b' TENSOR_MATCH\n    - global 'torch' FUNCTION_MATCH\n\nAt the top you can see the FX graph.\nNext, you see the original bytecode of the function, followed by the\nmodified bytecode generated by TorchDynamo. Finally, you see the guards\nwhich we covered above.\n\nIn the modified bytecode, ``__compiled_fn_0`` is the return value of\n``my_compiler()`` (the compiled graph). ``__resume_at_30_1`` and\n``__resume_at_38_2`` are both generated continuation functions that pick\nup execution after a graph break (at bytecode offsets 30 and 38). Each\nof these functions take the form:\n\n::\n\n   __resume_at_<offset>:\n       ... restore stack state if needed ...\n       JUMP_ABSOLUTE <offset> into toy_example\n       ... original bytecode of toy_example ...","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/deep-dive.rst","loc":{"lines":{"from":134,"to":155}}}}],["149",{"pageContent":"::\n\n   __resume_at_<offset>:\n       ... restore stack state if needed ...\n       JUMP_ABSOLUTE <offset> into toy_example\n       ... original bytecode of toy_example ...\n\nBy generating this `resume_at` function, we force the remainder of the\nfunction to be executed in a new Python frame which recursively\ntriggers TorchDynamo to restart its capture once execution reaches that\npoint for the first time.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/deep-dive.rst","loc":{"lines":{"from":155,"to":165}}}}],["150",{"pageContent":"Frequently Asked Questions\n==========================\n**Author**: `Mark Saroufim <https://github.com/msaroufim>`_\n\nAt a high level, the PyTorch 2.0 stack consists of a graph capture from\nPython code using dynamo and a backend compiler. In this example the\nbackend compiler consists of backward graph tracing using AOTAutograd\nand graph lowering using TorchInductor. There are of course many more\ncompilers available `here <https://github.com/pytorch/torchdynamo/blob/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd/README.md#existing-backend>`__\nbut for this document we will focus on inductor as a motivating example.\n\nTorchdynamo supports training, using AotAutograd to capture backwards:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":1,"to":12}}}}],["151",{"pageContent":"Torchdynamo supports training, using AotAutograd to capture backwards:\n\n   1. the ``.forward()`` graph and ``optimizer.step()`` is captured by torchdynamo’s python evalframe frontend\n   2. for each segment of ``.forward()`` that torchdynamo captures, it uses AotAutograd to generate a backward graph segment\n   3. each pair of forward, backward graph are (optionally) min-cut partitioned to save the minimal state between forward/backward\n   4. the forward, backward pairs are wrapped in autograd.function modules 5. usercode calling\\ ``.backward()`` still triggers eager’s autograd engine, which runs each ‘compiled backward’ graph as if it were one op, also running any non-compiled eager ops’ .backward() functions\n\nDo you support Distributed code?\n--------------------------------\n\nDDP has been tested and works, support for other distributed training\nlibraries is under discussion.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":12,"to":23}}}}],["152",{"pageContent":"Do you support Distributed code?\n--------------------------------\n\nDDP has been tested and works, support for other distributed training\nlibraries is under discussion.\n\nThe main reason why Distributed code is challenging with dynamo is\nbecause AOTAutograd unrolls both the forward and backward pass and\nprovides 2 graphs for backends to optimize. This is a problem for\ndistributed code because we’d like to ideally overlap communication\noperations with computations. Eager pytorch accomplishes this in\ndifferent ways for DDP/FSDP- using autograd hooks, module hooks, and\nmodifications/mutations of module states. In a naive application of\ndynamo, hooks that should run directly after an operation during\nbackwards may be delayed until after the entire compiled region of\nbackwards ops, due to how AOTAutograd compiled functions interact with\ndispatcher hooks.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":23,"to":39}}}}],["153",{"pageContent":"The basic strategy for optimizing DDP with Dynamo is outlined in\n`distributed.py <https://github.com/pytorch/pytorch/blob/master/torch/_dynamo/optimizations/distributed.py>`__\nwhere the main idea will be to graph break on `DDP bucket\nboundaries <https://pytorch.org/docs/stable/notes/ddp.html#internal-design>`__.\n\nWhen each node in DDP needs to synchronize its weights with the other\nnodes it organizes its gradients and parameters into buckets which\nreduces communication times and allows a node to broadcast a fraction of\nits gradients to other waiting nodes.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":41,"to":49}}}}],["154",{"pageContent":"Graph breaks in distributed code mean you can expect dynamo and its\nbackends to optimize the compute overhead of a distributed program but\nnot its communication overhead. Graph-breaks may interfere with\ncompilation speedups, if the reduced graph-size robs the compiler of\nfusion opportunities. However, there are diminishing returns with\nincreasing graph size since most of the current compute optimizations\nare local fusions. So in practice this approach may be sufficient.\n\nDo I still need to export whole graphs?\n---------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":51,"to":60}}}}],["155",{"pageContent":"Do I still need to export whole graphs?\n---------------------------------------\n\nFor the vast majority of models you probably don’t and you can use\n``torch._dynamo()`` optimize as is but there are a few situations where\nfull graphs are necessary and you can can ensure a full graph by simply\nrunning ``torch.dynamo(..., nopython=True)`` \\* Large scale training\nruns, think $250K+ that require pipeline parallelism and other advanced\nsharding strategies \\* Inference optimizers like\n`TensorRT <https://github.com/pytorch/TensorRT>`__ or\n`AITemplate <https://github.com/facebookincubator/AITemplate>`__ that rely\non fusing much more aggressively than training optimizers \\* Mobile training or\ninference.\n\nFuture work will include tracing communication operations into graphs,\ncoordinating these operations with compute optimizations, and optimizing\nthe communication operations.\n\nWhy is my code crashing?\n------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":60,"to":79}}}}],["156",{"pageContent":"Why is my code crashing?\n------------------------\n\nIf your code ran just fine without dynamo and started to crash with it\nenabled then the most important first step is figuring out which part of\nthe stack your failure occurred in so try running things in the below\norder and only try the next step if the previous step succeeded.\n\n1. ``torch.compile(..., backend=\"eager\")`` which only runs torchdynamo forward graph\n   capture and then runs the captured graph with PyTorch. If this fails\n   then there’s an issue with TorchDynamo.\n\n2. ``torch.compile(..., backend=\"aot_eager\")``\n   which runs torchdynamo to capture a forward graph, and then AOTAutograd\n   to trace the backward graph without any additional backend compiler\n   steps. PyTorch eager will then be used to run the forward and backward\n   graphs. If this fails then there’s an issue with AOTAutograd.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":79,"to":95}}}}],["157",{"pageContent":"3. ``torch.compile(..., backend=\"inductor\")`` which runs torchdynamo to capture a\n   forward graph, and then AOTAutograd to trace the backward graph with the\n   TorchInductor compiler. If this fails then there’s an issue with TorchInductor\n\nTorchDynamo Errors\n~~~~~~~~~~~~~~~~~~\n\nIf the error that is generated occurs with the ``\"eager\"`` backend, then\ntorchdynamo is the most likely source of the error.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":97,"to":105}}}}],["158",{"pageContent":"TorchDynamo Errors\n~~~~~~~~~~~~~~~~~~\n\nIf the error that is generated occurs with the ``\"eager\"`` backend, then\ntorchdynamo is the most likely source of the error.\n\nTo debug these issues we recommend setting\n``torch._dynamo.config.verbose=True`` to get a full stack trace to both\nthe error in torchdynamo and the user code. In addition to this flag,\nyou can also set the ``log_level`` of torchdynamo through\n``torch._dynamo.config.log_level``. The available levels are the\nfollowing: - ``logging.DEBUG``: Print every instruction that is\nencountered in addition to all below log levels - ``logging.INFO``:\nPrint each function that is compiled (original and modified bytecode)\nand the graph that is captured in addition to all below log levels -\n``logging.WARNING`` (default): Print graph breaks in addition to all\nbelow log levels - ``logging.ERROR``: Print errors only","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":105,"to":121}}}}],["159",{"pageContent":"If a model is sufficiently large, the logs can become overwhelming. If\nan error occurs deep within a model’s python code, it can be useful to\nexecute only the frame in which the error occurs to enable easier\ndebugging. There are 2 tools available to enable this:\n\n* ``env TORCHDYNAMO_DEBUG_FUNCTION=<desired_function_name>`` will only run TorchDynamo on functions with that name.\n\n* ``env torch._dynamo.config.replay_record_enabled = True``) which dumps an execution record when an error is encountered. This record can then be replayed to run only the frame where an error occurred.\n\nTorchInductor Errors\n--------------------\n\nWith TorchInductor as the chosen backend, AOTAutograd is used to\ngenerate the backward graph from the forward graph captured by\ntorchdynamo. It’s important to note that errors can occur during this\ntracing and also while TorchInductor lowers the forward and backward\ngraphs to GPU code or C++.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":123,"to":139}}}}],["160",{"pageContent":"A model can often consist of hundreds or thousands of FX nodes, so\nnarrowing the exact nodes where this problem occurred can be very\ndifficult which is why we highly recommend you use our minifier to\ncreate tiny reproducible examples of failures you’re seeing. We can\nminify errors that occur either at the AOTAutograd layer or Inductor\nlayer which you should try in the following order.\n\n1. ``env TORCHDYNAMO_REPRO_AFTER=\"aot\" python your_model.py``\n2.  ``env TORCHDYNAMO_REPRO_AFTER=\"dynamo\" python your_model.py``\n\nMinifying your error is the quickest path to getting it fixed.\n\nThe minifier will actually create a ``repro.py`` for you at the location\nset by ``env TORCHDYNAMO_REPRO_DIR`` so make you have right access to\nthat directory. You can then run ``python repro.py`` and confirm that\nyou are getting the same error.\n\n.. note::\n   For other compilers such as nvfuser, the process is similar but\n   instead you would leverage ``env TORCHDYNAMO_REPRO_AFTER=\"dynamo\" python your_model.py``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":141,"to":160}}}}],["161",{"pageContent":".. note::\n   For other compilers such as nvfuser, the process is similar but\n   instead you would leverage ``env TORCHDYNAMO_REPRO_AFTER=\"dynamo\" python your_model.py``.\n\nWhy is compilation slow?\n------------------------\n\nDynamo Compilation\n~~~~~~~~~~~~~~~~~~\n\nTorchDynamo has a builtin stats function for collecting and displaying\nthe time spent in each compilation phase. These stats can be accessed by\ncalling ``torch._dynamo.utils.compile_times()`` after executing\n``torch._dynamo``. By default, this returns a string representation of\nthe compile times spent in each TorchDynamo function by name.\n\nInductor Compilation\n~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":160,"to":177}}}}],["162",{"pageContent":"Inductor Compilation\n~~~~~~~~~~~~~~~~~~~~\n\nTorchInductor has a builtin stats and trace function for displaying time\nspent in each compilation phase, output code, output graph visualization\nand IR dump. ``env TORCH_COMPILE_DEBUG=1 python repro.py``. This is a\ndebugging tool designed to make it easier to debug/understand the\ninternals of TorchInductor with an output that will look something like\n`this <https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396>`__\n\nEach file in that debug trace can be enabled/disabled via\n``torch._inductor.config.trace.*``. The profile and the diagram are both\ndisabled by default since they are expensive to generate. See the\n`example debug directory\noutput <https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396>`__\nfor more examples.\n\nExcessive Recompilation\n~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":177,"to":195}}}}],["163",{"pageContent":"Excessive Recompilation\n~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen TorchDynamo compiles a function (or part of one), it makes certain\nassumptions about locals and globals in order to allow compiler\noptimizations, and expresses these assumptions as guards that check\nparticular values at runtime. If any of these guards fail, Dynamo will\nrecompile that function (or part) up to\n``torch._dynamo.config.cache_size_limit`` times. If your program is\nhitting the cache limit, you will first need to determine which guard is\nfailing and what part of your program is triggering it.\n\nThe `recompilation profiler <#recompilation-profiler>`__ automates the\nprocess of setting TorchDynamo’s cache limit to 1 and running your\nprogram under an observation-only ‘compiler’ that records the causes of\nany guard failures. You should be sure to run your program for at least\nas long (as many iterations) as you were running when you ran into\ntrouble, and the profiler will accumulate statistics over this duration.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":195,"to":212}}}}],["164",{"pageContent":".. code-block:: python\n\n   from torch._dynamo.utils import CompileProfiler\n\n   prof = CompileProfiler()\n\n   def my_model():\n       ...\n\n   profiler_model = torch.compile(my_model, backend=prof)\n   profiler_model()\n   print(prof.report())\n\nMany of the reasons for graph breaks and excessive recompilation will be\nfixed with upcoming support for `tracing dynamic tensor\nshapes <https://docs.google.com/document/d/1QJB-GOnbv-9PygGlOMXwiO9K6vVNm8sNg_olixJ9koc/edit?usp=sharing>`__,\nmore careful choices for guards and better tuned heuristics.\n\nWhy are you recompiling in production?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn some cases, you may not want unexpected compiles after a program has\nwarmed up. For example, if you are serving production traffic in a\nlatency critical application. For this, TorchDynamo provides an\nalternate mode where prior compiled graphs are used, but no new ones are\ngenerated:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":214,"to":241}}}}],["165",{"pageContent":".. code-block:: python\n\n   frozen_toy_example = dynamo.run(toy_example)\n   frozen_toy_example(torch.randn(10), torch.randn(10))\n\nHow are you speeding up my code?\n--------------------------------\n\nThere are 3 major ways to accelerate PyTorch code:\n\n1. Kernel fusion via vertical fusions which fuse sequential operations to avoid\n   excessive read/writes. For example, fuse 2 subsequent cosines means you\n   can can do 1 read 1 write instead 2 reads 2 writes 2. Horizontal fusion:\n   the simplest example being batching where a single matrix is multiplied\n   with a batch of examples but the more general scenario is a grouped GEMM\n   where a group of matrix multiplications are scheduled together\n\n2. Out of order execution: A general optimization for compilers, by looking ahead\n   at the exact data dependencies within a graph we can decide on the most\n   opportune time to execute a node and which buffers can be reused","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":241,"to":260}}}}],["166",{"pageContent":"3. Automatic work placement: Similar of the out of order execution point,\n   but by matching nodes of a graph to resources like physical hardware or\n   memory we can design an appropriate schedule\n\nThe above are general principles for accelerating PyTorch code but\ndifferent backends will each make different tradeoffs on what to\noptimize. For example Inductor first takes care of fusing whatever it\ncan and only then generates `Triton <https://openai.com/blog/triton/>`__\nkernels. It can also\n\nTriton in addition offers speedups because of automatic memory\ncoalescing, memory management and scheduling within each Streaming\nMultiprocessor and has been designed to handle tiled computations.\n\nHowever, regardless of the backend you use it’s best to use a benchmark\nand see approach so try out the PyTorch profiler, visually inspect the\ngenerated kernels and try to see what’s going on for yourself.\n\nWhy am I not seeing speedups?\n-----------------------------\n\nGraph Breaks\n~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":262,"to":284}}}}],["167",{"pageContent":"Why am I not seeing speedups?\n-----------------------------\n\nGraph Breaks\n~~~~~~~~~~~~\n\nThe main reason you won’t see the speedups you’d like to by using dynamo\nis excessive graph breaks. So what’s a graph break?\n\nGiven a program like:\n\n.. code-block:: python\n\n   def some_fun(x):\n       ...\n\n   torch.compile(some_fun)(x)\n   ...\n\nTorchdynamo will attempt to compile all of the torch/tensor operations\nwithin ``some_fun()`` into a single FX graph, but it may fail to capture\neverything into one graph.\n\nSome graph break reasons are insurmountable to TorchDynamo like calling\ninto a C extension other than torch is invisible to torchdynamo, and\ncould do arbitrary things without TorchDynamo being able to introduce\nnecessary guards to ensure that the compiled program would be safe to reuse.\n\n   To maximize performance, it’s important to have as few graph breaks\n   as possible.\n\nIdentifying the cause of a graph break\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":284,"to":316}}}}],["168",{"pageContent":"To maximize performance, it’s important to have as few graph breaks\n   as possible.\n\nIdentifying the cause of a graph break\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo identify all graph breaks in a program and the associated reasons for\nthe breaks, ``torch._dynamo.explain`` can be used. This tool runs\nTorchDynamo on the supplied function and aggregates the graph breaks\nthat are encountered. Here is an example usage:\n\n.. code-block:: python\n\n   import torch\n   import torch._dynamo as dynamo\n   def toy_example(a, b):\n       x = a / (torch.abs(a) + 1)\n       print(\"woo\")\n       if b.sum() < 0:\n           b = b * -1\n       return x * b\n   explanation, out_guards, graphs, ops_per_graph = dynamo.explain(toy_example, torch.randn(10), torch.randn(10))\n   print(explanation)\n   \"\"\"\n   Dynamo produced 3 graphs, with 2 graph break and 6 ops.\n    Break reasons:\n   1. call_function BuiltinVariable(print) [ConstantVariable(str)] {}\n      File \"t2.py\", line 16, in toy_example\n       print(\"woo\")","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":316,"to":344}}}}],["169",{"pageContent":"2. generic_jump\n      File \"t2.py\", line 17, in toy_example\n       if b.sum() < 0:\n    \"\"\"\n\nTo throw an error on the first graph break encountered you can use\ndisable python fallback by using ``nopython=True``, this should be\nfamiliar if you’ve worked with export based compilers.\n\n.. code-block:: python\n\n   def toy_example(a, b):\n      ...\n\n   torch.compile(toy_example, fullgraph=True, backend=<compiler>)\n\nWhy didn’t my code recompile when I changed it?\n-----------------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":346,"to":363}}}}],["170",{"pageContent":"def toy_example(a, b):\n      ...\n\n   torch.compile(toy_example, fullgraph=True, backend=<compiler>)\n\nWhy didn’t my code recompile when I changed it?\n-----------------------------------------------\n\nIf you went ahead and enabled dynamic shapes via\n``env TORCHDYNAMO_DYNAMIC_SHAPES=1 python model.py`` then your code\nwon’t recompile on shape changes. We’ve added support for dynamic shapes\nwhich avoids recompilations in the case when shapes vary by less than a\nfactor of 2. This is especially useful in scenarios like varying image\nsizes in CV or variable sequence length in NLP. In inference scenarios\nit’s often not possible to know what a batch size will be beforehand\nbecause you take what you can get from different client apps.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":363,"to":378}}}}],["171",{"pageContent":"In general, TorchDynamo tries very hard not to recompile things\nunnecessarily so if for example torchdynamo finds 3 graphs and your\nchange only modified one graph then only that graph will recompile. So\nanother tip to avoid potentially slow compilation times is to warmup a\nmodel by compiling it once after which subsequent compilations will be\nmuch faster. Cold start compile times is still a metric we track\nvisibly.\n\nWhy am I getting incorrect results?\n-----------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":380,"to":389}}}}],["172",{"pageContent":"Why am I getting incorrect results?\n-----------------------------------\n\nAccuracy issues can also be minified if you set the environment variable\n``TORCHDYNAMO_REPRO_LEVEL=4``, it operates with a similar git bisect\nmodel and a full repro might be something like\n``TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4`` the reason\nwe need this is downstream compilers will codegen code whether it’s\nTriton code or the C++ backend, the numerics from those downstream\ncompilers can be different in subtle ways yet have dramatic impact on\nyour training stability. So the accuracy debugger is very useful for us\nto detect bugs in our codegen or with a backend compiler.\n\nIf you'd like to ensure that random number generation is the same across both torch\nand triton then you can enable ``torch._inductor.config.fallback_random = True``\n\nWhy am I getting OOMs?\n----------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":389,"to":406}}}}],["173",{"pageContent":"Why am I getting OOMs?\n----------------------\n\nDynamo is still an alpha product so there’s a few sources of OOMs and if\nyou’re seeing an OOM try disabling the following configurations in this\norder and then open an issue on GitHub so we can solve the root problem\n1. If you’re using dynamic shapes try disabling them, we’ve disabled\nthem by default: ``env TORCHDYNAMO_DYNAMIC_SHAPES=0 python model.py`` 2.\nCUDA graphs with Triton are enabled by default in inductor but removing\nthem may alleviate some OOM issues: ``torch._inductor.config.triton.cudagraphs = False``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/faq.rst","loc":{"lines":{"from":406,"to":415}}}}],["174",{"pageContent":"Getting Started\n===============\n\nLet’s start with a simple example. Note that you are likely to see more\nsignificant speedups the newer your GPU is.\n\nThe below is a tutorial for inference, for a training specific tutorial, make sure to checkout `example on training <https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`__\n\n.. code:: python\n\n   import torch\n   def fn(x, y):\n       a = torch.cos(x).cuda()\n       b = torch.sin(y).cuda()\n       return a + b\n   new_fn = torch.compile(fn, backend=\"inductor\")\n   input_tensor = torch.randn(10000).to(device=\"cuda:0\")\n   a = new_fn(input_tensor, input_tensor)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":1,"to":18}}}}],["175",{"pageContent":"This example will not actually run faster. Its purpose is to demonstrate\nthe ``torch.cos()`` and ``torch.sin()`` features which are\nexamples of pointwise ops as in they operate element by element on a\nvector. A more famous pointwise op you might want to use would\nbe something like ``torch.relu()``. Pointwise ops in eager mode are\nsuboptimal because each one would need to read a tensor from\nmemory, make some changes, and then write back those changes. The single\nmost important optimization that inductor does is fusion. So back to our\nexample we can turn 2 reads and 2 writes into 1 read and 1 write which\nis crucial especially for newer GPUs where the bottleneck is memory\nbandwidth (how quickly you can send data to a GPU) rather than compute\n(how quickly your GPU can crunch floating point operations).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":20,"to":31}}}}],["176",{"pageContent":"Another major optimization that inductor makes available is automatic\nsupport for CUDA graphs.\nCUDA graphs help eliminate the overhead from launching individual\nkernels from a Python program which is especially relevant for newer GPUs.\n\nTorchDynamo supports many different backends but inductor specifically works\nby generating `Triton <https://github.com/openai/triton>`__ kernels and\nwe can inspect them by running ``TORCH_COMPILE_DEBUG=1 python trig.py``\nwith the actual generated kernel being\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":33,"to":43}}}}],["177",{"pageContent":".. code-block:: python\n\n   @pointwise(size_hints=[16384], filename=__file__, meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\n   @triton.jit\n   def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n       xnumel = 10000\n       xoffset = tl.program_id(0) * XBLOCK\n       xindex = xoffset + tl.reshape(tl.arange(0, XBLOCK), [XBLOCK])\n       xmask = xindex < xnumel\n       x0 = xindex\n       tmp0 = tl.load(in_ptr0 + (x0), xmask)\n       tmp1 = tl.sin(tmp0)\n       tmp2 = tl.sin(tmp1)\n       tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp2, xmask)\n\nAnd you can verify that fusing the two ``sin`` did actually occur\nbecause the two ``sin`` operations occur within a single Triton kernel\nand the temporary variables are held in registers with very fast access.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":43,"to":60}}}}],["178",{"pageContent":"You can read up a lot more on Triton’s performance\n`here <https://openai.com/blog/triton/>`__ but the key is it’s in Python\nso you can easily understand it even if you have not written all that\nmany CUDA kernels.\n\nNext, let’s try a real model like resnet50 from the PyTorch\nhub.\n\n.. code-block:: python\n\n   import torch\n   model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n   opt_model = torch.compile(model, backend=\"inductor\")\n   model(torch.randn(1,3,64,64))\n\nAnd that is not the only available backend, you can run in a REPL\n``torch._dynamo.list_backends()`` to see all the available backends. Try out the\n``cudagraphs`` or ``nvfuser`` next as inspiration.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":62,"to":79}}}}],["179",{"pageContent":"And that is not the only available backend, you can run in a REPL\n``torch._dynamo.list_backends()`` to see all the available backends. Try out the\n``cudagraphs`` or ``nvfuser`` next as inspiration.\n\nLet’s do something a bit more interesting now, our community frequently\nuses pretrained models from\n`transformers <https://github.com/huggingface/transformers>`__ or\n`TIMM <https://github.com/rwightman/pytorch-image-models>`__ and one of\nour design goals is for Dynamo and inductor to work out of the box with\nany model that people would like to author.\n\nSo we will directly download a pretrained model from the\nHuggingFace hub and optimize it:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":79,"to":93}}}}],["180",{"pageContent":"So we will directly download a pretrained model from the\nHuggingFace hub and optimize it:\n\n.. code-block:: python\n\n   import torch\n   from transformers import BertTokenizer, BertModel\n   # Copy pasted from here https://huggingface.co/bert-base-uncased\n   tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n   model = BertModel.from_pretrained(\"bert-base-uncased\").to(device=\"cuda:0\")\n   model = torch.compile(model, backend=\"inductor\") # This is the only line of code that we changed\n   text = \"Replace me by any text you'd like.\"\n   encoded_input = tokenizer(text, return_tensors='pt').to(device=\"cuda:0\")\n   output = model(**encoded_input)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":93,"to":106}}}}],["181",{"pageContent":"If you remove the ``to(device=\"cuda:0\")`` from the model and\n``encoded_input``, then Triton will generate C++ kernels that will be\noptimized for running on your CPU. You can inspect both Triton or C++\nkernels for BERT, they’re obviously more complex than the trigonometry\nexample we had above but you can similarly skim it and understand if you\nunderstand PyTorch.\n\nSimilarly let’s try out a TIMM example\n\n.. code-block:: python\n\n   import timm\n   import torch._dynamo as dynamo\n   import torch\n   model = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=2)\n   opt_model = torch.compile(model, backend=\"inductor\")\n   opt_model(torch.randn(64,3,7,7))\n\nOur goal with Dynamo and inductor is to build the highest coverage ML compiler\nwhich should work with any model you throw at it.\n\nExisting Backends\n~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":108,"to":130}}}}],["182",{"pageContent":"Our goal with Dynamo and inductor is to build the highest coverage ML compiler\nwhich should work with any model you throw at it.\n\nExisting Backends\n~~~~~~~~~~~~~~~~~\n\nTorchDynamo has a growing list of backends, which can be found in the\n`backends <https://github.com/pytorch/pytorch/blob/master/torch/_dynamo/backends/>`__ folder\nor ``torch._dynamo.list_backends()`` each of which with its optional dependencies.\n\nSome of the most commonly used backends include:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":130,"to":140}}}}],["183",{"pageContent":"Some of the most commonly used backends include:\n\n**Training & inference backends**:\n  * ``torch.compile(m, backend=\"inductor\")`` - Uses ``TorchInductor`` backend. `Read more <https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747>`__\n  * ``torch.compile(m, backend=\"aot_ts_nvfuser\")`` - nvFuser with AotAutograd/TorchScript. `Read more <https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593>`__\n  * ``torch.compile(m, backend=\"\"nvprims_nvfuser\")`` - nvFuser with PrimTorch. `Read more <https://dev-discuss.pytorch.org/t/tracing-with-primitives-update-1-nvfuser-and-its-primitives/593>`__\n  * ``torch.compile(m, backend=\"cudagraphs\")`` - cudagraphs with AotAutograd. `Read more <https://github.com/pytorch/torchdynamo/pull/757>`__","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":140,"to":146}}}}],["184",{"pageContent":"**Inference-only backends**:\n  * ``torch.compile(m, backend=\"onnxrt\")`` - Uses ONNXRT for inference on CPU/GPU. `Read more <https://onnxruntime.ai/>`__\n  * ``torch.compile(m, backend=\"tensorrt\")`` - Uses ONNXRT to run TensorRT for inference optimizations. `Read more <https://github.com/onnx/onnx-tensorrt>`__\n  * ``torch.compile(m, backend=\"ipex\")`` - Uses IPEX for inference on CPU. `Read more <https://github.com/intel/intel-extension-for-pytorch>`__\n  * ``torch.compile(m, backend=\"tvm\")`` - Uses Apache TVM for inference optimizations. `Read more <https://tvm.apache.org/>`__\n\nWhy do you need another way of optimizing PyTorch code?\n-------------------------------------------------------\n\nWhile a number of other code optimization tools exist in the PyTorch\necosystem, each of them has its own flow.\nHere is a few examples of existing methods and their limitations:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":148,"to":159}}}}],["185",{"pageContent":"While a number of other code optimization tools exist in the PyTorch\necosystem, each of them has its own flow.\nHere is a few examples of existing methods and their limitations:\n\n-  ``torch.jit.trace()`` is silently wrong if it cannot trace, for example:\n   during control flow\n-  ``torch.jit.script()`` requires modifications to user or library code\n   by adding type annotations and removing non PyTorch code\n-  ``torch.fx.symbolic_trace()`` either traces correctly or gives a hard\n   error but it’s limited to traceable code so still can’t handle\n   control flow\n-  ``torch._dynamo`` works out of the box and produces partial graphs.\n   It still has the option of producing a single graph with\n   ``nopython=True`` which are needed for `some\n   situations <./documentation/FAQ.md#do-i-still-need-to-export-whole-graphs>`__\n   but allows a smoother transition where partial graphs can be\n   optimized without code modification","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/get-started.rst","loc":{"lines":{"from":159,"to":175}}}}],["186",{"pageContent":"Guards Overview\n===============\n\nFrom a UX perspective, TorchDynamo is very easy to use. The user invokes\n``torchdynamo.optimize`` as an annotation:\n\n.. code-block:: python\n\n   @torchdynamo.optimize(my_compiler)\n   def fn_foo(bar):\n\nWhere a complete example looks like this:\n\n.. code-block:: python\n\n   from typing import List\n   import torch\n   from torch import _dynamo as torchdynamo\n   def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n       print(\"my_compiler() called with FX graph:\")\n       gm.graph.print_tabular()\n       return gm.forward  # return a python callable\n   @torchdynamo.optimize(my_compiler)\n   def toy_example(a, b):\n       x = a / (torch.abs(a) + 1)\n       if b.sum() < 0:\n           b = b * -1\n       return x * b\n   for _ in range(100):\n       toy_example(torch.randn(10), torch.randn(10))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":1,"to":30}}}}],["187",{"pageContent":"This allows TorchDynamo to capture the interpreted Python frames, grab\nany and all relevant information, and speed things up wherever it can.\nThe speedup comes from a few places, and can be rather dependent on the\nbackend (`my_compiler` in the example above) provided, but the one speedup\nthat is important in this section is **caching**. Caching itself is not\na direct speedup but a critical enablement that prevents\nrecompilation. We dig a hole with dynamo, and caching allows us to get\nout. It enables us to hold perf\nneutrality while then enabling backends - the true source of our\nspeedups.\n\nWith even a pass-through no-op backend provided:\n\n.. code-block:: python\n\n   def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n       return gm.forward\n\nWe can see TorchDynamo speeding up Python execution even on\nregular Python, not just PyTorch.\n\nCaching and Guards Overview\n---------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":32,"to":54}}}}],["188",{"pageContent":"We can see TorchDynamo speeding up Python execution even on\nregular Python, not just PyTorch.\n\nCaching and Guards Overview\n---------------------------\n\nTorchDynamo operates through caching transformed (by TorchDynamo) user\nbytecode. When TorchDynamo receives a frame for evaluation, it checks if the\n**objects referenced in the frame have changed** in certain ways, and if\nnot, TorchDynamo reads the previously transformed user bytecode to evaluate it.\nIn this section, we will focus on how we can identify whether or not the\n**objects referenced in the frame have changed**. This is a critical\npiece of functionality in TorchDynamo, because it drives the entire\ninvalidation lifecycle. This functionality is called **guards**.\n\nAt a very high level, the flow can be summarized like this:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":54,"to":69}}}}],["189",{"pageContent":"At a very high level, the flow can be summarized like this:\n\n1. TorchDynamo receives a Python frame.\n2. It converts the frame (1) passing it through instruction\n   translation.\n3. For the objects captured in (2), TorchDynamo creates tracking objects that\n   are:\n   * tracked on an output graph, which is an internal specialization\n   of a `torch.fx.Tracer`\n   * guards\n4. TorchDynamo processes the guard objects created in (3), turning them into a\n   generated Python function, `check_fn`, associated with a piece of code.\n5. The `check_fn` is evaluated whenever we encounter this code a\n   subsequent time - if a `check_fn` passes and evaluates to `True`, TorchDynamo\n   identifies the code in the cache and the code encountered here as same, and\n   can be safely used. If it fails and evaluates to `False`, TorchDynamo\n   identifies the code in the cache as not valid, and can be thrown out in\n   favor of a new entry, through recompilation or a graph break.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":69,"to":86}}}}],["190",{"pageContent":"Python Frame Evaluation and PEP 523\n-----------------------------------\n\nThe functionality of TorchDynamo is based on\n`PEP 523 <https://peps.python.org/pep-0523/>`__.\n\nTorchDynamo installs a frame evaluation function on Python by using\n`_PyInterpreterState_SetEvalFrameFunc`. TorchDynamo has a hook where\nPython can hand control back to us during evaluation.\n\nThe function we have installed is ``convert_frame`` or\n``convert_frame_assert`` in the ``nopython=True`` case, but glossing\nover that nuance for now, let’s take a look at ``convert_frame_assert``,\nas ``convert_frame`` proxies to it.\n\nWe can find it on `line 20 of convert_frame.py\n<https://github.com/pytorch/torchdynamo/blob/main/torchdynamo/convert_frame.py#L200>`__,\nwith a signature as follows:\n\n.. code-block:: python\n\n   def  convert_frame_assert(compiler_fn: Callable, one_graph=True):\n\nThis function wraps the entry point of where Python invokes TorchDynamo\nwith a frame:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":88,"to":114}}}}],["191",{"pageContent":".. code-block:: python\n\n   def  convert_frame_assert(compiler_fn: Callable, one_graph=True):\n\nThis function wraps the entry point of where Python invokes TorchDynamo\nwith a frame:\n\n.. code-block:: python\n\n   def  _convert_frame_assert(frame: types.FrameType, cache_size: int):\n\nHere is what this function does:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":114,"to":125}}}}],["192",{"pageContent":".. code-block:: python\n\n   def  _convert_frame_assert(frame: types.FrameType, cache_size: int):\n\nHere is what this function does:\n\n1. Checks if it has seen this ``code``\\ (see: f_code `here\n   <https://docs.python.org/3/library/inspect.html>`__) before and exits\n   early if it did.\n2. Checks if the code is an unsupported case.\n3. Checks if the ``cache_size`` (second arg above) crosses the limit\n   defined in the config, ``cache_size_limit``. If it has, the function\n   drops the frame and logs warnings. This helps to avoid constant\n   recompilation of a frame as it generally means that the frame is hot\n   in an unexpected way and caching it produces needless overhead,\n   as it is likely to get evicted the next time it is encountered.\n4. Passes the frame, alongside a function that creates an\n   ``InstructionTranslator`` through bytecode\n   transformation, via ``transform_code_object``. A few crucial things\n   happen under the hood here:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":125,"to":144}}}}],["193",{"pageContent":"1. New code is produced through ``transform_code_object``.\n\n   2. An FX tracer named ``output`` is produced through\n      ``InstructionTranslator``.\n\n      This can be a bit confusing,\n      as ``InstructionTranslator`` is not an `fx` tracer, but its stored\n      in a variable named tracer, and its output*\\ **is**\\ *an `fx`tracer.*\n\n   3. The function produces guards and stores them on ``output`` above.\n\n   4. The function produces ``output_instructions`` and stores them on\n      ``output`` above.\n\n   5. The function maps the newly produced transformed code to the initial code it\n      read off the frame. This mapping is worth remembering, we will\n      refer to it much later on below where we cover guard failures.\n\n5. Using the transformed code from 4.1 and the guards from 4.3,\n   the function produces a `GuardedCode`.\n\nNow that we have learned about frame evaluation, let’s review\n``InstructionTranslator``, and see how it turns the frame we handed\nit over into TorchDynamo internal types.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":146,"to":169}}}}],["194",{"pageContent":"Now that we have learned about frame evaluation, let’s review\n``InstructionTranslator``, and see how it turns the frame we handed\nit over into TorchDynamo internal types.\n\nInstructionTranslator\n---------------------\n\n`InstructionTranslator` does a lot! We won’t cover the details of\neverything it does, but most importantly for this document, it produces\na mapping of ``symbolic_locals`` which maintains a mapping from the\nframe’s ``f_locals`` to TorchDynamo internal Variable objects (more on these\nin a moment. ``symbolic_locals`` is filled via traversing the frame’s\nlocals:\n\n.. code-block:: python\n\n   self.symbolic_locals = collections.OrderedDict(\n       (k, VariableBuilder(self, LocalSource(k))(f_locals[k]))\n       for k in vars\n       if k in f_locals\n   )","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":169,"to":189}}}}],["195",{"pageContent":".. code-block:: python\n\n   self.symbolic_locals = collections.OrderedDict(\n       (k, VariableBuilder(self, LocalSource(k))(f_locals[k]))\n       for k in vars\n       if k in f_locals\n   )\n\nThe important component here  is the invocation of a call\ninto ``VariableBuilder``. ``VariableBuilder``\\ ’s call implementation\nproxies into a function called ``_wrap``, which in turn both constructs\ninstances of ``VariableTracker`` and calls ``make_guards`` on them. More\non that later.\n\nThis mapping, in turn, is critical as each Variable has associated\nguards, which are then passed to ``self.output``, the instance of\n``OutputGraph``, an fx tracer, mentioned in 4.2 of the section above. If\nyou recall, this ``OutputGraph``, stored in a variable called ``output``\nis where our guards are stored before being passed on to become\n``GuardedCode``\n\nHow does ``InstructionTranslator`` do this? At the heart of it, there is\na loop that is pumped, which drives a function ``step``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":189,"to":211}}}}],["196",{"pageContent":"How does ``InstructionTranslator`` do this? At the heart of it, there is\na loop that is pumped, which drives a function ``step``.\n\n``step`` is just that - a single processing step, taking exactly one\ninstruction and doing *something* with it.\n\n.. note:: These are real instructions processed by TorchDynamo’s\n   ``transform_code_object``, and it is pretty cool.\n\n.. note:: This section purposely skips the details of\n   `dis.get_instructions <https://docs.python.org/3/library/dis.html>`__.\n\nFor the example above, here is a snippet of a what a few\n``Instruction``\\'s may look like:\n\n.. code-block:: python\n\n   Instruction(opcode=124, opname='LOAD_FAST', arg=0, argval='b', offset=32, starts_line=8, is_jump_target=True, target=None)\n   Instruction(opcode=100, opname='LOAD_CONST', arg=3, argval=-1, offset=34, starts_line=None, is_jump_target=False, target=None)\n   Instruction(opcode=20, opname='BINARY_MULTIPLY', arg=None, argval=None, offset=36, starts_line=None, is_jump_target=False, target=None)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":211,"to":230}}}}],["197",{"pageContent":"This is the core functionality of this function. Take a look at the ``opname``,\nand then take a look at this little snippet from inside ``step``;\n\n.. code-block:: python\n\n   if not hasattr(self, inst.opname):\n       unimplemented(f\"missing: {inst.opname}\")\n   getattr(self, inst.opname)(inst)\n\nAs we can see, the function checks if the current class, the\n``InstructionTranslator`` has an attribute set matching the operator name\n(for example, ``LOAD_CONST``). If it does, the function invokes it, passing the\nwhole instruction object in. If it does not, the function drops the frame as\nunimplemented.\n\nFor the ``LOAD_CONST`` example, we can see that we do indeed support it,\nwith a relatively straightforward definition:\n\n::\n\n   def  LOAD_CONST(self, inst):\n   self.push(ConstantVariable(value=inst.argval))\n\nWe can see that this function creates a new instance of the class\n``ConstantVariable`` , with a value, in our example case, -1, and then\npushes it onto the stack.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":232,"to":257}}}}],["198",{"pageContent":"We can see that this function creates a new instance of the class\n``ConstantVariable`` , with a value, in our example case, -1, and then\npushes it onto the stack.\n\nThere are dozens of such methods - see ``symbolic_convert.py`` for all of\nthem. Generally, we implement as many matching methods to Python\nbytecode instructions as possible.\n\nAcross both the logic downstream of ``step`` and the logic from invoking\n``VariableBuilder`` - we now have a lot of ``VariableTracker``\\ s and of\ncourse, we’ve spoken about creating guards quiet a bit. Let’s dig into\nwhat Variables are, and get a little closer to understanding guards.\n\nVariables\n---------\n\nA ``ConstantVariable`` is an instance of\\ ``VariableTracker``.\n``VariableTracker`` represents a tracked Python local or stack value.\n\nWhen it comes to representing an object inside TorchDynamo, a\n``VariableTracker`` does exactly what it says - it tracks a given variable.\nIt is an extremely flexible class, but there are a few points to keep in\nmind:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":257,"to":279}}}}],["199",{"pageContent":"-  It manages the ``guard`` relationship around the underlying object\n   through:\n\n   -  ``make_guard``\n   -  ``replace_guards``\n   -  ``add_guard(s)``\n   -  ``propagate`` - ``propagate(*vars: List[List[\"VariableTracker\"]])`` -\n      Perhaps the most important of all, in that it combines guards from\n      all the provided ``VariableTracker`` instances passed in. It visits\n      the guards and combines the guards from these onto itself.\n\n-  It acts as a proxy on behalf of the underlying object, implementing\n   methods for the rest of TorchDynamo to get information about the\n   tracked object:\n\n   -  ``call_method``\n   -  ``call_function``\n   -  ``python_type``\n   -  ``as_proxy``\n   -  ``is/as_python_proxy``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":281,"to":300}}}}],["200",{"pageContent":"-  ``call_method``\n   -  ``call_function``\n   -  ``python_type``\n   -  ``as_proxy``\n   -  ``is/as_python_proxy``\n\n-  It stores the variable ``source`` of type ``Source``, from\n   ``torchdynamo/source.py``. This source type is a relatively self\n   contained class that helps us organize and bookkeep where the original\n   source came from, and helps provide convenience methods for things\n   like getting the name, and importantly for us, producing guards.\n\nAnd this class (``VariableTracker``) is built around subclassing,\nsomewhere between a full Abstract Base Class and fully fleshed out class\n- it leaves many methods raising ``NotImplementedError`` - with reliance on\nsubclasses. See ``torchdynamo/variables/`` for all subclasses to fulfill\ncontracts and custom behaviors.\n\nKnowing what we know now, we can see an example of how an instruction\nfrom ``dis``, ``BUILD_TUPLE``:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":300,"to":319}}}}],["201",{"pageContent":"Knowing what we know now, we can see an example of how an instruction\nfrom ``dis``, ``BUILD_TUPLE``:\n\n   ``BUILD_TUPLE(count)`` Creates a tuple consuming count items from the\n   stack, and pushes the resulting tuple onto the stack.\n\nIn our case, our signature will be a *little* different due to the way\nwe create ``Instruction`` objects, but the gist of it will be the same.\nInstead of passing in ``count``, we pass in an object with a little\nextra bookkeeping, and of course, we deal with turning regular old\npython objects into TorchDynamo notions:\n\n::\n\n   def BUILD_TUPLE(self, inst):\n       items = self.popn(inst.argval)\n       options = VariableTracker.propagate(items)\n       self.push(TupleVariable(items, **options))\n\nHere is what this code does:\n\n1. The function reads ``argval``, which in this case, is\n   analogous to ``counts`` in the pydoc for the equivalent instruction.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":319,"to":341}}}}],["202",{"pageContent":"Here is what this code does:\n\n1. The function reads ``argval``, which in this case, is\n   analogous to ``counts`` in the pydoc for the equivalent instruction.\n\n2. The function ``popn`` the items, in this case, the signature is\n   ``def  popn(self, n: int) -> List[TensorVariable]:`` this hints at an\n   underlying contract - we are returning ``TensorVariables``. If we\n   take a closer look at ``symbolic_convert.py`` and\n   ``InstructionTranslatorBase``/``InstructionTranslator``\\ we see that\n   the only thing pushed onto and popped from our stack are\n   ``VariableTracker``\\ s.\n\n3) The function calls ``VariableTracker.propagate``. This\n   takes the guards from every single item popped off the stack in 2,\n   and recursively traverses it and combines all the guards into\n   ``options``: ``py  return {      \"guards\": guards,  }``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":341,"to":357}}}}],["203",{"pageContent":"4) The function then makes a new instance of a ``VariableTracker``,\n   ``TupleVariable``\\ out of the ``items`` and ``options``. This then\n   allows us to install all the appropriate guards from the ``items``\n   that make up the new ``TupleVariable``\n\n.. note:: Where did the first guards come from? Propagation\n   is a good technique, but we need something created before it can be\n   propagated. ``VariableBuilder`` calls\n   ``make_guards`` as it creates ``VariableTracker`` instances, from\n   ``f_locals``. This in turn calls into the ``source``, to have it create\n   guards.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":359,"to":369}}}}],["204",{"pageContent":"After all this, bytecode translation is done and we are one step closer\nto producing ``GuardedCode``. We now understand how locals become\n``VariableTracker``\\ s, how instructions are handled, and where guards\nare called on for creation. Before we can go into seeing how code and\nguards are combined into a GuardedCode object, we need to dig a little\nbit into those ``make_guard`` and ``source.make_guard`` calls above. We\ncan then understand, what was going on when we made guards\nalongside, and on, ``VariableTracker`` instances.\n\nMaking Guards\n-------------\n\nGuards are just Python objects, of the class ``Guard``. Let's look at them\nin more detail.\n\nLooking at the definition of the dataclass (and therefore, ctor\nsignature), we see that it has a name, a source, and a create function.\n\n::\n\n   @dataclasses.dataclass\n   class Guard:\n       name: str\n       source: GuardSource\n       create_fn: Callable\n\nThe name should be the name of the variable.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":371,"to":397}}}}],["205",{"pageContent":"::\n\n   @dataclasses.dataclass\n   class Guard:\n       name: str\n       source: GuardSource\n       create_fn: Callable\n\nThe name should be the name of the variable.\n\nThe source here is an enum indicating what *kind* of source the guard\nbelongs to.\n\n.. note:: Not to be confused with ``Source`` and the other types\n   in ``source.py``, as stored on ``VariableTracker``.\n\n``create_fn`` provides the main functionality to transition from a simple\ndataclass to actually producing valid Python code to be invoked for\nknowing whether or not things have changed in between invocations, and\nwhether we can safely read from the code cache or not.\n\nThe most common code paths for getting an instance of a guard are\nthrough ``make_guards`` on ``VariableTracker``.\n``make_guards``->``source.make_guard``->``return Guard(self.name(), self.guard_source(), fn)``\n\nOr, in a concrete example:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":397,"to":424}}}}],["206",{"pageContent":"Or, in a concrete example:\n\n.. code-block:: python\n\n   ...\n   elif istype(value, range):\n       guards = self.make_guards(GuardBuilder.EQUALS_MATCH)\n       return RangeVariable(value=value, guards=guards)\n\nSince ``source`` was set at the construction time of this\n``VariableTracker``, all that was needed here was to provide the ``fn``,\n``GuardBuilder.EQUALS_MATCH`` to the ``create_fn`` field.\n\nThis ``create_fn`` must be a method on ``GuardBuilder``. The reason for\nthis becomes apparent in our next step. Once we have all the guards\ncreated for a frame, we move on to ``CheckFunctionManager`` and\n``compile_check_fn``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":424,"to":440}}}}],["207",{"pageContent":"Before the ``convert_frame`` function can produce a ``GuardedCode``,\nit needs to run the ``CheckFunctionManager``, with all the guards, to\nproduce a ``check_fn`` which will then, in turn get passed in alongside\nthe code into ``GuardedCode``. This is the same ``check_fn`` that we store in our\ncache entry, and the same one we run to know whether or not to retrieve\nthe code stored alongside. For reference, here is that code:\n\n.. code-block:: cpp\n\n   static CacheEntry *create_cache_entry(CacheEntry *next,\n                                         PyObject *guarded_code) {\n     CacheEntry *e = (CacheEntry *)malloc(sizeof(CacheEntry));\n     DEBUG_NULL_CHECK(e);\n     e->check_fn = PyObject_GetAttrString(guarded_code, \"check_fn\");\n     NULL_CHECK(e->check_fn);\n     e->code = (PyCodeObject *)PyObject_GetAttrString(guarded_code, \"code\");\n     NULL_CHECK(e->code);\n     e->next = next;\n     return e;\n   }","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":442,"to":461}}}}],["208",{"pageContent":"We now know how a ``check_fn`` function is used, and who makes it, and\nwhat it is composed of, but what we do not yet know is how. How does a\nlist of ``Guard`` objects become a function we can run later on?\n\nFirst, we iterate these guards:\n\n.. code-block:: python\n\n   for guard in sorted(guards or [], key=Guard.sort_key):\n       if not config.guard_nn_modules and guard.is_nn_module():\n           continue\n       guard.create(local_builder, global_builder)\n\nCalling ``guard.create`` runs that ``create_fn`` we set on the ``Guard``\nclass above (don’t confuse it with the ``check_fn`` we are working on\nproducing, the names are similar, so it can get a little confusing). In\nour example above, our ``create_fn`` is ``GuardBuilder.EQUALS_MATCH``.\nSo we are now invoking it, passing in the ``self``, the guard itself,\nin.\n\nThe signature is: ``def EQUALS_MATCH(self, guard: Guard):``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":463,"to":483}}}}],["209",{"pageContent":"The signature is: ``def EQUALS_MATCH(self, guard: Guard):``\n\nAnd internally to that function, we can use the ``name`` on the guard to\nget back our original object, querying it for data and type information,\nwhich in turn gets us to the most important bit: appending code.\n\nAt its simplest, ``EQUALS_MATCH`` appends just one line of code:\n``self.code.append(f\"{ref} == {val!r}\")``. Where ``ref`` is the name of\nthe variable, and ``val`` is the value. It might produce code like this:\n\n.. code-block::\n\n   y == 2\n\nThis is a basic example. But if we append a few other kinds of ``GuardBuilder``\nfunctions and then combine them all with\n``and`` in between each statement (as we do), we might get something\nlike this:\n\n.. code-block::\n\n   ___guarded_code.valid and ___check_type_id(y, 94367738391392) and y == 2 and ___check_tensors(x)\n\nHere is what this code performs:\n\n1. A check for ``.valid``\n2. A type ID check\n3. A value check\n4. A tensor check","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":483,"to":511}}}}],["210",{"pageContent":"Here is what this code performs:\n\n1. A check for ``.valid``\n2. A type ID check\n3. A value check\n4. A tensor check\n\nThis becomes the heart of the code our ``check_fn``, which in turn\nis evaluated the **next** time we encounter this code. It\nwill then check:\n\n1. Is this code still valid?\n2. If (1), Does ``y`` still have a type of ``94367738391392``?\n3. If (2), is ``y`` still 2?\n4. If (3), let’s check on if tensor ``x`` changed in some specific ways.\n\nIf all of these are still true, then we can use the code cached\nalongside this ``check_fn``.\n\n.. note:: For a deeper dive for how and where this happens\n   you can read ``static PyCodeObject *lookup(CacheEntry *e, PyObject *f_locals) {`` of\n   ``_eval_frame.c``.\n\nIf not, then, we can move on to recompiling the code anew, and storing\nthat in the cache alongside this code, and a whole new ``check_fn``,\nagain to be checked on yet another subsequent frame.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":511,"to":536}}}}],["211",{"pageContent":"If not, then, we can move on to recompiling the code anew, and storing\nthat in the cache alongside this code, and a whole new ``check_fn``,\nagain to be checked on yet another subsequent frame.\n\nThere are lots of other such functions on ``GuardBuilder`` which get\ncoalesced into, at times massive, strings which then get evaluated as\nPython code and stored into ``check_fn``. The example above\nillustrates of a simple case. To understand this functionality better, read\nthe other functions on ``GuardBuilder``, or better yet, dump the ``code`` variable\nin ``compile_check_fn`` to see what is getting produced,\nespecially on larger, real models.\n\nSummary\n-------\n\nIn this section, we have reviewed:\n\n- The role of ``.valid`` and invalidation around weak references (and potentially soon to be NN Moduleinvalidations).\n- How the C++ side of guard functions (``___check_type_id``, ``___check_tensors``, etc) operate\n- What happens when guards fail.\n- What happens if we produce invalid guard code.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":536,"to":556}}}}],["212",{"pageContent":"We covered how user provided code wrapped in a TorchDynamo context\ngoes on to get traced and tracked internally, organized into ``VariableTracker``\\ s\n``Source``\\ s and subsequently ``Guard``\\ s, and how those ``Guards`` in\nturn guide cache entry selection and invalidation when handing Python\ncode.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/guards-overview.rst","loc":{"lines":{"from":558,"to":562}}}}],["213",{"pageContent":".. currentmodule:: torch\n\ntorch.compile\n====================\n\n:func:`~torch.compile` was introduced in `PyTorch 2.0 <https://pytorch.org/get-started/pytorch-2.0/>`__\n\nOur default and supported backend is `inductor` with benchmarks `showing 30% to 2x speedups and 10% memory compression <https://github.com/pytorch/pytorch/issues/93794>`__\non real world models for both training and inference with a single line of code.\n\n.. note::\n    The :func:`~torch.compile` API is experimental and subject to change.\n\nThe simplest possible interesting program is the below which we go over in a lot more detail in `getting started <https://pytorch.org/docs/master/compile/get-started.html>`__\nshowing how to use :func:`~torch.compile` to speed up inference on a variety of real world models from both TIMM and HuggingFace which we\nco-announced `here <https://pytorch.org/blog/Accelerating-Hugging-Face-and-TIMM-models/>`__\n\n.. code:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/index.rst","loc":{"lines":{"from":1,"to":18}}}}],["214",{"pageContent":".. code:: python\n\n   import torch\n   def fn(x):\n       x = torch.cos(x).cuda()\n       x = torch.sin(x).cuda()\n       return x\n   compiled_fn = torch.compile(fn(torch.randn(10).cuda()))\n\nIf you happen to be running your model on an Ampere GPU, it's crucial to enable tensor cores. We will actually warn you to set\n``torch.set_float32_matmul_precision('high')``\n\n:func:`~torch.compile` works over :class:`~torch.nn.Module` as well as functions so you can pass in your entire training loop.\n\nThe above example was for inference but you can follow this tutorial for an `example on training <https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html>`__\n\n\nOptimizations\n-------------\n\nOptimizations can be passed in :func:`~torch.compile` with either a backend mode parameter or as passes. To understand what are the available options you can run\n``torch._inductor.list_options`` and ``torch._inductor.list_mode_options()``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/index.rst","loc":{"lines":{"from":18,"to":39}}}}],["215",{"pageContent":"The default backend is `inductor` which will likely be the most reliable and performant option for most users and library maintainers,\nother backends are there for power users who don't mind more experimental community support.\n\nThere is some nuance involved in benchmarking ``torch.compile`` so we've provided a utility to make this simpler with :func:`~torch.utils.benchmark.utils.compile.bench_all`\n\nYou can get the full list of community backends by running :func:`~torch._dynamo.list_backends`\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    compile\n\nTroubleshooting and Gotchas\n---------------------------\n\nIF you experience issues with models failing to compile, running of out of memory, recompiling too often, not giving accurate results,\nodds are you will find the right tool to solve your problem in our guides.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/index.rst","loc":{"lines":{"from":41,"to":58}}}}],["216",{"pageContent":".. WARNING::\n    A few features are still very much in development and not likely to work for most users. Please do not use these features\n    in production code and if you're a library maintainer please do not expose these options to your users\n    Dynamic shapes ``dynamic=true`` and max autotune ``mode=\"max-autotune\"`` which can be passed in to :func:`~torch.compile`.\n    Distributed training has some quirks which you can follow in the troubleshooting guide below. Model export is not ready yet.\n\n.. toctree::\n   :maxdepth: 1\n\n   troubleshooting\n   faq\n\nLearn more\n----------\n\nIf you can't wait to get started and want to learn more about the internals of the PyTorch 2.0 stack then\nplease check out the references below.\n\n.. toctree::\n   :maxdepth: 1\n\n   get-started\n   technical-overview\n   nn-module","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/index.rst","loc":{"lines":{"from":60,"to":83}}}}],["217",{"pageContent":"PyTorch 2.0 NNModule Support\n============================\n\n**Author**: `Will Constable <https://github.com/wconstab>`_\n\n`torch.compile` has special handling for torch.nn.Module objects, tracing them differently than it traces\narbitrary python classes, with the intent of producing faster code by making assumptions about the structure.\n\nThis doc describes some of the tradeoffs or edge cases that come up due to this specialization.\n\nNNModule Hooks Support\n----------------------\nPreviously, `torch.compile` had no support for hooks on nn.Modules, and if hooks were registered\nthey would simply be ignored in the compiled program. Indeed many users do not\nuse nn.Module hooks at all, or only use them for debug workflows, but there are valid use cases\nfor composing nn.Module hooks with `torch.compile`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/nn-module.rst","loc":{"lines":{"from":1,"to":16}}}}],["218",{"pageContent":"Hooks that are orchestrated via nn.Module.__call__ implementation include `_forward_pre_hooks`,\n`forward_hooks`, `_backward_pre_hooks`, and `_backward_hooks`, and will be referred to as 'call hooks'.\nThese hooks are partially supported by `torch.compile` with limitations described below.\n\nAnother category of hooks includes `_state_dict_hooks` and its `pre` and `load_` variants, and are still\nunsupported by `torch.compile`.\n\n`nn.Module.__call__` Hooks Usage and limitations\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nBy default, `torch.compile` will trace the contents of `nn.Module.__call__` which means it will encounter\nand run forward/pre-forward hooks.  If you install hooks before calling `torch.compile` and then do not remove\nor alter the hooks later, your use case should be supported by default.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/nn-module.rst","loc":{"lines":{"from":18,"to":29}}}}],["219",{"pageContent":"Backward/Pre-backward hooks are generally also supported, with similar caveats: currently graph-breaks in dynamo\noccur when accessing backward_hooks dicts, which is probably avoiable with some work.  Graph-breaks also impact the\ntiming of firing backward hooks, since graph-segments are run as autograd-functions which produce all their grads at\nthe same time.  Assuming it were possible for dynamo to not graph-break on the presence of backward-hooks, we would\nstill expect the backward hooks for a series of modules to all fire together after the whole compiled graph's backward\nran.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/nn-module.rst","loc":{"lines":{"from":31,"to":36}}}}],["220",{"pageContent":"**hooks on 'allowed modules'**\n`torch.compile` treats common modules such as torch.conv, as well as modules that are difficult to trace, specially\nby allowing them to be called opaquely in the dynamo graph instead of traced into by dynamo.  For such modules, hooks\ncurrently trigger a graph-break so that the affected modules run outside of dynamo.  Depending on the model, this could\nintroduce a significant performance regression, and additional work is required to improve this support.\n\n**skip_nnmodule_hook_guards**\nBy default, `torch._dynamo.config.skip_nnmodule_hook_guards` is set to True, meaning no guards will be installed\non each nn.Module hook dictionary, improving runtime by reducing guard execution time, at the cost of not noticing\nif any hook dict is changed after compilation.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/nn-module.rst","loc":{"lines":{"from":38,"to":47}}}}],["221",{"pageContent":"If you want to be able to remove or modify hooks after compilation and have `torch.compile` react appropriately\n(by recompiling), then you need to set `skip_nnmodule_hook_guards=False` and expect a runtime penalty for the added\nguards.\n\nTODO: confirm if backward/pre_backward hooks are working or not and document accordingly\n\nstate_dict Hooks\n~~~~~~~~~~~~~~~~\nState dict hooks have not yet been supported in `torch.compile`.\n\n\nTODO: warn_once if graph-breaking on hooks.  warn_once to point to this doc if hooks are present.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/nn-module.rst","loc":{"lines":{"from":49,"to":60}}}}],["222",{"pageContent":"Technical Overview\n====================\n\n**TorchDynamo** is a Python-level JIT compiler designed to make unmodified\nPyTorch programs faster. TorchDynamo hooks into the frame evaluation API\nin CPython (`PEP 523 <https://peps.python.org/pep-0523/>`__) to\ndynamically modify Python bytecode right before it is executed. It\nrewrites Python bytecode in order to extract sequences of PyTorch\noperations into an `FX Graph <https://pytorch.org/docs/stable/fx.html>`__\nwhich is then just-in-time compiled with a customizable backend.\nIt creates this FX Graph through bytecode analysis and is designed to\nmix Python execution with compiled backends to get the best of both\nworlds — usability and performance.\n\nTorchDynamo makes it easy to experiment with different compiler\nbackends to make PyTorch code faster with a single line decorator\n``torch._dynamo.optimize()`` which is wrapped for convenience by ``torch.compile()``\n\n.. image:: ../_static/img/dynamo/TorchDynamo.png","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/technical-overview.rst","loc":{"lines":{"from":1,"to":19}}}}],["223",{"pageContent":".. image:: ../_static/img/dynamo/TorchDynamo.png\n\n`TorchInductor` is one of the backends\nsupported by `TorchDynamo Graph <https://pytorch.org/docs/stable/fx.html>`__\ninto `Triton <https://github.com/openai/triton>`__ for GPUs or\n`C++/OpenMP <https://www.openmp.org/>`__ for CPUs. We have a\n`training performance dashboard <https://github.com/pytorch/torchdynamo/issues/681#issuecomment-1233828468>`__\nthat provides performance comparison for different training backends. You can read\nmore in the `TorchInductor post on PyTorch\ndev-discuss <https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747>`__.\n\n.. seealso::\n\n   * `TorchDynamo deep-dive video <https://www.youtube.com/watch?v=egZB5Uxki0I>`__\n   * `dev-discuss topics <https://dev-discuss.pytorch.org/search?q=TorchDynamo%20order%3Alatest>`__\n\n.. toctree::\n   :maxdepth: 1\n\n   guards-overview\n   custom-backends\n   deep-dive","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/technical-overview.rst","loc":{"lines":{"from":19,"to":40}}}}],["224",{"pageContent":"PyTorch 2.0 Troubleshooting\n===========================\n\n**Author**: `Michael Lazos <https://github.com/mlazos>`_\n\n`torch.compile` is still in active development, and many of the reasons for\ngraph breaks and excessive recompilation will be fixed with upcoming\nsupport for `tracing dynamic tensor\nshapes <https://docs.google.com/document/d/1QJB-GOnbv-9PygGlOMXwiO9K6vVNm8sNg_olixJ9koc/edit?usp=sharing>`__,\nmore careful choices for guards and better tuned heuristics.\n\nIn the meantime, you may need to diagnose a particular issue and\ndetermine if it is easy to work around with a change to your model, or\nfile an issue for support.\n\nAlso, we are actively developing debug tools, profilers, and improving our\nerrors/warnings. Please give us feedback if you have an issue with this\ninfra, or an idea for an improvement. Below is a table of the available\ntools and their typical usage. For additional help see\n`Diagnosing Runtime Errors <#diagnosing-runtime-errors>`__.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":1,"to":20}}}}],["225",{"pageContent":".. list-table:: Title\n   :widths: 25 25 50\n   :header-rows: 1","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":22,"to":24}}}}],["226",{"pageContent":"* - Tool\n     - Purpose\n     - Usage\n   * - Info logging\n     - View summarized steps of compilation\n     - ``torch._dynamo.config.log_level = logging.INFO``\n   * - Debug logging\n     - View detailed steps of compilation (print every instruction traced)\n     - ``torch._dynamo.config.log_level = logging.DEBUG`` and\n       ``torch._dynamo.config.verbose = True``\n   * - Minifier for any backend\n     - Find smallest subgraph which reproduces errors for any backend\n     - set environment variable ``TORCHDYNAMO_REPRO_AFTER=\"dynamo\"``\n   * - Minifier for ``TorchInductor``\n     - If the error is known to occur after `AOTAutograd`` find\n       smallest subgraph which reproduces errors during TorchInductor lowering\n     - set environment variable ``TORCHDYNAMO_REPRO_AFTER=\"aot\"``\n   * - Dynamo accuracy minifier\n     - Finds the smallest subgraph which reproduces an accuracy issue\n       between an eager model model and optimized model, when you\n       suspect the problem is in AOTAutograd","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":26,"to":46}}}}],["227",{"pageContent":"- Finds the smallest subgraph which reproduces an accuracy issue\n       between an eager model model and optimized model, when you\n       suspect the problem is in AOTAutograd\n     - ``TORCHDYNAMO_REPRO_AFTER=\"dynamo\" TORCHDYNAMO_REPRO_LEVEL=4``\n   * - Inductor accuracy minifier\n     - Finds the smallest subgraph which reproduces an accuracy issue\n       between an eager model model and optimized model, when you\n       suspect the problem is in the backend (e.g., inductor).\n       If this doesn't work, try the Dynamo accuracy minifier\n       instead.\n     - ``TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4``\n   * - ``torch._dynamo.explain``\n     - Find graph breaks and display reasoning for them\n     - ``torch._dynamo.explain(fn, *inputs)``\n   * - Record/Replay\n     - Record and replay frames which to reproduce errors during graph capture\n     - ``torch._dynamo.config.replay_record_enabled = True``\n   * - TorchDynamo function name filtering","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":46,"to":63}}}}],["228",{"pageContent":"* - Record/Replay\n     - Record and replay frames which to reproduce errors during graph capture\n     - ``torch._dynamo.config.replay_record_enabled = True``\n   * - TorchDynamo function name filtering\n     - Only compile functions with the given name to reduce noise when\n       debugging an issue\n     - set environment variable ``TORCHDYNAMO_DEBUG_FUNCTION=<name>``\n   * - TorchInductor Debug logging\n     - Print general TorchInductor debug info and generated Triton/C++ code\n     - ``torch._inductor.config.debug = True``\n   * - TorchInductor Tracing\n     - Show time taken in each TorchInductor stage + output code and graph\n       visualization\n     - set the environment variable TORCH_COMPILE_DEBUG=1 or\n       ``torch._inductor.config.trace.enabled = True``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":63,"to":77}}}}],["229",{"pageContent":"Diagnosing Runtime Errors\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBelow is the TorchDynamo compiler stack.\n\nAt a high level, the TorchDynamo stack consists of a graph capture from\nPython code (TorchDynamo) and a backend compiler. In this example, the\nbackend compiler consists of backward graph tracing (AOTAutograd) and\ngraph lowering (TorchInductor)*. Errors can occur in any component of\nthe stack and will provide full stack traces.\n\nYou may use info logging\n(``torch._dynamo.config.log_level = logging.INFO``) and look for\n``Step #: ...`` outputs in order to determine in which component the\nerror has occurred. Logs are made at the beginning and end of each step,\nso the step that an error should correspond to is the most recent logged\nstep whose end has not yet been logged. The steps correspond to the\nfollowing parts of the stack (according to the image above):\n\n==== ================\nStep Component\n==== ================\n1    TorchDynamo\n2    Compiler Backend\n3    TorchInductor\n==== ================","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":79,"to":104}}}}],["230",{"pageContent":"==== ================\nStep Component\n==== ================\n1    TorchDynamo\n2    Compiler Backend\n3    TorchInductor\n==== ================\n\nThe beginning and end of AOTAutograd is currently not logged, but we\nplan to add it soon.\n\nIf info logging is insufficient, then there are also some backend\noptions which can enable you to determine which component is causing the\nerror if you’re unable to understand the error message that is\ngenerated. These are the following:\n\n-  ``\"eager\"``: only runs torchdynamo forward graph capture and then\n   runs the captured graph with PyTorch. This provides an indication as\n   to whether TorchDynamo is raising the error.\n\n-  ``\"aot_eager\"``: runs torchdynamo to capture a forward graph, and\n   then AOTAutograd to trace the backward graph without any additional\n   backend compiler steps. PyTorch eager will then be used to run the\n   forward and backward graphs. This is useful to narrow down the issue\n   to AOTAutograd.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":104,"to":128}}}}],["231",{"pageContent":"The general procedure to narrow down an issue is the following:\n\n1. Run your program with the ``\"eager\"`` backend. If the error no longer\n   occurs, the issue is in the backend compiler that is being used (if\n   using TorchInductor, proceed to step 2. If not, see `this\n   section <#minifying-backend-compiler-errors>`__). If the error still\n   occurs with the ``\"eager\"`` backend, it is an `error while running\n   torchdynamo <#torchdynamo-errors>`__.\n\n2. This step is only necessary if ``TorchInductor`` is used as the backend\n   compiler. Run the model with the ``\"aot_eager\"`` backend. If this\n   backend raises an error then the error is occurring during\n   AOTAutograd tracing. If the error no longer occurs with this backend,\n   then `the error is in\n   TorchInductor\\* <#minifying-torchinductor-errors>`__.\n\nEach of these cases are analyzed in the following sections.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":130,"to":146}}}}],["232",{"pageContent":"Each of these cases are analyzed in the following sections.\n\n.. note:: The TorchInductor backend consists of\n   both AOTAutograd tracing and the TorchInductor compiler itself. We will\n   disambiguate by referring to ``TorchInductor`` as the backend, and\n   TorchInductor lowering as the phase which lowers the graph traced by\n   AOTAutograd.\n\nTorchdynamo Errors\n------------------\n\nIf the error that is generated occurs with the ``\"eager\"`` backend, then\nTorchDynamo is the most likely source of the error. Here is a sample code\nwhich will generate an error.\n\n.. code-block:: py\n\n   import torch\n\n   import torch._dynamo as dynamo\n\n\n   def test_assertion_error():\n       y = torch.ones(200, 200)\n       z = {y: 5}\n       return z\n\n   compiled_test_assertion_error = torch.compile(test_assertion_error, backend=\"eager\")\n\n   compiled_test_assertion_error()\n\nWhich will generate the following error:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":146,"to":179}}}}],["233",{"pageContent":"compiled_test_assertion_error = torch.compile(test_assertion_error, backend=\"eager\")\n\n   compiled_test_assertion_error()\n\nWhich will generate the following error:\n\n::\n\n   torch._dynamo.convert_frame: [ERROR] WON'T CONVERT test_assertion_error /scratch/mlazos/torchdynamo/../test/errors.py line 26\n   due to:\n   Traceback (most recent call last):\n     File \"/scratch/mlazos/torchdynamo/torchdynamo/symbolic_convert.py\", line 837, in BUILD_MAP\n       assert isinstance(k, ConstantVariable) or (\n   AssertionError\n\n   from user code:\n      File \"/scratch/mlazos/torchdynamo/../test/errors.py\", line 34, in test_assertion_error\n       z = {y: 5}\n\n   Set torch._dynamo.config.verbose=True for more information\n   ==========","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":179,"to":199}}}}],["234",{"pageContent":"Set torch._dynamo.config.verbose=True for more information\n   ==========\n\nAs the message suggests you can set\n``torch._dynamo.config.verbose=True`` to get a full stack trace to both\nthe error in TorchDynamo and the user code. In addition to this flag,\nyou can also set the ``log_level`` of torchdynamo through\n``torch._dynamo.config.log_level``. The available levels are the\nfollowing:\n\n- ``logging.DEBUG``: Print every instruction that is\n  encountered in addition to all below log levels.\n- ``logging.INFO``:\n  Print each function that is compiled (original and modified bytecode)\n  and the graph that is captured in addition to all below log levels.\n- ``logging.WARNING`` (default): Print graph breaks in addition to all\n  below log levels.\n- ``logging.ERROR``: Print errors only.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":199,"to":216}}}}],["235",{"pageContent":"If a model is sufficiently large, the logs can become overwhelming. If\nan error occurs deep within a model’s Python code, it can be useful to\nexecute only the frame in which the error occurs to enable easier\ndebugging. There are two tools available to enable this:\n\n- Setting the environment variable ``TORCHDYNAMO_DEBUG_FUNCTION`` to the desired function name will only run torchdynamo on functions with that name.\n- Enabling the record/replay tool (set ``torch._dynamo.config.replay_record_enabled = True``) which dumps an execution record when an error is encountered. This record can then be replayed to run only the frame where an error occurred.\n\nTorchInductor Errors\n--------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":218,"to":227}}}}],["236",{"pageContent":"TorchInductor Errors\n--------------------\n\nIf the error does not occur with the ``\"eager\"`` backend, then the\nbackend compiler is the source of the error (`example\nerror <https://gist.github.com/mlazos/2f13681e3cc6c43b3911f336327032de%5D>`__).\nThere are `different\nchoices <https://github.com/pytorch/torchdynamo/blob/0b8aaf340dad4777a080ef24bf09623f1aa6f3dd/README.md#existing-backends>`__\nfor backend compilers for TorchDynamo, with TorchInductor or nvfuser\nfitting the needs of most users. This section focuses on TorchInductor\nas the motivating example, but some tools will be usable with other\nbackend compilers.\n\nBelow is the portion of the stack which we are focusing on:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":227,"to":240}}}}],["237",{"pageContent":"With TorchInductor as the chosen backend, AOTAutograd is used to\ngenerate the backward graph from the forward graph captured by\ntorchdynamo. It is important to note that errors can occur during this\ntracing and also while TorchInductor lowers the forward and backward\ngraphs to GPU code or C++. A model can often consist of hundreds or\nthousands of FX nodes, so narrowing the exact nodes where this problem\noccurred can be very difficult. Fortunately, there are tools available to\nautomatically minify these input graphs to the nodes which are causing\nthe issue. The first step is to determine whether the error occurs\nduring tracing of the backward graph with AOTAutograd or during\nTorchInductor lowering. As mentioned above in step 2, the\n``\"aot_eager\"`` backend can be used to run only AOTAutograd in isolation\nwithout lowering. If the error still occurs with this backend, this\nindicates that the error is occurring during AOTAutograd tracing.\n\nHere is an example:\n\n.. code-block:: py\n\n   import torch","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":242,"to":261}}}}],["238",{"pageContent":"Here is an example:\n\n.. code-block:: py\n\n   import torch\n\n   import torch._dynamo as dynamo\n\n   model = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])\n\n   def test_backend_error():\n\n       y = torch.ones(200, 200)\n       x = torch.ones(200, 200)\n       z = x + y\n       a = torch.ops.aten._foobar(z)  # dummy function which errors\n       return model(a)\n\n\n   compiled_test_backend_error = torch.compile(test_backend_error, backend=\"inductor\")\n   compiled_test_backend_error()\n\nRunning this should give you this error with a longer stack trace below\nit:\n\n::\n\n   Traceback (most recent call last):\n     File \"/scratch/mlazos/torchdynamo/torchinductor/graph.py\", line 246, in call_function\n       return lowerings[target](*args, **kwargs)\n     File \"/scratch/mlazos/torchdynamo/torchinductor/lowering.py\", line 185, in wrapped\n       return decomp_fn(*args, **kwargs)\n     File \"/scratch/mlazos/torchdynamo/torchinductor/lowering.py\", line 810, in _foobar\n       assert False\n   AssertionError\n   ...","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":261,"to":296}}}}],["239",{"pageContent":"`error with full stack\ntrace <https://gist.github.com/mlazos/d6947854aa56d686800259a164c62100>`__\n\nIf you then change ``torch.compile(backend=\"inductor\")`` to\n``torch.compile(backend=\"aot_eager\")``, it will run without error, because\n`the\nissue <https://github.com/pytorch/torchdynamo/blob/d09e50fbee388d466b5252a63045643166006f77/torchinductor/lowering.py#:~:text=%23%20This%20shouldn%27t%20be,assert%20False>`__\nis in the TorchInductor lowering process, not in AOTAutograd.\n\nMinifying TorchInductor Errors\n------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":298,"to":308}}}}],["240",{"pageContent":"From here, let’s run the minifier to get a minimal repro. Setting the\nenvironment variable ``TORCHDYNAMO_REPRO_AFTER=“aot”`` (or setting\n``torch._dynamo.config.repro_after=\"aot\"`` directly) will generate a\nPython program which reduces the graph produced by AOTAutograd to the\nsmallest subgraph which reproduces the error. (See below for an example\nwhere we minify the graph produced by torchdynamo) Running the program\nwith this environment variable should show nearly `identical\noutput <https://gist.github.com/mlazos/0458ab828aa403c779fe73c012aa5982>`__,\nwith an additional line indicating where ``minifier_launcher.py`` has\nbeen written to. The output directory is configurable by setting\n``torch._dynamo.config.base_dir`` to a valid directory name. The final\nstep is to run the minifier and check that it runs successfully. A\nsuccessful run looks like\n`this <https://gist.github.com/mlazos/e6ea41ccce68a7b1b8a7a09acb1b206a>`__.\nIf the minifier runs successfully, it generates runnable python code","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":310,"to":324}}}}],["241",{"pageContent":"successful run looks like\n`this <https://gist.github.com/mlazos/e6ea41ccce68a7b1b8a7a09acb1b206a>`__.\nIf the minifier runs successfully, it generates runnable python code\nwhich reproduces the exact error. For our example this is the following\ncode:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":324,"to":328}}}}],["242",{"pageContent":".. code-block:: python\n\n   import torch\n   from torch import tensor, device\n   import torch.fx as fx\n   from torch._dynamo.testing import rand_strided\n   from math import inf\n   from torch.fx.experimental.proxy_tensor import make_fx\n\n   # torch version: 1.13.0a0+gitfddfc44\n   # torch cuda version: 11.6\n   # torch git version: fddfc4488afb207971c54ad4bf58130fdc8a4dc5\n\n\n   # CUDA Info:\n   # nvcc: NVIDIA (R) Cuda compiler driver\n   # Copyright (c) 2005-2022 NVIDIA Corporation\n   # Built on Thu_Feb_10_18:23:41_PST_2022\n   # Cuda compilation tools, release 11.6, V11.6.112\n   # Build cuda_11.6.r11.6/compiler.30978841_0\n\n   # GPU Hardware Info:\n   # NVIDIA A100-SXM4-40GB : 8\n\n   from torch.nn import *\n\n   class Repro(torch.nn.Module):\n       def __init__(self):\n           super().__init__()\n\n       def forward(self, add):\n           _foobar = torch.ops.aten._foobar.default(add);  add = None\n           return (_foobar,)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":330,"to":362}}}}],["243",{"pageContent":"def forward(self, add):\n           _foobar = torch.ops.aten._foobar.default(add);  add = None\n           return (_foobar,)\n\n   args = [((200, 200), (200, 1), torch.float32, 'cpu')]\n   args = [rand_strided(shape, stride, dtype, device) for shape, stride, dtype, device in args]\n   mod = make_fx(Repro())(*args)\n   from torch._inductor.compile_fx import compile_fx_inner\n\n   compiled = compile_fx_inner(mod, args)\n   compiled(*args)\n\nThe ``forward`` method of the ``Repro`` module contains the exact op\nwhich causes the issue. When filing an issue, please include any\nminified repros to aid in debugging.\n\nMinifying Backend Compiler Errors\n---------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":362,"to":379}}}}],["244",{"pageContent":"Minifying Backend Compiler Errors\n---------------------------------\n\nWith backend compilers other than TorchInductor the process for finding\nthe subgraph causing the error is nearly identical to the procedure in\n`errors in TorchInductor <#torchinductor-errors>`__ with one important\ncaveat. Namely, that the minifier will now be run on the graph that is\ntraced by TorchDynamo, not the output graph of AOTAutograd. Let’s walk\nthrough an example.\n\n.. code-block:: py\n\n   import torch\n\n   import torch._dynamo as dynamo\n\n   model = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])\n   # toy compiler which fails if graph contains relu\n   def toy_compiler(gm: torch.fx.GraphModule, _):\n       for node in gm.graph.nodes:\n           if node.target == torch.relu:\n               assert False\n\n       return gm\n\n\n   def test_backend_error():\n       y = torch.ones(200, 200)\n       x = torch.ones(200, 200)\n       z = x + y\n       a = torch.relu(z)\n       return model(a)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":379,"to":410}}}}],["245",{"pageContent":"return gm\n\n\n   def test_backend_error():\n       y = torch.ones(200, 200)\n       x = torch.ones(200, 200)\n       z = x + y\n       a = torch.relu(z)\n       return model(a)\n\n\n   compiled_test_backend_error = torch.compile(test_backend_error, backend=toy_compiler)\n   compiled_test_backend_error()\n\nIn order to run the code after TorchDynamo has traced the forward graph,\nyou can use the ``TORCHDYNAMO_REPRO_AFTER`` environment variable. Running\nthis program with ``TORCHDYNAMO_REPRO_AFTER=“dynamo”`` (or\n``torch._dynamo.config.repro_after=\"dynamo\"``) should produce `this\noutput <https://gist.github.com/mlazos/244e3d5b53667e44078e194762c0c92b>`__\\ and\nthe following code in ``{torch._dynamo.config.base_dir}/repro.py``.\n\n.. note:: The other option for TORCHDYNAMO_REPRO_AFTER are ``\"aot\"``, which\n   will run the minifier after the backward graph has been generated.\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":410,"to":434}}}}],["246",{"pageContent":".. note:: The other option for TORCHDYNAMO_REPRO_AFTER are ``\"aot\"``, which\n   will run the minifier after the backward graph has been generated.\n\n.. code-block:: python\n\n   import torch\n   import torch._dynamo as dynamo\n   from torch import tensor, device\n   import torch.fx as fx\n   from torch._dynamo.testing import rand_strided\n   from math import inf\n   from torch._dynamo.debug_utils import run_fwd_maybe_bwd\n\n   from torch.nn import *\n\n   class Repro(torch.nn.Module):\n       def __init__(self):\n           super().__init__()\n\n       def forward(self, add):\n           relu = torch.relu(add);  add = None\n           return (relu,)\n\n\n   mod = Repro().cuda()\n   opt_mod = torch.compile(mod, backend=\"None\")\n\n\n   args = [((200, 200), (200, 1), torch.float32, 'cpu', False)]\n   args = [rand_strided(sh, st, dt, dev).requires_grad_(rg) for (sh, st, dt, dev, rg) in args]\n\n\n   with torch.cuda.amp.autocast(enabled=False):\n       ref = run_fwd_maybe_bwd(mod, args)\n       res = run_fwd_maybe_bwd(opt_mod, args)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":434,"to":468}}}}],["247",{"pageContent":"with torch.cuda.amp.autocast(enabled=False):\n       ref = run_fwd_maybe_bwd(mod, args)\n       res = run_fwd_maybe_bwd(opt_mod, args)\n\nThe minifier successfully reduced the graph to the op that raises the\nerror in ``toy_compiler``. The other difference from the procedure in\n`TorhInductor Errors <#torchinductor-errors>`__ is that the minifier is\nautomatically run after encountering a backend compiler error. After a\nsuccessful run, the minifier writes ``repro.py`` to\n``torch._dynamo.config.base_dir``.\n\nPerformance Profiling\n~~~~~~~~~~~~~~~~~~~~~\n\nAccessing TorchDynamo Profiler\n------------------------------\n\nTorchDynamo has a builtin stats function for collecting and displaying\nthe time spent in each compilation phase. These stats can be accessed by\ncalling ``torch._dynamo.utils.compile_times()`` after executing\nTorch._Dynamo. By default, this returns a string representation of the\ncompile times spent in each TorchDynamo function by name.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":468,"to":489}}}}],["248",{"pageContent":"TorchInductor Debugging using TORCH_COMPILE_DEBUG\n-------------------------------------------------\n\nTorchInductor has a builtin stats and trace function for displaying time\nspent in each compilation phase, output code, output graph visualization\nand IR dump. This is a debugging tool designed to make it easier to\nunderstand and troubleshoot the internals of TorchInductor.\n\nLet's run an example with the following test program (repro.py):\n\n::\n\n  import torch\n\n  @torch.compile()\n  def test_model(x):\n      model = torch.nn.Sequential(\n          torch.nn.Linear(10, 10),\n          torch.nn.LayerNorm(10),\n          torch.nn.ReLU(),\n      )\n      return model(x)\n\n\n  y = test_model(torch.ones(10, 10))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":491,"to":515}}}}],["249",{"pageContent":"y = test_model(torch.ones(10, 10))\n\nSetting the environment variable ``TORCH_COMPILE_DEBUG=1`` will cause a\ndebug trace directory to be created, by default this directory will be in the current directory and named torch_compile_debug\n(this can be overridden in the torchdynamo configuration field ``debug_dir_root`` and also the env var TORCH_COMPILE_DEBUG_DIR).\nInside this directory, each run will have a separate folder named with the timestamp and process id of the run:\n::\n\n   $ env TORCH_COMPILE_DEBUG=1 python repro.py\n   $ cd torch_compile_debug\n   $ ls\n   run_2023_03_01_08_20_52_143510-pid_180167\n\nIn the run folder there will be a torchdynamo directory which contains debug logs, and an torchinductor\nfolder which contains a subfolder for each compiled kernel with inductor debug artifacts.\n\n::\n\n   $ cd\n   run_2023_03_01_08_20_52_143510-pid_180167\n   $ ls\n   torchinductor  torchdynamo","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":515,"to":536}}}}],["250",{"pageContent":"::\n\n   $ cd\n   run_2023_03_01_08_20_52_143510-pid_180167\n   $ ls\n   torchinductor  torchdynamo\n\nMoving further into the torchinductor directory, the \\*.log files are logs from the aot autograd phase of compilation, model__0_forward_1.0 contains the inductor debug artifacts.\n\n::\n\n   $ cd torchinductor\n   $ ls\n   aot_model___0_debug.log  model__0_forward_1.0\n   $ cd model__0_forward_1.0\n   $ ls\n   debug.log  fx_graph_readable.py  fx_graph_runnable.py  fx_graph_transformed.py  ir_post_fusion.txt  ir_pre_fusion.txt  output_code.py\n\nHere is a summary of the contents:\n - fx_graph_readable.py and fx_graph_runnable.py are the readable and runnable versions of the fx_graph received by inductor.\n - fx_graph_transformed.py is the fx graph after inductor has run all fx passes.\n - ir\\*.txt is the inductor ir pre and post fusion.\n - output_code.py is the compiled triton kernel for the subgraph.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":536,"to":558}}}}],["251",{"pageContent":"Here are `example debug directory contents\n<https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396>`__\nfor the test program:\n\n::\n\n  import torch\n\n  @torch.compile()\n  def test_model(x):\n      model = torch.nn.Sequential(\n          torch.nn.Linear(10, 10),\n          torch.nn.LayerNorm(10),\n          torch.nn.ReLU(),\n      )\n      return model(x)\n\n\n  y = test_model(torch.ones(10, 10))\n\nEach file in that debug trace can be enabled and disabled through\n``torch._inductor.config.trace.*``. The profile and the diagram are both\ndisabled by default since they are expensive to generate.\n\nA single node in this new debug format looks like:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":560,"to":586}}}}],["252",{"pageContent":"::\n\n   buf1: SchedulerNode(ComputedBuffer)\n   buf1.writes =\n       {   MemoryDep(name='buf1', index=0, size=()),\n           MemoryDep(name='buf1', index=0, size=(s0,))}\n   buf1.unmet_dependencies = {MemoryDep(name='buf0', index=c0, size=(s0,))}\n   buf1.met_dependencies = {MemoryDep(name='primals_2', index=c0, size=(s0,))}\n   buf1.group.device = cuda:0\n   buf1.group.iteration = (1, s0)\n   buf1.sizes = ([], [s0])\n   class buf1_loop_body:\n       var_ranges = {z0: s0}\n       index0 = z0\n       index1 = 0\n       def body(self, ops):\n           get_index = self.get_index('index0')\n           load = ops.load('buf0', get_index, False)\n           get_index_1 = self.get_index('index0')\n           load_1 = ops.load('primals_2', get_index_1, False)\n           add = ops.add(load, load_1)\n           get_index_2 = self.get_index('index1')\n           reduction = ops.reduction('buf1', torch.float32, torch.float32, 'sum', get_index_2, add)\n           return reduction","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":586,"to":609}}}}],["253",{"pageContent":"See the `example debug directory\noutput <https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396>`__\nfor more examples.\n\n..\n  _Memory Profiling\n  ----------------\n\n  TBD\n\nGraph Breaks\n------------\n\nGiven a program like this:\n\n.. code-block:: python\n\n   def some_fun(x):\n       ...\n\n   compiled_fun = torch.compile(some_fun, ...)\n   ...\n\nTorchDynamo will attempt to compile all of the torch/tensor operations\nwithin some_fun into a single FX graph, but it may fail to capture\neverything into one graph.\n\nSome graph break reasons are insurmountable to TorchDynamo, and can’t be\neasily fixed. - calling into a C extension other than torch is invisible\nto torchdynamo, and could do arbitrary things without TorchDynamo being\nable to introduce necessary `guards <./GuardsOverviewPt1.md>`__ to\nensure that the compiled program would be safe to reuse. Graph breaks\ncan hinder performance if the resulting fragments are small. To maximize\nperformance, it’s important to have as few graph breaks as possible.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":611,"to":644}}}}],["254",{"pageContent":"Identifying the Cause of a Graph Break\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo identify all graph breaks in a program and the associated reasons for\nthe breaks, ``torch._dynamo.explain`` can be used. This tool runs\nTorchDynamo on the supplied function and aggregates the graph breaks\nthat are encountered. Here is an example usage:\n\n.. code-block:: python\n\n   import torch\n   import torch._dynamo as dynamo\n   def toy_example(a, b):\n       x = a / (torch.abs(a) + 1)\n       print(\"woo\")\n       if b.sum() < 0:\n           b = b * -1\n       return x * b\n   explanation, out_guards, graphs, ops_per_graph = dynamo.explain(toy_example, torch.randn(10), torch.randn(10))\n   print(explanation)\n   \"\"\"\n   Dynamo produced 3 graphs, with 2 graph breaks and 6 ops.\n    Break reasons:\n   1. call_function BuiltinVariable(print) [ConstantVariable(str)] {}\n      File \"t2.py\", line 16, in toy_example\n       print(\"woo\")\n\n   2. generic_jump\n      File \"t2.py\", line 17, in toy_example\n       if b.sum() < 0:\n    \"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":646,"to":676}}}}],["255",{"pageContent":"2. generic_jump\n      File \"t2.py\", line 17, in toy_example\n       if b.sum() < 0:\n    \"\"\"\n\nOutputs include:\n\n- ``out_guards`` - a list of lists where each sublist contains the guards that must pass to ensure the traced graphs are valid.\n- ``graphs`` - a list of graph modules which were successfully traced.\n- ``ops_per_graph`` - a list of lists where each sublist contains the ops that are run in the graph.\n\nTo throw an error on the first graph break encountered, use the ``nopython``\nmode. This mode disables TorchDynamo’s Python fallback, and only\nsucceeds if the entire program is convertible into a single graph. Example\nusage:\n\n.. code-block:: python\n\n   def toy_example(a, b):\n      ...\n\n   compiled_toy = torch.compile(toy_example, fullgraph=True, backend=<compiler>)\n\nExcessive Recompilation\n-----------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":676,"to":700}}}}],["256",{"pageContent":".. code-block:: python\n\n   def toy_example(a, b):\n      ...\n\n   compiled_toy = torch.compile(toy_example, fullgraph=True, backend=<compiler>)\n\nExcessive Recompilation\n-----------------------\n\nWhen TorchDynamo compiles a function (or part of one), it makes certain\nassumptions about locals and globals in order to allow compiler\noptimizations, and expresses these assumptions as guards that check\nparticular values at runtime. If any of these guards fail, Dynamo will\nrecompile that function (or part) up to\n``torch._dynamo.config.cache_size_limit`` times. If your program is\nhitting the cache limit, you will first need to determine which guard is\nfailing and what part of your program is triggering it.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":700,"to":717}}}}],["257",{"pageContent":"The `compile profiler <https://github.com/pytorch/pytorch/blob/master/torch/_dynamo/utils.py>`__ automates the\nprocess of setting TorchDynamo’s cache limit to 1 and running your\nprogram under an observation-only 'compiler' that records the causes of\nany guard failures. You should be sure to run your program for at least\nas long (as many iterations) as you were running when you ran into\ntrouble, and the profiler will accumulate statistics over this duration.\n\nIf your program exhibits a bounded amount of dynamism, you may be able\nto tune the TorchDynamo cache limit to allow for each variation to be\ncompiled and cached, but if the cache limit is too high you may find the\ncost of recompilation outweighs any optimization benefits.\n\n::\n\n   torch._dynamo.config.cache_size_limit = <your desired cache limit>","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":719,"to":733}}}}],["258",{"pageContent":"::\n\n   torch._dynamo.config.cache_size_limit = <your desired cache limit>\n\nTorchdynamo plans to support many common cases of dynamic tensor shapes,\nsuch as varying batch size or sequence length. It does not plan to\nsupport rank-dynamism. In the meantime, setting a specific cache limit\ncan be used in coordination with bucketing techniques to achieve an\nacceptable number of recompilations for some dynamic models.\n\n.. code-block:: python\n\n   from torch._dynamo.utils import CompileProfiler\n\n   prof = CompileProfiler()\n\n   def my_model():\n       ...\n\n   profiler_model = torch.compile(my_model, backend=prof)\n   profiler_model()\n   print(prof.report())\n\nAccuracy Debugging\n~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":733,"to":757}}}}],["259",{"pageContent":"prof = CompileProfiler()\n\n   def my_model():\n       ...\n\n   profiler_model = torch.compile(my_model, backend=prof)\n   profiler_model()\n   print(prof.report())\n\nAccuracy Debugging\n~~~~~~~~~~~~~~~~~~\n\nAccuracy issues can also be minified if you set the environment variable\n``TORCHDYNAMO_REPRO_LEVEL=4``, it operates with a similar git bisect\nmodel and a full repro might be something like\n``TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4`` the reason\nwe need this is downstream compilers will codegen code whether it’s\nTriton code or the C++ backend, the numerics from those downstream\ncompilers can be different in subtle ways yet have dramatic impact on\nyour training stability. So the accuracy debugger is very useful for us\nto detect bugs in our codegen or with a backend compiler.\n\nIf you'd like to ensure that random number generation is the same across both torch\nand triton then you can enable ``torch._inductor.config.fallback_random = True``\n\nFile an Issue\n~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":757,"to":783}}}}],["260",{"pageContent":"If you'd like to ensure that random number generation is the same across both torch\nand triton then you can enable ``torch._inductor.config.fallback_random = True``\n\nFile an Issue\n~~~~~~~~~~~~~\n\nIf you experience problems with TorchDynamo, `file a GitHub\nissue <https://github.com/pytorch/torchdynamo/issues>`__.\n\nBefore filing an issue, read over the `README <../README.md>`__,\n`TROUBLESHOOTING <./TROUBLESHOOTING.md>`__, and search for similar\nissues.\n\nWhen filing an issue, include the information about your\nOS, Python< PyTorch, CUDA, and Triton versions info by running:\n\n.. code-block:: shell\n\n   python tools/verify_install.py\n\n-  A minimal repro script if possible, which can be generated by running\n   Minifier\n-  A description of the error\n-  The expected behavior\n-  A log (set ``torch._dynamo.config.log_file`` to a valid file name to\n   dump the logs to a file and\n   ``torch._dynamo.config.log_level = logging.DEBUG`` and\n   ``torch._dynamo.config.verbose = True``)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/compile/troubleshooting.rst","loc":{"lines":{"from":783,"to":810}}}}],["261",{"pageContent":".. _complex_numbers-doc:\n\nComplex Numbers\n===============\n\n.. note:: When using complex numbers, use Pytorch with CUDA 11.6 downloaded via pip wheel as described in\n   `Get Started <https://pytorch.org/get-started/locally/>`__ and select the CUDA 11.6 pip package.\n\nComplex numbers are numbers that can be expressed in the form :math:`a + bj`, where a and b are real numbers,\nand *j* is called the imaginary unit, which satisfies the equation :math:`j^2 = -1`. Complex numbers frequently occur in mathematics and\nengineering, especially in topics like signal processing. Traditionally many users and libraries (e.g., TorchAudio) have\nhandled complex numbers by representing the data in float tensors with shape :math:`(..., 2)` where the last\ndimension contains the real and imaginary values.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/complex_numbers.rst","loc":{"lines":{"from":1,"to":13}}}}],["262",{"pageContent":"Tensors of complex dtypes provide a more natural user experience while working with complex numbers. Operations on\ncomplex tensors (e.g., :func:`torch.mv`, :func:`torch.matmul`) are likely to be faster and more memory efficient\nthan operations on float tensors mimicking them. Operations involving complex numbers in PyTorch are optimized\nto use vectorized assembly instructions and specialized kernels (e.g. LAPACK, cuBlas).\n\n.. note::\n     Spectral operations in the `torch.fft module <https://pytorch.org/docs/stable/fft.html#torch-fft>`_ support\n     native complex tensors.\n\n.. warning ::\n     Complex tensors is a beta feature and subject to change.\n\nCreating Complex Tensors\n------------------------\n\nWe support two complex dtypes: `torch.cfloat` and `torch.cdouble`\n\n::\n\n     >>> x = torch.randn(2,2, dtype=torch.cfloat)\n     >>> x\n     tensor([[-0.4621-0.0303j, -0.2438-0.5874j],\n          [ 0.7706+0.1421j,  1.2110+0.1918j]])\n\n.. note::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/complex_numbers.rst","loc":{"lines":{"from":15,"to":39}}}}],["263",{"pageContent":"::\n\n     >>> x = torch.randn(2,2, dtype=torch.cfloat)\n     >>> x\n     tensor([[-0.4621-0.0303j, -0.2438-0.5874j],\n          [ 0.7706+0.1421j,  1.2110+0.1918j]])\n\n.. note::\n\n     The default dtype for complex tensors is determined by the default floating point dtype.\n     If the default floating point dtype is `torch.float64` then complex numbers are inferred to\n     have a dtype of `torch.complex128`, otherwise they are assumed to have a dtype of `torch.complex64`.\n\nAll factory functions apart from :func:`torch.linspace`, :func:`torch.logspace`, and :func:`torch.arange` are\nsupported for complex tensors.\n\nTransition from the old representation\n--------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/complex_numbers.rst","loc":{"lines":{"from":39,"to":56}}}}],["264",{"pageContent":"Transition from the old representation\n--------------------------------------\n\nUsers who currently worked around the lack of complex tensors with real tensors of shape :math:`(..., 2)`\ncan easily to switch using the complex tensors in their code using :func:`torch.view_as_complex`\nand :func:`torch.view_as_real`. Note that these functions don’t perform any copy and return a\nview of the input tensor.\n\n::\n\n     >>> x = torch.randn(3, 2)\n     >>> x\n     tensor([[ 0.6125, -0.1681],\n          [-0.3773,  1.3487],\n          [-0.0861, -0.7981]])\n     >>> y = torch.view_as_complex(x)\n     >>> y\n     tensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])\n     >>> torch.view_as_real(y)\n     tensor([[ 0.6125, -0.1681],\n          [-0.3773,  1.3487],\n          [-0.0861, -0.7981]])\n\nAccessing real and imag\n-----------------------\n\nThe real and imaginary values of a complex tensor can be accessed using the :attr:`real` and\n:attr:`imag`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/complex_numbers.rst","loc":{"lines":{"from":56,"to":83}}}}],["265",{"pageContent":"Accessing real and imag\n-----------------------\n\nThe real and imaginary values of a complex tensor can be accessed using the :attr:`real` and\n:attr:`imag`.\n\n.. note::\n     Accessing `real` and `imag` attributes doesn't allocate any memory, and in-place updates on the\n     `real` and `imag` tensors will update the original complex tensor. Also, the\n     returned `real` and `imag` tensors are not contiguous.\n\n::\n\n     >>> y.real\n     tensor([ 0.6125, -0.3773, -0.0861])\n     >>> y.imag\n     tensor([-0.1681,  1.3487, -0.7981])\n\n     >>> y.real.mul_(2)\n     tensor([ 1.2250, -0.7546, -0.1722])\n     >>> y\n     tensor([ 1.2250-0.1681j, -0.7546+1.3487j, -0.1722-0.7981j])\n     >>> y.real.stride()\n     (2,)\n\nAngle and abs\n-------------\n\nThe angle and absolute values of a complex tensor can be computed using :func:`torch.angle` and\n:func:`torch.abs`.\n\n::\n\n     >>> x1=torch.tensor([3j, 4+4j])\n     >>> x1.abs()\n     tensor([3.0000, 5.6569])\n     >>> x1.angle()\n     tensor([1.5708, 0.7854])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/complex_numbers.rst","loc":{"lines":{"from":83,"to":120}}}}],["266",{"pageContent":"::\n\n     >>> x1=torch.tensor([3j, 4+4j])\n     >>> x1.abs()\n     tensor([3.0000, 5.6569])\n     >>> x1.angle()\n     tensor([1.5708, 0.7854])\n\nLinear Algebra\n--------------\n\nMany linear algebra operations, like :func:`torch.matmul`, :func:`torch.svd`, :func:`torch.solve` etc., support complex numbers.\nIf you'd like to request an operation we don't currently support, please `search <https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+complex>`_\nif an issue has already been filed and if not, `file one <https://github.com/pytorch/pytorch/issues/new/choose>`_.\n\n\nSerialization\n-------------\n\nComplex tensors can be serialized, allowing data to be saved as complex values.\n\n::\n\n     >>> torch.save(y, 'complex_tensor.pt')\n     >>> torch.load('complex_tensor.pt')\n     tensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])\n\n\nAutograd\n--------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/complex_numbers.rst","loc":{"lines":{"from":120,"to":149}}}}],["267",{"pageContent":"::\n\n     >>> torch.save(y, 'complex_tensor.pt')\n     >>> torch.load('complex_tensor.pt')\n     tensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])\n\n\nAutograd\n--------\n\nPyTorch supports autograd for complex tensors. The gradient computed is the Conjugate Wirtinger derivative,\nthe negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus,\nall the existing optimizers work out of the box with complex parameters. For more details,\ncheck out the note :ref:`complex_autograd-doc`.\n\nWe do not fully support the following subsystems:\n\n* Quantization\n\n* JIT\n\n* Sparse Tensors\n\n* Distributed\n\nIf any of these would help your use case, please `search <https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+complex>`_\nif an issue has already been filed and if not, `file one <https://github.com/pytorch/pytorch/issues/new/choose>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/complex_numbers.rst","loc":{"lines":{"from":149,"to":175}}}}],["268",{"pageContent":"torch.__config__\n===================================\n\n.. automodule:: torch.__config__\n.. currentmodule:: torch.__config__\n\n.. autofunction:: show\n.. autofunction:: parallel_info","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/config_mod.rst","loc":{"lines":{"from":1,"to":8}}}}],["269",{"pageContent":"torch.utils.cpp_extension\n=========================\n\n.. currentmodule:: torch.utils.cpp_extension\n.. autofunction:: CppExtension\n.. autofunction:: CUDAExtension\n.. autofunction:: BuildExtension\n.. autofunction:: load\n.. autofunction:: load_inline\n.. autofunction:: include_paths\n.. autofunction:: get_compiler_abi_compatibility_and_version\n.. autofunction:: verify_ninja_availability\n.. autofunction:: is_ninja_available","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cpp_extension.rst","loc":{"lines":{"from":1,"to":13}}}}],["270",{"pageContent":"C++\n===================================\n.. Note::\n    If you are looking for the PyTorch C++ API docs, directly go `here <https://pytorch.org/cppdocs/>`__.\n\nPyTorch provides several features for working with C++, and it’s best to choose from them based on your needs. At a high level, the following support is available:\n\nTorchScript C++ API\n--------------------\n`TorchScript <https://pytorch.org/docs/stable/jit.html>`__ allows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in the `Loading a TorchScript Model in C++ tutorial <https://pytorch.org/tutorials/advanced/cpp_export.html>`__. This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cpp_index.rst","loc":{"lines":{"from":1,"to":10}}}}],["271",{"pageContent":"* Loading serialized TorchScript models saved from Python\n* Doing simple model modifications if needed (e.g. pulling out submodules)\n* Constructing the input and doing preprocessing using C++ Tensor API","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cpp_index.rst","loc":{"lines":{"from":12,"to":14}}}}],["272",{"pageContent":"Extending PyTorch and TorchScript with C++ Extensions\n------------------------------------------------------\nTorchScript can be augmented with user-supplied code through custom operators and custom classes.\nOnce registered with TorchScript, these operators and classes can be invoked in TorchScript code run from\nPython or from C++ as part of a serialized TorchScript model. The `Extending TorchScript with Custom C++ Operators <https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>`__ tutorial walks through interfacing TorchScript with OpenCV. In addition to wrapping a function call with a custom operator, C++ classes and structs can be bound into TorchScript through a pybind11-like interface which is explained in the `Extending TorchScript with Custom C++ Classes <https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html>`__ tutorial.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cpp_index.rst","loc":{"lines":{"from":16,"to":20}}}}],["273",{"pageContent":"Tensor and Autograd in C++\n---------------------------\nMost of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include:\n\n* ``torch::Tensor`` methods such as ``add`` / ``reshape`` / ``clone``. For the full list of methods available, please see: https://pytorch.org/cppdocs/api/classat_1_1_tensor.html\n* C++ tensor indexing API that looks and behaves the same as the Python API. For details on its usage, please see: https://pytorch.org/cppdocs/notes/tensor_indexing.html\n* The tensor autograd APIs and the ``torch::autograd`` package that are crucial for building dynamic neural networks in C++ frontend. For more details, please see: https://pytorch.org/tutorials/advanced/cpp_autograd.html","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cpp_index.rst","loc":{"lines":{"from":22,"to":28}}}}],["274",{"pageContent":"Authoring Models in C++\n------------------------\nThe \"author in TorchScript, infer in C++\" workflow requires model authoring to be done in TorchScript.\nHowever, there might be cases where the model has to be authored in C++ (e.g. in workflows where a Python\ncomponent is undesirable). To serve such use cases, we provide the full capability of authoring and training a neural net model purely in C++, with familiar components such as ``torch::nn`` / ``torch::nn::functional`` / ``torch::optim`` that closely resemble the Python API.\n\n* For an overview of the PyTorch C++ model authoring and training API, please see: https://pytorch.org/cppdocs/frontend.html\n* For a detailed tutorial on how to use the API, please see: https://pytorch.org/tutorials/advanced/cpp_frontend.html\n* Docs for components such as ``torch::nn`` / ``torch::nn::functional`` / ``torch::optim`` can be found at: https://pytorch.org/cppdocs/api/library_root.html","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cpp_index.rst","loc":{"lines":{"from":30,"to":38}}}}],["275",{"pageContent":"Packaging for C++\n------------------\nFor guidance on how to install and link with libtorch (the library that contains all of the above C++ APIs), please see: https://pytorch.org/cppdocs/installing.html. Note that on Linux there are two types of libtorch binaries provided: one compiled with GCC pre-cxx11 ABI and the other with GCC cxx11 ABI, and you should make the selection based on the GCC ABI your system is using.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cpp_index.rst","loc":{"lines":{"from":41,"to":43}}}}],["276",{"pageContent":".. currentmodule:: torch.cuda._sanitizer\n\nCUDA Stream Sanitizer\n=====================\n\n.. note::\n    This is a prototype feature, which means it is at an early stage\n    for feedback and testing, and its components are subject to change.\n\nOverview\n--------\n\n.. automodule:: torch.cuda._sanitizer\n\n\nUsage\n------\n\nHere is an example of a simple synchronization error in PyTorch:\n\n::\n\n    import torch\n\n    a = torch.rand(4, 2, device=\"cuda\")\n\n    with torch.cuda.stream(torch.cuda.Stream()):\n        torch.mul(a, 5, out=a)\n\nThe ``a`` tensor is initialized on the default stream and, without any synchronization\nmethods, modified on a new stream. The two kernels will run concurrently on the same tensor,\nwhich might cause the second kernel to read uninitialized data before the first one was able\nto write it, or the first kernel might overwrite part of the result of the second.\nWhen this script is run on the commandline with:\n::\n\n    TORCH_CUDA_SANITIZER=1 python example_error.py\n\nthe following output is printed by CSAN:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cuda._sanitizer.rst","loc":{"lines":{"from":1,"to":41}}}}],["277",{"pageContent":"TORCH_CUDA_SANITIZER=1 python example_error.py\n\nthe following output is printed by CSAN:\n\n::\n\n    ============================\n    CSAN detected a possible data race on tensor with data pointer 139719969079296\n    Access by stream 94646435460352 during kernel:\n    aten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)\n    writing to argument(s) self, out, and to the output\n    With stack trace:\n      File \"example_error.py\", line 6, in <module>\n        torch.mul(a, 5, out=a)\n      ...\n      File \"pytorch/torch/cuda/_sanitizer.py\", line 364, in _handle_kernel_launch\n        stack_trace = traceback.StackSummary.extract(","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cuda._sanitizer.rst","loc":{"lines":{"from":41,"to":57}}}}],["278",{"pageContent":"Previous access by stream 0 during kernel:\n    aten::rand(int[] size, *, int? dtype=None, Device? device=None) -> Tensor\n    writing to the output\n    With stack trace:\n      File \"example_error.py\", line 3, in <module>\n        a = torch.rand(10000, device=\"cuda\")\n      ...\n      File \"pytorch/torch/cuda/_sanitizer.py\", line 364, in _handle_kernel_launch\n        stack_trace = traceback.StackSummary.extract(\n\n    Tensor was allocated with stack trace:\n      File \"example_error.py\", line 3, in <module>\n        a = torch.rand(10000, device=\"cuda\")\n      ...\n      File \"pytorch/torch/cuda/_sanitizer.py\", line 420, in _handle_memory_allocation\n        traceback.StackSummary.extract(\n\nThis gives extensive insight into the origin of the error:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cuda._sanitizer.rst","loc":{"lines":{"from":59,"to":76}}}}],["279",{"pageContent":"This gives extensive insight into the origin of the error:\n\n- A tensor was incorrectly accessed from streams with ids: 0 (default stream) and 94646435460352 (new stream)\n- The tensor was allocated by invoking ``a = torch.rand(10000, device=\"cuda\")``\n- The faulty accesses were caused by operators\n    - ``a = torch.rand(10000, device=\"cuda\")`` on stream 0\n    - ``torch.mul(a, 5, out=a)`` on stream 94646435460352\n- The error message also displays the schemas of the invoked operators, along with a note\n  showing which arguments of the operators correspond to the affected tensor.\n\n  - In the example, it can be seen that tensor ``a`` corresponds to arguments ``self``, ``out``\n    and the ``output`` value of the invoked operator ``torch.mul``.\n\n.. seealso::\n    The list of supported torch operators and their schemas can be viewed\n    :doc:`here <torch>`.\n\nThe bug can be fixed by forcing the new stream to wait for the default stream:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cuda._sanitizer.rst","loc":{"lines":{"from":76,"to":95}}}}],["280",{"pageContent":".. seealso::\n    The list of supported torch operators and their schemas can be viewed\n    :doc:`here <torch>`.\n\nThe bug can be fixed by forcing the new stream to wait for the default stream:\n\n::\n\n    with torch.cuda.stream(torch.cuda.Stream()):\n        torch.cuda.current_stream().wait_stream(torch.cuda.default_stream())\n        torch.mul(a, 5, out=a)\n\nWhen the script is run again, there are no errors reported.\n\nAPI Reference\n-------------\n\n.. autofunction:: enable_cuda_sanitizer","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cuda._sanitizer.rst","loc":{"lines":{"from":95,"to":112}}}}],["281",{"pageContent":"torch.cuda\n===================================\n.. automodule:: torch.cuda\n.. currentmodule:: torch.cuda\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    StreamContext\n    can_device_access_peer\n    current_blas_handle\n    current_device\n    current_stream\n    default_stream\n    device\n    device_count\n    device_of\n    get_arch_list\n    get_device_capability\n    get_device_name\n    get_device_properties\n    get_gencode_flags\n    get_sync_debug_mode\n    init\n    ipc_collect\n    is_available\n    is_initialized\n    memory_usage\n    set_device\n    set_stream\n    set_sync_debug_mode\n    stream\n    synchronize\n    utilization\n    temperature\n    power_draw\n    clock_rate\n    OutOfMemoryError\n\nRandom Number Generator\n-------------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    get_rng_state\n    get_rng_state_all\n    set_rng_state\n    set_rng_state_all\n    manual_seed\n    manual_seed_all\n    seed\n    seed_all\n    initial_seed","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cuda.rst","loc":{"lines":{"from":1,"to":55}}}}],["282",{"pageContent":"get_rng_state\n    get_rng_state_all\n    set_rng_state\n    set_rng_state_all\n    manual_seed\n    manual_seed_all\n    seed\n    seed_all\n    initial_seed\n\n\nCommunication collectives\n-------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    comm.broadcast\n    comm.broadcast_coalesced\n    comm.reduce_add\n    comm.scatter\n    comm.gather\n\nStreams and events\n------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Stream\n    ExternalStream\n    Event\n\nGraphs (beta)\n-------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    is_current_stream_capturing\n    graph_pool_handle\n    CUDAGraph\n    graph\n    make_graphed_callables\n\n.. _cuda-memory-management-api:\n\nMemory management\n-----------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cuda.rst","loc":{"lines":{"from":55,"to":107}}}}],["283",{"pageContent":".. _cuda-memory-management-api:\n\nMemory management\n-----------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n     empty_cache\n     list_gpu_processes\n     mem_get_info\n     memory_stats\n     memory_summary\n     memory_snapshot\n     memory_allocated\n     max_memory_allocated\n     reset_max_memory_allocated\n     memory_reserved\n     max_memory_reserved\n     set_per_process_memory_fraction\n     memory_cached\n     max_memory_cached\n     reset_max_memory_cached\n     reset_peak_memory_stats\n     caching_allocator_alloc\n     caching_allocator_delete\n     get_allocator_backend\n     CUDAPluggableAllocator\n     change_current_allocator\n.. FIXME The following doesn't seem to exist. Is it supposed to?\n   https://github.com/pytorch/pytorch/issues/27785\n   .. autofunction:: reset_max_memory_reserved\n\nNVIDIA Tools Extension (NVTX)\n-----------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    nvtx.mark\n    nvtx.range_push\n    nvtx.range_pop","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cuda.rst","loc":{"lines":{"from":107,"to":149}}}}],["284",{"pageContent":"NVIDIA Tools Extension (NVTX)\n-----------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    nvtx.mark\n    nvtx.range_push\n    nvtx.range_pop\n\nJiterator (beta)\n-----------------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    jiterator._create_jit_fn\n    jiterator._create_multi_output_jit_fn\n\nStream Sanitizer (prototype)\n----------------------------\n\nCUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch.\nSee the :doc:`documentation <cuda._sanitizer>` for information on how to use it.\n\n.. toctree::\n    :hidden:\n\n    cuda._sanitizer","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cuda.rst","loc":{"lines":{"from":149,"to":178}}}}],["285",{"pageContent":".. note::\n\n    If the following conditions are satisfied:\n    1) cudnn is enabled,\n    2) input data is on the GPU\n    3) input data has dtype ``torch.float16``\n    4) V100 GPU is used,\n    5) input data is not in ``PackedSequence`` format\n    persistent algorithm can be selected to improve performance.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cudnn_persistent_rnn.rst","loc":{"lines":{"from":1,"to":9}}}}],["286",{"pageContent":".. warning::\n    There are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA.\n    You can enforce deterministic behavior by setting the following environment variables:\n\n    On CUDA 10.1, set environment variable ``CUDA_LAUNCH_BLOCKING=1``.\n    This may affect performance.\n\n    On CUDA 10.2 or later, set environment variable\n    (note the leading colon symbol)\n    ``CUBLAS_WORKSPACE_CONFIG=:16:8``\n    or\n    ``CUBLAS_WORKSPACE_CONFIG=:4096:2``.\n\n    See the `cuDNN 8 Release Notes`_ for more information.\n\n.. _cuDNN 8 Release Notes: https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_8.html","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/cudnn_rnn_determinism.rst","loc":{"lines":{"from":1,"to":16}}}}],["287",{"pageContent":"torch.utils.data\n===================================\n\n.. automodule:: torch.utils.data\n\nAt the heart of PyTorch data loading utility is the :class:`torch.utils.data.DataLoader`\nclass.  It represents a Python iterable over a dataset, with support for\n\n* `map-style and iterable-style datasets <Dataset Types_>`_,\n\n* `customizing data loading order <Data Loading Order and Sampler_>`_,\n\n* `automatic batching <Loading Batched and Non-Batched Data_>`_,\n\n* `single- and multi-process data loading <Single- and Multi-process Data Loading_>`_,\n\n* `automatic memory pinning <Memory Pinning_>`_.\n\nThese options are configured by the constructor arguments of a\n:class:`~torch.utils.data.DataLoader`, which has signature::\n\n    DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n               batch_sampler=None, num_workers=0, collate_fn=None,\n               pin_memory=False, drop_last=False, timeout=0,\n               worker_init_fn=None, *, prefetch_factor=2,\n               persistent_workers=False)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":1,"to":26}}}}],["288",{"pageContent":"The sections below describe in details the effects and usages of these options.\n\nDataset Types\n-------------\n\nThe most important argument of :class:`~torch.utils.data.DataLoader`\nconstructor is :attr:`dataset`, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets:\n\n* `map-style datasets <Map-style datasets_>`_,\n\n* `iterable-style datasets <Iterable-style datasets_>`_.\n\nMap-style datasets\n^^^^^^^^^^^^^^^^^^\n\nA map-style dataset is one that implements the :meth:`__getitem__` and\n:meth:`__len__` protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples.\n\nFor example, such a dataset, when accessed with ``dataset[idx]``, could read\nthe ``idx``-th image and its corresponding label from a folder on the disk.\n\nSee :class:`~torch.utils.data.Dataset` for more details.\n\nIterable-style datasets\n^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":28,"to":54}}}}],["289",{"pageContent":"See :class:`~torch.utils.data.Dataset` for more details.\n\nIterable-style datasets\n^^^^^^^^^^^^^^^^^^^^^^^\n\nAn iterable-style dataset is an instance of a subclass of :class:`~torch.utils.data.IterableDataset`\nthat implements the :meth:`__iter__` protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data.\n\nFor example, such a dataset, when called ``iter(dataset)``, could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time.\n\nSee :class:`~torch.utils.data.IterableDataset` for more details.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":54,"to":69}}}}],["290",{"pageContent":"See :class:`~torch.utils.data.IterableDataset` for more details.\n\n.. note:: When using a :class:`~torch.utils.data.IterableDataset` with\n          `multi-process data loading <Multi-process data loading_>`_. The same\n          dataset object is replicated on each worker process, and thus the\n          replicas must be configured differently to avoid duplicated data. See\n          :class:`~torch.utils.data.IterableDataset` documentations for how to\n          achieve this.\n\nData Loading Order and :class:`~torch.utils.data.Sampler`\n---------------------------------------------------------\n\nFor `iterable-style datasets <Iterable-style datasets_>`_, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":69,"to":84}}}}],["291",{"pageContent":"The rest of this section concerns the case with\n`map-style datasets <Map-style datasets_>`_. :class:`torch.utils.data.Sampler`\nclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets.  E.g., in the\ncommon case with stochastic gradient decent (SGD), a\n:class:`~torch.utils.data.Sampler` could randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD.\n\nA sequential or shuffled sampler will be automatically constructed based on the :attr:`shuffle` argument to a :class:`~torch.utils.data.DataLoader`.\nAlternatively, users may use the :attr:`sampler` argument to specify a\ncustom :class:`~torch.utils.data.Sampler` object that at each time yields\nthe next index/key to fetch.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":86,"to":98}}}}],["292",{"pageContent":"A custom :class:`~torch.utils.data.Sampler` that yields a list of batch\nindices at a time can be passed as the :attr:`batch_sampler` argument.\nAutomatic batching can also be enabled via :attr:`batch_size` and\n:attr:`drop_last` arguments. See\n`the next section <Loading Batched and Non-Batched Data_>`_ for more details\non this.\n\n.. note::\n  Neither :attr:`sampler` nor :attr:`batch_sampler` is compatible with\n  iterable-style datasets, since such datasets have no notion of a key or an\n  index.\n\nLoading Batched and Non-Batched Data\n------------------------------------\n\n:class:`~torch.utils.data.DataLoader` supports automatically collating\nindividual fetched data samples into batches via arguments\n:attr:`batch_size`, :attr:`drop_last`, :attr:`batch_sampler`, and\n:attr:`collate_fn` (which has a default function).\n\n\nAutomatic batching (default)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":100,"to":122}}}}],["293",{"pageContent":"Automatic batching (default)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThis is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first).\n\nWhen :attr:`batch_size` (default ``1``) is not ``None``, the data loader yields\nbatched samples instead of individual samples. :attr:`batch_size` and\n:attr:`drop_last` arguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecify :attr:`batch_sampler`, which yields a list of keys at a time.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":122,"to":133}}}}],["294",{"pageContent":".. note::\n  The :attr:`batch_size` and :attr:`drop_last` arguments essentially are used\n  to construct a :attr:`batch_sampler` from :attr:`sampler`. For map-style\n  datasets, the :attr:`sampler` is either provided by user or constructed\n  based on the :attr:`shuffle` argument. For iterable-style datasets, the\n  :attr:`sampler` is a dummy infinite one. See\n  `this section <Data Loading Order and Sampler_>`_ on more details on\n  samplers.\n\n.. note::\n  When fetching from\n  `iterable-style datasets <Iterable-style datasets_>`_ with\n  `multi-processing <Multi-process data loading_>`_, the :attr:`drop_last`\n  argument drops the last non-full batch of each worker's dataset replica.\n\nAfter fetching a list of samples using the indices from sampler, the function\npassed as the :attr:`collate_fn` argument is used to collate lists of samples\ninto batches.\n\nIn this case, loading from a map-style dataset is roughly equivalent with::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":135,"to":154}}}}],["295",{"pageContent":"In this case, loading from a map-style dataset is roughly equivalent with::\n\n    for indices in batch_sampler:\n        yield collate_fn([dataset[i] for i in indices])\n\nand loading from an iterable-style dataset is roughly equivalent with::\n\n    dataset_iter = iter(dataset)\n    for indices in batch_sampler:\n        yield collate_fn([next(dataset_iter) for _ in indices])\n\nA custom :attr:`collate_fn` can be used to customize collation, e.g., padding\nsequential data to max length of a batch. See\n`this section <dataloader-collate_fn_>`_ on more about :attr:`collate_fn`.\n\nDisable automatic batching\n^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":154,"to":170}}}}],["296",{"pageContent":"Disable automatic batching\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples.  Under these scenarios, it's likely\nbetter to not use automatic batching (where :attr:`collate_fn` is used to\ncollate the samples), but let the data loader directly return each member of\nthe :attr:`dataset` object.\n\nWhen both :attr:`batch_size` and :attr:`batch_sampler` are ``None`` (default\nvalue for :attr:`batch_sampler` is already ``None``), automatic batching is\ndisabled. Each sample obtained from the :attr:`dataset` is processed with the\nfunction passed as the :attr:`collate_fn` argument.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":170,"to":185}}}}],["297",{"pageContent":"**When automatic batching is disabled**, the default :attr:`collate_fn` simply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.\n\nIn this case, loading from a map-style dataset is roughly equivalent with::\n\n    for index in sampler:\n        yield collate_fn(dataset[index])\n\nand loading from an iterable-style dataset is roughly equivalent with::\n\n    for data in iter(dataset):\n        yield collate_fn(data)\n\nSee `this section <dataloader-collate_fn_>`_ on more about :attr:`collate_fn`.\n\n.. _dataloader-collate_fn:\n\nWorking with :attr:`collate_fn`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe use of :attr:`collate_fn` is slightly different when automatic batching is\nenabled or disabled.\n\n**When automatic batching is disabled**, :attr:`collate_fn` is called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the default :attr:`collate_fn` simply converts NumPy\narrays in PyTorch tensors.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":187,"to":213}}}}],["298",{"pageContent":"**When automatic batching is enabled**, :attr:`collate_fn` is called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes the behavior of the default :attr:`collate_fn`\n(:func:`~torch.utils.data.default_collate`).\n\nFor instance, if each data sample consists of a 3-channel image and an integral\nclass label, i.e., each element of the dataset returns a tuple\n``(image, class_index)``, the default :attr:`collate_fn` collates a list of\nsuch tuples into a single tuple of a batched image tensor and a batched class\nlabel Tensor. In particular, the default :attr:`collate_fn` has the following\nproperties:\n\n* It always prepends a new dimension as the batch dimension.\n\n* It automatically converts NumPy arrays and Python numerical values into\n  PyTorch Tensors.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":215,"to":231}}}}],["299",{"pageContent":"* It always prepends a new dimension as the batch dimension.\n\n* It automatically converts NumPy arrays and Python numerical values into\n  PyTorch Tensors.\n\n* It preserves the data structure, e.g., if each sample is a dictionary, it\n  outputs a dictionary with the same set of keys but batched Tensors as values\n  (or lists if the values can not be converted into Tensors). Same\n  for ``list`` s, ``tuple`` s, ``namedtuple`` s, etc.\n\nUsers may use customized :attr:`collate_fn` to achieve custom batching, e.g.,\ncollating along a dimension other than the first, padding sequences of\nvarious lengths, or adding support for custom data types.\n\nIf you run into a situation where the outputs of :class:`~torch.utils.data.DataLoader`\nhave dimensions or type that is different from your expectation, you may\nwant to check your :attr:`collate_fn`.\n\nSingle- and Multi-process Data Loading\n--------------------------------------\n\nA :class:`~torch.utils.data.DataLoader` uses single-process data loading by\ndefault.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":231,"to":253}}}}],["300",{"pageContent":"Single- and Multi-process Data Loading\n--------------------------------------\n\nA :class:`~torch.utils.data.DataLoader` uses single-process data loading by\ndefault.\n\nWithin a Python process, the\n`Global Interpreter Lock (GIL) <https://wiki.python.org/moin/GlobalInterpreterLock>`_\nprevents true fully parallelizing Python code across threads. To avoid blocking\ncomputation code with data loading, PyTorch provides an easy switch to perform\nmulti-process data loading by simply setting the argument :attr:`num_workers`\nto a positive integer.\n\nSingle-process data loading (default)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":253,"to":267}}}}],["301",{"pageContent":"Single-process data loading (default)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn this mode, data fetching is done in the same process a\n:class:`~torch.utils.data.DataLoader` is initialized.  Therefore, data loading\nmay block computing.  However, this mode may be preferred when resource(s) used\nfor sharing data among processes (e.g., shared memory, file descriptors) is\nlimited, or when the entire dataset is small and can be loaded entirely in\nmemory.  Additionally, single-process loading often shows more readable error\ntraces and thus is useful for debugging.\n\n\nMulti-process data loading\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSetting the argument :attr:`num_workers` as a positive integer will\nturn on multi-process data loading with the specified number of loader worker\nprocesses.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":267,"to":284}}}}],["302",{"pageContent":"Setting the argument :attr:`num_workers` as a positive integer will\nturn on multi-process data loading with the specified number of loader worker\nprocesses.\n\n.. warning::\n   After several iterations, the loader worker processes will consume\n   the same amount of CPU memory as the parent process for all Python\n   objects in the parent process which are accessed from the worker\n   processes.  This can be problematic if the Dataset contains a lot of\n   data (e.g., you are loading a very large list of filenames at Dataset\n   construction time) and/or you are using a lot of workers (overall\n   memory usage is ``number of workers * size of parent process``).  The\n   simplest workaround is to replace Python objects with non-refcounted\n   representations such as Pandas, Numpy or PyArrow objects.  Check out\n   `issue #13246\n   <https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662>`_\n   for more details on why this occurs and example code for how to\n   workaround these problems.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":284,"to":301}}}}],["303",{"pageContent":"In this mode, each time an iterator of a :class:`~torch.utils.data.DataLoader`\nis created (e.g., when you call ``enumerate(dataloader)``), :attr:`num_workers`\nworker processes are created. At this point, the :attr:`dataset`,\n:attr:`collate_fn`, and :attr:`worker_init_fn` are passed to each\nworker, where they are used to initialize, and fetch data. This means that\ndataset access together with its  internal IO, transforms\n(including :attr:`collate_fn`) runs in the worker process.\n\n:func:`torch.utils.data.get_worker_info()` returns various useful information\nin a worker process (including the worker id, dataset replica, initial seed,\netc.), and returns ``None`` in main process. Users may use this function in\ndataset code and/or :attr:`worker_init_fn` to individually configure each\ndataset replica, and to determine whether the code is running in a worker\nprocess. For example, this can be particularly helpful in sharding the dataset.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":303,"to":316}}}}],["304",{"pageContent":"For map-style datasets, the main process generates the indices using\n:attr:`sampler` and sends them to the workers. So any shuffle randomization is\ndone in the main process which guides loading by assigning indices to load.\n\nFor iterable-style datasets, since each worker process gets a replica of the\n:attr:`dataset` object, naive multi-process loading will often result in\nduplicated data. Using :func:`torch.utils.data.get_worker_info()` and/or\n:attr:`worker_init_fn`, users may configure each replica independently. (See\n:class:`~torch.utils.data.IterableDataset` documentations for how to achieve\nthis. ) For similar reasons, in multi-process loading, the :attr:`drop_last`\nargument drops the last non-full batch of each worker's iterable-style dataset\nreplica.\n\nWorkers are shut down once the end of the iteration is reached, or when the\niterator becomes garbage collected.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":318,"to":332}}}}],["305",{"pageContent":"Workers are shut down once the end of the iteration is reached, or when the\niterator becomes garbage collected.\n\n.. warning::\n  It is generally not recommended to return CUDA tensors in multi-process\n  loading because of many subtleties in using CUDA and sharing CUDA tensors in\n  multiprocessing (see :ref:`multiprocessing-cuda-note`). Instead, we recommend\n  using `automatic memory pinning <Memory Pinning_>`_ (i.e., setting\n  :attr:`pin_memory=True`), which enables fast data transfer to CUDA-enabled\n  GPUs.\n\nPlatform-specific behaviors\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nSince workers rely on Python :py:mod:`multiprocessing`, worker launch behavior is\ndifferent on Windows compared to Unix.\n\n* On Unix, :func:`fork()` is the default :py:mod:`multiprocessing` start method.\n  Using :func:`fork`, child workers typically can access the :attr:`dataset` and\n  Python argument functions directly through the cloned address space.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":332,"to":351}}}}],["306",{"pageContent":"* On Windows or MacOS, :func:`spawn()` is the default :py:mod:`multiprocessing` start method.\n  Using :func:`spawn()`, another interpreter is launched which runs your main script,\n  followed by the internal worker function that receives the :attr:`dataset`,\n  :attr:`collate_fn` and other arguments through :py:mod:`pickle` serialization.\n\nThis separate serialization means that you should take two steps to ensure you\nare compatible with Windows while using multi-process data loading:\n\n- Wrap most of you main script's code within ``if __name__ == '__main__':`` block,\n  to make sure it doesn't run again (most likely generating error) when each worker\n  process is launched. You can place your dataset and :class:`~torch.utils.data.DataLoader`\n  instance creation logic here, as it doesn't need to be re-executed in workers.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":353,"to":364}}}}],["307",{"pageContent":"- Make sure that any custom :attr:`collate_fn`, :attr:`worker_init_fn`\n  or :attr:`dataset` code is declared as top level definitions, outside of the\n  ``__main__`` check. This ensures that they are available in worker processes.\n  (this is needed since functions are pickled as references only, not ``bytecode``.)\n\n.. _data-loading-randomness:\n\nRandomness in multi-process data loading\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nBy default, each worker will have its PyTorch seed set to ``base_seed + worker_id``,\nwhere ``base_seed`` is a long generated by main process using its RNG (thereby,\nconsuming a RNG state mandatorily) or a specified :attr:`generator`. However, seeds for other\nlibraries may be duplicated upon initializing workers, causing each worker to return\nidentical random numbers. (See :ref:`this section <dataloader-workers-random-seed>` in FAQ.).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":366,"to":380}}}}],["308",{"pageContent":"In :attr:`worker_init_fn`, you may access the PyTorch seed set for each worker\nwith either :func:`torch.utils.data.get_worker_info().seed <torch.utils.data.get_worker_info>`\nor :func:`torch.initial_seed()`, and use it to seed other libraries before data\nloading.\n\nMemory Pinning\n--------------\n\nHost to GPU copies are much faster when they originate from pinned (page-locked)\nmemory. See :ref:`cuda-memory-pinning` for more details on when and how to use\npinned memory generally.\n\nFor data loading, passing :attr:`pin_memory=True` to a\n:class:`~torch.utils.data.DataLoader` will automatically put the fetched data\nTensors in pinned memory, and thus enables faster data transfer to CUDA-enabled\nGPUs.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":382,"to":397}}}}],["309",{"pageContent":"The default memory pinning logic only recognizes Tensors and maps and iterables\ncontaining Tensors.  By default, if the pinning logic sees a batch that is a\ncustom type (which will occur if you have a :attr:`collate_fn` that returns a\ncustom batch type), or if each element of your batch is a custom type, the\npinning logic will not recognize them, and it will return that batch (or those\nelements) without pinning the memory.  To enable memory pinning for custom\nbatch or data type(s), define a :meth:`pin_memory` method on your custom\ntype(s).\n\nSee the example below.\n\nExample::\n\n    class SimpleCustomBatch:\n        def __init__(self, data):\n            transposed_data = list(zip(*data))\n            self.inp = torch.stack(transposed_data[0], 0)\n            self.tgt = torch.stack(transposed_data[1], 0)\n\n        # custom memory pinning method on custom type\n        def pin_memory(self):\n            self.inp = self.inp.pin_memory()\n            self.tgt = self.tgt.pin_memory()\n            return self","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":399,"to":422}}}}],["310",{"pageContent":"# custom memory pinning method on custom type\n        def pin_memory(self):\n            self.inp = self.inp.pin_memory()\n            self.tgt = self.tgt.pin_memory()\n            return self\n\n    def collate_wrapper(batch):\n        return SimpleCustomBatch(batch)\n\n    inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n    tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n    dataset = TensorDataset(inps, tgts)\n\n    loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n                        pin_memory=True)\n\n    for batch_ndx, sample in enumerate(loader):\n        print(sample.inp.is_pinned())\n        print(sample.tgt.is_pinned())","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":422,"to":440}}}}],["311",{"pageContent":"for batch_ndx, sample in enumerate(loader):\n        print(sample.inp.is_pinned())\n        print(sample.tgt.is_pinned())\n\n\n.. autoclass:: DataLoader\n.. autoclass:: Dataset\n.. autoclass:: IterableDataset\n.. autoclass:: TensorDataset\n.. autoclass:: ConcatDataset\n.. autoclass:: ChainDataset\n.. autoclass:: Subset\n.. autofunction:: torch.utils.data._utils.collate.collate\n.. autofunction:: torch.utils.data.default_collate\n.. autofunction:: torch.utils.data.default_convert\n.. autofunction:: torch.utils.data.get_worker_info\n.. autofunction:: torch.utils.data.random_split\n.. autoclass:: torch.utils.data.Sampler\n.. autoclass:: torch.utils.data.SequentialSampler\n.. autoclass:: torch.utils.data.RandomSampler\n.. autoclass:: torch.utils.data.SubsetRandomSampler\n.. autoclass:: torch.utils.data.WeightedRandomSampler\n.. autoclass:: torch.utils.data.BatchSampler\n.. autoclass:: torch.utils.data.distributed.DistributedSampler","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":440,"to":463}}}}],["312",{"pageContent":".. These modules are documented as part of torch/data listing them here for\n.. now until we have a clearer fix\n.. py:module:: torch.utils.data.datapipes\n.. py:module:: torch.utils.data.datapipes.dataframe\n.. py:module:: torch.utils.data.datapipes.iter\n.. py:module:: torch.utils.data.datapipes.map\n.. py:module:: torch.utils.data.datapipes.utils","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/data.rst","loc":{"lines":{"from":466,"to":472}}}}],["313",{"pageContent":"DDP Communication Hooks\n=======================\n\nDDP communication hook is a generic interface to control how to communicate\ngradients across workers by overriding the vanilla allreduce in\n`DistributedDataParallel <https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.>`_.\nA few built-in communication hooks are provided,\nand users can easily apply any of these hooks to optimize communication.\nBesides, the hook interface can also support user-defined communication\nstrategies for more advanced use cases.\n\nHow to Use a Communication Hook?\n--------------------------------\n\nTo use a communication hook, the user just needs to let the DDP model register\nthe hook before the training loop as below.\n\n:func:`torch.nn.parallel.DistributedDataParallel.register_comm_hook`\n\nWhat Does a Communication Hook Operate On?\n------------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ddp_comm_hooks.rst","loc":{"lines":{"from":1,"to":21}}}}],["314",{"pageContent":":func:`torch.nn.parallel.DistributedDataParallel.register_comm_hook`\n\nWhat Does a Communication Hook Operate On?\n------------------------------------------\n\nA communication hook provides a flexible way to allreduce gradients.\nTherefore, it mainly operates on the gradients on each replica before allreduce,\nwhich are bucketized to increase the overlap between communication and computation.\nParticularly, :class:`torch.distributed.GradBucket` represents a bucket of gradient tensors to be allreduced.\n\n.. autoclass:: torch.distributed.GradBucket\n\n.. autofunction:: torch.distributed.GradBucket.index\n.. autofunction:: torch.distributed.GradBucket.buffer\n.. autofunction:: torch.distributed.GradBucket.gradients\n.. autofunction:: torch.distributed.GradBucket.is_last\n.. autofunction:: torch.distributed.GradBucket.set_buffer\n.. autofunction:: torch.distributed.GradBucket.parameters\n\nDefault Communication Hooks\n---------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ddp_comm_hooks.rst","loc":{"lines":{"from":21,"to":41}}}}],["315",{"pageContent":"Default Communication Hooks\n---------------------------\n\nDefault communication hooks are simple **stateless** hooks, so the input state\nin ``register_comm_hook`` is either a process group or ``None``.\nThe input ``bucket`` is a :class:`torch.distributed.GradBucket` object.\n\n.. currentmodule:: torch.distributed.algorithms.ddp_comm_hooks.default_hooks\n.. autofunction:: allreduce_hook\n.. autofunction:: fp16_compress_hook\n.. autofunction:: bf16_compress_hook\n\nAdditionally, a communication hook wrapper is provided to support :meth:`~fp16_compress_hook` or :meth:`~bf16_compress_hook` as a wrapper,\nwhich can be combined with other communication hooks.\n\n.. autofunction:: fp16_compress_wrapper\n.. autofunction:: bf16_compress_wrapper\n\nPowerSGD Communication Hook\n---------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ddp_comm_hooks.rst","loc":{"lines":{"from":41,"to":60}}}}],["316",{"pageContent":".. autofunction:: fp16_compress_wrapper\n.. autofunction:: bf16_compress_wrapper\n\nPowerSGD Communication Hook\n---------------------------\n\nPowerSGD (`Vogels et al., NeurIPS 2019 <https://arxiv.org/abs/1905.13727>`_)\nis a gradient compression algorithm, which can provide very high compression\nrates and accelerate bandwidth-bound distributed training.\nThis algorithm needs to maintain both some hyperparameters and the internal\nstate. Therefore, PowerSGD communication hook is a **stateful** hook,\nand the user needs to provide a state object defined as below.\n\nPowerSGD State\n^^^^^^^^^^^^^^^^\n\n.. currentmodule:: torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook\n.. autoclass:: PowerSGDState\n\nPowerSGD Hooks\n^^^^^^^^^^^^^^^^\n\n.. warning ::\n    PowerSGD typically requires extra memory of the same size as the model's\n    gradients to enable error feedback, which can compensate for biased\n    compressed communication and improve accuracy.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ddp_comm_hooks.rst","loc":{"lines":{"from":60,"to":85}}}}],["317",{"pageContent":".. warning ::\n    PowerSGD hooks may conflict with `Apex automatic mixed precision package <https://github.com/NVIDIA/apex>`_.\n    Please use PyTorch `native automatic mixed precision package <https://pytorch.org/docs/stable/amp.html>`_\n    instead.\n\n.. autofunction:: powerSGD_hook\n.. autofunction:: batched_powerSGD_hook\n\nDebugging Communication Hooks\n-----------------------------\n\nAs the name implies, debugging communication hooks are **only** used for debugging and performance optimization purpose.\n\n.. currentmodule:: torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks\n\n.. warning ::\n    Debugging communication hooks do not necessarily output the correct results.\n\n.. autofunction:: noop_hook\n\nCheckpointing of Communication Hooks\n------------------------------------\n\n.. currentmodule:: torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ddp_comm_hooks.rst","loc":{"lines":{"from":87,"to":110}}}}],["318",{"pageContent":".. autofunction:: noop_hook\n\nCheckpointing of Communication Hooks\n------------------------------------\n\n.. currentmodule:: torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook\n\nA stateful communication hook can be saved as a part of model checkpointing to enable trainer restarts.\nTo make a hook serializable, ``__setstate__`` and ``__getstate__`` should be defined.\n\n.. warning ::\n    ``__getstate__`` should exclude non-serializable attributes from a returned dictionary.\n\n.. warning ::\n    ``__setstate__`` should properly initialize non-serializable attributes, excluded from a provided ``state``.\n\n:class:`PowerSGDState` has ``__setstate__`` and ``__getstate__`` implemented and can be used as a reference.\n\n.. class:: PowerSGDState\n    :noindex:\n\n    .. automethod:: PowerSGDState.__getstate__\n    .. automethod:: PowerSGDState.__setstate__\n\nHere is a simple, end-to-end example of saving and reloading PowerSGD state and hook.\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ddp_comm_hooks.rst","loc":{"lines":{"from":110,"to":136}}}}],["319",{"pageContent":".. automethod:: PowerSGDState.__getstate__\n    .. automethod:: PowerSGDState.__setstate__\n\nHere is a simple, end-to-end example of saving and reloading PowerSGD state and hook.\n\n::\n\n    import os\n    import sys\n    import tempfile\n    import torch\n    import torch.distributed as dist\n    import torch.nn as nn\n    import torch.optim as optim\n\n    from torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook as powerSGD\n\n    class SimpleModel(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.fc1 = nn.Linear(24,24)\n            self.relu = nn.ReLU()\n            self.fc2 = nn.Linear(24,12)\n\n        def forward(self, x):\n            return self.fc2(self.relu(self.fc1(x)))\n\n    def setup(rank, world_size):\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = '12355'\n\n        # initialize the process group\n        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ddp_comm_hooks.rst","loc":{"lines":{"from":136,"to":168}}}}],["320",{"pageContent":"# initialize the process group\n        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n    def cleanup():\n        dist.destroy_process_group()\n\n    def run_demo(demo_fn, world_size):\n        mp.spawn(\n            demo_fn,\n            args=(world_size,),\n            nprocs=world_size,\n            join=True)\n\n    def demo_serialization(rank, world_size):\n        setup(rank, world_size)\n\n        CHECKPOINT = tempfile.gettempdir() + \"/checkpoint.pt\"\n\n        model = SimpleModel().to(rank)\n        ddp_model = DistributedDataParallel(model, device_ids=[rank])\n\n        powersgd_hook = powerSGD.powerSGD_hook\n        powersgd_state = powerSGD.PowerSGDState(process_group=None)\n\n        optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n        ddp_model.register_comm_hook(powersgd_state, powersgd_hook)\n\n        state = {\n            'state_dict': ddp_model.state_dict(),\n            'comm_hook': hook,\n            'comm_hook_state': hook_state}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ddp_comm_hooks.rst","loc":{"lines":{"from":168,"to":198}}}}],["321",{"pageContent":"state = {\n            'state_dict': ddp_model.state_dict(),\n            'comm_hook': hook,\n            'comm_hook_state': hook_state}\n\n        if rank == 0:\n            torch.save(state, CHECKPOINT)\n\n        dist.barrier()\n        map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n        checkpoint = torch.load(CHECKPOINT, map_location=map_location)\n\n        ddp_model.load_state_dict(checkpoint['state_dict'])\n        powersgd_hook = checkpoint['comm_hook']\n        powersgd_state = checkpoint['comm_hook_state']\n\n        ddp_model.register_comm_hook(powersgd_state, powersgd_hook)\n\n        if rank == 0:\n            os.remove(CHECKPOINT)\n\n        cleanup()\n\n    if __name__ == \"__main__\":\n        n_gpus = torch.cuda.device_count()\n        assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n        world_size = n_gpus\n        run_demo(demo_serialization, world_size)\n\nAcknowledgements\n----------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ddp_comm_hooks.rst","loc":{"lines":{"from":198,"to":228}}}}],["322",{"pageContent":"Acknowledgements\n----------------\n\nMany thanks to PowerSGD paper author **Thijs Vogels** for the code review on\nPowerSGD communication hook, as well as the\n`comparison experiments <https://observablehq.com/@tvogels/powersgd-benchmark>`_,\nwhich show that the performance of PowerSGD communication hook is on par with\nthe implementation in the original `paper <https://arxiv.org/abs/1905.13727>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ddp_comm_hooks.rst","loc":{"lines":{"from":228,"to":235}}}}],["323",{"pageContent":"torch::deploy has been moved to pytorch/multipy\n===============================================\n\n``torch::deploy`` has been moved to its new home at `https://github.com/pytorch/multipy <https://github.com/pytorch/multipy>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/deploy.rst","loc":{"lines":{"from":1,"to":4}}}}],["324",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\nGeneric Join Context Manager\n============================\nThe generic join context manager facilitates distributed training on uneven\ninputs. This page outlines the API of the relevant classes: :class:`Join`,\n:class:`Joinable`, and :class:`JoinHook`. For a tutorial, see\n`Distributed Training with Uneven Inputs Using the Join Context Manager`_.\n\n.. autoclass:: torch.distributed.algorithms.Join\n    :members:\n\n.. autoclass:: torch.distributed.algorithms.Joinable\n    :members:\n\n.. autoclass:: torch.distributed.algorithms.JoinHook\n    :members:\n\n.. _Distributed Training with Uneven Inputs Using the Join Context Manager: https://pytorch.org/tutorials/advanced/generic_join.html","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.algorithms.join.rst","loc":{"lines":{"from":1,"to":20}}}}],["325",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\nDistributed Checkpoint - torch.distributed.checkpoint\n=====================================================\n\n\nDistributed Checkpoint (DCP) support loading and saving models from multiple ranks in parallel.\nIt handles load-time resharding which enables saving in one cluster topology and loading into another.\n\nDCP is different than `torch.save` and `torch.load` in a few significant ways:\n\n* It produces multiple files per checkpoint, with at least one per rank.\n* It operates in place, meaning that the model should allocate its data first and DCP uses that storage instead.\n\nThe entrypoints to load and save a checkpoint are the following:\n\n\n.. automodule:: torch.distributed.checkpoint\n\n.. currentmodule:: torch.distributed.checkpoint\n\n.. autofunction::  load_state_dict\n.. autofunction::  save_state_dict","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.checkpoint.rst","loc":{"lines":{"from":1,"to":24}}}}],["326",{"pageContent":".. automodule:: torch.distributed.checkpoint\n\n.. currentmodule:: torch.distributed.checkpoint\n\n.. autofunction::  load_state_dict\n.. autofunction::  save_state_dict\n\nThis `example <https://github.com/pytorch/pytorch/blob/master/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py>`_ shows how to use Pytorch Distributed Checkpoint to save a FSDP model.\n\n\nThe following types define the IO interface used during checkpoint:\n\n.. autoclass:: torch.distributed.checkpoint.StorageReader\n  :members:\n\n.. autoclass:: torch.distributed.checkpoint.StorageWriter\n  :members:\n\nThe following types define the planner interface used during checkpoint:\n\n.. autoclass:: torch.distributed.checkpoint.LoadPlanner\n  :members:\n\n.. autoclass:: torch.distributed.checkpoint.LoadPlan\n  :members:\n\n.. autoclass:: torch.distributed.checkpoint.ReadItem\n  :members:\n\n.. autoclass:: torch.distributed.checkpoint.SavePlanner\n  :members:\n\n.. autoclass:: torch.distributed.checkpoint.SavePlan\n  :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.checkpoint.rst","loc":{"lines":{"from":24,"to":57}}}}],["327",{"pageContent":".. autoclass:: torch.distributed.checkpoint.ReadItem\n  :members:\n\n.. autoclass:: torch.distributed.checkpoint.SavePlanner\n  :members:\n\n.. autoclass:: torch.distributed.checkpoint.SavePlan\n  :members:\n\n.. autoclass:: torch.distributed.checkpoint.WriteItem\n  :members:\n\nWe provide a filesystem based storage layer:\n\n.. autoclass:: torch.distributed.checkpoint.FileSystemReader\n  :members:\n\n.. autoclass:: torch.distributed.checkpoint.FileSystemWriter\n  :members:\n\nWe provide default implementations of `LoadPlanner` and `SavePlanner` that\ncan handle all of torch.distributed constructs such as FSDP, DDP, ShardedTensor and DistributedTensor.\n\n.. autoclass:: torch.distributed.checkpoint.DefaultSavePlanner\n  :members:\n\n.. autoclass:: torch.distributed.checkpoint.DefaultLoadPlanner\n  :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.checkpoint.rst","loc":{"lines":{"from":57,"to":84}}}}],["328",{"pageContent":"Torch Distributed Elastic\n============================\n\nMakes distributed PyTorch fault-tolerant and elastic.\n\nGet Started\n---------------\n.. toctree::\n   :maxdepth: 1\n   :caption: Usage\n\n   elastic/quickstart\n   elastic/train_script\n   elastic/examples\n\nDocumentation\n---------------\n\n.. toctree::\n   :maxdepth: 1\n   :caption: API\n\n   elastic/run\n   elastic/agent\n   elastic/multiprocessing\n   elastic/errors\n   elastic/rendezvous\n   elastic/timer\n   elastic/metrics\n   elastic/events\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Advanced\n\n   elastic/customization\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Plugins\n\n   elastic/kubernetes","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.elastic.rst","loc":{"lines":{"from":1,"to":42}}}}],["329",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\nDistributed Optimizers\n======================\n\n.. warning ::\n    Distributed optimizer is not currently supported when using CUDA tensors\n\n.. automodule:: torch.distributed.optim\n    :members: DistributedOptimizer, PostLocalSGDOptimizer, ZeroRedundancyOptimizer","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.optim.rst","loc":{"lines":{"from":1,"to":11}}}}],["330",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\nDistributed communication package - torch.distributed\n=====================================================\n\n.. note ::\n    Please refer to `PyTorch Distributed Overview <https://pytorch.org/tutorials/beginner/dist_overview.html>`__\n    for a brief introduction to all features related to distributed training.\n\n.. automodule:: torch.distributed\n.. currentmodule:: torch.distributed\n\nBackends\n--------\n\n``torch.distributed`` supports three built-in backends, each with\ndifferent capabilities. The table below shows which functions are available\nfor use with CPU / CUDA tensors.\nMPI supports CUDA only if the implementation used to build PyTorch supports it.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":1,"to":20}}}}],["331",{"pageContent":"+----------------+-----------+-----------+-----------+\n| Backend        | ``gloo``  | ``mpi``   | ``nccl``  |\n+----------------+-----+-----+-----+-----+-----+-----+\n| Device         | CPU | GPU | CPU | GPU | CPU | GPU |\n+================+=====+=====+=====+=====+=====+=====+\n| send           | ✓   | ✘   | ✓   | ?   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+\n| recv           | ✓   | ✘   | ✓   | ?   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+\n| broadcast      | ✓   | ✓   | ✓   | ?   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+\n| all_reduce     | ✓   | ✓   | ✓   | ?   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+\n| reduce         | ✓   | ✘   | ✓   | ?   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+\n| all_gather     | ✓   | ✘   | ✓   | ?   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+\n| gather         | ✓   | ✘   | ✓   | ?   | ✘   | ✓   |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":23,"to":40}}}}],["332",{"pageContent":"| all_gather     | ✓   | ✘   | ✓   | ?   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+\n| gather         | ✓   | ✘   | ✓   | ?   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+\n| scatter        | ✓   | ✘   | ✓   | ?   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+\n| reduce_scatter | ✘   | ✘   | ✘   | ✘   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+\n| all_to_all     | ✘   | ✘   | ✓   | ?   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+\n| barrier        | ✓   | ✘   | ✓   | ?   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":40,"to":51}}}}],["333",{"pageContent":"Backends that come with PyTorch\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype).\nBy default for Linux, the Gloo and NCCL backends are built and included in PyTorch\ndistributed (NCCL only when building with CUDA). MPI is an optional backend that can only be\nincluded if you build PyTorch from source. (e.g. building PyTorch on a host that has MPI\ninstalled.)\n\n.. note ::\n    As of PyTorch v1.8, Windows supports all collective communications backend but NCCL,\n    If  the `init_method` argument of :func:`init_process_group` points to a file it must adhere\n    to the following schema:\n\n    - Local file system, ``init_method=\"file:///d:/tmp/some_file\"``\n    - Shared file system, ``init_method=\"file://////{machine_name}/{share_folder_name}/some_file\"``\n\n    Same as on Linux platform, you can enable TcpStore by setting environment variables,\n    MASTER_ADDR and MASTER_PORT.\n\nWhich backend to use?\n^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":54,"to":75}}}}],["334",{"pageContent":"Same as on Linux platform, you can enable TcpStore by setting environment variables,\n    MASTER_ADDR and MASTER_PORT.\n\nWhich backend to use?\n^^^^^^^^^^^^^^^^^^^^^\n\nIn the past, we were often asked: \"which backend should I use?\".\n\n- Rule of thumb\n\n  - Use the NCCL backend for distributed **GPU** training\n  - Use the Gloo backend for distributed **CPU** training.\n\n- GPU hosts with InfiniBand interconnect\n\n  - Use NCCL, since it's the only backend that currently supports\n    InfiniBand and GPUDirect.\n\n- GPU hosts with Ethernet interconnect\n\n  - Use NCCL, since it currently provides the best distributed GPU\n    training performance, especially for multiprocess single-node or\n    multi-node distributed training. If you encounter any problem with\n    NCCL, use Gloo as the fallback option. (Note that Gloo currently\n    runs slower than NCCL for GPUs.)\n\n- CPU hosts with InfiniBand interconnect","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":75,"to":101}}}}],["335",{"pageContent":"- CPU hosts with InfiniBand interconnect\n\n  - If your InfiniBand has enabled IP over IB, use Gloo, otherwise,\n    use MPI instead. We are planning on adding InfiniBand support for\n    Gloo in the upcoming releases.\n\n- CPU hosts with Ethernet interconnect\n\n  - Use Gloo, unless you have specific reasons to use MPI.\n\nCommon environment variables\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nChoosing the network interface to use\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nBy default, both the NCCL and Gloo backends will try to find the right network interface to use.\nIf the automatically detected interface is not correct, you can override it using the following\nenvironment variables (applicable to the respective backend):\n\n* **NCCL_SOCKET_IFNAME**, for example ``export NCCL_SOCKET_IFNAME=eth0``\n* **GLOO_SOCKET_IFNAME**, for example ``export GLOO_SOCKET_IFNAME=eth0``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":101,"to":122}}}}],["336",{"pageContent":"* **NCCL_SOCKET_IFNAME**, for example ``export NCCL_SOCKET_IFNAME=eth0``\n* **GLOO_SOCKET_IFNAME**, for example ``export GLOO_SOCKET_IFNAME=eth0``\n\nIf you're using the Gloo backend, you can specify multiple interfaces by separating\nthem by a comma, like this: ``export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3``.\nThe backend will dispatch operations in a round-robin fashion across these interfaces.\nIt is imperative that all processes specify the same number of interfaces in this variable.\n\nOther NCCL environment variables\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n**Debugging** - in case of NCCL failure, you can set ``NCCL_DEBUG=INFO`` to print an explicit\nwarning message as well as basic NCCL initialization information.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":122,"to":134}}}}],["337",{"pageContent":"**Debugging** - in case of NCCL failure, you can set ``NCCL_DEBUG=INFO`` to print an explicit\nwarning message as well as basic NCCL initialization information.\n\nYou may also use ``NCCL_DEBUG_SUBSYS`` to get more details about a specific\naspect of NCCL. For example, ``NCCL_DEBUG_SUBSYS=COLL`` would print logs of\ncollective calls, which may be helpful when debugging hangs, especially those\ncaused by collective type or message size mismatch. In case of topology\ndetection failure, it would be helpful to set ``NCCL_DEBUG_SUBSYS=GRAPH``\nto inspect the detailed detection result and save as reference if further help\nfrom NCCL team is needed.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":134,"to":143}}}}],["338",{"pageContent":"**Performance tuning** - NCCL performs automatic tuning based on its topology detection to save users'\ntuning effort. On some socket-based systems, users may still try tuning\n``NCCL_SOCKET_NTHREADS`` and ``NCCL_NSOCKS_PERTHREAD`` to increase socket\nnetwork bandwidth. These two environment variables have been pre-tuned by NCCL\nfor some cloud providers, such as AWS or GCP.\n\nFor a full list of NCCL environment variables, please refer to\n`NVIDIA NCCL's official documentation <https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html>`_\n\n\n.. _distributed-basics:\n\nBasics\n------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":145,"to":158}}}}],["339",{"pageContent":".. _distributed-basics:\n\nBasics\n------\n\nThe `torch.distributed` package provides PyTorch support and communication primitives\nfor multiprocess parallelism across several computation nodes running on one or more\nmachines. The class :func:`torch.nn.parallel.DistributedDataParallel` builds on this\nfunctionality to provide synchronous distributed training as a wrapper around any\nPyTorch model. This differs from the kinds of parallelism provided by\n:doc:`multiprocessing` and :func:`torch.nn.DataParallel` in that it supports\nmultiple network-connected machines and in that the user must explicitly launch a separate\ncopy of the main training script for each process.\n\nIn the single-machine synchronous case, `torch.distributed` or the\n:func:`torch.nn.parallel.DistributedDataParallel` wrapper may still have advantages over other\napproaches to data-parallelism, including :func:`torch.nn.DataParallel`:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":158,"to":174}}}}],["340",{"pageContent":"* Each process maintains its own optimizer and performs a complete optimization step with each\n  iteration. While this may appear redundant, since the gradients have already been gathered\n  together and averaged across processes and are thus the same for every process, this means\n  that no parameter broadcast step is needed, reducing time spent transferring tensors between\n  nodes.\n* Each process contains an independent Python interpreter, eliminating the extra interpreter\n  overhead and \"GIL-thrashing\" that comes from driving several execution threads, model\n  replicas, or GPUs from a single Python process. This is especially important for models that\n  make heavy use of the Python runtime, including models with recurrent layers or many small\n  components.\n\nInitialization\n--------------\n\nThe package needs to be initialized using the :func:`torch.distributed.init_process_group`\nfunction before calling any other methods. This blocks until all processes have\njoined.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":176,"to":192}}}}],["341",{"pageContent":"The package needs to be initialized using the :func:`torch.distributed.init_process_group`\nfunction before calling any other methods. This blocks until all processes have\njoined.\n\n.. autofunction:: is_available\n\n.. autofunction:: init_process_group\n\n.. autofunction:: is_initialized\n\n.. autofunction:: is_mpi_available\n\n.. autofunction:: is_nccl_available\n\n.. autofunction:: is_gloo_available\n\n.. autofunction:: is_torchelastic_launched\n\n--------------------------------------------------------------------------------\n\nCurrently three initialization methods are supported:\n\nTCP initialization\n^^^^^^^^^^^^^^^^^^\n\nThere are two ways to initialize using TCP, both requiring a network address\nreachable from all processes and a desired ``world_size``. The first way\nrequires specifying an address that belongs to the rank 0 process. This\ninitialization method requires that all processes have manually specified ranks.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":192,"to":220}}}}],["342",{"pageContent":"Note that multicast address is not supported anymore in the latest distributed\npackage. ``group_name`` is deprecated as well.\n\n::\n\n    import torch.distributed as dist\n\n    # Use address of one of the machines\n    dist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                            rank=args.rank, world_size=4)\n\nShared file-system initialization\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAnother initialization method makes use of a file system that is shared and\nvisible from all machines in a group, along with a desired ``world_size``. The URL should start\nwith ``file://`` and contain a path to a non-existent file (in an existing\ndirectory) on a shared file system. File-system initialization will automatically\ncreate that file if it doesn't exist, but will not delete the file. Therefore, it\nis your responsibility to make sure that the file is cleaned up before the next\n:func:`init_process_group` call on the same file path/name.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":222,"to":242}}}}],["343",{"pageContent":"Note that automatic rank assignment is not supported anymore in the latest\ndistributed package and ``group_name`` is deprecated as well.\n\n.. warning::\n    This method assumes that the file system supports locking using ``fcntl`` - most\n    local systems and NFS support it.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":244,"to":249}}}}],["344",{"pageContent":".. warning::\n    This method will always create the file and try its best to clean up and remove\n    the file at the end of the program. In other words, each initialization with\n    the file init method will need a brand new empty file in order for the initialization\n    to succeed. If the same file used by the previous initialization (which happens not\n    to get cleaned up) is used again, this is unexpected behavior and can often cause\n    deadlocks and failures. Therefore, even though this method will try its best to clean up\n    the file, if the auto-delete happens to be unsuccessful, it is your responsibility\n    to ensure that the file is removed at the end of the training to prevent the same\n    file to be reused again during the next time. This is especially important\n    if you plan to call :func:`init_process_group` multiple times on the same file name.\n    In other words, if the file is not removed/cleaned up and you call","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":251,"to":262}}}}],["345",{"pageContent":"if you plan to call :func:`init_process_group` multiple times on the same file name.\n    In other words, if the file is not removed/cleaned up and you call\n    :func:`init_process_group` again on that file, failures are expected.\n    The rule of thumb here is that, make sure that the file is non-existent or\n    empty every time :func:`init_process_group` is called.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":262,"to":266}}}}],["346",{"pageContent":"::\n\n    import torch.distributed as dist\n\n    # rank should always be specified\n    dist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                            world_size=4, rank=args.rank)\n\nEnvironment variable initialization\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThis method will read the configuration from environment variables, allowing\none to fully customize how the information is obtained. The variables to be set\nare:\n\n* ``MASTER_PORT`` - required; has to be a free port on machine with rank 0\n* ``MASTER_ADDR`` - required (except for rank 0); address of rank 0 node\n* ``WORLD_SIZE`` - required; can be set either here, or in a call to init function\n* ``RANK`` - required; can be set either here, or in a call to init function\n\nThe machine with rank 0 will be used to set up all connections.\n\nThis is the default method, meaning that ``init_method`` does not have to be specified (or\ncan be ``env://``).\n\nPost-Initialization\n-------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":268,"to":294}}}}],["347",{"pageContent":"This is the default method, meaning that ``init_method`` does not have to be specified (or\ncan be ``env://``).\n\nPost-Initialization\n-------------------\n\nOnce :func:`torch.distributed.init_process_group` was run, the following functions can be used. To\ncheck whether the process group has already been initialized use :func:`torch.distributed.is_initialized`.\n\n.. autoclass:: Backend\n    :members:\n\n.. autofunction:: get_backend\n\n.. autofunction:: get_rank\n\n.. autofunction:: get_world_size\n\n--------------------------------------------------------------------------------\n\nDistributed Key-Value Store\n---------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":294,"to":315}}}}],["348",{"pageContent":".. autofunction:: get_rank\n\n.. autofunction:: get_world_size\n\n--------------------------------------------------------------------------------\n\nDistributed Key-Value Store\n---------------------------\n\nThe distributed package comes with a distributed key-value store, which can be\nused to share information between processes in the group as well as to\ninitialize the distributed package in\n:func:`torch.distributed.init_process_group` (by explicitly creating the store\nas an alternative to specifying ``init_method``.) There are 3 choices for\nKey-Value Stores: :class:`~torch.distributed.TCPStore`,\n:class:`~torch.distributed.FileStore`, and :class:`~torch.distributed.HashStore`.\n\n.. autoclass:: Store\n.. autoclass:: TCPStore\n.. autoclass:: HashStore\n.. autoclass:: FileStore\n.. autoclass:: PrefixStore","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":315,"to":336}}}}],["349",{"pageContent":".. autoclass:: Store\n.. autoclass:: TCPStore\n.. autoclass:: HashStore\n.. autoclass:: FileStore\n.. autoclass:: PrefixStore\n\n.. autofunction:: torch.distributed.Store.set\n.. autofunction:: torch.distributed.Store.get\n.. autofunction:: torch.distributed.Store.add\n.. autofunction:: torch.distributed.Store.compare_set\n.. autofunction:: torch.distributed.Store.wait\n.. autofunction:: torch.distributed.Store.num_keys\n.. autofunction:: torch.distributed.Store.delete_key\n.. autofunction:: torch.distributed.Store.set_timeout\n\nGroups\n------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":336,"to":352}}}}],["350",{"pageContent":"Groups\n------\n\nBy default collectives operate on the default group (also called the world) and\nrequire all processes to enter the distributed function call. However, some workloads can benefit\nfrom more fine-grained communication. This is where distributed groups come\ninto play. :func:`~torch.distributed.new_group` function can be\nused to create new groups, with arbitrary subsets of all processes. It returns\nan opaque group handle that can be given as a ``group`` argument to all collectives\n(collectives are distributed functions to exchange information in certain well-known programming patterns).\n\n\n.. autofunction:: new_group\n\n.. autofunction:: get_group_rank\n\n.. autofunction:: get_global_rank\n\n.. autofunction:: get_process_group_ranks\n\nPoint-to-point communication\n----------------------------\n\n.. autofunction:: send\n\n.. autofunction:: recv","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":352,"to":377}}}}],["351",{"pageContent":".. autofunction:: get_global_rank\n\n.. autofunction:: get_process_group_ranks\n\nPoint-to-point communication\n----------------------------\n\n.. autofunction:: send\n\n.. autofunction:: recv\n\n:func:`~torch.distributed.isend` and :func:`~torch.distributed.irecv`\nreturn distributed request objects when used. In general, the type of this object is unspecified\nas they should never be created manually, but they are guaranteed to support two methods:\n\n* ``is_completed()`` - returns True if the operation has finished\n* ``wait()`` - will block the process until the operation is finished.\n  ``is_completed()`` is guaranteed to return True once it returns.\n\n.. autofunction:: isend\n\n.. autofunction:: irecv\n\n.. autofunction:: batch_isend_irecv\n\n.. autoclass:: P2POp\n\nSynchronous and asynchronous collective operations\n--------------------------------------------------\nEvery collective operation function supports the following two kinds of operations,\ndepending on the setting of the ``async_op`` flag passed into the collective:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":377,"to":407}}}}],["352",{"pageContent":"**Synchronous operation** - the default mode, when ``async_op`` is set to ``False``.\nWhen the function returns, it is guaranteed that\nthe collective operation is performed. In the case of CUDA operations, it is not guaranteed\nthat the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any\nfurther function calls utilizing the output of the collective call will behave as expected. For CUDA collectives,\nfunction calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of\nsynchronization under the scenario of running under different streams. For details on CUDA semantics such as stream\nsynchronization, see `CUDA Semantics <https://pytorch.org/docs/stable/notes/cuda.html>`__.\nSee the below script to see examples of differences in these semantics for CPU and CUDA operations.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":409,"to":417}}}}],["353",{"pageContent":"**Asynchronous operation** - when ``async_op`` is set to True. The collective operation function\nreturns a distributed request object. In general, you don't need to create it manually and it\nis guaranteed to support two methods:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":419,"to":421}}}}],["354",{"pageContent":"* ``is_completed()`` - in the case of CPU collectives, returns ``True`` if completed. In the case of CUDA operations,\n  returns ``True`` if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the\n  default stream without further synchronization.\n* ``wait()`` - in the case of CPU collectives, will block the process until the operation is completed. In the case\n  of CUDA collectives, will block until the operation has been successfully enqueued onto a CUDA stream and the\n  output can be utilized on the default stream without further synchronization.\n* ``get_future()`` - returns ``torch._C.Future`` object. Supported for NCCL, also supported for most operations on GLOO\n  and MPI, except for peer to peer operations.\n  Note: as we continue adopting Futures and merging APIs, ``get_future()`` call might become redundant.\n\n**Example**","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":423,"to":433}}}}],["355",{"pageContent":"**Example**\n\nThe following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\nIt shows the explicit need to synchronize when using collective outputs on different CUDA streams:\n\n::\n\n    # Code runs on each rank.\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    output = torch.tensor([rank]).cuda(rank)\n    s = torch.cuda.Stream()\n    handle = dist.all_reduce(output, async_op=True)\n    # Wait ensures the operation is enqueued, but not necessarily complete.\n    handle.wait()\n    # Using result on non-default stream.\n    with torch.cuda.stream(s):\n        s.wait_stream(torch.cuda.default_stream())\n        output.add_(100)\n    if rank == 0:\n        # if the explicit call to wait_stream was omitted, the output below will be\n        # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n        # the value after the add completed.\n        print(output)\n\n\nCollective functions\n--------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":433,"to":459}}}}],["356",{"pageContent":"Collective functions\n--------------------\n\n.. autofunction:: broadcast\n\n.. autofunction:: broadcast_object_list\n\n.. autofunction:: all_reduce\n\n.. autofunction:: reduce\n\n.. autofunction:: all_gather\n\n.. autofunction:: all_gather_into_tensor\n\n.. autofunction:: all_gather_object\n\n.. autofunction:: gather\n\n.. autofunction:: gather_object\n\n.. autofunction:: scatter\n\n.. autofunction:: scatter_object_list\n\n.. autofunction:: reduce_scatter\n\n.. autofunction:: reduce_scatter_tensor\n\n.. autofunction:: all_to_all_single\n\n.. autofunction:: all_to_all\n\n.. autofunction:: barrier\n\n.. autofunction:: monitored_barrier\n\n.. autoclass:: ReduceOp\n\n.. class:: reduce_op\n\n    Deprecated enum-like class for reduction operations: ``SUM``, ``PRODUCT``,\n    ``MIN``, and ``MAX``.\n\n    :class:`~torch.distributed.ReduceOp` is recommended to use instead.\n\nProfiling Collective Communication\n-----------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":459,"to":506}}}}],["357",{"pageContent":":class:`~torch.distributed.ReduceOp` is recommended to use instead.\n\nProfiling Collective Communication\n-----------------------------------------\n\nNote that you can use ``torch.profiler`` (recommended, only available after 1.8.1)  or ``torch.autograd.profiler`` to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (``gloo``,\n``nccl``, ``mpi``) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:\n\n::\n\n    import torch\n    import torch.distributed as dist\n    with torch.profiler():\n        tensor = torch.randn(20, 10)\n        dist.all_reduce(tensor)\n\nPlease refer to the `profiler documentation <https://pytorch.org/docs/master/profiler.html>`__ for a full overview of profiler features.\n\n\nMulti-GPU collective functions\n------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":506,"to":526}}}}],["358",{"pageContent":"Please refer to the `profiler documentation <https://pytorch.org/docs/master/profiler.html>`__ for a full overview of profiler features.\n\n\nMulti-GPU collective functions\n------------------------------\n\n.. warning::\n    The multi-GPU functions will be deprecated. If you must use them, please revisit our documentation later.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":526,"to":533}}}}],["359",{"pageContent":".. warning::\n    The multi-GPU functions will be deprecated. If you must use them, please revisit our documentation later.\n\nIf you have more than one GPU on each node, when using the NCCL and Gloo backend,\n:func:`~torch.distributed.broadcast_multigpu`\n:func:`~torch.distributed.all_reduce_multigpu`\n:func:`~torch.distributed.reduce_multigpu`\n:func:`~torch.distributed.all_gather_multigpu` and\n:func:`~torch.distributed.reduce_scatter_multigpu` support distributed collective\noperations among multiple GPUs within each node. These functions can potentially\nimprove the overall distributed training performance and be easily used by\npassing a list of tensors. Each Tensor in the passed tensor list needs\nto be on a separate GPU device of the host where the function is called. Note\nthat the length of the tensor list needs to be identical among all the\ndistributed processes. Also note that currently the multi-GPU collective\nfunctions are only supported by the NCCL backend.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":533,"to":548}}}}],["360",{"pageContent":"For example, if the system we use for distributed training has 2 nodes, each\nof which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\nlike to all-reduce. The following code can serve as a reference:\n\nCode running on Node 0\n\n::\n\n    import torch\n    import torch.distributed as dist\n\n    dist.init_process_group(backend=\"nccl\",\n                            init_method=\"file:///distributed_test\",\n                            world_size=2,\n                            rank=0)\n    tensor_list = []\n    for dev_idx in range(torch.cuda.device_count()):\n        tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\n    dist.all_reduce_multigpu(tensor_list)\n\nCode running on Node 1\n\n::\n\n    import torch\n    import torch.distributed as dist","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":550,"to":576}}}}],["361",{"pageContent":"dist.all_reduce_multigpu(tensor_list)\n\nCode running on Node 1\n\n::\n\n    import torch\n    import torch.distributed as dist\n\n    dist.init_process_group(backend=\"nccl\",\n                            init_method=\"file:///distributed_test\",\n                            world_size=2,\n                            rank=1)\n    tensor_list = []\n    for dev_idx in range(torch.cuda.device_count()):\n        tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\n    dist.all_reduce_multigpu(tensor_list)\n\nAfter the call, all 16 tensors on the two nodes will have the all-reduced value\nof 16\n\n.. autofunction:: broadcast_multigpu\n\n.. autofunction:: all_reduce_multigpu\n\n.. autofunction:: reduce_multigpu\n\n.. autofunction:: all_gather_multigpu\n\n.. autofunction:: reduce_scatter_multigpu\n\n\n.. _distributed-launch:\n\nThird-party backends\n--------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":576,"to":612}}}}],["362",{"pageContent":".. autofunction:: reduce_multigpu\n\n.. autofunction:: all_gather_multigpu\n\n.. autofunction:: reduce_scatter_multigpu\n\n\n.. _distributed-launch:\n\nThird-party backends\n--------------------\n\nBesides the builtin GLOO/MPI/NCCL backends, PyTorch distributed supports\nthird-party backends through a run-time register mechanism.\nFor references on how to develop a third-party backend through C++ Extension,\nplease refer to `Tutorials - Custom C++ and CUDA Extensions <https://pytorch.org/\ntutorials/advanced/cpp_extension.html>`_ and\n``test/cpp_extensions/cpp_c10d_extension.cpp``. The capability of third-party\nbackends are decided by their own implementations.\n\nThe new backend derives from ``c10d::ProcessGroup`` and registers the backend\nname and the instantiating interface through :func:`torch.distributed.Backend.register_backend`\nwhen imported.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":612,"to":634}}}}],["363",{"pageContent":"The new backend derives from ``c10d::ProcessGroup`` and registers the backend\nname and the instantiating interface through :func:`torch.distributed.Backend.register_backend`\nwhen imported.\n\nWhen manually importing this backend and invoking :func:`torch.distributed.init_process_group`\nwith the corresponding backend name, the ``torch.distributed`` package runs on\nthe new backend.\n\n.. warning::\n    The support of third-party backend is experimental and subject to change.\n\nLaunch utility\n--------------\n\nThe `torch.distributed` package also provides a launch utility in\n`torch.distributed.launch`. This helper utility can be used to launch\nmultiple processes per node for distributed training.\n\n\n.. automodule:: torch.distributed.launch\n\n\nSpawn utility\n-------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":634,"to":657}}}}],["364",{"pageContent":".. automodule:: torch.distributed.launch\n\n\nSpawn utility\n-------------\n\nThe :ref:`multiprocessing-doc` package also provides a ``spawn``\nfunction in :func:`torch.multiprocessing.spawn`. This helper function\ncan be used to spawn multiple processes. It works by passing in the\nfunction that you want to run and spawns N processes to run it. This\ncan be used for multiprocess distributed training as well.\n\nFor references on how to use it, please refer to `PyTorch example - ImageNet\nimplementation <https://github.com/pytorch/examples/tree/master/imagenet>`_\n\nNote that this function requires Python 3.4 or higher.\n\nDebugging ``torch.distributed`` applications\n------------------------------------------------------\n\nDebugging distributed applications can be challenging due to hard to understand hangs, crashes, or inconsistent behavior across ranks. ``torch.distributed`` provides\na suite of tools to help debug training applications in a self-serve fashion:\n\nMonitored Barrier\n^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":657,"to":681}}}}],["365",{"pageContent":"Monitored Barrier\n^^^^^^^^^^^^^^^^^\n\nAs of v1.10, :func:`torch.distributed.monitored_barrier` exists as an alternative to :func:`torch.distributed.barrier` which fails with helpful information about which rank may be faulty\nwhen crashing, i.e. not all ranks calling into :func:`torch.distributed.monitored_barrier` within the provided timeout. :func:`torch.distributed.monitored_barrier` implements a host-side\nbarrier using ``send``/``recv`` communication primitives in a process similar to acknowledgements, allowing rank 0 to report which rank(s) failed to acknowledge\nthe barrier in time. As an example, consider the following function where rank 1 fails to call into :func:`torch.distributed.monitored_barrier` (in practice this could be due\nto an application bug or hang in a previous collective):\n\n::\n\n    import os\n    from datetime import timedelta\n\n    import torch\n    import torch.distributed as dist\n    import torch.multiprocessing as mp","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":681,"to":697}}}}],["366",{"pageContent":"::\n\n    import os\n    from datetime import timedelta\n\n    import torch\n    import torch.distributed as dist\n    import torch.multiprocessing as mp\n\n\n    def worker(rank):\n        dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n        # monitored barrier requires gloo process group to perform host-side sync.\n        group_gloo = dist.new_group(backend=\"gloo\")\n        if rank not in [1]:\n            dist.monitored_barrier(group=group_gloo, timeout=timedelta(seconds=2))\n\n\n    if __name__ == \"__main__\":\n        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n        os.environ[\"MASTER_PORT\"] = \"29501\"\n        mp.spawn(worker, nprocs=2, args=())\n\nThe following error message is produced on rank 0, allowing the user to determine which rank(s) may be faulty and investigate further:\n\n::\n\n  RuntimeError: Rank 1 failed to pass monitoredBarrier in 2000 ms\n   Original exception:\n  [gloo/transport/tcp/pair.cc:598] Connection closed by peer [2401:db00:eef0:1100:3560:0:1c05:25d]:8594","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":697,"to":726}}}}],["367",{"pageContent":"::\n\n  RuntimeError: Rank 1 failed to pass monitoredBarrier in 2000 ms\n   Original exception:\n  [gloo/transport/tcp/pair.cc:598] Connection closed by peer [2401:db00:eef0:1100:3560:0:1c05:25d]:8594\n\n\n``TORCH_DISTRIBUTED_DEBUG``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWith ``TORCH_CPP_LOG_LEVEL=INFO``, the environment variable ``TORCH_DISTRIBUTED_DEBUG``  can be used to trigger additional useful logging and collective synchronization checks to ensure all ranks\nare synchronized appropriately. ``TORCH_DISTRIBUTED_DEBUG`` can be set to either ``OFF`` (default), ``INFO``, or ``DETAIL`` depending on the debugging level\nrequired. Please note that the most verbose option, ``DETAIL`` may impact the application performance and thus should only be used when debugging issues.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":726,"to":738}}}}],["368",{"pageContent":"Setting ``TORCH_DISTRIBUTED_DEBUG=INFO`` will result in additional debug logging when models trained with :func:`torch.nn.parallel.DistributedDataParallel` are initialized, and\n``TORCH_DISTRIBUTED_DEBUG=DETAIL`` will additionally log runtime performance statistics a select number of iterations. These runtime statistics\ninclude data such as forward time, backward time, gradient communication time, etc. As an example, given the following application:\n\n::\n\n    import os\n\n    import torch\n    import torch.distributed as dist\n    import torch.multiprocessing as mp\n\n\n    class TwoLinLayerNet(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.a = torch.nn.Linear(10, 10, bias=False)\n            self.b = torch.nn.Linear(10, 1, bias=False)\n\n        def forward(self, x):\n            a = self.a(x)\n            b = self.b(x)\n            return (a, b)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":740,"to":762}}}}],["369",{"pageContent":"def forward(self, x):\n            a = self.a(x)\n            b = self.b(x)\n            return (a, b)\n\n\n    def worker(rank):\n        dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n        torch.cuda.set_device(rank)\n        print(\"init model\")\n        model = TwoLinLayerNet().cuda()\n        print(\"init ddp\")\n        ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n\n        inp = torch.randn(10, 10).cuda()\n        print(\"train\")\n\n        for _ in range(20):\n            output = ddp_model(inp)\n            loss = output[0] + output[1]\n            loss.sum().backward()\n\n\n    if __name__ == \"__main__\":\n        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n        os.environ[\"MASTER_PORT\"] = \"29501\"\n        os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n        os.environ[\n            \"TORCH_DISTRIBUTED_DEBUG\"\n        ] = \"DETAIL\"  # set to DETAIL for runtime logging.\n        mp.spawn(worker, nprocs=2, args=())","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":762,"to":792}}}}],["370",{"pageContent":"The following logs are rendered at initialization time:\n\n::\n\n  I0607 16:10:35.739390 515217 logger.cpp:173] [Rank 0]: DDP Initialized with:\n  broadcast_buffers: 1\n  bucket_cap_bytes: 26214400\n  find_unused_parameters: 0\n  gradient_as_bucket_view: 0\n  is_multi_device_module: 0\n  iteration: 0\n  num_parameter_tensors: 2\n  output_device: 0\n  rank: 0\n  total_parameter_size_bytes: 440\n  world_size: 2\n  backend_name: nccl\n  bucket_sizes: 440\n  cuda_visible_devices: N/A\n  device_ids: 0\n  dtypes: float\n  master_addr: localhost\n  master_port: 29501\n  module_name: TwoLinLayerNet\n  nccl_async_error_handling: N/A\n  nccl_blocking_wait: N/A\n  nccl_debug: WARN\n  nccl_ib_timeout: N/A\n  nccl_nthreads: N/A\n  nccl_socket_ifname: N/A\n  torch_distributed_debug: INFO\n\n\nThe following logs are rendered during runtime (when ``TORCH_DISTRIBUTED_DEBUG=DETAIL`` is set):\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":794,"to":829}}}}],["371",{"pageContent":"The following logs are rendered during runtime (when ``TORCH_DISTRIBUTED_DEBUG=DETAIL`` is set):\n\n::\n\n  I0607 16:18:58.085681 544067 logger.cpp:344] [Rank 1 / 2] Training TwoLinLayerNet unused_parameter_size=0\n   Avg forward compute time: 40838608\n   Avg backward compute time: 5983335\n  Avg backward comm. time: 4326421\n   Avg backward comm/comp overlap time: 4207652\n  I0607 16:18:58.085693 544066 logger.cpp:344] [Rank 0 / 2] Training TwoLinLayerNet unused_parameter_size=0\n   Avg forward compute time: 42850427\n   Avg backward compute time: 3885553\n  Avg backward comm. time: 2357981\n   Avg backward comm/comp overlap time: 2234674","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":829,"to":842}}}}],["372",{"pageContent":"In addition, ``TORCH_DISTRIBUTED_DEBUG=INFO`` enhances crash logging in :func:`torch.nn.parallel.DistributedDataParallel` due to unused parameters in the model. Currently, ``find_unused_parameters=True``\nmust be passed into :func:`torch.nn.parallel.DistributedDataParallel` initialization if there are parameters that may be unused in the forward pass, and as of v1.10, all model outputs are required\nto be used in loss computation as :func:`torch.nn.parallel.DistributedDataParallel` does not support unused parameters in the backwards pass. These constraints are challenging especially for larger\nmodels, thus when crashing with an error, :func:`torch.nn.parallel.DistributedDataParallel` will log the fully qualified name of all parameters that went unused. For example, in the above application,\nif we modify ``loss`` to be instead computed as ``loss = output[1]``, then ``TwoLinLayerNet.a`` does not receive a gradient in the backwards pass, and","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":845,"to":849}}}}],["373",{"pageContent":"if we modify ``loss`` to be instead computed as ``loss = output[1]``, then ``TwoLinLayerNet.a`` does not receive a gradient in the backwards pass, and\nthus results in ``DDP`` failing. On a crash, the user is passed information about parameters which went unused, which may be challenging to manually find for large models:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":849,"to":850}}}}],["374",{"pageContent":"::\n\n  RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing\n   the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\n  making sure all `forward` function outputs participate in calculating loss.\n  If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return va\n  lue of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n  Parameters which did not receive grad for rank 0: a.weight\n  Parameter indices which did not receive grad for rank 0: 0","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":853,"to":861}}}}],["375",{"pageContent":"Setting ``TORCH_DISTRIBUTED_DEBUG=DETAIL`` will trigger additional consistency and synchronization checks on every collective call issued by the user\neither directly or indirectly (such as DDP ``allreduce``). This is done by creating a wrapper process group that wraps all process groups returned by\n:func:`torch.distributed.init_process_group` and :func:`torch.distributed.new_group` APIs. As a result, these APIs will return a wrapper process group that can be used exactly like a regular process\ngroup, but performs consistency checks before dispatching the collective to an underlying process group. Currently, these checks include a :func:`torch.distributed.monitored_barrier`,\nwhich ensures all ranks complete their outstanding collective calls and reports ranks which are stuck. Next, the collective itself is checked for consistency by\nensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":864,"to":869}}}}],["376",{"pageContent":"ensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the\napplication crashes, rather than a hang or uninformative error message. As an example, consider the following function which has mismatched input shapes into\n:func:`torch.distributed.all_reduce`:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":869,"to":871}}}}],["377",{"pageContent":"::\n\n    import torch\n    import torch.distributed as dist\n    import torch.multiprocessing as mp\n\n\n    def worker(rank):\n        dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n        torch.cuda.set_device(rank)\n        tensor = torch.randn(10 if rank == 0 else 20).cuda()\n        dist.all_reduce(tensor)\n        torch.cuda.synchronize(device=rank)\n\n\n    if __name__ == \"__main__\":\n        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n        os.environ[\"MASTER_PORT\"] = \"29501\"\n        os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n        os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n        mp.spawn(worker, nprocs=2, args=())\n\nWith the ``NCCL`` backend, such an application would likely result in a hang which can be challenging to root-cause in nontrivial scenarios. If the user enables\n``TORCH_DISTRIBUTED_DEBUG=DETAIL`` and reruns the application, the following error message reveals the root cause:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":873,"to":898}}}}],["378",{"pageContent":"::\n\n    work = default_pg.allreduce([tensor], opts)\n    RuntimeError: Error when verifying shape tensors for collective ALLREDUCE on rank 0. This likely indicates that input shapes into the collective are mismatched across ranks. Got shapes:  10\n    [ torch.LongTensor{1} ]\n\n.. note::\n    For fine-grained control of the debug level during runtime the functions :func:`torch.distributed.set_debug_level`, :func:`torch.distributed.set_debug_level_from_env`, and\n    :func:`torch.distributed.get_debug_level` can also be used.\n\nIn addition, `TORCH_DISTRIBUTED_DEBUG=DETAIL` can be used in conjunction with `TORCH_SHOW_CPP_STACKTRACES=1` to log the entire callstack when a collective desynchronization is detected. These\ncollective desynchronization checks will work for all applications that use ``c10d`` collective calls backed by process groups created with the\n:func:`torch.distributed.init_process_group` and :func:`torch.distributed.new_group` APIs.\n\nLogging\n-------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":898,"to":913}}}}],["379",{"pageContent":"Logging\n-------\n\nIn addition to explicit debugging support via :func:`torch.distributed.monitored_barrier` and ``TORCH_DISTRIBUTED_DEBUG``, the underlying C++ library of ``torch.distributed`` also outputs log\nmessages at various levels. These messages can be helpful to understand the execution state of a distributed training job and to troubleshoot problems such as network connection failures. The\nfollowing matrix shows how the log level can be adjusted via the combination of ``TORCH_CPP_LOG_LEVEL`` and ``TORCH_DISTRIBUTED_DEBUG`` environment variables.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":913,"to":918}}}}],["380",{"pageContent":"+-------------------------+-----------------------------+------------------------+\n| ``TORCH_CPP_LOG_LEVEL`` | ``TORCH_DISTRIBUTED_DEBUG`` |   Effective Log Level  |\n+=========================+=============================+========================+\n| ``ERROR``               | ignored                     | Error                  |\n+-------------------------+-----------------------------+------------------------+\n| ``WARNING``             | ignored                     | Warning                |\n+-------------------------+-----------------------------+------------------------+\n| ``INFO``                | ignored                     | Info                   |\n+-------------------------+-----------------------------+------------------------+\n| ``INFO``                | ``INFO``                    | Debug                  |\n+-------------------------+-----------------------------+------------------------+\n| ``INFO``                | ``DETAIL``                  | Trace (a.k.a. All)     |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":920,"to":931}}}}],["381",{"pageContent":"+-------------------------+-----------------------------+------------------------+\n| ``INFO``                | ``DETAIL``                  | Trace (a.k.a. All)     |\n+-------------------------+-----------------------------+------------------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":931,"to":933}}}}],["382",{"pageContent":"Distributed has a custom Exception type derived from `RuntimeError` called `torch.distributed.DistBackendError`. This exception is thrown when a backend-specific error occurs. For example, if\nthe `NCCL` backend is used and the user attempts to use a GPU that is not available to the `NCCL` library.\n\n.. autoclass:: torch.distributed.DistBackendError\n\n.. warning::\n    The DistBackendError exception type is an experimental feature is subject to change.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":935,"to":941}}}}],["383",{"pageContent":".. autoclass:: torch.distributed.DistBackendError\n\n.. warning::\n    The DistBackendError exception type is an experimental feature is subject to change.\n\n.. Distributed modules that are missing specific entries.\n.. Adding them here for tracking purposes until they are more permanently fixed.\n.. py:module:: torch.distributed.algorithms\n.. py:module:: torch.distributed.algorithms.ddp_comm_hooks\n.. py:module:: torch.distributed.algorithms.model_averaging\n.. py:module:: torch.distributed.elastic\n.. py:module:: torch.distributed.elastic.utils\n.. py:module:: torch.distributed.elastic.utils.data\n.. py:module:: torch.distributed.launcher\n.. py:module:: torch.distributed.nn\n.. py:module:: torch.distributed.nn.api\n.. py:module:: torch.distributed.nn.jit\n.. py:module:: torch.distributed.nn.jit.templates\n.. py:module:: torch.distributed.pipeline\n.. py:module:: torch.distributed.pipeline.sync\n.. py:module:: torch.distributed.pipeline.sync.skip\n.. py:module:: torch.distributed.tensor","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.rst","loc":{"lines":{"from":941,"to":962}}}}],["384",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\nTensor Parallelism - torch.distributed.tensor.parallel\n======================================================\n\nTensor Parallelism(TP) is built on top of the PyTorch DistributedTensor\n(`DTensor <https://github.com/pytorch/pytorch/blob/master/torch/distributed/_tensor/README.md>`__)\nand provides several parallelism styles: Rowwise, Colwise and Pairwise Parallelism.\n\n.. warning ::\n    Tensor Parallelism APIs are experimental and subject to change.\n\nThe entrypoint to parallelize your ``nn.Module`` using Tensor Parallelism is:\n\n.. automodule:: torch.distributed.tensor.parallel\n\n.. currentmodule:: torch.distributed.tensor.parallel\n\n.. autofunction::  parallelize_module\n\nTensor Parallelism supports the following parallel styles:\n\n.. autoclass:: torch.distributed.tensor.parallel.style.RowwiseParallel\n  :members:\n\n.. autoclass:: torch.distributed.tensor.parallel.style.ColwiseParallel\n  :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.tensor.parallel.rst","loc":{"lines":{"from":1,"to":28}}}}],["385",{"pageContent":".. autoclass:: torch.distributed.tensor.parallel.style.RowwiseParallel\n  :members:\n\n.. autoclass:: torch.distributed.tensor.parallel.style.ColwiseParallel\n  :members:\n\n.. autoclass:: torch.distributed.tensor.parallel.style.PairwiseParallel\n  :members:\n\n.. warning ::\n    Sequence Parallelism are still in experimental and no evaluation has been done.\n\n.. autoclass:: torch.distributed.tensor.parallel.style.PairwiseSequenceParallel\n  :members:\n\nSince Tensor Parallelism is built on top of DTensor, we need to specify the\ninput and output placement of the module with DTensors so it can expectedly\ninteracts with the module before and after. The followings are functions\nused for input/output preparation:\n\n\n.. currentmodule:: torch.distributed.tensor.parallel.style","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.tensor.parallel.rst","loc":{"lines":{"from":28,"to":49}}}}],["386",{"pageContent":".. currentmodule:: torch.distributed.tensor.parallel.style\n\n.. autofunction::  make_input_replicate_1d\n.. autofunction::  make_input_reshard_replicate\n.. autofunction::  make_input_shard_1d\n.. autofunction::  make_input_shard_1d_last_dim\n.. autofunction::  make_output_replicate_1d\n.. autofunction::  make_output_reshard_tensor\n.. autofunction::  make_output_shard_1d\n.. autofunction::  make_output_tensor\n\nCurrently, there are some constraints which makes it hard for the `nn.MultiheadAttention`\nmodule to work out of box for Tensor Parallelism, so we built this multihead_attention\nmodule for Tensor Parallelism users. Also, in ``parallelize_module``, we automatically\nswap ``nn.MultiheadAttention`` to this custom module when specifying ``PairwiseParallel``.\n\n.. autoclass:: torch.distributed.tensor.parallel.multihead_attention_tp.TensorParallelMultiheadAttention\n  :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.tensor.parallel.rst","loc":{"lines":{"from":49,"to":66}}}}],["387",{"pageContent":".. autoclass:: torch.distributed.tensor.parallel.multihead_attention_tp.TensorParallelMultiheadAttention\n  :members:\n\nWe also enabled 2D parallelism to integrate with ``FullyShardedDataParallel``.\nUsers just need to call the following API explicitly:\n\n\n.. currentmodule:: torch.distributed.tensor.parallel.fsdp\n.. autofunction::  enable_2d_with_fsdp","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributed.tensor.parallel.rst","loc":{"lines":{"from":66,"to":74}}}}],["388",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\nProbability distributions - torch.distributions\n==================================================\n\n.. automodule:: torch.distributions\n.. currentmodule:: torch.distributions\n\n:hidden:`Distribution`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.distribution\n.. autoclass:: Distribution\n    :members:\n    :show-inheritance:\n\n:hidden:`ExponentialFamily`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.exp_family\n.. autoclass:: ExponentialFamily\n    :members:\n    :show-inheritance:\n\n:hidden:`Bernoulli`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.bernoulli\n.. autoclass:: Bernoulli\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Beta`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.beta\n.. autoclass:: Beta\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Binomial`\n~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributions.rst","loc":{"lines":{"from":1,"to":45}}}}],["389",{"pageContent":":hidden:`Beta`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.beta\n.. autoclass:: Beta\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Binomial`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.binomial\n.. autoclass:: Binomial\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Categorical`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.categorical\n.. autoclass:: Categorical\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Cauchy`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.cauchy\n.. autoclass:: Cauchy\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Chi2`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.chi2\n.. autoclass:: Chi2\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`ContinuousBernoulli`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributions.rst","loc":{"lines":{"from":45,"to":91}}}}],["390",{"pageContent":".. currentmodule:: torch.distributions.chi2\n.. autoclass:: Chi2\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`ContinuousBernoulli`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.continuous_bernoulli\n.. autoclass:: ContinuousBernoulli\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Dirichlet`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.dirichlet\n.. autoclass:: Dirichlet\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Exponential`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.exponential\n.. autoclass:: Exponential\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`FisherSnedecor`\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.fishersnedecor\n.. autoclass:: FisherSnedecor\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Gamma`\n~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributions.rst","loc":{"lines":{"from":91,"to":134}}}}],["391",{"pageContent":".. currentmodule:: torch.distributions.fishersnedecor\n.. autoclass:: FisherSnedecor\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Gamma`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.gamma\n.. autoclass:: Gamma\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Geometric`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.geometric\n.. autoclass:: Geometric\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Gumbel`\n~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.gumbel\n.. autoclass:: Gumbel\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`HalfCauchy`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.half_cauchy\n.. autoclass:: HalfCauchy\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`HalfNormal`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.half_normal\n.. autoclass:: HalfNormal\n    :members:\n    :undoc-members:\n    :show-inheritance:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributions.rst","loc":{"lines":{"from":134,"to":183}}}}],["392",{"pageContent":":hidden:`HalfNormal`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.half_normal\n.. autoclass:: HalfNormal\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Independent`\n~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.independent\n.. autoclass:: Independent\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Kumaraswamy`\n~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.kumaraswamy\n.. autoclass:: Kumaraswamy\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`LKJCholesky`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.lkj_cholesky\n.. autoclass:: LKJCholesky\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Laplace`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.laplace\n.. autoclass:: Laplace\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`LogNormal`\n~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributions.rst","loc":{"lines":{"from":183,"to":229}}}}],["393",{"pageContent":".. currentmodule:: torch.distributions.laplace\n.. autoclass:: Laplace\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`LogNormal`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.log_normal\n.. autoclass:: LogNormal\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`LowRankMultivariateNormal`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.lowrank_multivariate_normal\n.. autoclass:: LowRankMultivariateNormal\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`MixtureSameFamily`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.mixture_same_family\n.. autoclass:: MixtureSameFamily\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Multinomial`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.multinomial\n.. autoclass:: Multinomial\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`MultivariateNormal`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributions.rst","loc":{"lines":{"from":229,"to":272}}}}],["394",{"pageContent":".. currentmodule:: torch.distributions.multinomial\n.. autoclass:: Multinomial\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`MultivariateNormal`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.multivariate_normal\n.. autoclass:: MultivariateNormal\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`NegativeBinomial`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.negative_binomial\n.. autoclass:: NegativeBinomial\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Normal`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.normal\n.. autoclass:: Normal\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`OneHotCategorical`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.one_hot_categorical\n.. autoclass:: OneHotCategorical\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Pareto`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributions.rst","loc":{"lines":{"from":272,"to":315}}}}],["395",{"pageContent":".. currentmodule:: torch.distributions.one_hot_categorical\n.. autoclass:: OneHotCategorical\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Pareto`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.pareto\n.. autoclass:: Pareto\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Poisson`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.poisson\n.. autoclass:: Poisson\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`RelaxedBernoulli`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.relaxed_bernoulli\n.. autoclass:: RelaxedBernoulli\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`LogitRelaxedBernoulli`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.relaxed_bernoulli\n.. autoclass:: LogitRelaxedBernoulli\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`RelaxedOneHotCategorical`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributions.rst","loc":{"lines":{"from":315,"to":358}}}}],["396",{"pageContent":":hidden:`RelaxedOneHotCategorical`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.relaxed_categorical\n.. autoclass:: RelaxedOneHotCategorical\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`StudentT`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.studentT\n.. autoclass:: StudentT\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`TransformedDistribution`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.transformed_distribution\n.. autoclass:: TransformedDistribution\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Uniform`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.uniform\n.. autoclass:: Uniform\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`VonMises`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.von_mises\n.. autoclass:: VonMises\n    :members:\n    :undoc-members:\n    :show-inheritance:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributions.rst","loc":{"lines":{"from":358,"to":401}}}}],["397",{"pageContent":":hidden:`VonMises`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.von_mises\n.. autoclass:: VonMises\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Weibull`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.weibull\n.. autoclass:: Weibull\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:hidden:`Wishart`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. currentmodule:: torch.distributions.wishart\n.. autoclass:: Wishart\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n`KL Divergence`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automodule:: torch.distributions.kl\n.. currentmodule:: torch.distributions.kl\n\n.. autofunction:: kl_divergence\n.. autofunction:: register_kl\n\n`Transforms`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automodule:: torch.distributions.transforms\n    :members:\n    :member-order: bysource\n\n`Constraints`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automodule:: torch.distributions.constraints\n    :members:\n    :member-order: bysource\n\n`Constraint Registry`\n~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributions.rst","loc":{"lines":{"from":401,"to":452}}}}],["398",{"pageContent":"`Constraints`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automodule:: torch.distributions.constraints\n    :members:\n    :member-order: bysource\n\n`Constraint Registry`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automodule:: torch.distributions.constraint_registry\n    :members:\n    :member-order: bysource","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/distributions.rst","loc":{"lines":{"from":452,"to":464}}}}],["399",{"pageContent":"torch.utils.dlpack\n==================\n\n.. currentmodule:: torch.utils.dlpack\n\n.. autofunction:: from_dlpack\n.. autofunction:: to_dlpack","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/dlpack.rst","loc":{"lines":{"from":1,"to":7}}}}],["400",{"pageContent":"Elastic Agent\n==============\n\n.. automodule:: torch.distributed.elastic.agent\n.. currentmodule:: torch.distributed.elastic.agent\n\nServer\n--------\n\n.. automodule:: torch.distributed.elastic.agent.server\n\nBelow is a diagram of an agent that manages a local group of workers.\n\n.. image:: agent_diagram.jpg\n\nConcepts\n--------\n\nThis section describes the high-level classes and concepts that\nare relevant to understanding the role of the ``agent`` in torchelastic.\n\n.. currentmodule:: torch.distributed.elastic.agent.server\n\n.. autoclass:: ElasticAgent\n   :members:\n\n.. autoclass:: WorkerSpec\n   :members:\n\n.. autoclass:: WorkerState\n   :members:\n\n.. autoclass:: Worker\n   :members:\n\n.. autoclass:: WorkerGroup\n   :members:\n\nImplementations\n-------------------\n\nBelow are the agent implementations provided by torchelastic.\n\n.. currentmodule:: torch.distributed.elastic.agent.server.local_elastic_agent\n.. autoclass:: LocalElasticAgent\n\n\nExtending the Agent\n---------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/agent.rst","loc":{"lines":{"from":1,"to":49}}}}],["401",{"pageContent":".. currentmodule:: torch.distributed.elastic.agent.server.local_elastic_agent\n.. autoclass:: LocalElasticAgent\n\n\nExtending the Agent\n---------------------\n\nTo extend the agent you can implement ```ElasticAgent`` directly, however\nwe recommend you extend ``SimpleElasticAgent`` instead, which provides\nmost of the scaffolding and leaves you with a few specific abstract methods\nto implement.\n\n.. currentmodule:: torch.distributed.elastic.agent.server\n.. autoclass:: SimpleElasticAgent\n   :members:\n   :private-members:\n\n.. autoclass:: torch.distributed.elastic.agent.server.api.RunResult\n\n\nWatchdog in the Agent\n---------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/agent.rst","loc":{"lines":{"from":49,"to":70}}}}],["402",{"pageContent":".. autoclass:: torch.distributed.elastic.agent.server.api.RunResult\n\n\nWatchdog in the Agent\n---------------------\n\nA named pipe based watchdog can be enabled in ```LocalElasticAgent``` if an\nenvironment variable ``TORCHELASTIC_ENABLE_FILE_TIMER`` with value 1 has\nbeen defined in the ```LocalElasticAgent``` process.\nOptionally, another environment variable ```TORCHELASTIC_TIMER_FILE```\ncan be set with a unique file name for the named pipe. If the environment\nvariable ```TORCHELASTIC_TIMER_FILE``` is not set, ```LocalElasticAgent```\nwill internally create a unique file name and set it to the environment\nvariable ```TORCHELASTIC_TIMER_FILE```, and this environment variable will\nbe propagated to the worker processes to allow them to connect to the same\nnamed pipe that ```LocalElasticAgent``` uses.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/agent.rst","loc":{"lines":{"from":70,"to":85}}}}],["403",{"pageContent":"Customization\n=============\n\nThis section describes how to customize TorchElastic to fit your needs.\n\nLauncher\n------------------------\n\nThe launcher program that ships with TorchElastic\nshould be sufficient for most use-cases (see :ref:`launcher-api`).\nYou can implement a custom launcher by\nprogrammatically creating an agent and passing it specs for your workers as\nshown below.\n\n.. code-block:: python\n\n  # my_launcher.py\n\n  if __name__ == \"__main__\":\n    args = parse_args(sys.argv[1:])\n    rdzv_handler = RendezvousHandler(...)\n    spec = WorkerSpec(\n        local_world_size=args.nproc_per_node,\n        fn=trainer_entrypoint_fn,\n        args=(trainer_entrypoint_fn args.fn_args,...),\n        rdzv_handler=rdzv_handler,\n        max_restarts=args.max_restarts,\n        monitor_interval=args.monitor_interval,\n    )","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/customization.rst","loc":{"lines":{"from":1,"to":29}}}}],["404",{"pageContent":"agent = LocalElasticAgent(spec, start_method=\"spawn\")\n    try:\n        run_result = agent.run()\n        if run_result.is_failed():\n            print(f\"worker 0 failed with: run_result.failures[0]\")\n        else:\n            print(f\"worker 0 return value is: run_result.return_values[0]\")\n    except Exception ex:\n        # handle exception\n\n\nRendezvous Handler\n------------------------\n\nTo implement your own rendezvous, extend ``torch.distributed.elastic.rendezvous.RendezvousHandler``\nand implement its methods.\n\n.. warning:: Rendezvous handlers are tricky to implement. Before you begin\n          make sure you completely understand the properties of rendezvous.\n          Please refer to :ref:`rendezvous-api` for more information.\n\nOnce implemented you can pass your custom rendezvous handler to the worker\nspec when creating the agent.\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/customization.rst","loc":{"lines":{"from":31,"to":55}}}}],["405",{"pageContent":"Once implemented you can pass your custom rendezvous handler to the worker\nspec when creating the agent.\n\n.. code-block:: python\n\n    spec = WorkerSpec(\n        rdzv_handler=MyRendezvousHandler(params),\n        ...\n    )\n    elastic_agent = LocalElasticAgent(spec, start_method=start_method)\n    elastic_agent.run(spec.role)\n\n\nMetric Handler\n-----------------------------\n\nTorchElastic emits platform level metrics (see :ref:`metrics-api`).\nBy default metrics are emitted to `/dev/null` so you will not see them.\nTo have the metrics pushed to a metric handling service in your infrastructure,\nimplement a `torch.distributed.elastic.metrics.MetricHandler` and `configure` it in your\ncustom launcher.\n\n.. code-block:: python\n\n  # my_launcher.py\n\n  import torch.distributed.elastic.metrics as metrics\n\n  class MyMetricHandler(metrics.MetricHandler):\n      def emit(self, metric_data: metrics.MetricData):\n          # push metric_data to your metric sink\n\n  def main():\n    metrics.configure(MyMetricHandler())","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/customization.rst","loc":{"lines":{"from":55,"to":88}}}}],["406",{"pageContent":"def main():\n    metrics.configure(MyMetricHandler())\n\n    spec = WorkerSpec(...)\n    agent = LocalElasticAgent(spec)\n    agent.run()\n\nEvents Handler\n-----------------------------\n\nTorchElastic supports events recording (see :ref:`events-api`).\nThe events module defines API that allows you to record events and\nimplement custom EventHandler. EventHandler is used for publishing events\nproduced during torchelastic execution to different sources, e.g.  AWS CloudWatch.\nBy default it uses `torch.distributed.elastic.events.NullEventHandler` that ignores\nevents. To configure custom events handler you need to implement\n`torch.distributed.elastic.events.EventHandler` interface and `configure` it\nin your custom launcher.\n\n.. code-block:: python\n\n  # my_launcher.py\n\n  import torch.distributed.elastic.events as events\n\n  class MyEventHandler(events.EventHandler):\n      def record(self, event: events.Event):\n          # process event\n\n  def main():\n    events.configure(MyEventHandler())","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/customization.rst","loc":{"lines":{"from":88,"to":118}}}}],["407",{"pageContent":"class MyEventHandler(events.EventHandler):\n      def record(self, event: events.Event):\n          # process event\n\n  def main():\n    events.configure(MyEventHandler())\n\n    spec = WorkerSpec(...)\n    agent = LocalElasticAgent(spec)\n    agent.run()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/customization.rst","loc":{"lines":{"from":118,"to":127}}}}],["408",{"pageContent":".. _elastic_errors-api:\n\nError Propagation\n==================\n\n.. automodule:: torch.distributed.elastic.multiprocessing.errors\n\nMethods and Classes\n---------------------\n\n.. currentmodule:: torch.distributed.elastic.multiprocessing.errors\n\n.. autofunction:: torch.distributed.elastic.multiprocessing.errors.record\n\n.. autoclass:: ChildFailedError\n\n.. autoclass:: ErrorHandler\n\n.. autoclass:: ProcessFailure","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/errors.rst","loc":{"lines":{"from":1,"to":19}}}}],["409",{"pageContent":".. _events-api:\n\nEvents\n============================\n\n.. automodule:: torch.distributed.elastic.events\n\nAPI Methods\n------------\n\n.. autofunction:: torch.distributed.elastic.events.record\n\n.. autofunction:: torch.distributed.elastic.events.get_logging_handler\n\nEvent Objects\n-----------------\n\n.. currentmodule:: torch.distributed.elastic.events.api\n\n.. autoclass:: torch.distributed.elastic.events.api.Event\n\n.. autoclass:: torch.distributed.elastic.events.api.EventSource\n\n.. autoclass:: torch.distributed.elastic.events.api.EventMetadataValue","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/events.rst","loc":{"lines":{"from":1,"to":24}}}}],["410",{"pageContent":"Examples\n==========================\n\nPlease refer to the `elastic/examples README <https://github.com/pytorch/elastic/tree/master/examples>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/examples.rst","loc":{"lines":{"from":1,"to":4}}}}],["411",{"pageContent":"TorchElastic Kubernetes\n==========================\n\nPlease refer to our GitHub's `Kubernetes README <https://github.com/pytorch/elastic/tree/master/kubernetes>`_\nfor more information on Elastic Job Controller and custom resource definition.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/kubernetes.rst","loc":{"lines":{"from":1,"to":5}}}}],["412",{"pageContent":".. _metrics-api:\n\nMetrics\n=========\n\n.. automodule:: torch.distributed.elastic.metrics\n\n\nMetric Handlers\n-----------------\n\n.. currentmodule:: torch.distributed.elastic.metrics.api\n\nBelow are the metric handlers that come included with torchelastic.\n\n.. autoclass:: MetricHandler\n\n.. autoclass:: ConsoleMetricHandler\n\n.. autoclass:: NullMetricHandler\n\n\n\nMethods\n------------\n\n.. autofunction:: torch.distributed.elastic.metrics.configure\n\n.. autofunction:: torch.distributed.elastic.metrics.prof\n\n.. autofunction:: torch.distributed.elastic.metrics.put_metric","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/metrics.rst","loc":{"lines":{"from":1,"to":31}}}}],["413",{"pageContent":":github_url: https://github.com/pytorch/elastic\n\nMultiprocessing\n================\n\n.. automodule:: torch.distributed.elastic.multiprocessing\n\nStarting Multiple Workers\n---------------------------\n\n.. autofunction:: torch.distributed.elastic.multiprocessing.start_processes\n\nProcess Context\n----------------\n\n.. currentmodule:: torch.distributed.elastic.multiprocessing.api\n\n.. autoclass:: PContext\n\n.. autoclass:: MultiprocessContext\n\n.. autoclass:: SubprocessContext\n\n.. autoclass:: RunProcsResult","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/multiprocessing.rst","loc":{"lines":{"from":1,"to":24}}}}],["414",{"pageContent":"Quickstart\n===========\n\nTo launch a **fault-tolerant** job, run the following on all nodes.\n\n.. code-block:: bash\n\n    torchrun\n       --nnodes=NUM_NODES\n       --nproc-per-node=TRAINERS_PER_NODE\n       --max-restarts=NUM_ALLOWED_FAILURES\n       --rdzv-id=JOB_ID\n       --rdzv-backend=c10d\n       --rdzv-endpoint=HOST_NODE_ADDR\n       YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n\nTo launch an **elastic** job, run the following on at least ``MIN_SIZE`` nodes\nand at most ``MAX_SIZE`` nodes.\n\n.. code-block:: bash\n\n    torchrun\n        --nnodes=MIN_SIZE:MAX_SIZE\n        --nproc-per-node=TRAINERS_PER_NODE\n        --max-restarts=NUM_ALLOWED_FAILURES_OR_MEMBERSHIP_CHANGES\n        --rdzv-id=JOB_ID\n        --rdzv-backend=c10d\n        --rdzv-endpoint=HOST_NODE_ADDR\n        YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/quickstart.rst","loc":{"lines":{"from":1,"to":30}}}}],["415",{"pageContent":".. note::\n   TorchElastic models failures as membership changes. When a node fails,\n   this is treated as a \"scale down\" event. When the failed node is replaced by\n   the scheduler, it is a \"scale up\" event. Hence for both fault tolerant\n   and elastic jobs, ``--max-restarts`` is used to control the total number of\n   restarts before giving up, regardless of whether the restart was caused\n   due to a failure or a scaling event.\n\n``HOST_NODE_ADDR``, in form <host>[:<port>] (e.g. node1.example.com:29400),\nspecifies the node and the port on which the C10d rendezvous backend should be\ninstantiated and hosted. It can be any node in your training cluster, but\nideally you should pick a node that has a high bandwidth.\n\n.. note::\n   If no port number is specified ``HOST_NODE_ADDR`` defaults to 29400.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/quickstart.rst","loc":{"lines":{"from":32,"to":46}}}}],["416",{"pageContent":".. note::\n   If no port number is specified ``HOST_NODE_ADDR`` defaults to 29400.\n\n.. note::\n   The ``--standalone`` option can be passed to launch a single node job with a\n   sidecar rendezvous backend. You don’t have to pass ``--rdzv-id``,\n   ``--rdzv-endpoint``, and ``--rdzv-backend`` when the ``--standalone`` option\n   is used.\n\n\n.. note::\n   Learn more about writing your distributed training script\n   `here <train_script.html>`_.\n\nIf ``torchrun`` does not meet your requirements you may use our APIs directly\nfor more powerful customization. Start by taking a look at the\n`elastic agent <agent.html>`_ API.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/quickstart.rst","loc":{"lines":{"from":46,"to":62}}}}],["417",{"pageContent":".. _rendezvous-api:\n\nRendezvous\n==========\n\n.. automodule:: torch.distributed.elastic.rendezvous\n\nBelow is a state diagram describing how rendezvous works.\n\n.. image:: etcd_rdzv_diagram.png\n\nRegistry\n--------\n\n.. autoclass:: RendezvousParameters\n   :members:\n\n.. autoclass:: RendezvousHandlerRegistry\n   :members:\n\n.. automodule:: torch.distributed.elastic.rendezvous.registry\n\nHandler\n-------\n\n.. currentmodule:: torch.distributed.elastic.rendezvous\n\n.. autoclass:: RendezvousHandler\n   :members:\n\nExceptions\n----------\n.. autoclass:: RendezvousError\n.. autoclass:: RendezvousClosedError\n.. autoclass:: RendezvousTimeoutError\n.. autoclass:: RendezvousConnectionError\n.. autoclass:: RendezvousStateError\n\nImplementations\n---------------\n\nDynamic Rendezvous\n******************\n\n.. currentmodule:: torch.distributed.elastic.rendezvous.dynamic_rendezvous\n\n.. autofunction:: create_handler\n\n.. autoclass:: DynamicRendezvousHandler()\n   :members: from_backend\n\n.. autoclass:: RendezvousBackend\n   :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/rendezvous.rst","loc":{"lines":{"from":1,"to":53}}}}],["418",{"pageContent":".. autofunction:: create_handler\n\n.. autoclass:: DynamicRendezvousHandler()\n   :members: from_backend\n\n.. autoclass:: RendezvousBackend\n   :members:\n\n.. autoclass:: RendezvousTimeout\n   :members:\n\nC10d Backend\n^^^^^^^^^^^^\n\n.. currentmodule:: torch.distributed.elastic.rendezvous.c10d_rendezvous_backend\n\n.. autofunction:: create_backend\n\n.. autoclass:: C10dRendezvousBackend\n   :members:\n\nEtcd Backend\n^^^^^^^^^^^^\n\n.. currentmodule:: torch.distributed.elastic.rendezvous.etcd_rendezvous_backend\n\n.. autofunction:: create_backend\n\n.. autoclass:: EtcdRendezvousBackend\n   :members:\n\nEtcd Rendezvous (Legacy)\n************************\n\n.. warning::\n    The ``DynamicRendezvousHandler`` class supersedes the ``EtcdRendezvousHandler``\n    class, and is recommended for most users. ``EtcdRendezvousHandler`` is in\n    maintenance mode and will be deprecated in the future.\n\n.. currentmodule:: torch.distributed.elastic.rendezvous.etcd_rendezvous\n\n.. autoclass:: EtcdRendezvousHandler\n\nEtcd Store\n**********","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/rendezvous.rst","loc":{"lines":{"from":53,"to":97}}}}],["419",{"pageContent":".. currentmodule:: torch.distributed.elastic.rendezvous.etcd_rendezvous\n\n.. autoclass:: EtcdRendezvousHandler\n\nEtcd Store\n**********\n\nThe ``EtcdStore`` is the C10d ``Store`` instance type returned by\n``next_rendezvous()`` when etcd is used as the rendezvous backend.\n\n.. currentmodule:: torch.distributed.elastic.rendezvous.etcd_store\n\n.. autoclass:: EtcdStore\n   :members:\n\nEtcd Server\n***********\n\nThe ``EtcdServer`` is a convenience class that makes it easy for you to\nstart and stop an etcd server on a subprocess. This is useful for testing\nor single-node (multi-worker) deployments where manually setting up an\netcd server on the side is cumbersome.\n\n.. warning:: For production and multi-node deployments please consider\n             properly deploying a highly available etcd server as this is\n             the single point of failure for your distributed jobs.\n\n.. currentmodule:: torch.distributed.elastic.rendezvous.etcd_server\n\n.. autoclass:: EtcdServer","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/rendezvous.rst","loc":{"lines":{"from":97,"to":126}}}}],["420",{"pageContent":".. _launcher-api:\n\ntorchrun (Elastic Launch)\n======================================\n\n.. automodule:: torch.distributed.run","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/run.rst","loc":{"lines":{"from":1,"to":6}}}}],["421",{"pageContent":"Expiration Timers\n==================\n\n.. automodule:: torch.distributed.elastic.timer\n.. currentmodule:: torch.distributed.elastic.timer\n\nClient Methods\n---------------\n.. autofunction:: torch.distributed.elastic.timer.configure\n\n.. autofunction:: torch.distributed.elastic.timer.expires\n\nServer/Client Implementations\n------------------------------\nBelow are the timer server and client pairs that are provided by torchelastic.\n\n.. note:: Timer server and clients always have to be implemented and used\n          in pairs since there is a messaging protocol between the server\n          and client.\n\nBelow is a pair of timer server and client that is implemented based on\na ``multiprocess.Queue``.\n\n.. autoclass:: LocalTimerServer\n\n.. autoclass:: LocalTimerClient\n\nBelow is another pair of timer server and client that is implemented\nbased on a named pipe.\n\n.. autoclass:: FileTimerServer\n\n.. autoclass:: FileTimerClient\n\n\nWriting a custom timer server/client\n--------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/timer.rst","loc":{"lines":{"from":1,"to":37}}}}],["422",{"pageContent":".. autoclass:: FileTimerServer\n\n.. autoclass:: FileTimerClient\n\n\nWriting a custom timer server/client\n--------------------------------------\n\nTo write your own timer server and client extend the\n``torch.distributed.elastic.timer.TimerServer`` for the server and\n``torch.distributed.elastic.timer.TimerClient`` for the client. The\n``TimerRequest`` object is used to pass messages between\nthe server and client.\n\n.. autoclass:: TimerRequest\n   :members:\n\n.. autoclass:: TimerServer\n   :members:\n\n.. autoclass:: TimerClient\n   :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/timer.rst","loc":{"lines":{"from":37,"to":58}}}}],["423",{"pageContent":".. _elastic_train_script:\n\nTrain script\n-------------\n\nIf your train script works with ``torch.distributed.launch`` it will continue\nworking with ``torchrun`` with these differences:\n\n1. No need to manually pass ``RANK``, ``WORLD_SIZE``,\n   ``MASTER_ADDR``, and ``MASTER_PORT``.\n\n2. ``rdzv_backend`` and ``rdzv_endpoint`` can be provided. For most users\n   this will be set to ``c10d`` (see `rendezvous <rendezvous.html>`_). The default\n   ``rdzv_backend`` creates a non-elastic rendezvous where ``rdzv_endpoint`` holds\n   the master address.\n\n3. Make sure you have a ``load_checkpoint(path)`` and\n   ``save_checkpoint(path)`` logic in your script. When any number of\n   workers fail we restart all the workers with the same program\n   arguments so you will lose progress up to the most recent checkpoint\n   (see `elastic launch <run.html>`_).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/train_script.rst","loc":{"lines":{"from":1,"to":21}}}}],["424",{"pageContent":"4. ``use_env`` flag has been removed. If you were parsing local rank by parsing\n   the ``--local-rank`` option, you need to get the local rank from the\n   environment variable ``LOCAL_RANK`` (e.g. ``int(os.environ[\"LOCAL_RANK\"])``).\n\nBelow is an expository example of a training script that checkpoints on each\nepoch, hence the worst-case progress lost on failure is one full epoch worth\nof training.\n\n.. code-block:: python\n\n  def main():\n       args = parse_args(sys.argv[1:])\n       state = load_checkpoint(args.checkpoint_path)\n       initialize(state)\n\n       # torch.distributed.run ensures that this will work\n       # by exporting all the env vars needed to initialize the process group\n       torch.distributed.init_process_group(backend=args.backend)\n\n       for i in range(state.epoch, state.total_num_epochs)\n            for batch in iter(state.dataset)\n                train(batch, state.model)\n\n            state.epoch += 1\n            save_checkpoint(state)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/train_script.rst","loc":{"lines":{"from":23,"to":47}}}}],["425",{"pageContent":"state.epoch += 1\n            save_checkpoint(state)\n\nFor concrete examples of torchelastic-compliant train scripts, visit\nour `examples <examples.html>`_ page.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/elastic/train_script.rst","loc":{"lines":{"from":47,"to":51}}}}],["426",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\ntorch.fft\n=========\n\nDiscrete Fourier transforms and related functions.\n\n.. automodule:: torch.fft\n.. currentmodule:: torch.fft\n\nFast Fourier Transforms\n-----------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    fft\n    ifft\n    fft2\n    ifft2\n    fftn\n    ifftn\n    rfft\n    irfft\n    rfft2\n    irfft2\n    rfftn\n    irfftn\n    hfft\n    ihfft\n    hfft2\n    ihfft2\n    hfftn\n    ihfftn\n\nHelper Functions\n----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    fftfreq\n    rfftfreq\n    fftshift\n    ifftshift","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fft.rst","loc":{"lines":{"from":1,"to":48}}}}],["427",{"pageContent":"FullyShardedDataParallel\n========================\n\n.. automodule:: torch.distributed.fsdp\n\n.. autoclass:: torch.distributed.fsdp.FullyShardedDataParallel\n  :members:\n\n.. autoclass:: torch.distributed.fsdp.BackwardPrefetch\n  :members:\n\n.. autoclass:: torch.distributed.fsdp.ShardingStrategy\n  :members:\n\n.. autoclass:: torch.distributed.fsdp.MixedPrecision\n  :members:\n\n.. autoclass:: torch.distributed.fsdp.CPUOffload\n  :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fsdp.rst","loc":{"lines":{"from":1,"to":19}}}}],["428",{"pageContent":"torch.func API Reference\n========================\n\n.. currentmodule:: torch.func\n\n.. automodule:: torch.func\n\nFunction Transforms\n-------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n     vmap\n     grad\n     grad_and_value\n     vjp\n     jvp\n     linearize\n     jacrev\n     jacfwd\n     hessian\n     functionalize\n\nUtilities for working with torch.nn.Modules\n-------------------------------------------\n\nIn general, you can transform over a function that calls a ``torch.nn.Module``.\nFor example, the following is an example of computing a jacobian of a function\nthat takes three values and returns three values:\n\n.. code-block:: python\n\n    model = torch.nn.Linear(3, 3)\n\n    def f(x):\n        return model(x)\n\n    x = torch.randn(3)\n    jacobian = jacrev(f)(x)\n    assert jacobian.shape == (3, 3)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.api.rst","loc":{"lines":{"from":1,"to":41}}}}],["429",{"pageContent":".. code-block:: python\n\n    model = torch.nn.Linear(3, 3)\n\n    def f(x):\n        return model(x)\n\n    x = torch.randn(3)\n    jacobian = jacrev(f)(x)\n    assert jacobian.shape == (3, 3)\n\nHowever, if you want to do something like compute a jacobian over the parameters\nof the model, then there needs to be a way to construct a function where the\nparameters are the inputs to the function.\nThat's what :func:`functional_call` is for:\nit accepts an nn.Module, the transformed ``parameters``, and the inputs to the\nModule's forward pass. It returns the value of running the Module's forward pass\nwith the replaced parameters.\n\nHere's how we would compute the Jacobian over the parameters\n\n.. code-block:: python\n\n    model = torch.nn.Linear(3, 3)\n\n    def f(params, x):\n        return torch.func.functional_call(model, params, x)\n\n    x = torch.randn(3)\n    jacobian = jacrev(f)(dict(model.named_parameters()), x)\n\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.api.rst","loc":{"lines":{"from":41,"to":75}}}}],["430",{"pageContent":"x = torch.randn(3)\n    jacobian = jacrev(f)(dict(model.named_parameters()), x)\n\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    functional_call\n    stack_module_state\n    replace_all_batch_norm_modules_\n\nIf you're looking for information on fixing Batch Norm modules, please follow the\nguidance here\n\n.. toctree::\n   :maxdepth: 1\n\n   func.batch_norm","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.api.rst","loc":{"lines":{"from":75,"to":93}}}}],["431",{"pageContent":"Patching Batch Norm\n===================\n\nWhat's happening?\n-----------------\nBatch Norm requires in-place updates to running_mean and running_var of the same size as the input.\nFunctorch does not support inplace update to a regular tensor that takes in a batched tensor (i.e.\n``regular.add_(batched)`` is not allowed). So when vmapping over a batch of inputs to a single module,\nwe end up with this error\n\nHow to fix\n----------\nOne of the best supported ways is to switch BatchNorm for GroupNorm. Options 1 and 2 support this\n\nAll of these options assume that you don't need running stats. If you're using a module this means\nthat it's assumed you won't use batch norm in evaluation mode. If you have a use case that involves\nrunning batch norm with vmap in evaluation mode, please file an issue\n\nOption 1: Change the BatchNorm\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIf you want to change for GroupNorm, anywhere that you have BatchNorm, replace it with:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.batch_norm.rst","loc":{"lines":{"from":1,"to":23}}}}],["432",{"pageContent":"Option 1: Change the BatchNorm\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIf you want to change for GroupNorm, anywhere that you have BatchNorm, replace it with:\n\n.. code-block:: python\n\n    BatchNorm2d(C, G, track_running_stats=False)\n\nHere ``C`` is the same ``C`` as in the original BatchNorm. ``G`` is the number of groups to\nbreak ``C`` into. As such, ``C % G == 0`` and as a fallback, you can set ``C == G``, meaning\neach channel will be treated separately.\n\nIf you must use BatchNorm and you've built the module yourself, you can change the module to\nnot use running stats. In other words, anywhere that there's a BatchNorm module, set the\n``track_running_stats`` flag to be False\n\n.. code-block:: python\n\n    BatchNorm2d(64, track_running_stats=False)\n\n\nOption 2: torchvision parameter\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nSome torchvision models, like resnet and regnet, can take in a ``norm_layer`` parameter. These are\noften defaulted to be BatchNorm2d if they've been defaulted.\n\nInstead you can set it to be GroupNorm.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.batch_norm.rst","loc":{"lines":{"from":23,"to":49}}}}],["433",{"pageContent":"Instead you can set it to be GroupNorm.\n\n.. code-block:: python\n\n    import torchvision\n    from functools import partial\n    torchvision.models.resnet18(norm_layer=lambda c: GroupNorm(num_groups=g, c))\n\nHere, once again, ``c % g == 0`` so as a fallback, set ``g = c``.\n\nIf you are attached to BatchNorm, be sure to use a version that doesn't use running stats\n\n.. code-block:: python\n\n    import torchvision\n    from functools import partial\n    torchvision.models.resnet18(norm_layer=partial(BatchNorm2d, track_running_stats=False))\n\nOption 3: functorch's patching\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nfunctorch has added some functionality to allow for quick, in-place patching of the module to not\nuse running stats. Changing the norm layer is more fragile, so we have not offered that. If you\nhave a net where you want the BatchNorm to not use running stats, you can run\n``replace_all_batch_norm_modules_`` to update the module in-place to not use running stats\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.batch_norm.rst","loc":{"lines":{"from":49,"to":74}}}}],["434",{"pageContent":".. code-block:: python\n\n    from torch.func import replace_all_batch_norm_modules_\n    replace_all_batch_norm_modules_(net)\n\nOption 4: eval mode\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nWhen run under eval mode, the running_mean and running_var will not be updated. Therefore, vmap can support this mode\n\n.. code-block:: python\n\n    model.eval()\n    vmap(model)(x)\n    model.train()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.batch_norm.rst","loc":{"lines":{"from":74,"to":87}}}}],["435",{"pageContent":"Migrating from functorch to torch.func\n======================================\n\ntorch.func, previously known as \"functorch\", is\n`JAX-like <https://github.com/google/jax>`_ composable function transforms for PyTorch.\n\nfunctorch started as an out-of-tree library over at\nthe `pytorch/functorch <https://github.com/pytorch/functorch>`_ repository.\nOur goal has always been to upstream functorch directly into PyTorch and provide\nit as a core PyTorch library.\n\nAs the final step of the upstream, we've decided to migrate from being a top level package\n(``functorch``) to being a part of PyTorch to reflect how the function transforms are\nintegrated directly into PyTorch core. As of PyTorch 2.0, we are deprecating\n``import functorch`` and ask that users migrate to the newest APIs, which we\nwill maintain going forward. ``import functorch`` will be kept around to maintain\nbackwards compatibility for a couple of releases.\n\nfunction transforms\n-------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.migrating.rst","loc":{"lines":{"from":1,"to":20}}}}],["436",{"pageContent":"function transforms\n-------------------\n\nThe following APIs are a drop-in replacement for the following\n`functorch APIs <https://pytorch.org/functorch/1.13/functorch.html>`_.\nThey are fully backwards compatible.\n\n\n==============================  =======================================\nfunctorch API                    PyTorch API (as of PyTorch 2.0)\n==============================  =======================================\nfunctorch.vmap                  :func:`torch.vmap` or :func:`torch.func.vmap`\nfunctorch.grad                  :func:`torch.func.grad`\nfunctorch.vjp                   :func:`torch.func.vjp`\nfunctorch.jvp                   :func:`torch.func.jvp`\nfunctorch.jacrev                :func:`torch.func.jacrev`\nfunctorch.jacfwd                :func:`torch.func.jacfwd`\nfunctorch.hessian               :func:`torch.func.hessian`\nfunctorch.functionalize         :func:`torch.func.functionalize`\n==============================  =======================================","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.migrating.rst","loc":{"lines":{"from":20,"to":39}}}}],["437",{"pageContent":"Furthermore, if you are using torch.autograd.functional APIs, please try out\nthe :mod:`torch.func` equivalents instead. :mod:`torch.func` function\ntransforms are more composable and more performant in many cases.\n\n=========================================== =======================================\ntorch.autograd.functional API               torch.func API (as of PyTorch 2.0)\n=========================================== =======================================\n:func:`torch.autograd.functional.vjp`       :func:`torch.func.grad` or :func:`torch.func.vjp`\n:func:`torch.autograd.functional.jvp`       :func:`torch.func.jvp`\n:func:`torch.autograd.functional.jacobian`  :func:`torch.func.jacrev` or :func:`torch.func.jacfwd`\n:func:`torch.autograd.functional.hessian`   :func:`torch.func.hessian`\n=========================================== =======================================\n\nNN module utilities\n-------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.migrating.rst","loc":{"lines":{"from":41,"to":55}}}}],["438",{"pageContent":"NN module utilities\n-------------------\n\nWe've changed the APIs to apply function transforms over NN modules to make them\nfit better into the PyTorch design philosophy. The new API is different, so\nplease read this section carefully.\n\nfunctorch.make_functional\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n:func:`torch.func.functional_call` is the replacement for\n`functorch.make_functional <https://pytorch.org/functorch/1.13/generated/functorch.make_functional.html#functorch.make_functional>`_\nand\n`functorch.make_functional_with_buffers <https://pytorch.org/functorch/1.13/generated/functorch.make_functional_with_buffers.html#functorch.make_functional_with_buffers>`_.\nHowever, it is not a drop-in replacement.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.migrating.rst","loc":{"lines":{"from":55,"to":69}}}}],["439",{"pageContent":"If you're in a hurry, you can use\n`helper functions in this gist <https://gist.github.com/zou3519/7769506acc899d83ef1464e28f22e6cf>`_\nthat emulate the behavior of functorch.make_functional and functorch.make_functional_with_buffers.\nWe recommend using :func:`torch.func.functional_call` directly because it is a more explicit\nand flexible API.\n\nConcretely, functorch.make_functional returns a functional module and parameters.\nThe functional module accepts parameters and inputs to the model as arguments.\n:func:`torch.func.functional_call` allows one to call the forward pass of an existing\nmodule using new parameters and buffers and inputs.\n\nHere's an example of how to compute gradients of parameters of a model using functorch\nvs :mod:`torch.func`::\n\n    # ---------------\n    # using functorch\n    # ---------------\n    import torch\n    import functorch\n    inputs = torch.randn(64, 3)\n    targets = torch.randn(64, 3)\n    model = torch.nn.Linear(3, 3)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.migrating.rst","loc":{"lines":{"from":71,"to":92}}}}],["440",{"pageContent":"fmodel, params = functorch.make_functional(model)\n\n    def compute_loss(params, inputs, targets):\n        prediction = fmodel(params, inputs)\n        return torch.nn.functional.mse_loss(prediction, targets)\n\n    grads = functorch.grad(compute_loss)(params, inputs, targets)\n\n    # ------------------------------------\n    # using torch.func (as of PyTorch 2.0)\n    # ------------------------------------\n    import torch\n    inputs = torch.randn(64, 3)\n    targets = torch.randn(64, 3)\n    model = torch.nn.Linear(3, 3)\n\n    params = dict(model.named_parameters())\n\n    def compute_loss(params, inputs, targets):\n        prediction = torch.func.functional_call(model, params, (inputs,))\n        return torch.nn.functional.mse_loss(prediction, targets)\n\n    grads = torch.func.grad(compute_loss)(params, inputs, targets)\n\nAnd here's an example of how to compute jacobians of model parameters::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.migrating.rst","loc":{"lines":{"from":94,"to":118}}}}],["441",{"pageContent":"grads = torch.func.grad(compute_loss)(params, inputs, targets)\n\nAnd here's an example of how to compute jacobians of model parameters::\n\n    # ---------------\n    # using functorch\n    # ---------------\n    import torch\n    import functorch\n    inputs = torch.randn(64, 3)\n    model = torch.nn.Linear(3, 3)\n\n    fmodel, params = functorch.make_functional(model)\n    jacobians = functorch.jacrev(fmodel)(params, inputs)\n\n    # ------------------------------------\n    # using torch.func (as of PyTorch 2.0)\n    # ------------------------------------\n    import torch\n    from torch.func import jacrev, functional_call\n    inputs = torch.randn(64, 3)\n    model = torch.nn.Linear(3, 3)\n\n    params = dict(model.named_parameters())\n    # jacrev computes jacobians of argnums=0 by default.\n    # We set it to 1 to compute jacobians of params\n    jacobians = jacrev(functional_call, argnums=1)(model, params, (inputs,))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.migrating.rst","loc":{"lines":{"from":118,"to":144}}}}],["442",{"pageContent":"Note that it is important for memory consumption that you should only carry\naround a single copy of your parameters. ``model.named_parameters()`` does not copy\nthe parameters. If in your model training you update the parameters of the model\nin-place, then the ``nn.Module`` that is your model has the single copy of the\nparameters and everything is OK.\n\nHowever, if you want to carry your parameters around in a dictionary and update\nthem out-of-place, then there are two copies of parameters: the one in the\ndictionary and the one in the ``model``. In this case, you should change\n``model`` to not hold memory by converting it to the meta device via\n``model.to('meta')``.\n\nfunctorch.combine_state_for_ensemble\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.migrating.rst","loc":{"lines":{"from":146,"to":159}}}}],["443",{"pageContent":"functorch.combine_state_for_ensemble\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPlease use :func:`torch.func.stack_module_state` instead of\n`functorch.combine_state_for_ensemble <https://pytorch.org/functorch/1.13/generated/functorch.combine_state_for_ensemble.html>`_\n:func:`torch.func.stack_module_state` returns two dictionaries, one of stacked parameters, and\none of stacked buffers, that can then be used with :func:`torch.vmap` and :func:`torch.func.functional_call`\nfor ensembling.\n\nFor example, here is an example of how to ensemble over a very simple model::\n\n    import torch\n    num_models = 5\n    batch_size = 64\n    in_features, out_features = 3, 3\n    models = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]\n    data = torch.randn(batch_size, 3)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.migrating.rst","loc":{"lines":{"from":159,"to":175}}}}],["444",{"pageContent":"# ---------------\n    # using functorch\n    # ---------------\n    import functorch\n    fmodel, params, buffers = functorch.combine_state_for_ensemble(models)\n    output = functorch.vmap(fmodel, (0, 0, None))(params, buffers, data)\n    assert output.shape == (num_models, batch_size, out_features)\n\n    # ------------------------------------\n    # using torch.func (as of PyTorch 2.0)\n    # ------------------------------------\n    import copy\n\n    # Construct a version of the model with no memory by putting the Tensors on\n    # the meta device.\n    base_model = copy.deepcopy(models[0])\n    base_model.to('meta')\n\n    params, buffers = torch.func.stack_module_state(models)\n\n    # It is possible to vmap directly over torch.func.functional_call,\n    # but wrapping it in a function makes it clearer what is going on.\n    def call_single_model(params, buffers, data):\n        return torch.func.functional_call(base_model, (params, buffers), (data,))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.migrating.rst","loc":{"lines":{"from":177,"to":200}}}}],["445",{"pageContent":"output = torch.vmap(call_single_model, (0, 0, None))(params, buffers, data)\n    assert output.shape == (num_models, batch_size, out_features)\n\n\nfunctorch.compile\n-----------------\n\nWe are no longer supporting functorch.compile (also known as AOTAutograd)\nas a frontend for compilation in PyTorch; we have integrated AOTAutograd\ninto PyTorch's compilation story. If you are a user, please use\n:func:`torch.compile` instead.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.migrating.rst","loc":{"lines":{"from":202,"to":212}}}}],["446",{"pageContent":"torch.func\n==========\n\n.. currentmodule:: torch.func\n\ntorch.func, previously known as \"functorch\", is\n`JAX-like <https://github.com/google/jax>`_ composable function transforms for PyTorch.\n\n.. note::\n   This library is currently in `beta <https://pytorch.org/blog/pytorch-feature-classification-changes/#beta>`_.\n   What this means is that the features generally work (unless otherwise documented)\n   and we (the PyTorch team) are committed to bringing this library forward. However, the APIs\n   may change under user feedback and we don't have full coverage over PyTorch operations.\n\n   If you have suggestions on the API or use-cases you'd like to be covered, please\n   open an GitHub issue or reach out. We'd love to hear about how you're using the library.\n\nWhat are composable function transforms?\n----------------------------------------\n\n- A \"function transform\" is a higher-order function that accepts a numerical function\n  and returns a new function that computes a different quantity.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.rst","loc":{"lines":{"from":1,"to":22}}}}],["447",{"pageContent":"- A \"function transform\" is a higher-order function that accepts a numerical function\n  and returns a new function that computes a different quantity.\n\n- :mod:`torch.func` has auto-differentiation transforms (``grad(f)`` returns a function that\n  computes the gradient of ``f``), a vectorization/batching transform (``vmap(f)``\n  returns a function that computes ``f`` over batches of inputs), and others.\n\n- These function transforms can compose with each other arbitrarily. For example,\n  composing ``vmap(grad(f))`` computes a quantity called per-sample-gradients that\n  stock PyTorch cannot efficiently compute today.\n\nWhy composable function transforms?\n-----------------------------------\n\nThere are a number of use cases that are tricky to do in PyTorch today:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.rst","loc":{"lines":{"from":22,"to":36}}}}],["448",{"pageContent":"Why composable function transforms?\n-----------------------------------\n\nThere are a number of use cases that are tricky to do in PyTorch today:\n\n- computing per-sample-gradients (or other per-sample quantities)\n- running ensembles of models on a single machine\n- efficiently batching together tasks in the inner-loop of MAML\n- efficiently computing Jacobians and Hessians\n- efficiently computing batched Jacobians and Hessians\n\nComposing :func:`vmap`, :func:`grad`, and :func:`vjp` transforms allows us to express the above without designing a separate subsystem for each.\nThis idea of composable function transforms comes from the `JAX framework <https://github.com/google/jax>`_.\n\nRead More\n---------\n\n.. toctree::\n   :maxdepth: 2\n\n   func.whirlwind_tour\n   func.api\n   func.ux_limitations\n   func.migrating","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.rst","loc":{"lines":{"from":36,"to":59}}}}],["449",{"pageContent":".. currentmodule:: torch.func\n\n.. _ux-limitations:\n\nUX Limitations\n==============\n\ntorch.func, like `JAX <https://github.com/google/jax>`_, has restrictions around\nwhat can be transformed. In general, JAX’s limitations are that transforms\nonly work with pure functions: that is, functions where the output is completely\ndetermined by the input and that do not involve side effects (like mutation).\n\nWe have a similar guarantee: our transforms work well with pure functions.\nHowever, we do support certain in-place operations. On one hand, writing code\ncompatible with function transforms may involve changing how you write PyTorch\ncode, on the other hand, you may find that our transforms let you express things\nthat were previously difficult to express in PyTorch.\n\nGeneral limitations\n-------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":1,"to":20}}}}],["450",{"pageContent":"General limitations\n-------------------\n\nAll torch.func transforms share a limitation in that a function should not\nassign to global variables. Instead, all outputs to a function must be returned\nfrom the function. This restriction comes from how torch.func is implemented:\neach transform wraps Tensor inputs in special torch.func Tensor subclasses\nthat facilitate the transform.\n\nSo, instead of the following:\n\n::\n\n  import torch\n  from torch.func import grad\n\n  # Don't do this\n  intermediate = None\n\n  def f(x):\n    global intermediate\n    intermediate = x.sin()\n    z = intermediate.sin()\n    return z\n\n  x = torch.randn([])\n  grad_x = grad(f)(x)\n\nPlease rewrite ``f`` to return ``intermediate``:\n\n::\n\n  def f(x):\n    intermediate = x.sin()\n    z = intermediate.sin()\n    return z, intermediate\n\n  grad_x, intermediate = grad(f, has_aux=True)(x)\n\ntorch.autograd APIs\n-------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":20,"to":60}}}}],["451",{"pageContent":"::\n\n  def f(x):\n    intermediate = x.sin()\n    z = intermediate.sin()\n    return z, intermediate\n\n  grad_x, intermediate = grad(f, has_aux=True)(x)\n\ntorch.autograd APIs\n-------------------\n\nIf you are trying to use a ``torch.autograd`` API like ``torch.autograd.grad``\nor ``torch.autograd.backward`` inside of a function being transformed by\n:func:`vmap` or one of torch.func's AD transforms (:func:`vjp`, :func:`jvp`,\n:func:`jacrev`, :func:`jacfwd`), the transform may not be able to transform over it.\nIf it is unable to do so, you'll receive an error message.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":60,"to":76}}}}],["452",{"pageContent":"This is a fundamental design limitation in how PyTorch's AD support is implemented\nand the reason why we designed the torch.func library. Please instead use the torch.func\nequivalents of the ``torch.autograd`` APIs:\n- ``torch.autograd.grad``, ``Tensor.backward`` -> ``torch.func.vjp`` or ``torch.func.grad``\n- ``torch.autograd.functional.jvp`` -> ``torch.func.jvp``\n- ``torch.autograd.functional.jacobian`` -> ``torch.func.jacrev`` or ``torch.func.jacfwd``\n- ``torch.autograd.functional.hessian`` -> ``torch.func.hessian``\n\nvmap limitations\n----------------\n\n.. note::\n  :func:`vmap` is our most restrictive transform.\n  The grad-related transforms (:func:`grad`, :func:`vjp`, :func:`jvp`) do not\n  have these limitations. :func:`jacfwd` (and :func:`hessian`, which is\n  implemented with :func:`jacfwd`) is a composition of :func:`vmap` and\n  :func:`jvp` so it also has these limitations.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":78,"to":94}}}}],["453",{"pageContent":"``vmap(func)`` is a transform that returns a function that maps ``func`` over\nsome new dimension of each input Tensor. The mental model for vmap is that it is\nlike running a for-loop: for pure functions (i.e. in the absence of side\neffects), ``vmap(f)(x)`` is equivalent to:\n\n::\n\n  torch.stack([f(x_i) for x_i in x.unbind(0)])\n\nMutation: Arbitrary mutation of Python data structures\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn the presence of side effects, :func:`vmap` no longer acts like it is running\na for-loop. For example, the following function:\n\n::\n\n  def f(x, list):\n    list.pop()\n    print(\"hello!\")\n    return x.sum(0)\n\n  x = torch.randn(3, 1)\n  lst = [0, 1, 2, 3]\n\n  result = vmap(f, in_dims=(0, None))(x, lst)\n\nwill print \"hello!\" once and pop only one element from ``lst``.\n\n\n:func:`vmap` executes ``f`` a single time, so all side effects only happen once.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":96,"to":126}}}}],["454",{"pageContent":"result = vmap(f, in_dims=(0, None))(x, lst)\n\nwill print \"hello!\" once and pop only one element from ``lst``.\n\n\n:func:`vmap` executes ``f`` a single time, so all side effects only happen once.\n\nThis is a consequence of how vmap is implemented. torch.func has a special,\ninternal BatchedTensor class. ``vmap(f)(*inputs)`` takes all Tensor inputs,\nturns them into BatchedTensors, and calls ``f(*batched_tensor_inputs)``.\nBatchedTensor overrides the PyTorch API to produce batched (i.e. vectorized)\nbehavior for each PyTorch operator.\n\n\nMutation: in-place PyTorch Operations\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nYou might be here due to receiving an error about vmap-incompatible in-place\noperations. :func:`vmap` will raise an error if it encounters an unsupported PyTorch\nin-place operation and it will succeed otherwise. Unsupported operations\nare those that would cause a Tensor with more elements to be written to a\nTensor with fewer elements. Here's an example of how this can occur:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":126,"to":149}}}}],["455",{"pageContent":"::\n\n  def f(x, y):\n    x.add_(y)\n    return x\n\n  x = torch.randn(1)\n  y = torch.randn(3, 1)  # When vmapped over, looks like it has shape [1]\n\n  # Raises an error because `x` has fewer elements than `y`.\n  vmap(f, in_dims=(None, 0))(x, y)\n\n``x`` is a Tensor with one element, ``y`` is a Tensor with three elements.\n``x + y`` has three elements (due to broadcasting), but attempting to write\nthree elements back into ``x``, which only has one element, raises an error\ndue to attempting to write three elements into a Tensor with a single element.\n\nThere is no problem if the Tensor being written to is batched under\n:func:`~torch.vmap` (i.e. it is being vmapped over).\n\n::\n\n  def f(x, y):\n    x.add_(y)\n    return x\n\n  x = torch.randn(3, 1)\n  y = torch.randn(3, 1)\n  expected = x + y\n\n  # Does not raise an error because x is being vmapped over.\n  vmap(f, in_dims=(0, 0))(x, y)\n  assert torch.allclose(x, expected)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":149,"to":181}}}}],["456",{"pageContent":"x = torch.randn(3, 1)\n  y = torch.randn(3, 1)\n  expected = x + y\n\n  # Does not raise an error because x is being vmapped over.\n  vmap(f, in_dims=(0, 0))(x, y)\n  assert torch.allclose(x, expected)\n\nOne common fix for this is to replace calls to factory functions with\ntheir \"new_*\" equivalent. For example:\n\n- Replace :func:`torch.zeros` with :meth:`Tensor.new_zeros`\n- Replace :func:`torch.empty` with :meth:`Tensor.new_empty`\n\nTo see why this helps, consider the following.\n\n::\n\n  def diag_embed(vec):\n    assert vec.dim() == 1\n    result = torch.zeros(vec.shape[0], vec.shape[0])\n    result.diagonal().copy_(vec)\n    return result\n\n  vecs = torch.tensor([[0., 1, 2], [3., 4, 5]])\n\n  # RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible ...\n  vmap(diag_embed)(vecs)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":181,"to":208}}}}],["457",{"pageContent":"vecs = torch.tensor([[0., 1, 2], [3., 4, 5]])\n\n  # RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible ...\n  vmap(diag_embed)(vecs)\n\nInside of :func:`~torch.vmap`, ``result`` is a Tensor of shape [3, 3].\nHowever, although ``vec`` looks like it has shape [3], ``vec`` actually has\nunderlying shape [2, 3].\nIt is not possible to copy ``vec`` into ``result.diagonal()``, which has\nshape [3], because it has too many elements.\n\n::\n\n  def diag_embed(vec):\n    assert vec.dim() == 1\n    result = vec.new_zeros(vec.shape[0], vec.shape[0])\n    result.diagonal().copy_(vec)\n    return result\n\n  vecs = torch.tensor([[0., 1, 2], [3., 4, 5]])\n  vmap(diag_embed)(vecs)\n\nReplacing :func:`torch.zeros` with :meth:`Tensor.new_zeros` makes it so that\n``result`` has an underlying Tensor of shape [2, 3, 3], so it is now possible\nto copy ``vec``, which has underlying shape [2, 3], into ``result.diagonal()``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":208,"to":232}}}}],["458",{"pageContent":"Mutation: out= PyTorch Operations\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n:func:`vmap` doesn't support the ``out=`` keyword argument in PyTorch operations.\nIt will error out gracefully if it encounters that in your code.\n\nThis is not a fundamental limitation; we could theoretically support this in the\nfuture but we have chosen not to for now.\n\nData-dependent Python control flow\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nWe don't yet support ``vmap`` over data-dependent control flow. Data-dependent\ncontrol flow is when the condition of an if-statement, while-loop, or\nfor-loop is a Tensor that is being ``vmap``'ed over. For example, the\nfollowing will raise an error message:\n\n::\n\n  def relu(x):\n    if x > 0:\n      return x\n    return 0\n\n  x = torch.randn(3)\n  vmap(relu)(x)\n\nHowever, any control flow that is not dependent on the values in ``vmap``'ed\ntensors will work:\n\n::\n\n  def custom_dot(x):\n    if x.dim() == 1:\n      return torch.dot(x, x)\n    return (x * x).sum()\n\n  x = torch.randn(3)\n  vmap(custom_dot)(x)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":235,"to":271}}}}],["459",{"pageContent":"::\n\n  def custom_dot(x):\n    if x.dim() == 1:\n      return torch.dot(x, x)\n    return (x * x).sum()\n\n  x = torch.randn(3)\n  vmap(custom_dot)(x)\n\nJAX supports transforming over\n`data-dependent control flow <https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators>`_\nusing special control flow operators (e.g. ``jax.lax.cond``, ``jax.lax.while_loop``).\nWe're investigating adding equivalents of those to PyTorch.\n\nData-dependent operations (.item())\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nWe do not (and will not) support vmap over a user-defined function that calls\n``.item()`` on a Tensor. For example, the following will raise an error message:\n\n::\n\n  def f(x):\n    return x.item()\n\n  x = torch.randn(3)\n  vmap(f)(x)\n\nPlease try to rewrite your code to not use ``.item()`` calls.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":271,"to":299}}}}],["460",{"pageContent":"::\n\n  def f(x):\n    return x.item()\n\n  x = torch.randn(3)\n  vmap(f)(x)\n\nPlease try to rewrite your code to not use ``.item()`` calls.\n\nYou may also encounter an error message about using ``.item()`` but you might\nnot have used it. In those cases, it is possible that PyTorch internally is\ncalling ``.item()`` -- please file an issue on GitHub and we'll fix\nPyTorch internals.\n\nDynamic shape operations (nonzero and friends)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n``vmap(f)`` requires that ``f`` applied to every \"example\" in your input\nreturns a Tensor with the same shape. Operations such as ``torch.nonzero``,\n``torch.is_nonzero`` are not supported and will error as a result.\n\nTo see why, consider the following example:\n\n::\n\n  xs = torch.tensor([[0, 1, 2], [0, 0, 3]])\n  vmap(torch.nonzero)(xs)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":299,"to":325}}}}],["461",{"pageContent":"To see why, consider the following example:\n\n::\n\n  xs = torch.tensor([[0, 1, 2], [0, 0, 3]])\n  vmap(torch.nonzero)(xs)\n\n``torch.nonzero(xs[0])`` returns a Tensor of shape 2;\nbut ``torch.nonzero(xs[1])`` returns a Tensor of shape 1.\nWe are unable to construct a single Tensor as an output;\nthe output would need to be a ragged Tensor (and PyTorch does not yet have\nthe concept of a ragged Tensor).\n\n\nRandomness\n----------\nThe user's intention when calling a random operation can be unclear. Specifically, some users may want\nthe random behavior to be the same across batches while others may want it to differ across batches.\nTo address this, ``vmap`` takes a randomness flag.\n\nThe flag can only be passed to vmap and can take on 3 values, \"error,\" \"different,\" or \"same,\" defaulting\nto error. Under \"error\" mode, any call to a random function will produce an error asking the user to use\none of the other two flags based on their use case.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":325,"to":347}}}}],["462",{"pageContent":"Under \"different\" randomness, elements in a batch produce different random values. For instance,\n\n::\n\n  def add_noise(x):\n    y = torch.randn(())  # y will be different across the batch\n    return x + y\n\n  x = torch.ones(3)\n  result = vmap(add_noise, randomness=\"different\")(x)  # we get 3 different values\n\nUnder \"same\" randomness, elements in a batch produce same random values. For instance,\n\n::\n\n  def add_noise(x):\n    y = torch.randn(())  # y will be the same across the batch\n    return x + y\n\n  x = torch.ones(3)\n  result = vmap(add_noise, randomness=\"same\")(x)  # we get the same value, repeated 3 times\n\n\n.. warning::\n    Our system only determine the randomness behavior of PyTorch operators and cannot control the\n    behavior of other libraries, like numpy. This is similar to JAX's limitations with their solutions","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":349,"to":374}}}}],["463",{"pageContent":".. note::\n    Multiple vmap calls using either type of supported randomness will not produce\n    the same results. Like with standard PyTorch, a user can get randomness reproducibility through\n    either using ``torch.manual_seed()`` outside of vmap or by using generators.\n\n.. note::\n    Finally, our randomness differs from JAX because we aren't using a stateless PRNG, in part because PyTorch\n    doesn't have full support for a stateless PRNG. Instead, we've introduced a flag system to allow for the\n    most common forms of randomness that we see. If your use case does not fit these forms of randomness, please\n    file an issue.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.ux_limitations.rst","loc":{"lines":{"from":376,"to":385}}}}],["464",{"pageContent":"torch.func Whirlwind Tour\n=========================\n\nWhat is torch.func?\n-------------------\n\n.. currentmodule:: torch.func\n\ntorch.func, previously known as functorch, is a library for\n`JAX <https://github.com/google/jax>`_-like composable function transforms in\nPyTorch.\n\n- A \"function transform\" is a higher-order function that accepts a numerical\n  function and returns a new function that computes a different quantity.\n- torch.func has auto-differentiation transforms (``grad(f)`` returns a function\n  that computes the gradient of ``f``), a vectorization/batching transform\n  (``vmap(f)`` returns a function that computes ``f`` over batches of inputs),\n  and others.\n- These function transforms can compose with each other arbitrarily. For\n  example, composing ``vmap(grad(f))`` computes a quantity called\n  per-sample-gradients that stock PyTorch cannot efficiently compute today.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.whirlwind_tour.rst","loc":{"lines":{"from":1,"to":21}}}}],["465",{"pageContent":"Why composable function transforms?\n-----------------------------------\nThere are a number of use cases that are tricky to do in PyTorch today:\n- computing per-sample-gradients (or other per-sample quantities)\n\n- running ensembles of models on a single machine\n- efficiently batching together tasks in the inner-loop of MAML\n- efficiently computing Jacobians and Hessians\n- efficiently computing batched Jacobians and Hessians\n\nComposing :func:`vmap`, :func:`grad`, :func:`vjp`, and :func:`jvp` transforms\nallows us to express the above without designing a separate subsystem for each.\n\nWhat are the transforms?\n------------------------\n\n:func:`grad` (gradient computation)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n``grad(func)`` is our gradient computation transform. It returns a new function\nthat computes the gradients of ``func``. It assumes ``func`` returns a single-element\nTensor and by default it computes the gradients of the output of ``func`` w.r.t.\nto the first input.\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.whirlwind_tour.rst","loc":{"lines":{"from":23,"to":47}}}}],["466",{"pageContent":".. code-block:: python\n\n    import torch\n    from torch.func import grad\n    x = torch.randn([])\n    cos_x = grad(lambda x: torch.sin(x))(x)\n    assert torch.allclose(cos_x, x.cos())\n\n    # Second-order gradients\n    neg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)\n    assert torch.allclose(neg_sin_x, -x.sin())\n\n:func:`vmap` (auto-vectorization)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nNote: :func:`vmap` imposes restrictions on the code that it can be used on. For more\ndetails, please see :ref:`ux-limitations`.\n\n``vmap(func)(*inputs)`` is a transform that adds a dimension to all Tensor\noperations in ``func``. ``vmap(func)`` returns a new function that maps ``func``\nover some dimension (default: 0) of each Tensor in inputs.\n\nvmap is useful for hiding batch dimensions: one can write a function func that\nruns on examples and then lift it to a function that can take batches of\nexamples with ``vmap(func)``, leading to a simpler modeling experience:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.whirlwind_tour.rst","loc":{"lines":{"from":47,"to":73}}}}],["467",{"pageContent":".. code-block:: python\n\n    import torch\n    from torch.func import vmap\n    batch_size, feature_size = 3, 5\n    weights = torch.randn(feature_size, requires_grad=True)\n\n    def model(feature_vec):\n        # Very simple linear model with activation\n        assert feature_vec.dim() == 1\n        return feature_vec.dot(weights).relu()\n\n    examples = torch.randn(batch_size, feature_size)\n    result = vmap(model)(examples)\n\nWhen composed with :func:`grad`, :func:`vmap` can be used to compute per-sample-gradients:\n\n.. code-block:: python\n\n    from torch.func import vmap\n    batch_size, feature_size = 3, 5\n\n    def model(weights,feature_vec):\n        # Very simple linear model with activation\n        assert feature_vec.dim() == 1\n        return feature_vec.dot(weights).relu()\n\n    def compute_loss(weights, example, target):\n        y = model(weights, example)\n        return ((y - target) ** 2).mean()  # MSELoss","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.whirlwind_tour.rst","loc":{"lines":{"from":73,"to":102}}}}],["468",{"pageContent":"def compute_loss(weights, example, target):\n        y = model(weights, example)\n        return ((y - target) ** 2).mean()  # MSELoss\n\n    weights = torch.randn(feature_size, requires_grad=True)\n    examples = torch.randn(batch_size, feature_size)\n    targets = torch.randn(batch_size)\n    inputs = (weights,examples, targets)\n    grad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)\n\n:func:`vjp` (vector-Jacobian product)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe :func:`vjp` transform applies ``func`` to ``inputs`` and returns a new function\nthat computes the vector-Jacobian product (vjp) given some ``cotangents`` Tensors.\n\n.. code-block:: python\n\n    from torch.func import vjp\n\n    inputs = torch.randn(3)\n    func = torch.sin\n    cotangents = (torch.randn(3),)\n\n    outputs, vjp_fn = vjp(func, inputs); vjps = vjp_fn(*cotangents)\n\n:func:`jvp` (Jacobian-vector product)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.whirlwind_tour.rst","loc":{"lines":{"from":102,"to":129}}}}],["469",{"pageContent":"outputs, vjp_fn = vjp(func, inputs); vjps = vjp_fn(*cotangents)\n\n:func:`jvp` (Jacobian-vector product)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe :func:`jvp` transforms computes Jacobian-vector-products and is also known as\n\"forward-mode AD\". It is not a higher-order function unlike most other transforms,\nbut it returns the outputs of ``func(inputs)`` as well as the jvps.\n\n.. code-block:: python\n\n    from torch.func import jvp\n    x = torch.randn(5)\n    y = torch.randn(5)\n    f = lambda x, y: (x * y)\n    _, out_tangent = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))\n    assert torch.allclose(out_tangent, x + y)\n\n:func:`jacrev`, :func:`jacfwd`, and :func:`hessian`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe :func:`jacrev` transform returns a new function that takes in ``x`` and returns\nthe Jacobian of the function with respect to ``x`` using reverse-mode AD.\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.whirlwind_tour.rst","loc":{"lines":{"from":129,"to":153}}}}],["470",{"pageContent":"The :func:`jacrev` transform returns a new function that takes in ``x`` and returns\nthe Jacobian of the function with respect to ``x`` using reverse-mode AD.\n\n.. code-block:: python\n\n    from torch.func import jacrev\n    x = torch.randn(5)\n    jacobian = jacrev(torch.sin)(x)\n    expected = torch.diag(torch.cos(x))\n    assert torch.allclose(jacobian, expected)\n\n:func:`jacrev` can be composed with :func:`vmap` to produce batched jacobians:\n\n.. code-block:: python\n\n    x = torch.randn(64, 5)\n    jacobian = vmap(jacrev(torch.sin))(x)\n    assert jacobian.shape == (64, 5, 5)\n\n:func:`jacfwd` is a drop-in replacement for jacrev that computes Jacobians using\nforward-mode AD:\n\n.. code-block:: python\n\n    from torch.func import jacfwd\n    x = torch.randn(5)\n    jacobian = jacfwd(torch.sin)(x)\n    expected = torch.diag(torch.cos(x))\n    assert torch.allclose(jacobian, expected)\n\nComposing :func:`jacrev` with itself or :func:`jacfwd` can produce hessians:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.whirlwind_tour.rst","loc":{"lines":{"from":153,"to":185}}}}],["471",{"pageContent":"Composing :func:`jacrev` with itself or :func:`jacfwd` can produce hessians:\n\n.. code-block:: python\n\n    def f(x):\n        return x.sin().sum()\n\n    x = torch.randn(5)\n    hessian0 = jacrev(jacrev(f))(x)\n    hessian1 = jacfwd(jacrev(f))(x)\n\n:func:`hessian` is a convenience function that combines jacfwd and jacrev:\n\n.. code-block:: python\n\n    from torch.func import hessian\n\n    def f(x):\n        return x.sin().sum()\n\n    x = torch.randn(5)\n    hess = hessian(f)(x)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/func.whirlwind_tour.rst","loc":{"lines":{"from":185,"to":206}}}}],["472",{"pageContent":".. currentmodule:: torch.futures\n\n.. _futures-docs:\n\ntorch.futures\n=============\n\nThis package provides a :class:`~torch.futures.Future` type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non :class:`~torch.futures.Future` objects. Currently, the\n:class:`~torch.futures.Future` type is primarily used by the\n:ref:`distributed-rpc-framework`.\n\n.. automodule:: torch.futures\n\n.. autoclass:: Future\n    :inherited-members:\n\n.. autofunction:: collect_all\n.. autofunction:: wait_all","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/futures.rst","loc":{"lines":{"from":1,"to":20}}}}],["473",{"pageContent":".. currentmodule:: torch.fx\n\ntorch.fx\n=============\n\nOverview\n--------\n.. automodule:: torch.fx\n\n.. _Writing Transformations:\n\n\nWriting Transformations\n-----------------------\n\nWhat is an FX transform? Essentially, it's a function that looks like this.\n\n::\n\n    import torch\n    import torch.fx\n\n    def transform(m: nn.Module,\n                  tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n        # Step 1: Acquire a Graph representing the code in `m`\n\n        # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n        # fx.Tracer.trace and constructing a GraphModule. We'll\n        # split that out in our transform to allow the caller to\n        # customize tracing behavior.\n        graph : torch.fx.Graph = tracer_class().trace(m)\n\n        # Step 2: Modify this Graph or create a new one\n        graph = ...\n\n        # Step 3: Construct a Module to return\n        return torch.fx.GraphModule(m, graph)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":1,"to":37}}}}],["474",{"pageContent":"# Step 2: Modify this Graph or create a new one\n        graph = ...\n\n        # Step 3: Construct a Module to return\n        return torch.fx.GraphModule(m, graph)\n\nYour transform will take in a :class:`torch.nn.Module`, acquire a :class:`Graph`\nfrom it, do some modifications, and return a new\n:class:`torch.nn.Module`. You should think of the :class:`torch.nn.Module` that your FX\ntransform returns as identical to a regular :class:`torch.nn.Module` -- you can pass it to another\nFX transform, you can pass it to TorchScript, or you can\nrun it. Ensuring that the inputs and outputs of your FX transform are a\n:class:`torch.nn.Module` will allow for composability.\n\n.. note::\n\n    It is also possible to modify an existing :class:`GraphModule` instead of\n    creating a new one, like so::\n\n        import torch\n        import torch.fx\n\n        def transform(m : nn.Module) -> nn.Module:\n            gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n            # Modify gm.graph\n            # <...>","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":37,"to":63}}}}],["475",{"pageContent":"def transform(m : nn.Module) -> nn.Module:\n            gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n            # Modify gm.graph\n            # <...>\n\n            # Recompile the forward() method of `gm` from its Graph\n            gm.recompile()\n\n            return gm\n\n    Note that you MUST call :meth:`GraphModule.recompile` to bring the generated\n    ``forward()`` method on the ``GraphModule`` in sync with the modified :class:`Graph`.\n\nGiven that you’ve passed in a :class:`torch.nn.Module` that has been traced into a\n:class:`Graph`, there are now two primary approaches you can take to building a new\n:class:`Graph`.\n\nA Quick Primer on Graphs\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nFull treatment of the semantics of graphs can be found in the :class:`Graph`\ndocumentation, but we are going to cover the basics here. A :class:`Graph` is\na data structure that represents a method on a :class:`GraphModule`. The\ninformation that this requires is:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":63,"to":87}}}}],["476",{"pageContent":"- What are the inputs to the method?\n- What are the operations that run inside the method?\n- What is the output (i.e. return) value from the method?\n\nAll three of these concepts are represented with :class:`Node` instances.\nLet's see what we mean by that with a short example:\n\n::\n\n    import torch\n    import torch.fx\n\n    class MyModule(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.param = torch.nn.Parameter(torch.rand(3, 4))\n            self.linear = torch.nn.Linear(4, 5)\n\n        def forward(self, x):\n            return torch.topk(torch.sum(\n                self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\n    m = MyModule()\n    gm = torch.fx.symbolic_trace(m)\n\n    gm.graph.print_tabular()\n\nHere we define a module ``MyModule`` for demonstration purposes, instantiate it,\nsymbolically trace it, then call the :meth:`Graph.print_tabular` method to print\nout a table showing the nodes of this :class:`Graph`:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":89,"to":118}}}}],["477",{"pageContent":"+---------------+---------------+----------------------------+--------------------+-------------+\n    | opcode        | name          | target                     | args               | kwargs      |\n    +===============+===============+============================+====================+=============+\n    | placeholder   | x             | x                          | ()                 | {}          |\n    +---------------+---------------+----------------------------+--------------------+-------------+\n    | get_attr      | linear_weight | linear.weight              | ()                 | {}          |\n    +---------------+---------------+----------------------------+--------------------+-------------+\n    | call_function | add_1         | <built-in function add>    | (x, linear_weight) | {}          |\n    +---------------+---------------+----------------------------+--------------------+-------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":120,"to":128}}}}],["478",{"pageContent":"+---------------+---------------+----------------------------+--------------------+-------------+\n    | call_module   | linear_1      | linear                     | (add_1,)           | {}          |\n    +---------------+---------------+----------------------------+--------------------+-------------+\n    | call_method   | relu_1        | relu                       | (linear_1,)        | {}          |\n    +---------------+---------------+----------------------------+--------------------+-------------+\n    | call_function | sum_1         | <built-in method sum ...>  | (relu_1,)          | {'dim': -1} |\n    +---------------+---------------+----------------------------+--------------------+-------------+\n    | call_function | topk_1        | <built-in method topk ...> | (sum_1, 3)         | {}          |\n    +---------------+---------------+----------------------------+--------------------+-------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":128,"to":136}}}}],["479",{"pageContent":"+---------------+---------------+----------------------------+--------------------+-------------+\n    | output        | output        | output                     | (topk_1,)          | {}          |\n    +---------------+---------------+----------------------------+--------------------+-------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":136,"to":138}}}}],["480",{"pageContent":"We can use this information to answer the questions we posed above.\n\n- What are the inputs to the method? In FX, method inputs are specified\n  via special ``placeholder`` nodes. In this case, we have a single\n  ``placeholder`` node with a ``target`` of ``x``, meaning we have\n  a single (non-self) argument named x.\n- What are the operations within the method? The ``get_attr``,\n  ``call_function``, ``call_module``, and ``call_method`` nodes\n  represent the operations in the method. A full treatment of\n  the semantics of all of these can be found in the :class:`Node`\n  documentation.\n- What is the return value of the method? The return value in a\n  :class:`Graph` is specified by a special ``output`` node.\n\nGiven that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a :class:`Graph`.\n\nGraph Manipulation\n^^^^^^^^^^^^^^^^^^\n\nDirect Graph Manipulation\n~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":140,"to":161}}}}],["481",{"pageContent":"Graph Manipulation\n^^^^^^^^^^^^^^^^^^\n\nDirect Graph Manipulation\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nOne approach to building this new :class:`Graph` is to directly manipulate your old\none. To aid in this, we can simply take the :class:`Graph` we obtain from symbolic\ntracing and modify it. For example, let’s say we desire to replace\n:func:`torch.add` calls with :func:`torch.mul` calls.\n\n::\n\n    import torch\n    import torch.fx\n\n    # Sample module\n    class M(torch.nn.Module):\n        def forward(self, x, y):\n            return torch.add(x, y)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":161,"to":180}}}}],["482",{"pageContent":"::\n\n    import torch\n    import torch.fx\n\n    # Sample module\n    class M(torch.nn.Module):\n        def forward(self, x, y):\n            return torch.add(x, y)\n\n    def transform(m: torch.nn.Module,\n                  tracer_class : type = fx.Tracer) -> torch.nn.Module:\n        graph : fx.Graph = tracer_class().trace(m)\n        # FX represents its Graph as an ordered list of\n        # nodes, so we can iterate through them.\n        for node in graph.nodes:\n            # Checks if we're calling a function (i.e:\n            # torch.add)\n            if node.op == 'call_function':\n                # The target attribute is the function\n                # that call_function calls.\n                if node.target == torch.add:\n                    node.target = torch.mul\n\n        graph.lint() # Does some checks to make sure the\n                     # Graph is well-formed.\n\n        return fx.GraphModule(m, graph)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":180,"to":207}}}}],["483",{"pageContent":"graph.lint() # Does some checks to make sure the\n                     # Graph is well-formed.\n\n        return fx.GraphModule(m, graph)\n\n\nWe can also do more involved :class:`Graph` rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the :class:`Graph` documentation. An\nexample of using these APIs to append a :func:`torch.relu` call\ncan be found below.\n\n::\n\n    # Specifies the insertion point. Any nodes added to the\n    # Graph within this scope will be inserted after `node`\n    with traced.graph.inserting_after(node):\n        # Insert a new `call_function` node calling `torch.relu`\n        new_node = traced.graph.call_function(\n            torch.relu, args=(node,))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":207,"to":227}}}}],["484",{"pageContent":"# We want all places that used the value of `node` to\n        # now use that value after the `relu` call we've added.\n        # We use the `replace_all_uses_with` API to do this.\n        node.replace_all_uses_with(new_node)\n\nFor simple transformations that only consist of substitutions, you can also\nmake use of the `subgraph rewriter. <https://github.com/pytorch/pytorch/blob/master/torch/fx/subgraph_rewriter.py>`__\n\nSubgraph Rewriting With replace_pattern()\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":229,"to":238}}}}],["485",{"pageContent":"Subgraph Rewriting With replace_pattern()\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFX also provides another level of automation on top of direct graph manipulation.\nThe :func:`replace_pattern` API is essentially a \"find/replace\" tool for editing\n:class:`Graph`\\s. It allows you to specify a ``pattern`` and ``replacement`` function\nand it will trace through those functions, find instances of the group of operations\nin the ``pattern`` graph, and replace those instances with copies of the ``replacement``\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex.\n\nGraph Manipulation Examples\n~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":238,"to":250}}}}],["486",{"pageContent":"Graph Manipulation Examples\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  `Replace one\n   op <https://github.com/pytorch/examples/blob/master/fx/replace_op.py>`__\n-  `Conv/Batch Norm\n   fusion <https://github.com/pytorch/pytorch/blob/40cbf342d3c000712da92cfafeaca651b3e0bd3e/torch/fx/experimental/optimization.py#L50>`__\n-  `replace_pattern: Basic usage <https://github.com/pytorch/examples/blob/master/fx/subgraph_rewriter_basic_use.py>`__\n-  `Quantization <https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization>`__\n-  `Invert Transformation <https://github.com/pytorch/examples/blob/master/fx/invert.py>`__\n\nProxy/Retracing\n^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":250,"to":262}}}}],["487",{"pageContent":"Proxy/Retracing\n^^^^^^^^^^^^^^^\n\nAnother way of manipulating :class:`Graph`\\s is by reusing the :class:`Proxy`\nmachinery used in symbolic tracing. For example, let’s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\n``F.relu(x)`` call into ``(x > 0) * x``. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the ``F.relu``, and then clean up the original\n``F.relu``. However, we can automate this process by using :class:`Proxy`\nobjects to automatically record operations into the :class:`Graph`.\n\nTo use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with :class:`Proxy` objects as arguments.\nThese :class:`Proxy` objects will capture the operations that are performed\non them and append them to the :class:`Graph`.\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":262,"to":280}}}}],["488",{"pageContent":"::\n\n    # Note that this decomposition rule can be read as regular Python\n    def relu_decomposition(x):\n        return (x > 0) * x\n\n    decomposition_rules = {}\n    decomposition_rules[F.relu] = relu_decomposition","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":280,"to":287}}}}],["489",{"pageContent":"def decompose(model: torch.nn.Module,\n                  tracer_class : type = fx.Tracer) -> torch.nn.Module:\n        \"\"\"\n        Decompose `model` into smaller constituent operations.\n        Currently,this only supports decomposing ReLU into its\n        mathematical definition: (x > 0) * x\n        \"\"\"\n        graph : fx.Graph = tracer_class().trace(model)\n        new_graph = fx.Graph()\n        env = {}\n        tracer = torch.fx.proxy.GraphAppendingTracer(new_graph)\n        for node in graph.nodes:\n            if node.op == 'call_function' and node.target in decomposition_rules:\n                # By wrapping the arguments with proxies,\n                # we can dispatch to the appropriate\n                # decomposition rule and implicitly add it\n                # to the Graph by symbolically tracing it.\n                proxy_args = [\n                    fx.Proxy(env[x.name], tracer) if isinstance(x, fx.Node) else x for x in node.args]","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":289,"to":307}}}}],["490",{"pageContent":"# to the Graph by symbolically tracing it.\n                proxy_args = [\n                    fx.Proxy(env[x.name], tracer) if isinstance(x, fx.Node) else x for x in node.args]\n                output_proxy = decomposition_rules[node.target](*proxy_args)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":307,"to":310}}}}],["491",{"pageContent":"# Operations on `Proxy` always yield new `Proxy`s, and the\n                # return value of our decomposition rule is no exception.\n                # We need to extract the underlying `Node` from the `Proxy`\n                # to use it in subsequent iterations of this transform.\n                new_node = output_proxy.node\n                env[node.name] = new_node\n            else:\n                # Default case: we don't have a decomposition rule for this\n                # node, so just copy the node over into the new graph.\n                new_node = new_graph.node_copy(node, lambda x: env[x.name])\n                env[node.name] = new_node\n        return fx.GraphModule(model, new_graph)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":312,"to":323}}}}],["492",{"pageContent":"In addition to avoiding explicit graph manipulation, using :class:`Proxy`\\s\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. Note that while calling :class:`Proxy` we also\npassed a tracer pointing to the underlying variable `graph`. This is done so\nif in case the operations in graph are n-ary (e.g. add is a binary operator)\nthe call to :class:`Proxy` does not create multiple instances of a graph\ntracer which can lead to unexpected runtime errors. We recommend this method\nof using :class:`Proxy` especially when the underlying operators can not be\nsafely assumed to be unary.\n\nA worked example of using :class:`Proxy`\\s for :class:`Graph` manipulation\ncan be found\n`here <https://github.com/pytorch/examples/blob/master/fx/proxy_based_graph_creation.py>`__.\n\nThe Interpreter Pattern\n^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":325,"to":342}}}}],["493",{"pageContent":"The Interpreter Pattern\n^^^^^^^^^^^^^^^^^^^^^^^\n\nA useful code organizational pattern in FX is to loop over all the :class:`Node`\\s\nin a :class:`Graph` and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with :class:`Proxy`\\s. For example, suppose we want to run a\n:class:`GraphModule` and record the :class:`torch.Tensor` shape and dtype\nproperties on the nodes as we see them at runtime. That might look like:\n\n::\n\n    import torch\n    import torch.fx\n    from torch.fx.node import Node\n\n    from typing import Dict","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":342,"to":358}}}}],["494",{"pageContent":"::\n\n    import torch\n    import torch.fx\n    from torch.fx.node import Node\n\n    from typing import Dict\n\n    class ShapeProp:\n        \"\"\"\n        Shape propagation. This class takes a `GraphModule`.\n        Then, its `propagate` method executes the `GraphModule`\n        node-by-node with the given arguments. As each operation\n        executes, the ShapeProp class stores away the shape and\n        element type for the output values of each operation on\n        the `shape` and `dtype` attributes of the operation's\n        `Node`.\n        \"\"\"\n        def __init__(self, mod):\n            self.mod = mod\n            self.graph = mod.graph\n            self.modules = dict(self.mod.named_modules())\n\n        def propagate(self, *args):\n            args_iter = iter(args)\n            env : Dict[str, Node] = {}\n\n            def load_arg(a):\n                return torch.fx.graph.map_arg(a, lambda n: env[n.name])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":358,"to":386}}}}],["495",{"pageContent":"def load_arg(a):\n                return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n            def fetch_attr(target : str):\n                target_atoms = target.split('.')\n                attr_itr = self.mod\n                for i, atom in enumerate(target_atoms):\n                    if not hasattr(attr_itr, atom):\n                        raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n                    attr_itr = getattr(attr_itr, atom)\n                return attr_itr","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":386,"to":396}}}}],["496",{"pageContent":"for node in self.graph.nodes:\n                if node.op == 'placeholder':\n                    result = next(args_iter)\n                elif node.op == 'get_attr':\n                    result = fetch_attr(node.target)\n                elif node.op == 'call_function':\n                    result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n                elif node.op == 'call_method':\n                    self_obj, *args = load_arg(node.args)\n                    kwargs = load_arg(node.kwargs)\n                    result = getattr(self_obj, node.target)(*args, **kwargs)\n                elif node.op == 'call_module':\n                    result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":398,"to":410}}}}],["497",{"pageContent":"# This is the only code specific to shape propagation.\n                # you can delete this `if` branch and this becomes\n                # a generic GraphModule interpreter.\n                if isinstance(result, torch.Tensor):\n                    node.shape = result.shape\n                    node.dtype = result.dtype\n\n                env[node.name] = result\n\n            return load_arg(self.graph.result)\n\nAs you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe :class:`Interpreter` class, which encompasses the above logic\nin a way that certain aspects of the interpreter's execution can\nbe overridden via method overrides.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":412,"to":427}}}}],["498",{"pageContent":"In addition to executing operations, we can also generate a new\n`Graph` by feeding :class:`Proxy` values through an interpreter.\nSimilarly, we provide the :class:`Transformer` class to encompass\nthis pattern. :class:`Transformer` behaves similarly to\n:class:`Interpreter`, but instead of calling the ``run`` method to\nget a concrete output value from the Module, you would call the\n:meth:`Transformer.transform` method to return a new\n:class:`GraphModule` which was subject to any transformation rules\nyou installed as overridden methods.\n\nExamples of the Interpreter Pattern\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n-  `Shape\n   Propagation <https://github.com/pytorch/pytorch/blob/master/torch/fx/passes/shape_prop.py>`__\n-  `Performance Profiler <https://github.com/pytorch/tutorials/pull/1319>`__\n\n\nDebugging\n-----------\n\nIntroduction\n^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":429,"to":451}}}}],["499",{"pageContent":"Debugging\n-----------\n\nIntroduction\n^^^^^^^^^^^^^^^^\n\nOften in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code.\n\nIf you’re not familiar with debuggers, please see the auxiliary section\n:ref:`Available Debuggers`.\n\n\nCommon Pitfalls in Transform Authoring\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":451,"to":468}}}}],["500",{"pageContent":"If you’re not familiar with debuggers, please see the auxiliary section\n:ref:`Available Debuggers`.\n\n\nCommon Pitfalls in Transform Authoring\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n* Nondeterministic ``set`` iteration order. In Python, the ``set`` datatype is\n  unordered. Using ``set`` to contain collections of objects like ``Node``\\ s,\n  for example, can cause unexpected nondeterminism. An example is iterating\n  over a set of ``Node``\\ s to insert them into a ``Graph``. Because the\n  ``set`` data type is unordered, the ordering of the operations in the output\n  program will be nondeterministic and can change across program invocations.\n  The recommended alternative is to use a ``dict`` data type, which is\n  `insertion ordered <https://mail.python.org/pipermail/python-dev/2017-December/151283.html>`_\n  as of Python 3.7 (and as of cPython 3.6). A ``dict`` can be used equivalently\n  to a set by storing values to be deduplicated in the keys of the ``dict``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":468,"to":484}}}}],["501",{"pageContent":"Checking Correctness of Modules\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBecause the output of most deep learning modules consists of floating\npoint :class:`torch.Tensor` instances, checking for equivalence between\nthe results of two :class:`torch.nn.Module` is not as straightforward\nas doing a simple equality check. To motivate this, let's use an\nexample:\n\n::\n\n    import torch\n    import torch.fx\n    import torchvision.models as models\n\n    def transform(m : torch.nn.Module) -> torch.nn.Module:\n        gm = torch.fx.symbolic_trace(m)\n\n        # Imagine we're doing some transforms here\n        # <...>\n\n        gm.recompile()\n\n        return gm\n\n    resnet18 = models.resnet18()\n    transformed_resnet18 = transform(resnet18)\n\n    input_image = torch.randn(5, 3, 224, 224)\n\n    assert resnet18(input_image) == transformed_resnet18(input_image)\n    \"\"\"\n    RuntimeError: Boolean value of Tensor with more than one value is ambiguous\n    \"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":486,"to":519}}}}],["502",{"pageContent":"assert resnet18(input_image) == transformed_resnet18(input_image)\n    \"\"\"\n    RuntimeError: Boolean value of Tensor with more than one value is ambiguous\n    \"\"\"\n\nHere, we've tried to check equality of the values of two deep learning\nmodels with the ``==`` equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\n`here <https://floating-point-gui.de/errors/comparison/>`__ for more\ndetails). We can use :func:`torch.allclose` instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold:\n\n::\n\n    assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":519,"to":537}}}}],["503",{"pageContent":"::\n\n    assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))\n\nThis is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation.\n\nDebugging the Generated Code\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBecause FX generates the ``forward()`` function on :class:`GraphModule`\\s, using\ntraditional debugging techniques like ``print`` statements or ``pdb`` is\nnot as straightforward. Luckily, we have several techniques we can use\nfor debugging the generated code.\n\nUse ``pdb``\n~~~~~~~~~~~~~\nInvoke ``pdb`` to step into the running program. Although the code that\nrepresents the :class:`Graph` is not in any source file, we can still step\ninto it manually using ``pdb`` when the forward pass is invoked.\n\n::\n\n    import torch\n    import torch.fx\n    import torchvision.models as models","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":537,"to":562}}}}],["504",{"pageContent":"::\n\n    import torch\n    import torch.fx\n    import torchvision.models as models\n\n    def my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n        graph = tracer_class().trace(inp)\n        # Transformation logic here\n        # <...>\n\n        # Return new Module\n        return fx.GraphModule(inp, graph)\n\n    my_module = models.resnet18()\n    my_module_transformed = my_pass(my_module)\n\n    input_value = torch.randn(5, 3, 224, 224)\n\n    # When this line is executed at runtime, we will be dropped into an\n    # interactive `pdb` prompt. We can use the `step` or `s` command to\n    # step into the execution of the next line\n    import pdb; pdb.set_trace()\n\n    my_module_transformed(input_value)\n\n.. _Print the Generated Code:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":562,"to":588}}}}],["505",{"pageContent":"my_module_transformed(input_value)\n\n.. _Print the Generated Code:\n\nPrint the Generated Code\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\nIf you’d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with ``pdb``. In that case, one\napproach is to simply copy-paste the generated ``forward`` pass into\nyour code and examine it from there.\n\n::\n\n    # Assume that `traced` is a GraphModule that has undergone some\n    # number of transforms\n\n    # Copy this code for later\n    print(traced)\n    # Print the code generated from symbolic tracing. This outputs:\n    \"\"\"\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n    \"\"\"\n\n    # Subclass the original Module\n    class SubclassM(M):\n        def __init__(self):\n            super().__init__()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":588,"to":617}}}}],["506",{"pageContent":"# Subclass the original Module\n    class SubclassM(M):\n        def __init__(self):\n            super().__init__()\n\n        # Paste the generated `forward` function (the one we printed and\n        # copied above) here\n        def forward(self, y):\n            x = self.x\n            add_1 = x + y;  x = y = None\n            return add_1\n\n    # Create an instance of the original, untraced Module. Then, create an\n    # instance of the Module with the copied `forward` function. We can\n    # now compare the output of both the original and the traced version.\n    pre_trace = M()\n    post_trace = SubclassM()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":617,"to":633}}}}],["507",{"pageContent":"Use the ``to_folder`` Function From ``GraphModule``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n:meth:`GraphModule.to_folder` is a method in ``GraphModule`` that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in :ref:`Print the Generated Code`,\nit may be easier to examine modules and parameters using ``to_folder``.\n\n::\n\n    m = symbolic_trace(M())\n    m.to_folder(\"foo\", \"Bar\")\n    from foo import Bar\n    y = Bar()\n\nAfter running the above example, we can then look at the code within\n``foo/module.py`` and modify it as desired (e.g. adding ``print``\nstatements or using ``pdb``) to debug the generated code.\n\nDebugging the Transformation\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":635,"to":654}}}}],["508",{"pageContent":"Debugging the Transformation\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nNow that we've identified that a transformation is creating incorrect\ncode, it's time to debug the transformation itself. First, we'll check\nthe :ref:`Limitations of Symbolic Tracing` section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our ``GraphModule``\ntransformation. There may be a quick answer in\n:ref:`Writing Transformations`, but, if not, there are several ways to\nexamine our traced module:\n\n::\n\n    # Sample Module\n    class M(torch.nn.Module):\n        def forward(self, x, y):\n            return x + y\n\n    # Create an instance of `M`\n    m = M()\n\n    # Symbolically trace an instance of `M` (returns a GraphModule). In\n    # this example, we'll only be discussing how to inspect a\n    # GraphModule, so we aren't showing any sample transforms for the\n    # sake of brevity.\n    traced = symbolic_trace(m)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":654,"to":680}}}}],["509",{"pageContent":"# Print the code produced by tracing the module.\n    print(traced)\n    # The generated `forward` function is:\n    \"\"\"\n    def forward(self, x, y):\n        add = x + y;  x = y = None\n        return add\n    \"\"\"\n\n    # Print the internal Graph.\n    print(traced.graph)\n    # This print-out returns:\n    \"\"\"\n    graph():\n        %x : [#users=1] = placeholder[target=x]\n        %y : [#users=1] = placeholder[target=y]\n        %add : [#users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {})\n        return add\n    \"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":682,"to":700}}}}],["510",{"pageContent":"# Print a tabular representation of the internal Graph.\n    traced.graph.print_tabular()\n    # This gives us:\n    \"\"\"\n    opcode         name    target                   args    kwargs\n    -------------  ------  -----------------------  ------  --------\n    placeholder    x       x                        ()      {}\n    placeholder    y       y                        ()      {}\n    call_function  add     <built-in function add>  (x, y)  {}\n    output         output  output                   (add,)  {}\n    \"\"\"\n\nUsing the utility functions above, we can compare our traced Module\nbefore and after we've applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it's still\nnot clear what's going wrong, a debugger like ``pdb`` can be a good\nnext step.\n\nGoing off of the example above, consider the following code:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":702,"to":722}}}}],["511",{"pageContent":"Going off of the example above, consider the following code:\n\n::\n\n    # Sample user-defined function\n    def transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n        # Get the Graph from our traced Module\n        g = tracer_class().trace(module)\n\n        \"\"\"\n        Transformations on `g` go here\n        \"\"\"\n\n        return fx.GraphModule(module, g)\n\n    # Transform the Graph\n    transformed = transform_graph(traced)\n\n    # Print the new code after our transforms. Check to see if it was\n    # what we expected\n    print(transformed)\n\nUsing the above example, let’s say that the call to ``print(traced)``\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a ``pdb`` session. We can see\nwhat’s happening during the transform by breaking on\n``transform_graph(traced)``, then pressing ``s`` to “step into” the call\nto ``transform_graph(traced)``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":722,"to":749}}}}],["512",{"pageContent":"We may also have good luck by editing the ``print_tabular`` method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node’s ``input_nodes`` and ``users``.)\n\n.. _Available Debuggers:\n\nAvailable Debuggers\n^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":751,"to":758}}}}],["513",{"pageContent":"The most common Python debugger is\n`pdb <https://docs.python.org/3/library/pdb.html>`__. You can start\nyour program in “debug mode” with ``pdb`` by typing\n``python -m pdb FILENAME.py`` into the command line, where ``FILENAME``\nis the name of the file you want to debug. After that, you can use the\n``pdb`` `debugger commands\n<https://docs.python.org/3/library/pdb.html#debugger-commands>`__\nto move through your running program stepwise. It’s common to set a\nbreakpoint (``b LINE-NUMBER``) when you start ``pdb``, then call ``c`` to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using ``s`` or ``n``) to get to the part\nof the code you want to examine. Alternatively, you can write\n``import pdb; pdb.set_trace()`` before the line you want to break at.\nIf you add ``pdb.set_trace()``, your program will automatically start\nin debug mode when you run it. (In other words, you can just type\n``python FILENAME.py`` into the command line instead of","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":760,"to":775}}}}],["514",{"pageContent":"If you add ``pdb.set_trace()``, your program will automatically start\nin debug mode when you run it. (In other words, you can just type\n``python FILENAME.py`` into the command line instead of\n``python -m pdb FILENAME.py``.) Once you're running your file in\ndebug mode, you can step through the code and examine your program's\ninternal state using certain commands. There are many excellent\ntutorials on ``pdb`` online, including RealPython’s\n`“Python Debugging With Pdb” <https://realpython.com/python-debugging-pdb/>`__.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":775,"to":782}}}}],["515",{"pageContent":"IDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use ``pdb`` by pulling up a terminal\nwindow in your IDE (e.g. View → Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around ``pdb``).\n\n.. _Limitations of Symbolic Tracing:\n\nLimitations of Symbolic Tracing\n-------------------------------\n\nFX uses a system of **symbolic tracing** (a.k.a `symbolic\nexecution <https://en.wikipedia.org/wiki/Symbolic_execution>`__)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is **tracing** in that it executes the program (really a\n:class:`torch.nn.Module` or function) to record operations. It is\n**symbolic** in that the data flowing through the program during this\nexecution is not real data, but rather symbols (:class:`Proxy` in FX parlance).\n\nAlthough symbolic tracing works for most neural net code, it has some\nlimitations.\n\nDynamic Control Flow\n^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":784,"to":806}}}}],["516",{"pageContent":"Although symbolic tracing works for most neural net code, it has some\nlimitations.\n\nDynamic Control Flow\n^^^^^^^^^^^^^^^^^^^^\n\nThe main limitation of symbolic tracing is it does not currently support\n*dynamic control flow*. That is, loops or ``if`` statements where the\ncondition may depend on the input values of the program.\n\nFor example, let’s examine the following program:\n\n::\n\n    def func_to_trace(x):\n        if x.sum() > 0:\n            return torch.relu(x)\n        else:\n            return torch.neg(x)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":806,"to":824}}}}],["517",{"pageContent":"For example, let’s examine the following program:\n\n::\n\n    def func_to_trace(x):\n        if x.sum() > 0:\n            return torch.relu(x)\n        else:\n            return torch.neg(x)\n\n    traced = torch.fx.symbolic_trace(func_to_trace)\n    \"\"\"\n      <...>\n      File \"dyn.py\", line 6, in func_to_trace\n        if x.sum() > 0:\n      File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n        return self.tracer.to_bool(self)\n      File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n        raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\n    torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n    \"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":824,"to":844}}}}],["518",{"pageContent":"The condition to the ``if`` statement relies on the value of ``x.sum()``,\nwhich relies on the value of ``x``, a function input. Since\n``x`` can change (i.e. if you pass a new input tensor to the traced\nfunction), this is *dynamic control flow*. The traceback walks back up\nthrough your code to show you where this situation happens.\n\nStatic Control Flow\n~~~~~~~~~~~~~~~~~~~\n\nOn the other hand, so-called *static control flow* is supported. Static\ncontrol flow is loops or ``if`` statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model’s architecture based on\nhyper-parameters. As a concrete example:\n\n::\n\n    import torch\n    import torch.fx\n\n    class MyModule(torch.nn.Module):\n        def __init__(self, do_activation : bool = False):\n            super().__init__()\n            self.do_activation = do_activation\n            self.linear = torch.nn.Linear(512, 512)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":846,"to":870}}}}],["519",{"pageContent":"def forward(self, x):\n            x = self.linear(x)\n            # This if-statement is so-called static control flow.\n            # Its condition does not depend on any input values\n            if self.do_activation:\n                x = torch.relu(x)\n            return x\n\n    without_activation = MyModule(do_activation=False)\n    with_activation = MyModule(do_activation=True)\n\n    traced_without_activation = torch.fx.symbolic_trace(without_activation)\n    print(traced_without_activation.code)\n    \"\"\"\n    def forward(self, x):\n        linear_1 = self.linear(x);  x = None\n        return linear_1\n    \"\"\"\n\n    traced_with_activation = torch.fx.symbolic_trace(with_activation)\n    print(traced_with_activation.code)\n    \"\"\"\n    import torch\n    def forward(self, x):\n        linear_1 = self.linear(x);  x = None\n        relu_1 = torch.relu(linear_1);  linear_1 = None\n        return relu_1\n    \"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":872,"to":899}}}}],["520",{"pageContent":"The if-statement ``if self.do_activation`` does not depend on any\nfunction inputs, thus it is static. ``do_activation`` can be considered\nto be a hyper-parameter, and the traces of different instances of\n``MyModule`` with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.\n\nMany instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to ``Module`` attributes or by binding concrete values to arguments\nduring symbolic tracing:\n\n::\n\n        def f(x, flag):\n            if flag: return x\n            else: return x*2\n\n        fx.symbolic_trace(f) # Fails!\n\n        fx.symbolic_trace(f, concrete_args={'flag': True})","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":901,"to":921}}}}],["521",{"pageContent":"::\n\n        def f(x, flag):\n            if flag: return x\n            else: return x*2\n\n        fx.symbolic_trace(f) # Fails!\n\n        fx.symbolic_trace(f, concrete_args={'flag': True})\n\nIn the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\n:ref:`Customizing Tracing`) or function (see\n:func:`wrap`) rather than tracing through them.\n\nNon-\\ ``torch`` Functions\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nFX uses ``__torch_function__`` as the mechanism by which it intercepts\ncalls (see the `technical\noverview <https://github.com/pytorch/pytorch/blob/master/torch/fx/OVERVIEW.md#technical-details>`__\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the ``math`` module, are not covered by\n``__torch_function__``, but we would still like to capture them in\nsymbolic tracing. For example:\n\n::\n\n    import torch\n    import torch.fx\n    from math import sqrt","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":921,"to":951}}}}],["522",{"pageContent":"::\n\n    import torch\n    import torch.fx\n    from math import sqrt\n\n    def normalize(x):\n        \"\"\"\n        Normalize `x` by the size of the batch dimension\n        \"\"\"\n        return x / sqrt(len(x))\n\n    # It's valid Python code\n    normalize(torch.rand(3, 4))\n\n    traced = torch.fx.symbolic_trace(normalize)\n    \"\"\"\n      <...>\n      File \"sqrt.py\", line 9, in normalize\n        return x / sqrt(len(x))\n      File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n        raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\n    RuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n    \"\"\"\n\nThe error tells us that the built-in function ``len`` is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the :func:`wrap` API:\n\n::\n\n    torch.fx.wrap('len')\n    torch.fx.wrap('sqrt')","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":951,"to":983}}}}],["523",{"pageContent":"::\n\n    torch.fx.wrap('len')\n    torch.fx.wrap('sqrt')\n\n    traced = torch.fx.symbolic_trace(normalize)\n\n    print(traced.code)\n    \"\"\"\n    import math\n    def forward(self, x):\n        len_1 = len(x)\n        sqrt_1 = math.sqrt(len_1);  len_1 = None\n        truediv = x / sqrt_1;  x = sqrt_1 = None\n        return truediv\n    \"\"\"\n\n.. _Customizing Tracing:\n\nCustomizing Tracing with the ``Tracer`` class\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe :class:`Tracer` class is the class that underlies the\nimplementation of ``symbolic_trace``. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:\n\n::\n\n    class MyCustomTracer(torch.fx.Tracer):\n        # Inside here you can override various methods\n        # to customize tracing. See the `Tracer` API\n        # reference\n        pass\n\n\n    # Let's use this custom tracer to trace through this module\n    class MyModule(torch.nn.Module):\n        def forward(self, x):\n            return torch.relu(x) + torch.ones(3, 4)\n\n    mod = MyModule()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":983,"to":1023}}}}],["524",{"pageContent":"mod = MyModule()\n\n    traced_graph = MyCustomTracer().trace(mod)\n    # trace() returns a Graph. Let's wrap it up in a\n    # GraphModule to make it runnable\n    traced = torch.fx.GraphModule(mod, traced_graph)\n\nLeaf Modules\n~~~~~~~~~~~~\n\nLeaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard ``torch.nn`` module instances. For example:\n\n::\n\n    class MySpecialSubmodule(torch.nn.Module):\n        def forward(self, x):\n            return torch.neg(x)\n\n    class MyModule(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.linear = torch.nn.Linear(3, 4)\n            self.submod = MySpecialSubmodule()\n\n        def forward(self, x):\n            return self.submod(self.linear(x))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":1023,"to":1050}}}}],["525",{"pageContent":"def forward(self, x):\n            return self.submod(self.linear(x))\n\n    traced = torch.fx.symbolic_trace(MyModule())\n    print(traced.code)\n    # `linear` is preserved as a call, yet `submod` is traced though.\n    # This is because the default set of \"Leaf Modules\" includes all\n    # standard `torch.nn` modules.\n    \"\"\"\n    import torch\n    def forward(self, x):\n        linear_1 = self.linear(x);  x = None\n        neg_1 = torch.neg(linear_1);  linear_1 = None\n        return neg_1\n    \"\"\"\n\nThe set of leaf modules can be customized by overriding\n:meth:`Tracer.is_leaf_module`.\n\nMiscellanea\n^^^^^^^^^^^\n\n-  Tensor constructors (e.g. ``torch.zeros``, ``torch.ones``,\n   ``torch.rand``, ``torch.randn``, ``torch.sparse_coo_tensor``)\n   are currently not traceable.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":1050,"to":1074}}}}],["526",{"pageContent":"Miscellanea\n^^^^^^^^^^^\n\n-  Tensor constructors (e.g. ``torch.zeros``, ``torch.ones``,\n   ``torch.rand``, ``torch.randn``, ``torch.sparse_coo_tensor``)\n   are currently not traceable.\n\n   -  The deterministic constructors (``zeros``, ``ones``) can be used\n      and the value they produce will be embedded in the trace as a\n      constant. This is only problematic if the arguments to these\n      constructors refers to dynamic input sizes. In this case,\n      ``ones_like`` or ``zeros_like`` may be a viable substitute.\n   -  Nondeterministic constructors (``rand``, ``randn``) will have a\n      single random value embedded in the trace. This is likely not the\n      intended behavior. One workaround is to wrap ``torch.randn`` in a ``torch.fx.wrap`` function and call that instead.\n\n    ::\n\n        @torch.fx.wrap\n        def torch_randn(x, shape):\n            return torch.randn(shape)\n\n        def f(x):\n            return x + torch_randn(x, 5)\n        fx.symbolic_trace(f)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":1074,"to":1098}}}}],["527",{"pageContent":"::\n\n        @torch.fx.wrap\n        def torch_randn(x, shape):\n            return torch.randn(shape)\n\n        def f(x):\n            return x + torch_randn(x, 5)\n        fx.symbolic_trace(f)\n\n   -  This behavior may be fixed in a future release.\n\n-  Type annotations\n\n   -  Python 3-style type annotations (e.g.\n      ``func(x : torch.Tensor, y : int) -> torch.Tensor``) are supported\n      and will be preserved by symbolic tracing.\n   -  Python 2-style comment type annotations\n      ``# type: (torch.Tensor, int) -> torch.Tensor`` are not currently\n      supported.\n   -  Annotations on local names within a function are not currently\n      supported.\n\n\n-  Gotcha around ``training`` flag and submodules\n\n   -  When using functionals like ``torch.nn.functional.dropout``, it will be common for the training argument to be passed in as ``self.training``. During FX tracing, this will likely be baked in as a constant value.\n\n    ::\n\n        import torch\n        import torch.fx","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":1098,"to":1129}}}}],["528",{"pageContent":"::\n\n        import torch\n        import torch.fx\n\n        class DropoutRepro(torch.nn.Module):\n          def forward(self, x):\n            return torch.nn.functional.dropout(x, training=self.training)\n\n\n        traced = torch.fx.symbolic_trace(DropoutRepro())\n        print(traced.code)\n        \"\"\"\n        def forward(self, x):\n          dropout = torch.nn.functional.dropout(x, p = 0.5, training = True, inplace = False);  x = None\n          return dropout\n        \"\"\"\n\n        traced.eval()\n\n        x = torch.randn(5, 3)\n        torch.testing.assert_close(traced(x), x)\n        \"\"\"\n        AssertionError: Tensor-likes are not close!\n\n        Mismatched elements: 15 / 15 (100.0%)\n        Greatest absolute difference: 1.6207983493804932 at index (0, 2) (up to 1e-05 allowed)\n        Greatest relative difference: 1.0 at index (0, 0) (up to 0.0001 allowed)\n        \"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":1129,"to":1157}}}}],["529",{"pageContent":"- However, when the standard ``nn.Dropout()`` submodule is used, the training flag is encapsulated and--because of the preservation of the ``nn.Module`` object model--can be changed.\n\n    ::\n\n        class DropoutRepro2(torch.nn.Module):\n          def __init__(self):\n            super().__init__()\n            self.drop = torch.nn.Dropout()\n\n          def forward(self, x):\n            return self.drop(x)\n\n        traced = torch.fx.symbolic_trace(DropoutRepro2())\n        print(traced.code)\n        \"\"\"\n        def forward(self, x):\n          drop = self.drop(x);  x = None\n          return drop\n        \"\"\"\n\n        traced.eval()\n\n        x = torch.randn(5, 3)\n        torch.testing.assert_close(traced(x), x)\n\n  - Because of this difference, consider marking modules that interact with the ``training`` flag dynamically as leaf modules.\n\n\nAPI Reference\n-------------\n\n.. autofunction:: torch.fx.symbolic_trace\n\n.. autofunction:: torch.fx.wrap\n\n.. autoclass:: torch.fx.GraphModule\n  :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":1159,"to":1195}}}}],["530",{"pageContent":"API Reference\n-------------\n\n.. autofunction:: torch.fx.symbolic_trace\n\n.. autofunction:: torch.fx.wrap\n\n.. autoclass:: torch.fx.GraphModule\n  :members:\n\n  .. automethod:: __init__\n\n.. autoclass:: torch.fx.Graph\n  :members:\n\n  .. automethod:: __init__\n\n.. autoclass:: torch.fx.Node\n  :members:\n\n.. autoclass:: torch.fx.Tracer\n  :members:\n  :inherited-members:\n\n.. autoclass:: torch.fx.Proxy\n\n.. autoclass:: torch.fx.Interpreter\n  :members:\n\n.. autoclass:: torch.fx.Transformer\n  :members:\n\n.. autofunction:: torch.fx.replace_pattern","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":1195,"to":1227}}}}],["531",{"pageContent":".. autoclass:: torch.fx.Proxy\n\n.. autoclass:: torch.fx.Interpreter\n  :members:\n\n.. autoclass:: torch.fx.Transformer\n  :members:\n\n.. autofunction:: torch.fx.replace_pattern\n\n\n.. The experimental and passes submodules are missing docs.\n.. Adding it here for coverage but this doesn't add anything to the\n.. rendered doc.\n.. py:module:: torch.fx.passes\n.. py:module:: torch.fx.passes.infra\n.. py:module:: torch.fx.passes.backends\n.. py:module:: torch.fx.passes.utils\n.. py:module:: torch.fx.passes.tests\n.. py:module:: torch.fx.experimental\n.. py:module:: torch.fx.experimental.unification\n.. py:module:: torch.fx.experimental.unification.multipledispatch\n.. py:module:: torch.fx.experimental.migrate_gradual_types\n.. py:module:: torch.fx.passes.dialect\n.. py:module:: torch.fx.passes.dialect.common","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/fx.rst","loc":{"lines":{"from":1227,"to":1251}}}}],["532",{"pageContent":"torch.hub\n===================================\nPytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.\n\nPublishing models\n-----------------\n\nPytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a GitHub repository by adding a simple ``hubconf.py`` file;\n\n``hubconf.py`` can have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish).\n\n::\n\n    def entrypoint_name(*args, **kwargs):\n        # args & kwargs are optional, for models which take positional/keyword arguments.\n        ...","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/hub.rst","loc":{"lines":{"from":1,"to":18}}}}],["533",{"pageContent":"::\n\n    def entrypoint_name(*args, **kwargs):\n        # args & kwargs are optional, for models which take positional/keyword arguments.\n        ...\n\nHow to implement an entrypoint?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nHere is a code snippet specifies an entrypoint for ``resnet18`` model if we expand\nthe implementation in ``pytorch/vision/hubconf.py``.\nIn most case importing the right function in ``hubconf.py`` is sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script in\n`pytorch/vision repo <https://github.com/pytorch/vision/blob/master/hubconf.py>`_\n\n::\n\n    dependencies = ['torch']\n    from torchvision.models.resnet import resnet18 as _resnet18","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/hub.rst","loc":{"lines":{"from":18,"to":36}}}}],["534",{"pageContent":"::\n\n    dependencies = ['torch']\n    from torchvision.models.resnet import resnet18 as _resnet18\n\n    # resnet18 is the name of entrypoint\n    def resnet18(pretrained=False, **kwargs):\n        \"\"\" # This docstring shows up in hub.help()\n        Resnet18 model\n        pretrained (bool): kwargs, load pretrained weights into the model\n        \"\"\"\n        # Call the model, load pretrained weights\n        model = _resnet18(pretrained=pretrained, **kwargs)\n        return model","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/hub.rst","loc":{"lines":{"from":36,"to":49}}}}],["535",{"pageContent":"- ``dependencies`` variable is a **list** of package names required to **load** the model. Note this might\n  be slightly different from dependencies required for training a model.\n- ``args`` and ``kwargs`` are passed along to the real callable function.\n- Docstring of the function works as a help message. It explains what does the model do and what\n  are the allowed positional/keyword arguments. It's highly recommended to add a few examples here.\n- Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.\n- Callables prefixed with underscore are considered as helper functions which won't show up in :func:`torch.hub.list()`.\n- Pretrained weights can either be stored locally in the GitHub repo, or loadable by\n  :func:`torch.hub.load_state_dict_from_url()`. If less than 2GB, it's recommended to attach it to a `project release <https://help.github.com/en/articles/distributing-large-binaries>`_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/hub.rst","loc":{"lines":{"from":52,"to":60}}}}],["536",{"pageContent":":func:`torch.hub.load_state_dict_from_url()`. If less than 2GB, it's recommended to attach it to a `project release <https://help.github.com/en/articles/distributing-large-binaries>`_\n  and use the url from the release.\n  In the example above ``torchvision.models.resnet.resnet18`` handles ``pretrained``, alternatively you can put the following logic in the entrypoint definition.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/hub.rst","loc":{"lines":{"from":60,"to":62}}}}],["537",{"pageContent":"::\n\n    if pretrained:\n        # For checkpoint saved in local GitHub repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\n        dirname = os.path.dirname(__file__)\n        checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\n        state_dict = torch.load(checkpoint)\n        model.load_state_dict(state_dict)\n\n        # For checkpoint saved elsewhere\n        checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n        model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n\n\nImportant Notice\n^^^^^^^^^^^^^^^^\n\n- The published models should be at least in a branch/tag. It can't be a random commit.\n\n\nLoading models from Hub\n-----------------------\n\nPytorch Hub provides convenient APIs to explore all available models in hub\nthrough :func:`torch.hub.list()`, show docstring and examples through\n:func:`torch.hub.help()` and load the pre-trained models using\n:func:`torch.hub.load()`.\n\n\n.. automodule:: torch.hub\n\n.. autofunction:: list","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/hub.rst","loc":{"lines":{"from":64,"to":95}}}}],["538",{"pageContent":".. automodule:: torch.hub\n\n.. autofunction:: list\n\n.. autofunction:: help\n\n.. autofunction:: load\n\n.. autofunction:: download_url_to_file\n\n.. autofunction:: load_state_dict_from_url\n\nRunning a loaded model:\n^^^^^^^^^^^^^^^^^^^^^^^\n\nNote that ``*args`` and ``**kwargs`` in :func:`torch.hub.load()` are used to\n**instantiate** a model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is\n\n- ``dir(model)`` to see all available methods of the model.\n- ``help(model.foo)`` to check what arguments ``model.foo`` takes to run\n\nTo help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It's also helpful\nto include a minimal working example.\n\nWhere are my downloaded models saved?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe locations are used in the order of","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/hub.rst","loc":{"lines":{"from":95,"to":125}}}}],["539",{"pageContent":"Where are my downloaded models saved?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe locations are used in the order of\n\n- Calling ``hub.set_dir(<PATH_TO_HUB_DIR>)``\n- ``$TORCH_HOME/hub``, if environment variable ``TORCH_HOME`` is set.\n- ``$XDG_CACHE_HOME/torch/hub``, if environment variable ``XDG_CACHE_HOME`` is set.\n- ``~/.cache/torch/hub``\n\n.. autofunction:: get_dir\n\n.. autofunction:: set_dir\n\nCaching logic\n^^^^^^^^^^^^^\n\nBy default, we don't clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned by :func:`~torch.hub.get_dir()`.\n\nUsers can force a reload by calling ``hub.load(..., force_reload=True)``. This will delete\nthe existing GitHub folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/hub.rst","loc":{"lines":{"from":125,"to":147}}}}],["540",{"pageContent":"Known limitations:\n^^^^^^^^^^^^^^^^^^\nTorch hub works by importing the package as if it was installed. There are some side effects\nintroduced by importing in Python. For example, you can see new items in Python caches\n``sys.modules`` and ``sys.path_importer_cache`` which is normal Python behavior.\nThis also means that you may have import errors when importing different models\nfrom different repos, if the repos have the same sub-package names (typically, a\n``model`` subpackage). A workaround for these kinds of import errors is to\nremove the offending sub-package from the ``sys.modules`` dict; more details can\nbe found in `this GitHub issue\n<https://github.com/pytorch/hub/issues/243#issuecomment-942403391>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/hub.rst","loc":{"lines":{"from":150,"to":160}}}}],["541",{"pageContent":"A known limitation that is worth mentioning here: users **CANNOT** load two different branches of\nthe same repo in the **same python process**. It's just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it's totally fine to load them in separate processes.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/hub.rst","loc":{"lines":{"from":162,"to":165}}}}],["542",{"pageContent":".. PyTorch documentation master file, created by\n   sphinx-quickstart on Fri Dec 23 13:31:47 2016.\n   You can adapt this file completely to your liking, but it should at least\n   contain the root `toctree` directive.\n\n:github_url: https://github.com/pytorch/pytorch\n\nPyTorch documentation\n===================================\n\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\n\nFeatures described in this documentation are classified by release status:\n\n  *Stable:*  These features will be maintained long-term and there should generally\n  be no major performance limitations or gaps in documentation.\n  We also expect to maintain backwards compatibility (although\n  breaking changes can happen and notice will be given one release ahead\n  of time).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/index.rst","loc":{"lines":{"from":1,"to":19}}}}],["543",{"pageContent":"*Beta:*  These features are tagged as Beta because the API may change based on\n  user feedback, because the performance needs to improve, or because\n  coverage across operators is not yet complete. For Beta features, we are\n  committing to seeing the feature through to the Stable classification.\n  We are not, however, committing to backwards compatibility.\n\n  *Prototype:*  These features are typically not available as part of\n  binary distributions like PyPI or Conda, except sometimes behind run-time\n  flags, and are at an early stage for feedback and testing.\n\n.. toctree::\n   :glob:\n   :maxdepth: 1\n   :caption: Community\n\n   community/*\n\n.. toctree::\n   :glob:\n   :maxdepth: 1\n   :caption: Developer Notes\n\n   notes/*\n\n.. toctree::\n   :glob:\n   :maxdepth: 1\n   :caption: torch.compile\n\n   compile/index\n   compile/get-started\n   compile/troubleshooting\n   compile/faq\n   compile/technical-overview\n   compile/guards-overview\n   compile/custom-backends\n   compile/deep-dive\n   ir","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/index.rst","loc":{"lines":{"from":21,"to":58}}}}],["544",{"pageContent":"compile/index\n   compile/get-started\n   compile/troubleshooting\n   compile/faq\n   compile/technical-overview\n   compile/guards-overview\n   compile/custom-backends\n   compile/deep-dive\n   ir\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Language Bindings\n\n   cpp_index\n   Javadoc <https://pytorch.org/javadoc/>\n   torch::deploy <deploy>\n\n.. toctree::\n   :glob:\n   :maxdepth: 2\n   :caption: Python API","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/index.rst","loc":{"lines":{"from":58,"to":79}}}}],["545",{"pageContent":"torch\n   nn\n   nn.functional\n   tensors\n   tensor_attributes\n   tensor_view\n   torch.amp <amp>\n   torch.autograd <autograd>\n   torch.library <library>\n   cuda\n   mps\n   torch.backends <backends>\n   torch.distributed <distributed>\n   torch.distributed.algorithms.join <distributed.algorithms.join>\n   torch.distributed.elastic <distributed.elastic>\n   torch.distributed.fsdp <fsdp>\n   torch.distributed.optim <distributed.optim>\n   torch.distributed.tensor.parallel <distributed.tensor.parallel>\n   torch.distributed.checkpoint <distributed.checkpoint>\n   torch.distributions <distributions>\n   torch._dynamo <_dynamo>\n   torch.fft <fft>\n   torch.func <func>\n   futures\n   fx\n   torch.hub <hub>\n   torch.jit <jit>\n   torch.linalg <linalg>\n   torch.monitor <monitor>\n   torch.signal <signal>\n   torch.special <special>\n   torch.overrides\n   torch.package <package>\n   profiler\n   nn.init\n   onnx\n   onnx_diagnostics\n   optim\n   complex_numbers\n   ddp_comm_hooks\n   pipeline\n   quantization\n   rpc\n   torch.random <random>\n   masked","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/index.rst","loc":{"lines":{"from":81,"to":125}}}}],["546",{"pageContent":"torch.overrides\n   torch.package <package>\n   profiler\n   nn.init\n   onnx\n   onnx_diagnostics\n   optim\n   complex_numbers\n   ddp_comm_hooks\n   pipeline\n   quantization\n   rpc\n   torch.random <random>\n   masked\n   torch.nested <nested>\n   sparse\n   storage\n   torch.testing <testing>\n   torch.utils.benchmark <benchmark_utils>\n   torch.utils.bottleneck <bottleneck>\n   torch.utils.checkpoint <checkpoint>\n   torch.utils.cpp_extension <cpp_extension>\n   torch.utils.data <data>\n   torch.utils.jit <jit_utils>\n   torch.utils.dlpack <dlpack>\n   torch.utils.mobile_optimizer <mobile_optimizer>\n   torch.utils.model_zoo <model_zoo>\n   torch.utils.tensorboard <tensorboard>\n   type_info\n   named_tensor\n   name_inference\n   torch.__config__ <config_mod>","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/index.rst","loc":{"lines":{"from":125,"to":156}}}}],["547",{"pageContent":".. toctree::\n   :maxdepth: 1\n   :caption: Libraries\n\n   torchaudio <https://pytorch.org/audio/stable>\n   TorchData <https://pytorch.org/data>\n   TorchRec <https://pytorch.org/torchrec>\n   TorchServe <https://pytorch.org/serve>\n   torchtext <https://pytorch.org/text/stable>\n   torchvision <https://pytorch.org/vision/stable>\n   PyTorch on XLA Devices <https://pytorch.org/xla/>\n\nIndices and tables\n==================\n\n* :ref:`genindex`\n* :ref:`modindex`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/index.rst","loc":{"lines":{"from":158,"to":174}}}}],["548",{"pageContent":"IRs\n===============\n\nPyTorch 2.0 offers two set of IRs for backends to interface with: Core Aten IR and Prims IR.\n\nCore Aten IR\n--------------------\n\nCore aten ops is the core subset of aten operators that can be used to compose other operators.\nCore aten IR is fully functional, and there is no `inplace` or `_out` variants in this opset.\nIn contrast to Prims IR, core aten ops reuses the existing aten ops in \"native_functions.yaml\",\nand it doesn't further decompose ops into explicit type promotion and broadcasting ops.\nThis opset is designed to serve as the functional IR to interface with backends.\n\n.. warning::\n  This opset is still under active development, more ops will be added in the future.\n\n.. csv-table::\n   :file: ../build/ir/aten_ops.csv\n   :widths: auto\n   :header-rows: 1\n\nPrims IR\n-----------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ir.rst","loc":{"lines":{"from":1,"to":24}}}}],["549",{"pageContent":".. csv-table::\n   :file: ../build/ir/aten_ops.csv\n   :widths: auto\n   :header-rows: 1\n\nPrims IR\n-----------\n\nPrims IR is a set of primitive operators that can be used to compose other operators.\nPrims IR is a lower level opset than core aten IR, and it further decomposes ops into explicit\ntype promotion and broadcasting ops: prims.convert_element_type and prims.broadcast_in_dim.\nThis opset is designed to interface with compiler backends.\n\n.. warning::\n  This opset is still under active development, more ops will be added in the future.\n\n.. csv-table::\n   :file: ../build/ir/prims_ops.csv\n   :widths: auto\n   :header-rows: 1","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/ir.rst","loc":{"lines":{"from":24,"to":43}}}}],["550",{"pageContent":"TorchScript\n===========\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Builtin Functions\n   :hidden:\n\n   torch.jit.supported_ops <jit_builtin_functions>\n\n\n.. toctree::\n    :maxdepth: 1\n    :caption: Language Reference\n    :hidden:\n\n    jit_language_reference\n\n\n.. toctree::\n    :maxdepth: 1\n\n    jit_language_reference_v2\n\n\n.. contents:: :local:\n    :depth: 2\n\n.. automodule:: torch.jit\n.. currentmodule:: torch.jit\n\nTorchScript is a way to create serializable and optimizable models from PyTorch code.\nAny TorchScript program can be saved from a Python\nprocess and loaded in a process where there is no Python dependency.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":1,"to":34}}}}],["551",{"pageContent":"We provide tools to incrementally transition a model from a pure Python program\nto a TorchScript program that can be run independently from Python, such as in a standalone C++ program.\nThis makes it possible to train models in PyTorch using familiar tools in Python and then export\nthe model via TorchScript to a production environment where Python programs may be disadvantageous\nfor performance and multi-threading reasons.\n\nFor a gentle introduction to TorchScript, see the `Introduction to TorchScript <https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html>`_ tutorial.\n\nFor an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see the\n`Loading a PyTorch Model in C++ <https://pytorch.org/tutorials/advanced/cpp_export.html>`_ tutorial.\n\nCreating TorchScript Code\n--------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":36,"to":52}}}}],["552",{"pageContent":"Creating TorchScript Code\n--------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    script\n    trace\n    script_if_tracing\n    trace_module\n    fork\n    wait\n    ScriptModule\n    ScriptFunction\n    freeze\n    optimize_for_inference\n    enable_onednn_fusion\n    onednn_fusion_enabled\n    set_fusion_strategy\n    strict_fusion\n    save\n    load\n    ignore\n    unused\n    isinstance\n    Attribute\n    annotate\n\nMixing Tracing and Scripting\n----------------------------\n\nIn many cases either tracing or scripting is an easier approach for converting a model to TorchScript.\nTracing and scripting can be composed to suit the particular requirements\nof a part of a model.\n\nScripted functions can call traced functions. This is particularly useful when you need\nto use control-flow around a simple feed-forward model. For instance the beam search\nof a sequence to sequence model will typically be written in script but can call an\nencoder module generated using tracing.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":52,"to":91}}}}],["553",{"pageContent":".. testsetup::\n\n    # These are hidden from the docs, but these are necessary for `doctest`\n    # since the `inspect` module doesn't play nicely with the execution\n    # environment for `doctest`\n    import torch\n\n    original_script = torch.jit.script\n    def script_wrapper(obj, *args, **kwargs):\n        obj.__module__ = 'FakeMod'\n        return original_script(obj, *args, **kwargs)\n\n    torch.jit.script = script_wrapper\n\n    original_trace = torch.jit.trace\n    def trace_wrapper(obj, *args, **kwargs):\n        obj.__module__ = 'FakeMod'\n        return original_trace(obj, *args, **kwargs)\n\n    torch.jit.trace = trace_wrapper\n\n\nExample (calling a traced function in script):\n\n.. testcode::\n\n    import torch\n\n    def foo(x, y):\n        return 2 * x + y\n\n    traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n    @torch.jit.script\n    def bar(x):\n        return traced_foo(x, x)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":94,"to":129}}}}],["554",{"pageContent":"import torch\n\n    def foo(x, y):\n        return 2 * x + y\n\n    traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n    @torch.jit.script\n    def bar(x):\n        return traced_foo(x, x)\n\nTraced functions can call script functions. This is useful when a small part of\na model requires some control-flow even though most of the model is just a feed-forward\nnetwork. Control-flow inside of a script function called by a traced function is\npreserved correctly.\n\nExample (calling a script function in a traced function):\n\n.. testcode::\n\n    import torch\n\n    @torch.jit.script\n    def foo(x, y):\n        if x.max() > y.max():\n            r = x\n        else:\n            r = y\n        return r\n\n\n    def bar(x, y, z):\n        return foo(x, y) + z\n\n    traced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3)))\n\nThis composition also works for ``nn.Module``\\s as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":129,"to":166}}}}],["555",{"pageContent":"This composition also works for ``nn.Module``\\s as well, where it can be used to generate\na submodule using tracing that can be called from the methods of a script module.\n\nExample (using a traced module):\n\n.. testcode::\n    :skipif: torchvision is None\n\n    import torch\n    import torchvision\n\n    class MyScriptModule(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68])\n                                            .resize_(1, 3, 1, 1))\n            self.resnet = torch.jit.trace(torchvision.models.resnet18(),\n                                          torch.rand(1, 3, 224, 224))\n\n        def forward(self, input):\n            return self.resnet(input - self.means)\n\n    my_script_module = torch.jit.script(MyScriptModule())\n\n\nTorchScript Language\n--------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":166,"to":192}}}}],["556",{"pageContent":"def forward(self, input):\n            return self.resnet(input - self.means)\n\n    my_script_module = torch.jit.script(MyScriptModule())\n\n\nTorchScript Language\n--------------------\n\nTorchScript is a statically typed subset of Python, so many Python features apply\ndirectly to TorchScript. See the full :ref:`language-reference` for details.\n\n\n.. _builtin functions:\n\nBuilt-in Functions and Modules\n------------------------------\n\nTorchScript supports the use of most PyTorch functions and many Python built-ins.\nSee :ref:`builtin-functions` for a full reference of supported functions.\n\nPyTorch Functions and Modules\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTorchScript supports a subset of the tensor and neural network\nfunctions that PyTorch provides. Most methods on Tensor as well as functions in\nthe ``torch`` namespace, all functions in ``torch.nn.functional`` and\nmost modules from ``torch.nn`` are supported in TorchScript.\n\nSee :ref:`jit_unsupported` for a list of unsupported PyTorch functions and modules.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":192,"to":221}}}}],["557",{"pageContent":"See :ref:`jit_unsupported` for a list of unsupported PyTorch functions and modules.\n\n\nPython Functions and Modules\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nMany of Python's `built-in functions <https://docs.python.org/3/library/functions.html>`_ are supported in TorchScript.\nThe :any:`math` module is also supported (see :ref:`math-module` for details), but no other Python modules\n(built-in or third party) are supported.\n\n\nPython Language Reference Comparison\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFor a full listing of supported Python features, see :ref:`python-language-reference`.\n\nDebugging\n---------\n\n.. _`disable TorchScript`:\n\nDisable JIT for Debugging\n~~~~~~~~~~~~~~~~~~~~~~~~~\n.. envvar:: PYTORCH_JIT","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":221,"to":243}}}}],["558",{"pageContent":"Debugging\n---------\n\n.. _`disable TorchScript`:\n\nDisable JIT for Debugging\n~~~~~~~~~~~~~~~~~~~~~~~~~\n.. envvar:: PYTORCH_JIT\n\nSetting the environment variable ``PYTORCH_JIT=0`` will disable all script\nand tracing annotations. If there is hard-to-debug error in one of your\nTorchScript models, you can use this flag to force everything to run using native\nPython. Since TorchScript (scripting and tracing) is disabled with this flag,\nyou can use tools like ``pdb`` to debug the model code.  For example::\n\n    @torch.jit.script\n    def scripted_fn(x : torch.Tensor):\n        for i in range(12):\n            x = x + x\n        return x\n\n    def fn(x):\n        x = torch.neg(x)\n        import pdb; pdb.set_trace()\n        return scripted_fn(x)\n\n    traced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\n    traced_fn(torch.rand(3, 4))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":243,"to":270}}}}],["559",{"pageContent":"def fn(x):\n        x = torch.neg(x)\n        import pdb; pdb.set_trace()\n        return scripted_fn(x)\n\n    traced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\n    traced_fn(torch.rand(3, 4))\n\nDebugging this script with ``pdb`` works except for when we invoke the\n:func:`@torch.jit.script <torch.jit.script>` function. We can globally disable\nJIT, so that we can call the :func:`@torch.jit.script <torch.jit.script>`\nfunction as a normal Python function and not compile it. If the above script\nis called ``disable_jit_example.py``, we can invoke it like so::\n\n    $ PYTORCH_JIT=0 python disable_jit_example.py\n\nand we will be able to step into the :func:`@torch.jit.script\n<torch.jit.script>` function as a normal Python function. To disable the\nTorchScript compiler for a specific function, see\n:func:`@torch.jit.ignore <torch.jit.ignore>`.\n\n.. _inspecting-code:\n\nInspecting Code\n~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":270,"to":294}}}}],["560",{"pageContent":".. _inspecting-code:\n\nInspecting Code\n~~~~~~~~~~~~~~~\n\nTorchScript provides a code pretty-printer for all :class:`ScriptModule` instances. This\npretty-printer gives an interpretation of the script method's code as valid\nPython syntax. For example:\n\n.. testcode::\n\n    @torch.jit.script\n    def foo(len):\n        # type: (int) -> torch.Tensor\n        rv = torch.zeros(3, 4)\n        for i in range(len):\n            if i < 10:\n                rv = rv - 1.0\n            else:\n                rv = rv + 1.0\n        return rv\n\n    print(foo.code)\n\n.. testoutput::\n    :hide:\n\n    ...\n\nA :class:`ScriptModule` with a single ``forward`` method will have an attribute\n``code``, which you can use to inspect the :class:`ScriptModule`'s code.\nIf the :class:`ScriptModule` has more than one method, you will need to access\n``.code`` on the method itself and not the module. We can inspect the\ncode of a method named ``foo`` on a :class:`ScriptModule` by accessing ``.foo.code``.\nThe example above produces this output: ::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":294,"to":328}}}}],["561",{"pageContent":"def foo(len: int) -> Tensor:\n        rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)\n        rv0 = rv\n        for i in range(len):\n            if torch.lt(i, 10):\n                rv1 = torch.sub(rv0, 1., 1)\n            else:\n                rv1 = torch.add(rv0, 1., 1)\n            rv0 = rv1\n        return rv0\n\nThis is TorchScript's compilation of the code for the ``forward`` method.\nYou can use this to ensure TorchScript (tracing or scripting) has captured\nyour model code correctly.\n\n\n.. _interpreting-graphs:\n\nInterpreting Graphs\n~~~~~~~~~~~~~~~~~~~\nTorchScript also has a representation at a lower level than the code pretty-\nprinter, in the form of IR graphs.\n\nTorchScript uses a static single assignment (SSA) intermediate representation\n(IR) to represent computation. The instructions in this format consist of\nATen (the C++ backend of PyTorch) operators and other primitive operators,\nincluding control flow operators for loops and conditionals. As an example:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":330,"to":356}}}}],["562",{"pageContent":".. testcode::\n\n    @torch.jit.script\n    def foo(len):\n        # type: (int) -> torch.Tensor\n        rv = torch.zeros(3, 4)\n        for i in range(len):\n            if i < 10:\n                rv = rv - 1.0\n            else:\n                rv = rv + 1.0\n        return rv\n\n    print(foo.graph)\n\n.. testoutput::\n    :hide:\n\n    ...\n\n``graph`` follows the same rules described in the :ref:`inspecting-code` section\nwith regard to ``forward`` method lookup.\n\nThe example script above produces the graph::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":358,"to":381}}}}],["563",{"pageContent":"graph(%len.1 : int):\n      %24 : int = prim::Constant[value=1]()\n      %17 : bool = prim::Constant[value=1]() # test.py:10:5\n      %12 : bool? = prim::Constant()\n      %10 : Device? = prim::Constant()\n      %6 : int? = prim::Constant()\n      %1 : int = prim::Constant[value=3]() # test.py:9:22\n      %2 : int = prim::Constant[value=4]() # test.py:9:25\n      %20 : int = prim::Constant[value=10]() # test.py:11:16\n      %23 : float = prim::Constant[value=1]() # test.py:12:23\n      %4 : int[] = prim::ListConstruct(%1, %2)\n      %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10\n      %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5\n        block0(%i.1 : int, %rv.14 : Tensor):\n          %21 : bool = aten::lt(%i.1, %20) # test.py:11:12\n          %rv.13 : Tensor = prim::If(%21) # test.py:11:9\n            block0():\n              %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18\n              -> (%rv.3)\n            block1():","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":383,"to":402}}}}],["564",{"pageContent":"%rv.13 : Tensor = prim::If(%21) # test.py:11:9\n            block0():\n              %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18\n              -> (%rv.3)\n            block1():\n              %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18\n              -> (%rv.6)\n          -> (%17, %rv.13)\n      return (%rv)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":402,"to":410}}}}],["565",{"pageContent":"Take the instruction ``%rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10`` for\nexample.\n\n* ``%rv.1 : Tensor`` means we assign the output to a (unique) value named ``rv.1``, that value is of ``Tensor`` type and that we do not know its concrete shape.\n* ``aten::zeros`` is the operator (equivalent to ``torch.zeros``) and the input list ``(%4, %6, %6, %10, %12)`` specifies which values in scope should be passed as inputs. The schema for built-in functions like ``aten::zeros`` can be found at `Builtin Functions`_.\n* ``# test.py:9:10`` is the location in the original source file that generated this instruction. In this case, it is a file named `test.py`, on line 9, and at character 10.\n\nNotice that operators can also have associated ``blocks``, namely the\n``prim::Loop`` and ``prim::If`` operators. In the graph print-out, these\noperators are formatted to reflect their equivalent source code forms\nto facilitate easy debugging.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":413,"to":423}}}}],["566",{"pageContent":"Graphs can be inspected as shown to confirm that the computation described\nby a :class:`ScriptModule` is correct, in both automated and manual fashion, as\ndescribed below.\n\nTracer\n~~~~~~\n\n\nTracing Edge Cases\n^^^^^^^^^^^^^^^^^^\nThere are some edge cases that exist where the trace of a given Python\nfunction/module will not be representative of the underlying code. These\ncases can include:\n\n* Tracing of control flow that is dependent on inputs (e.g. tensor shapes)\n* Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment)\n\nNote that these cases may in fact be traceable in the future.\n\n\nAutomatic Trace Checking\n^^^^^^^^^^^^^^^^^^^^^^^^\nOne way to automatically catch many errors in traces is by using ``check_inputs``\non the ``torch.jit.trace()`` API. ``check_inputs`` takes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":425,"to":450}}}}],["567",{"pageContent":"def loop_in_traced_fn(x):\n        result = x[0]\n        for i in range(x.size(0)):\n            result = result * x[i]\n        return result\n\n    inputs = (torch.rand(3, 4, 5),)\n    check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\n    traced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)\n\nGives us the following diagnostic information::\n\n    ERROR: Graphs differed across invocations!\n    Graph diff:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":452,"to":466}}}}],["568",{"pageContent":"graph(%x : Tensor) {\n                %1 : int = prim::Constant[value=0]()\n                %2 : int = prim::Constant[value=0]()\n                %result.1 : Tensor = aten::select(%x, %1, %2)\n                %4 : int = prim::Constant[value=0]()\n                %5 : int = prim::Constant[value=0]()\n                %6 : Tensor = aten::select(%x, %4, %5)\n                %result.2 : Tensor = aten::mul(%result.1, %6)\n                %8 : int = prim::Constant[value=0]()\n                %9 : int = prim::Constant[value=1]()\n                %10 : Tensor = aten::select(%x, %8, %9)\n            -   %result : Tensor = aten::mul(%result.2, %10)\n            +   %result.3 : Tensor = aten::mul(%result.2, %10)\n            ?          ++\n                %12 : int = prim::Constant[value=0]()\n                %13 : int = prim::Constant[value=2]()\n                %14 : Tensor = aten::select(%x, %12, %13)\n            +   %result : Tensor = aten::mul(%result.3, %14)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":468,"to":485}}}}],["569",{"pageContent":"%13 : int = prim::Constant[value=2]()\n                %14 : Tensor = aten::select(%x, %12, %13)\n            +   %result : Tensor = aten::mul(%result.3, %14)\n            +   %16 : int = prim::Constant[value=0]()\n            +   %17 : int = prim::Constant[value=3]()\n            +   %18 : Tensor = aten::select(%x, %16, %17)\n            -   %15 : Tensor = aten::mul(%result, %14)\n            ?     ^                                 ^\n            +   %19 : Tensor = aten::mul(%result, %18)\n            ?     ^                                 ^\n            -   return (%15);\n            ?             ^\n            +   return (%19);\n            ?             ^\n                }","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":485,"to":499}}}}],["570",{"pageContent":"This message indicates to us that the computation differed between when\nwe first traced it and when we traced it with the ``check_inputs``. Indeed,\nthe loop within the body of ``loop_in_traced_fn`` depends on the shape\nof the input ``x``, and thus when we try another ``x`` with a different\nshape, the trace differs.\n\nIn this case, data-dependent control flow like this can be captured using\n:func:`torch.jit.script` instead:\n\n.. testcode::\n\n    def fn(x):\n        result = x[0]\n        for i in range(x.size(0)):\n            result = result * x[i]\n        return result\n\n    inputs = (torch.rand(3, 4, 5),)\n    check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\n    scripted_fn = torch.jit.script(fn)\n    print(scripted_fn.graph)\n    #print(str(scripted_fn.graph).strip())\n\n    for input_tuple in [inputs] + check_inputs:\n        torch.testing.assert_close(fn(*input_tuple), scripted_fn(*input_tuple))\n\n.. testoutput::\n    :hide:\n\n    ...\n\n\nWhich produces::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":502,"to":535}}}}],["571",{"pageContent":"for input_tuple in [inputs] + check_inputs:\n        torch.testing.assert_close(fn(*input_tuple), scripted_fn(*input_tuple))\n\n.. testoutput::\n    :hide:\n\n    ...\n\n\nWhich produces::\n\n    graph(%x : Tensor) {\n        %5 : bool = prim::Constant[value=1]()\n        %1 : int = prim::Constant[value=0]()\n        %result.1 : Tensor = aten::select(%x, %1, %1)\n        %4 : int = aten::size(%x, %1)\n        %result : Tensor = prim::Loop(%4, %5, %result.1)\n        block0(%i : int, %7 : Tensor) {\n            %10 : Tensor = aten::select(%x, %1, %i)\n            %result.2 : Tensor = aten::mul(%7, %10)\n            -> (%5, %result.2)\n        }\n        return (%result);\n    }\n\nTracer Warnings\n^^^^^^^^^^^^^^^\nThe tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor:\n\n.. testcode::\n\n    def fill_row_zero(x):\n        x[0] = torch.rand(*x.shape[1:2])\n        return x","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":535,"to":570}}}}],["572",{"pageContent":".. testcode::\n\n    def fill_row_zero(x):\n        x[0] = torch.rand(*x.shape[1:2])\n        return x\n\n    traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\n    print(traced.graph)\n\n.. testoutput::\n    :hide:\n\n    ...\n\nProduces several warnings and a graph which simply returns the input::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":570,"to":584}}}}],["573",{"pageContent":"...\n\nProduces several warnings and a graph which simply returns the input::\n\n    fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n        x[0] = torch.rand(*x.shape[1:2])\n    fill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n    Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)\n        traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\n    graph(%0 : Float(3, 4)) {\n        return (%0);\n    }","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":584,"to":595}}}}],["574",{"pageContent":"We can fix this by modifying the code to not use the in-place update, but\nrather build up the result tensor out-of-place with ``torch.cat``:\n\n.. testcode::\n\n    def fill_row_zero(x):\n        x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0)\n        return x\n\n    traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\n    print(traced.graph)\n\n.. testoutput::\n    :hide:\n\n    ...\n\nFrequently Asked Questions\n--------------------------\n\nQ: I would like to train a model on GPU and do inference on CPU. What are the\nbest practices?\n\n   First convert your model from GPU to CPU and then save it, like so: ::\n\n      cpu_model = gpu_model.cpu()\n      sample_input_cpu = sample_input_gpu.cpu()\n      traced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)\n      torch.jit.save(traced_cpu, \"cpu.pt\")\n\n      traced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\n      torch.jit.save(traced_gpu, \"gpu.pt\")\n\n      # ... later, when using the model:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":597,"to":630}}}}],["575",{"pageContent":"traced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\n      torch.jit.save(traced_gpu, \"gpu.pt\")\n\n      # ... later, when using the model:\n\n      if use_gpu:\n        model = torch.jit.load(\"gpu.pt\")\n      else:\n        model = torch.jit.load(\"cpu.pt\")\n\n      model(input)\n\n   This is recommended because the tracer may witness tensor creation on a\n   specific device, so casting an already-loaded model may have unexpected\n   effects. Casting the model *before* saving it ensures that the tracer has\n   the correct device information.\n\n\nQ: How do I store attributes on a :class:`ScriptModule`?\n\n    Say we have a model like:\n\n    .. testcode::\n\n        import torch\n\n        class Model(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.x = 2\n\n            def forward(self):\n                return self.x\n\n        m = torch.jit.script(Model())","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":630,"to":664}}}}],["576",{"pageContent":"def forward(self):\n                return self.x\n\n        m = torch.jit.script(Model())\n\n\n\n    If ``Model`` is instantiated it will result in a compilation error\n    since the compiler doesn't know about ``x``. There are 4 ways to inform the\n    compiler of attributes on :class:`ScriptModule`:\n\n    1. ``nn.Parameter`` - Values wrapped in ``nn.Parameter`` will work as they\n    do on ``nn.Module``\\s\n\n    2. ``register_buffer`` - Values wrapped in ``register_buffer`` will work as\n    they do on ``nn.Module``\\s. This is equivalent to an attribute (see 4) of type\n    ``Tensor``.\n\n    3. Constants - Annotating a class member as ``Final`` (or adding it to a list called\n    ``__constants__`` at the class definition level) will mark the contained names\n    as constants. Constants are saved directly in the code of the model. See\n    `builtin-constants` for details.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":664,"to":685}}}}],["577",{"pageContent":"4. Attributes - Values that are a `supported type` can be added as mutable\n    attributes. Most types can be inferred but some may need to be specified, see\n    `module attributes` for details.\n\nQ: I would like to trace module's method but I keep getting this error:\n\n``RuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient``\n\n    This error usually means that the method you are tracing uses a module's parameters and\n    you are passing the module's method instead of the module instance (e.g. ``my_module_instance.forward`` vs ``my_module_instance``).\n\n      - Invoking ``trace`` with a module's method captures module parameters (which may require gradients) as **constants**.\n      - On the other hand, invoking ``trace`` with module's instance (e.g. ``my_module``) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":687,"to":699}}}}],["578",{"pageContent":"To trace a specific method on a module, see :func:`torch.jit.trace_module <torch.jit.trace_module>`\n\nKnown Issues\n---------------\n\nIf you're using ``Sequential`` with TorchScript, the inputs of some\nof the ``Sequential`` submodules may be falsely inferred to be\n``Tensor``, even if they're annotated otherwise. The canonical\nsolution is to subclass ``nn.Sequential`` and redeclare ``forward``\nwith the input typed correctly.\n\nAppendix\n--------\n\nMigrating to PyTorch 1.2 Recursive Scripting API\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThis section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can\nskip this section. There are two main changes to the TorchScript API with PyTorch 1.2.\n\n1. :func:`torch.jit.script <torch.jit.script>` will now attempt to recursively compile functions,\nmethods, and classes that it encounters. Once you call ``torch.jit.script``,\ncompilation is \"opt-out\", rather than \"opt-in\".","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":701,"to":722}}}}],["579",{"pageContent":"2. ``torch.jit.script(nn_module_instance)`` is now the preferred way to create\n:class:`ScriptModule`\\s, instead of inheriting from ``torch.jit.ScriptModule``.\nThese changes combine to provide a simpler, easier-to-use API for converting\nyour ``nn.Module``\\s into :class:`ScriptModule`\\s, ready to be optimized and executed in a\nnon-Python environment.\n\nThe new usage looks like this:\n\n.. testcode::\n\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n    class Model(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\n    my_model = Model()\n    my_scripted_model = torch.jit.script(my_model)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":724,"to":749}}}}],["580",{"pageContent":"* The module's ``forward`` is compiled by default. Methods called from ``forward`` are lazily compiled in the order they are used in ``forward``.\n* To compile a method other than ``forward`` that is not called from ``forward``, add ``@torch.jit.export``.\n* To stop the compiler from compiling a method, add :func:`@torch.jit.ignore <torch.jit.ignore>` or :func:`@torch.jit.unused <torch.jit.unused>`. ``@ignore`` leaves the\n* method as a call to python, and ``@unused`` replaces it with an exception. ``@ignored`` cannot be exported; ``@unused`` can.\n* Most attribute types can be inferred, so ``torch.jit.Attribute`` is not necessary. For empty container types, annotate their types using `PEP 526-style <https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations>`_ class annotations.\n* Constants can be marked with a ``Final`` class annotation instead of adding the name of the member to ``__constants__``.\n* Python 3 type hints can be used in place of ``torch.jit.annotate``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":752,"to":758}}}}],["581",{"pageContent":"As a result of these changes, the following items are considered deprecated and should not appear in new code:\n  * The ``@torch.jit.script_method`` decorator\n  * Classes that inherit from ``torch.jit.ScriptModule``\n  * The ``torch.jit.Attribute`` wrapper class\n  * The ``__constants__`` array\n  * The ``torch.jit.annotate`` function\n\nModules\n^^^^^^^\n.. warning::\n\n    The :func:`@torch.jit.ignore <torch.jit.ignore>` annotation's behavior changes in\n    PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function\n    or method callable from code that is exported. To get this functionality back,\n    use ``@torch.jit.unused()``. ``@torch.jit.ignore`` is now equivalent\n    to ``@torch.jit.ignore(drop=False)``. See :func:`@torch.jit.ignore <torch.jit.ignore>`\n    and :func:`@torch.jit.unused<torch.jit.unused>` for details.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":760,"to":776}}}}],["582",{"pageContent":"When passed to the :func:`torch.jit.script <torch.jit.script>` function, a ``torch.nn.Module``\\'s data is\ncopied to a :class:`ScriptModule` and the TorchScript compiler compiles the module.\nThe module's ``forward`` is compiled by default. Methods called from ``forward`` are\nlazily compiled in the order they are used in ``forward``, as well as any\n``@torch.jit.export`` methods.\n\n.. autofunction:: export\n\nFunctions\n^^^^^^^^^\nFunctions don't change much, they can be decorated with :func:`@torch.jit.ignore <torch.jit.ignore>` or :func:`torch.jit.unused <torch.jit.unused>` if needed.\n\n.. testcode::\n\n    # Same behavior as pre-PyTorch 1.2\n    @torch.jit.script\n    def some_fn():\n        return 2\n\n    # Marks a function as ignored, if nothing\n    # ever calls it then this has no effect\n    @torch.jit.ignore\n    def some_fn2():\n        return 2","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":778,"to":801}}}}],["583",{"pageContent":"# Marks a function as ignored, if nothing\n    # ever calls it then this has no effect\n    @torch.jit.ignore\n    def some_fn2():\n        return 2\n\n    # As with ignore, if nothing calls it then it has no effect.\n    # If it is called in script it is replaced with an exception.\n    @torch.jit.unused\n    def some_fn3():\n      import pdb; pdb.set_trace()\n      return 4\n\n    # Doesn't do anything, this function is already\n    # the main entry point\n    @torch.jit.export\n    def some_fn4():\n        return 2\n\nTorchScript Classes\n^^^^^^^^^^^^^^^^^^^\n\n.. warning::\n\n    TorchScript class support is experimental. Currently it is best suited\n    for simple record-like types (think a ``NamedTuple`` with methods\n    attached).\n\nEverything in a user defined `TorchScript Class <torchscript-class>`_ is\nexported by default, functions can be decorated with :func:`@torch.jit.ignore\n<torch.jit.ignore>` if needed.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":801,"to":831}}}}],["584",{"pageContent":"Everything in a user defined `TorchScript Class <torchscript-class>`_ is\nexported by default, functions can be decorated with :func:`@torch.jit.ignore\n<torch.jit.ignore>` if needed.\n\nAttributes\n^^^^^^^^^^\nThe TorchScript compiler needs to know the types of `module attributes`. Most types\ncan be inferred from the value of the member. Empty lists and dicts cannot have their\ntypes inferred and must have their types annotated with `PEP 526-style <https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations>`_ class annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resulting :class:`ScriptModule`\n\n\nOld API:\n\n.. testcode::\n\n    from typing import Dict\n    import torch\n\n    class MyModule(torch.jit.ScriptModule):\n        def __init__(self):\n            super().__init__()\n            self.my_dict = torch.jit.Attribute({}, Dict[str, int])\n            self.my_int = torch.jit.Attribute(20, int)\n\n    m = MyModule()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":831,"to":857}}}}],["585",{"pageContent":"m = MyModule()\n\nNew API:\n\n.. testcode::\n\n    from typing import Dict\n\n    class MyModule(torch.nn.Module):\n        my_dict: Dict[str, int]\n\n        def __init__(self):\n            super().__init__()\n            # This type cannot be inferred and must be specified\n            self.my_dict = {}\n\n            # The attribute type here is inferred to be `int`\n            self.my_int = 20\n\n        def forward(self):\n            pass\n\n    m = torch.jit.script(MyModule())\n\n\nConstants\n^^^^^^^^^\nThe ``Final`` type constructor can be used to mark members as `constant`. If members are not marked constant, they will be copied to the resulting :class:`ScriptModule` as an attribute. Using ``Final`` opens opportunities for optimization if the value is known to be fixed and gives additional type safety.\n\nOld API:\n\n.. testcode::\n\n    class MyModule(torch.jit.ScriptModule):\n        __constants__ = ['my_constant']\n\n        def __init__(self):\n            super().__init__()\n            self.my_constant = 2","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":857,"to":895}}}}],["586",{"pageContent":"Old API:\n\n.. testcode::\n\n    class MyModule(torch.jit.ScriptModule):\n        __constants__ = ['my_constant']\n\n        def __init__(self):\n            super().__init__()\n            self.my_constant = 2\n\n        def forward(self):\n            pass\n    m = MyModule()\n\nNew API:\n\n::\n\n    from typing import Final\n\n    class MyModule(torch.nn.Module):\n\n        my_constant: Final[int]\n\n        def __init__(self):\n            super().__init__()\n            self.my_constant = 2\n\n        def forward(self):\n            pass\n\n    m = torch.jit.script(MyModule())\n\n.. _Python 3 type hints:\n\nVariables\n^^^^^^^^^\nContainers are assumed to have type ``Tensor`` and be non-optional (see\n`Default Types` for more information). Previously, ``torch.jit.annotate`` was used to\ntell the TorchScript compiler what the type should be. Python 3 style type hints are\nnow supported.\n\n.. testcode::\n\n    import torch\n    from typing import Dict, Optional","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":895,"to":941}}}}],["587",{"pageContent":".. testcode::\n\n    import torch\n    from typing import Dict, Optional\n\n    @torch.jit.script\n    def make_dict(flag: bool):\n        x: Dict[str, int] = {}\n        x['hi'] = 2\n        b: Optional[int] = None\n        if flag:\n            b = 2\n        return x, b\n\nFusion Backends\n~~~~~~~~~~~~~~~\nThere are a couple of fusion backends available to optimize TorchScript execution. The default fuser on CPUs is NNC, which can perform fusions for both CPUs and GPUs. The default fuser on GPUs is NVFuser, which supports a wider range of operators and has demonstrated generated kernels with improved throughput. See the  `NVFuser documentation <https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/codegen/cuda/README.md>`_ for more details on usage and debugging.\n\n\nReferences\n~~~~~~~~~~\n.. toctree::\n    :maxdepth: 1\n\n    jit_python_reference\n    jit_unsupported","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":941,"to":966}}}}],["588",{"pageContent":"References\n~~~~~~~~~~\n.. toctree::\n    :maxdepth: 1\n\n    jit_python_reference\n    jit_unsupported\n\n.. This package is missing doc. Adding it here for coverage\n.. This does not add anything to the rendered page.\n.. py:module:: torch.jit.mobile","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit.rst","loc":{"lines":{"from":966,"to":976}}}}],["589",{"pageContent":".. _builtin-functions:\n\nTorchScript Builtins\n====================\n\nThis is a full reference of functions and Tensor methods accessible in TorchScript\n\n.. contents:: :local:\n\n.. automodule:: torch.jit.supported_ops","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_builtin_functions.rst","loc":{"lines":{"from":1,"to":10}}}}],["590",{"pageContent":".. contents::\n    :local:\n    :depth: 2\n\n\n.. testsetup::\n\n    # These are hidden from the docs, but these are necessary for `doctest`\n    # since the `inspect` module doesn't play nicely with the execution\n    # environment for `doctest`\n    import torch\n\n    original_script = torch.jit.script\n    def script_wrapper(obj, *args, **kwargs):\n        obj.__module__ = 'FakeMod'\n        return original_script(obj, *args, **kwargs)\n\n    torch.jit.script = script_wrapper\n\n    original_trace = torch.jit.trace\n    def trace_wrapper(obj, *args, **kwargs):\n        obj.__module__ = 'FakeMod'\n        return original_trace(obj, *args, **kwargs)\n\n    torch.jit.trace = trace_wrapper\n\n.. _language-reference:\n\nTorchScript Language Reference\n==============================","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":1,"to":30}}}}],["591",{"pageContent":"torch.jit.trace = trace_wrapper\n\n.. _language-reference:\n\nTorchScript Language Reference\n==============================\n\nTorchScript is a statically typed subset of Python that can either be written directly (using\nthe :func:`@torch.jit.script <torch.jit.script>` decorator) or generated automatically from Python code via\ntracing. When using tracing, code is automatically converted into this subset of\nPython by recording only the actual operators on tensors and simply executing and\ndiscarding the other surrounding Python code.\n\nWhen writing TorchScript directly using ``@torch.jit.script`` decorator, the programmer must\nonly use the subset of Python supported in TorchScript. This section documents\nwhat is supported in TorchScript as if it were a language reference for a stand\nalone language. Any features of Python not mentioned in this reference are not\npart of TorchScript. See `Builtin Functions` for a complete reference of available\nPyTorch tensor methods, modules, and functions.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":30,"to":48}}}}],["592",{"pageContent":"As a subset of Python, any valid TorchScript function is also a valid Python\nfunction. This makes it possible to `disable TorchScript` and debug the\nfunction using standard Python tools like ``pdb``. The reverse is not true: there\nare many valid Python programs that are not valid TorchScript programs.\nInstead, TorchScript focuses specifically on the features of Python that are\nneeded to represent neural network models in PyTorch.\n\n.. _types:\n.. _supported type:\n\nTypes\n~~~~~\n\nThe largest difference between TorchScript and the full Python language is that\nTorchScript only supports a small set of types that are needed to express neural\nnet models. In particular, TorchScript supports:\n\n.. csv-table::\n   :header: \"Type\", \"Description\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":50,"to":68}}}}],["593",{"pageContent":".. csv-table::\n   :header: \"Type\", \"Description\"\n\n   \"``Tensor``\", \"A PyTorch tensor of any dtype, dimension, or backend\"\n   \"``Tuple[T0, T1, ..., TN]``\", \"A tuple containing subtypes ``T0``, ``T1``, etc. (e.g. ``Tuple[Tensor, Tensor]``)\"\n   \"``bool``\", \"A boolean value\"\n   \"``int``\", \"A scalar integer\"\n   \"``float``\", \"A scalar floating point number\"\n   \"``str``\", \"A string\"\n   \"``List[T]``\", \"A list of which all members are type ``T``\"\n   \"``Optional[T]``\", \"A value which is either None or type ``T``\"\n   \"``Dict[K, V]``\", \"A dict with key type ``K`` and value type ``V``. Only ``str``, ``int``, and ``float`` are allowed as key types.\"\n   \"``T``\", \"A `TorchScript Class`_\"\n   \"``E``\", \"A `TorchScript Enum`_\"\n   \"``NamedTuple[T0, T1, ...]``\", \"A :func:`collections.namedtuple <collections.namedtuple>` tuple type\"\n   \"``Union[T0, T1, ...]``\", \"One of the subtypes ``T0``, ``T1``, etc.\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":68,"to":83}}}}],["594",{"pageContent":"Unlike Python, each variable in TorchScript function must have a single static type.\nThis makes it easier to optimize TorchScript functions.\n\nExample (a type mismatch)\n\n.. testcode::\n\n    import torch\n\n    @torch.jit.script\n    def an_error(x):\n        if x:\n            r = torch.rand(1)\n        else:\n            r = 4\n        return r\n\n\n.. testoutput::\n\n     Traceback (most recent call last):\n       ...\n     RuntimeError: ...\n\n     Type mismatch: r is set to type Tensor in the true branch and type int in the false branch:\n     @torch.jit.script\n     def an_error(x):\n         if x:\n         ~~~~~\n             r = torch.rand(1)\n             ~~~~~~~~~~~~~~~~~\n         else:\n         ~~~~~\n             r = 4\n             ~~~~~ <--- HERE\n         return r\n     and was used here:\n         else:\n             r = 4\n         return r\n                ~ <--- HERE...","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":85,"to":125}}}}],["595",{"pageContent":"Unsupported Typing Constructs\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTorchScript does not support all features and types of the :mod:`typing` module. Some of these\nare more fundamental things that are unlikely to be added in the future while others\nmay be added if there is enough user demand to make it a priority.\n\nThese types and features from the :mod:`typing` module are unavailable in TorchScript.\n\n.. csv-table::\n   :header: \"Item\", \"Description\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":127,"to":136}}}}],["596",{"pageContent":".. csv-table::\n   :header: \"Item\", \"Description\"\n\n   \":any:`typing.Any`\", \":any:`typing.Any` is currently in development but not yet released\"\n   \":any:`typing.NoReturn`\", \"Not implemented\"\n   \":any:`typing.Sequence`\", \"Not implemented\"\n   \":any:`typing.Callable`\", \"Not implemented\"\n   \":any:`typing.Literal`\", \"Not implemented\"\n   \":any:`typing.ClassVar`\", \"Not implemented\"\n   \":any:`typing.Final`\", \"This is supported for :any:`module attributes <Module Attributes>` class attribute annotations but not for functions\"\n   \":any:`typing.AnyStr`\", \"TorchScript does not support :any:`bytes` so this type is not used\"\n   \":any:`typing.overload`\", \":any:`typing.overload` is currently in development but not yet released\"\n   \"Type aliases\", \"Not implemented\"\n   \"Nominal vs structural subtyping\", \"Nominal typing is in development, but structural typing is not\"\n   \"NewType\", \"Unlikely to be implemented\"\n   \"Generics\", \"Unlikely to be implemented\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":136,"to":151}}}}],["597",{"pageContent":"Any other functionality from the :any:`typing` module not explicitly listed in this documentation is unsupported.\n\nDefault Types\n^^^^^^^^^^^^^\n\nBy default, all parameters to a TorchScript function are assumed to be Tensor.\nTo specify that an argument to a TorchScript function is another type, it is possible to use\nMyPy-style type annotations using the types listed above.\n\n.. testcode::\n\n    import torch\n\n    @torch.jit.script\n    def foo(x, tup):\n        # type: (int, Tuple[Tensor, Tensor]) -> Tensor\n        t0, t1 = tup\n        return t0 + t1 + x\n\n    print(foo(3, (torch.rand(3), torch.rand(3))))\n\n.. testoutput::\n    :hide:\n\n    ...\n\n.. note::\n  It is also possible to annotate types with Python 3 type hints from the\n  ``typing`` module.\n\n  .. testcode::\n\n    import torch\n    from typing import Tuple\n\n    @torch.jit.script\n    def foo(x: int, tup: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        t0, t1 = tup\n        return t0 + t1 + x\n\n    print(foo(3, (torch.rand(3), torch.rand(3))))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":153,"to":193}}}}],["598",{"pageContent":"@torch.jit.script\n    def foo(x: int, tup: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        t0, t1 = tup\n        return t0 + t1 + x\n\n    print(foo(3, (torch.rand(3), torch.rand(3))))\n\n  .. testoutput::\n    :hide:\n\n    ...\n\n\nAn empty list is assumed to be ``List[Tensor]`` and empty dicts\n``Dict[str, Tensor]``. To instantiate an empty list or dict of other types,\nuse `Python 3 type hints`.\n\nExample (type annotations for Python 3):\n\n.. testcode::\n\n    import torch\n    import torch.nn as nn\n    from typing import Dict, List, Tuple\n\n    class EmptyDataStructures(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n\n        def forward(self, x: torch.Tensor) -> Tuple[List[Tuple[int, float]], Dict[str, int]]:\n            # This annotates the list to be a `List[Tuple[int, float]]`\n            my_list: List[Tuple[int, float]] = []\n            for i in range(10):\n                my_list.append((i, x.item()))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":193,"to":226}}}}],["599",{"pageContent":"my_dict: Dict[str, int] = {}\n            return my_list, my_dict\n\n    x = torch.jit.script(EmptyDataStructures())\n\n\n\n\nOptional Type Refinement\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nTorchScript will refine the type of a variable of type ``Optional[T]`` when\na comparison to ``None`` is made inside the conditional of an if-statement or checked in an ``assert``.\nThe compiler can reason about multiple ``None`` checks that are combined with\n``and``, ``or``, and ``not``. Refinement will also occur for else blocks of if-statements\nthat are not explicitly written.\n\nThe ``None`` check must be within the if-statement's condition; assigning\na ``None`` check to a variable and using it in the if-statement's condition will\nnot refine the types of variables in the check.\nOnly local variables will be refined, an attribute like ``self.x`` will not and must assigned to\na local variable to be refined.\n\n\nExample (refining types on parameters and locals):\n\n.. testcode::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":228,"to":254}}}}],["600",{"pageContent":"Example (refining types on parameters and locals):\n\n.. testcode::\n\n    import torch\n    import torch.nn as nn\n    from typing import Optional\n\n    class M(nn.Module):\n        z: Optional[int]\n\n        def __init__(self, z):\n            super().__init__()\n            # If `z` is None, its type cannot be inferred, so it must\n            # be specified (above)\n            self.z = z\n\n        def forward(self, x, y, z):\n            # type: (Optional[int], Optional[int], Optional[int]) -> int\n            if x is None:\n                x = 1\n                x = x + 1\n\n            # Refinement for an attribute by assigning it to a local\n            z = self.z\n            if y is not None and z is not None:\n                x = y + z\n\n            # Refinement via an `assert`\n            assert z is not None\n            x += z\n            return x\n\n    module = torch.jit.script(M(2))\n    module = torch.jit.script(M(None))\n\n\n.. _TorchScript Class:\n.. _TorchScript Classes:\n.. _torchscript-classes:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":254,"to":293}}}}],["601",{"pageContent":"module = torch.jit.script(M(2))\n    module = torch.jit.script(M(None))\n\n\n.. _TorchScript Class:\n.. _TorchScript Classes:\n.. _torchscript-classes:\n\nTorchScript Classes\n^^^^^^^^^^^^^^^^^^^\n\n.. warning::\n\n    TorchScript class support is experimental. Currently it is best suited\n    for simple record-like types (think a ``NamedTuple`` with methods\n    attached).\n\nPython classes can be used in TorchScript if they are annotated with :func:`@torch.jit.script <torch.jit.script>`,\nsimilar to how you would declare a TorchScript function:\n\n.. testcode::\n    :skipif: True  # TODO: fix the source file resolving so this can be tested\n\n    @torch.jit.script\n    class Foo:\n      def __init__(self, x, y):\n        self.x = x\n\n      def aug_add_x(self, inc):\n        self.x += inc\n\n\nThis subset is restricted:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":293,"to":325}}}}],["602",{"pageContent":"@torch.jit.script\n    class Foo:\n      def __init__(self, x, y):\n        self.x = x\n\n      def aug_add_x(self, inc):\n        self.x += inc\n\n\nThis subset is restricted:\n\n* All functions must be valid TorchScript functions (including ``__init__()``).\n* Classes must be new-style classes, as we use ``__new__()`` to construct them with pybind11.\n* TorchScript classes are statically typed. Members can only be declared by assigning to\n  self in the ``__init__()`` method.\n\n    For example, assigning to ``self`` outside of the ``__init__()`` method: ::\n\n        @torch.jit.script\n        class Foo:\n          def assign_x(self):\n            self.x = torch.rand(2, 3)\n\n    Will result in: ::\n\n        RuntimeError:\n        Tried to set nonexistent attribute: x. Did you forget to initialize it in __init__()?:\n        def assign_x(self):\n          self.x = torch.rand(2, 3)\n          ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":325,"to":354}}}}],["603",{"pageContent":"* No expressions except method definitions are allowed in the body of the class.\n* No support for inheritance or any other polymorphism strategy, except for inheriting\n  from ``object`` to specify a new-style class.\n\nAfter a class is defined, it can be used in both TorchScript and Python interchangeably\nlike any other TorchScript type:\n\n::\n\n    # Declare a TorchScript class\n    @torch.jit.script\n    class Pair:\n      def __init__(self, first, second):\n        self.first = first\n        self.second = second\n\n    @torch.jit.script\n    def sum_pair(p):\n      # type: (Pair) -> Tensor\n      return p.first + p.second\n\n    p = Pair(torch.rand(2, 3), torch.rand(2, 3))\n    print(sum_pair(p))\n\n\n.. _TorchScript Enum:\n.. _TorchScript Enums:\n.. _torchscript-enums:\n\nTorchScript Enums\n^^^^^^^^^^^^^^^^^^^\n\nPython enums can be used in TorchScript without any extra annotation or code:\n\n::\n\n    from enum import Enum\n\n\n    class Color(Enum):\n        RED = 1\n        GREEN = 2","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":356,"to":397}}}}],["604",{"pageContent":"TorchScript Enums\n^^^^^^^^^^^^^^^^^^^\n\nPython enums can be used in TorchScript without any extra annotation or code:\n\n::\n\n    from enum import Enum\n\n\n    class Color(Enum):\n        RED = 1\n        GREEN = 2\n\n    @torch.jit.script\n    def enum_fn(x: Color, y: Color) -> bool:\n        if x == Color.RED:\n            return True\n\n        return x == y\n\nAfter an enum is defined, it can be used in both TorchScript and Python interchangeably\nlike any other TorchScript type. The type of the values of an enum must be ``int``,\n``float``, or ``str``. All values must be of the same type; heterogenous types for enum\nvalues are not supported.\n\n\nNamed Tuples\n^^^^^^^^^^^^\nTypes produced by :func:`collections.namedtuple <collections.namedtuple>` can be used in TorchScript.\n\n.. testcode::\n\n    import torch\n    import collections\n\n    Point = collections.namedtuple('Point', ['x', 'y'])\n\n    @torch.jit.script\n    def total(point):\n        # type: (Point) -> Tensor\n        return point.x + point.y","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":397,"to":438}}}}],["605",{"pageContent":"Point = collections.namedtuple('Point', ['x', 'y'])\n\n    @torch.jit.script\n    def total(point):\n        # type: (Point) -> Tensor\n        return point.x + point.y\n\n    p = Point(x=torch.rand(3), y=torch.rand(3))\n    print(total(p))\n\n.. testoutput::\n    :hide:\n\n    ...\n\n\n.. _jit_iterables:\n\nIterables\n^^^^^^^^^\n\nSome functions (for example, :any:`zip` and :any:`enumerate`) can only operate on iterable types.\nIterable types in TorchScript include ``Tensor``\\s, lists, tuples, dictionaries, strings,\n:any:`torch.nn.ModuleList` and :any:`torch.nn.ModuleDict`.\n\n\nExpressions\n~~~~~~~~~~~\n\nThe following Python Expressions are supported.\n\nLiterals\n^^^^^^^^\n::\n\n    True\n    False\n    None\n    'string literals'\n    \"string literals\"\n    3  # interpreted as int\n    3.4  # interpreted as a float\n\nList Construction\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nAn empty list is assumed have type ``List[Tensor]``.\nThe types of other list literals are derived from the type of the members.\nSee `Default Types`_ for more details.\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":438,"to":487}}}}],["606",{"pageContent":"::\n\n    [3, 4]\n    []\n    [torch.rand(3), torch.rand(4)]\n\n\n\nTuple Construction\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n::\n\n    (3, 4)\n    (3,)\n\n\nDict Construction\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nAn empty dict is assumed have type ``Dict[str, Tensor]``.\nThe types of other dict literals are derived from the type of the members.\nSee `Default Types`_ for more details.\n\n::\n\n    {'hello': 3}\n    {}\n    {'a': torch.rand(3), 'b': torch.rand(4)}\n\n\nVariables\n^^^^^^^^^\nSee `Variable Resolution`_ for how variables are resolved.\n\n::\n\n    my_variable_name\n\nArithmetic Operators\n^^^^^^^^^^^^^^^^^^^^\n::\n\n    a + b\n    a - b\n    a * b\n    a / b\n    a ^ b\n    a @ b\n\nComparison Operators\n^^^^^^^^^^^^^^^^^^^^\n::\n\n    a == b\n    a != b\n    a < b\n    a > b\n    a <= b\n    a >= b\n\nLogical Operators\n^^^^^^^^^^^^^^^^^\n::\n\n    a and b\n    a or b\n    not b\n\nSubscripts and Slicing\n^^^^^^^^^^^^^^^^^^^^^^\n::\n\n    t[0]\n    t[-1]\n    t[0:2]\n    t[1:]\n    t[:1]\n    t[:]\n    t[0, 1]\n    t[0, 1:2]\n    t[0, :1]\n    t[-1, 1:, 0]\n    t[1:, -1, 0]\n    t[i:j, i]","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":487,"to":569}}}}],["607",{"pageContent":"Subscripts and Slicing\n^^^^^^^^^^^^^^^^^^^^^^\n::\n\n    t[0]\n    t[-1]\n    t[0:2]\n    t[1:]\n    t[:1]\n    t[:]\n    t[0, 1]\n    t[0, 1:2]\n    t[0, :1]\n    t[-1, 1:, 0]\n    t[1:, -1, 0]\n    t[i:j, i]\n\nFunction Calls\n^^^^^^^^^^^^^^\nCalls to `builtin functions`\n\n::\n\n    torch.rand(3, dtype=torch.int)\n\nCalls to other script functions:\n\n.. testcode::\n\n    import torch\n\n    @torch.jit.script\n    def foo(x):\n        return x + 1\n\n    @torch.jit.script\n    def bar(x):\n        return foo(x)\n\nMethod Calls\n^^^^^^^^^^^^\nCalls to methods of builtin types like tensor: ``x.mm(y)``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":569,"to":610}}}}],["608",{"pageContent":"@torch.jit.script\n    def bar(x):\n        return foo(x)\n\nMethod Calls\n^^^^^^^^^^^^\nCalls to methods of builtin types like tensor: ``x.mm(y)``\n\nOn modules, methods must be compiled before they can be called. The TorchScript\ncompiler recursively compiles methods it sees when compiling other methods. By default,\ncompilation starts on the ``forward`` method. Any methods called by ``forward`` will\nbe compiled, and any methods called by those methods, and so on. To start compilation at\na method other than ``forward``, use the :func:`@torch.jit.export <torch.jit.export>` decorator\n(``forward`` implicitly is marked ``@torch.jit.export``).\n\nCalling a submodule directly (e.g. ``self.resnet(input)``) is equivalent to\ncalling its ``forward`` method (e.g. ``self.resnet.forward(input)``).\n\n.. testcode::\n    :skipif: torchvision is None\n\n    import torch\n    import torch.nn as nn\n    import torchvision","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":610,"to":633}}}}],["609",{"pageContent":".. testcode::\n    :skipif: torchvision is None\n\n    import torch\n    import torch.nn as nn\n    import torchvision\n\n    class MyModule(nn.Module):\n        def __init__(self):\n            super().__init__()\n            means = torch.tensor([103.939, 116.779, 123.68])\n            self.means = torch.nn.Parameter(means.resize_(1, 3, 1, 1))\n            resnet = torchvision.models.resnet18()\n            self.resnet = torch.jit.trace(resnet, torch.rand(1, 3, 224, 224))\n\n        def helper(self, input):\n            return self.resnet(input - self.means)\n\n        def forward(self, input):\n            return self.helper(input)\n\n        # Since nothing in the model calls `top_level_method`, the compiler\n        # must be explicitly told to compile this method\n        @torch.jit.export\n        def top_level_method(self, input):\n            return self.other_helper(input)\n\n        def other_helper(self, input):\n            return input + 10","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":633,"to":661}}}}],["610",{"pageContent":"def other_helper(self, input):\n            return input + 10\n\n    # `my_script_module` will have the compiled methods `forward`, `helper`,\n    # `top_level_method`, and `other_helper`\n    my_script_module = torch.jit.script(MyModule())\n\n\nTernary Expressions\n^^^^^^^^^^^^^^^^^^^\n::\n\n    x if x > y else y\n\nCasts\n^^^^^\n::\n\n    float(ten)\n    int(3.5)\n    bool(ten)\n    str(2)``\n\nAccessing Module Parameters\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n::\n\n    self.my_parameter\n    self.my_submodule.my_parameter\n\n\nStatements\n~~~~~~~~~~\n\nTorchScript supports the following types of statements:\n\nSimple Assignments\n^^^^^^^^^^^^^^^^^^\n::\n\n    a = b\n    a += b # short-hand for a = a + b, does not operate in-place on a\n    a -= b\n\nPattern Matching Assignments\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n::\n\n    a, b = tuple_or_list\n    a, b, *c = a_tuple\n\nMultiple Assignments\n::\n\n    a = b, c = tup\n\nPrint Statements\n^^^^^^^^^^^^^^^^\n::\n\n    print(\"the result of an add:\", a + b)\n\nIf Statements\n^^^^^^^^^^^^^\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":661,"to":725}}}}],["611",{"pageContent":"a, b = tuple_or_list\n    a, b, *c = a_tuple\n\nMultiple Assignments\n::\n\n    a = b, c = tup\n\nPrint Statements\n^^^^^^^^^^^^^^^^\n::\n\n    print(\"the result of an add:\", a + b)\n\nIf Statements\n^^^^^^^^^^^^^\n::\n\n    if a < 4:\n        r = -a\n    elif a < 3:\n        r = a + a\n    else:\n        r = 3 * a\n\nIn addition to bools, floats, ints, and Tensors can be used in a conditional\nand will be implicitly casted to a boolean.\n\nWhile Loops\n^^^^^^^^^^^\n::\n\n    a = 0\n    while a < 4:\n        print(a)\n        a += 1\n\n\nFor loops with range\n^^^^^^^^^^^^^^^^^^^^\n::\n\n    x = 0\n    for i in range(10):\n        x *= i\n\nFor loops over tuples\n^^^^^^^^^^^^^^^^^^^^^\nThese unroll the loop, generating a body for\neach member of the tuple. The body must type-check correctly for each member.\n\n::\n\n    tup = (3, torch.rand(4))\n    for x in tup:\n        print(x)\n\n\nFor loops over constant nn.ModuleList\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":725,"to":784}}}}],["612",{"pageContent":"::\n\n    tup = (3, torch.rand(4))\n    for x in tup:\n        print(x)\n\n\nFor loops over constant nn.ModuleList\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTo use a ``nn.ModuleList`` inside a compiled method, it must be marked\nconstant by adding the name of the attribute to the ``__constants__``\nlist for the type. For loops over a ``nn.ModuleList`` will unroll the body of the\nloop at compile time, with each member of the constant module list.\n\n.. testcode::\n\n    class SubModule(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.weight = nn.Parameter(torch.randn(2))\n\n        def forward(self, input):\n            return self.weight + input\n\n    class MyModule(torch.nn.Module):\n        __constants__ = ['mods']\n\n        def __init__(self):\n            super().__init__()\n            self.mods = torch.nn.ModuleList([SubModule() for i in range(10)])\n\n        def forward(self, v):\n            for module in self.mods:\n                v = module(v)\n            return v","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":784,"to":819}}}}],["613",{"pageContent":"def forward(self, v):\n            for module in self.mods:\n                v = module(v)\n            return v\n\n\n    m = torch.jit.script(MyModule())\n\n\n\nBreak and Continue\n^^^^^^^^^^^^^^^^^^\n::\n\n    for i in range(5):\n        if i == 1:\n            continue\n        if i == 3:\n            break\n        print(i)\n\nReturn\n^^^^^^\n::\n\n    return a, b\n\nVariable Resolution\n~~~~~~~~~~~~~~~~~~~\n\nTorchScript supports a subset of Python's variable resolution (i.e. scoping)\nrules. Local variables behave the same as in Python, except for the restriction\nthat a variable must have the same type along all paths through a function.\nIf a variable has a different type on different branches of an if statement, it\nis an error to use it after the end of the if statement.\n\nSimilarly, a variable is not allowed to be used if it is only *defined* along some\npaths through the function.\n\nExample:\n\n.. testcode::\n\n    @torch.jit.script\n    def foo(x):\n        if x < 0:\n            y = 4\n        print(y)\n\n.. testoutput::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":819,"to":868}}}}],["614",{"pageContent":"Example:\n\n.. testcode::\n\n    @torch.jit.script\n    def foo(x):\n        if x < 0:\n            y = 4\n        print(y)\n\n.. testoutput::\n\n     Traceback (most recent call last):\n       ...\n     RuntimeError: ...\n\n     y is not defined in the false branch...\n     @torch.jit.script...\n     def foo(x):\n         if x < 0:\n         ~~~~~~~~~\n             y = 4\n             ~~~~~ <--- HERE\n         print(y)\n     and was used here:\n         if x < 0:\n             y = 4\n         print(y)\n               ~ <--- HERE...\n\nNon-local variables are resolved to Python values at compile time when the\nfunction is defined. These values are then converted into TorchScript values using\nthe rules described in `Use of Python Values`_.\n\nUse of Python Values\n~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":868,"to":903}}}}],["615",{"pageContent":"Use of Python Values\n~~~~~~~~~~~~~~~~~~~~\n\nTo make writing TorchScript more convenient, we allow script code to refer\nto Python values in the surrounding scope. For instance, any time there is a\nreference to ``torch``, the TorchScript compiler is actually resolving it to the\n``torch`` Python module when the function is declared.  These Python values are\nnot a first class part of TorchScript. Instead they are de-sugared at compile-time\ninto the primitive types that TorchScript supports. This depends\non the dynamic type of the Python valued referenced when compilation occurs.\nThis section describes the rules that are used when accessing Python values in TorchScript.\n\nFunctions\n^^^^^^^^^\n\nTorchScript can call Python functions. This functionality is very useful when\nincrementally converting a model to TorchScript. The model can be moved function-by-function\nto TorchScript, leaving calls to Python functions in place. This way you can incrementally\ncheck the correctness of the model as you go.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":903,"to":921}}}}],["616",{"pageContent":".. autofunction:: torch.jit.is_scripting\n\n.. autofunction:: torch.jit.is_tracing\n\n\nAttribute Lookup On Python Modules\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTorchScript can lookup attributes on modules. `Builtin functions` like ``torch.add``\nare accessed this way. This allows TorchScript to call functions defined in\nother modules.\n\n.. _constant:\n\nPython-defined Constants\n^^^^^^^^^^^^^^^^^^^^^^^^\nTorchScript also provides a way to use constants that are defined in Python.\nThese can be used to hard-code hyper-parameters into the function, or to\ndefine universal constants. There are two ways of specifying that a Python\nvalue should be treated as a constant.\n\n1. Values looked up as attributes of a module are assumed to be constant:\n\n.. testcode::\n\n    import math\n    import torch\n\n    @torch.jit.script\n    def fn():\n        return math.pi\n\n2. Attributes of a ScriptModule can be marked constant by annotating them with ``Final[T]``\n\n::\n\n    import torch\n    import torch.nn as nn","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":924,"to":960}}}}],["617",{"pageContent":"@torch.jit.script\n    def fn():\n        return math.pi\n\n2. Attributes of a ScriptModule can be marked constant by annotating them with ``Final[T]``\n\n::\n\n    import torch\n    import torch.nn as nn\n\n    class Foo(nn.Module):\n        # `Final` from the `typing_extensions` module can also be used\n        a : torch.jit.Final[int]\n\n        def __init__(self):\n            super().__init__()\n            self.a = 1 + 4\n\n        def forward(self, input):\n            return self.a + input\n\n    f = torch.jit.script(Foo())\n\nSupported constant Python types are\n\n* ``int``\n* ``float``\n* ``bool``\n* ``torch.device``\n* ``torch.layout``\n* ``torch.dtype``\n* tuples containing supported types\n* ``torch.nn.ModuleList`` which can be used in a TorchScript for loop\n\n\n\n\n.. _module attributes:\n\nModule Attributes\n^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":960,"to":1001}}}}],["618",{"pageContent":".. _module attributes:\n\nModule Attributes\n^^^^^^^^^^^^^^^^^\n\nThe ``torch.nn.Parameter`` wrapper and ``register_buffer`` can be used to assign\ntensors to a module. Other values assigned to a module that is compiled\nwill be added to the compiled module if their types can be inferred. All `types`_\navailable in TorchScript can be used as module attributes. Tensor attributes are\nsemantically the same as buffers. The type of empty lists and dictionaries and ``None``\nvalues cannot be inferred and must be specified via\n`PEP 526-style <https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations>`_ class annotations.\nIf a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute\nto the resulting :class:`ScriptModule`.\n\nExample:\n\n.. testcode::\n\n    from typing import List, Dict\n\n    class Foo(nn.Module):\n        # `words` is initialized as an empty list, so its type must be specified\n        words: List[str]","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":1001,"to":1024}}}}],["619",{"pageContent":"Example:\n\n.. testcode::\n\n    from typing import List, Dict\n\n    class Foo(nn.Module):\n        # `words` is initialized as an empty list, so its type must be specified\n        words: List[str]\n\n        # The type could potentially be inferred if `a_dict` (below) was not\n        # empty, but this annotation ensures `some_dict` will be made into the\n        # proper type\n        some_dict: Dict[str, int]\n\n        def __init__(self, a_dict):\n            super().__init__()\n            self.words = []\n            self.some_dict = a_dict\n\n            # `int`s can be inferred\n            self.my_int = 10\n\n        def forward(self, input):\n            # type: (str) -> int\n            self.words.append(input)\n            return self.some_dict[input] + self.my_int\n\n    f = torch.jit.script(Foo({'hi': 2}))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference.rst","loc":{"lines":{"from":1024,"to":1052}}}}],["620",{"pageContent":".. testsetup::\n\n    # These are hidden from the docs, but these are necessary for `doctest`\n    # since the `inspect` module doesn't play nicely with the execution\n    # environment for `doctest`\n    import torch\n\n    original_script = torch.jit.script\n    def script_wrapper(obj, *args, **kwargs):\n        obj.__module__ = 'FakeMod'\n        return original_script(obj, *args, **kwargs)\n\n    torch.jit.script = script_wrapper\n\n    original_trace = torch.jit.trace\n    def trace_wrapper(obj, *args, **kwargs):\n        obj.__module__ = 'FakeMod'\n        return original_trace(obj, *args, **kwargs)\n\n    torch.jit.trace = trace_wrapper\n\n.. _language-reference-v2:\n\nTorchScript Language Reference\n==============================","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1,"to":25}}}}],["621",{"pageContent":"torch.jit.trace = trace_wrapper\n\n.. _language-reference-v2:\n\nTorchScript Language Reference\n==============================\n\nThis reference manual describes the syntax and core semantics of the TorchScript language.\nTorchScript is a statically typed subset of the Python language. This document explains the supported features of\nPython in TorchScript and also how the language diverges from regular Python. Any features of Python that are not mentioned in\nthis reference manual are not part of TorchScript. TorchScript focuses specifically on the features of Python that are needed to\nrepresent neural network models in PyTorch.\n\n.. contents::\n    :local:\n    :depth: 1\n\n.. _type_system:\n\nTerminology\n~~~~~~~~~~~\n\nThis document uses the following terminologies:\n\n.. list-table::\n   :widths: 25 25\n   :header-rows: 1","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":25,"to":51}}}}],["622",{"pageContent":".. contents::\n    :local:\n    :depth: 1\n\n.. _type_system:\n\nTerminology\n~~~~~~~~~~~\n\nThis document uses the following terminologies:\n\n.. list-table::\n   :widths: 25 25\n   :header-rows: 1\n\n   * - Pattern\n     - Notes\n   * - ``::=``\n     - Indicates that the given symbol is defined as.\n   * - ``\" \"``\n     - Represents real keywords and delimiters that are part of the syntax.\n   * - ``A | B``\n     - Indicates either A or B.\n   * - ``( )``\n     - Indicates grouping.\n   * - ``[]``\n     - Indicates optional.\n   * - ``A+``\n     - Indicates a regular expression where term A is repeated at least once.\n   * - ``A*``\n     - Indicates a regular expression where term A is repeated zero or more times.\n\nType System\n~~~~~~~~~~~\nTorchScript is a statically typed subset of Python. The largest difference between TorchScript and the full Python language is that TorchScript only supports a small set of types that are needed to express\nneural net models.\n\nTorchScript Types\n^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":51,"to":89}}}}],["623",{"pageContent":"TorchScript Types\n^^^^^^^^^^^^^^^^^\n\nThe TorchScript type system consists of ``TSType`` and ``TSModuleType`` as defined below.\n\n::\n\n    TSAllType ::= TSType | TSModuleType\n    TSType    ::= TSMetaType | TSPrimitiveType | TSStructuralType | TSNominalType\n\n``TSType`` represents the majority of TorchScript types that are composable and that can be used in TorchScript type annotations.\n``TSType`` refers to any of the following:\n\n* Meta Types, e.g., ``Any``\n* Primitive Types, e.g., ``int``, ``float``, and ``str``\n* Structural Types, e.g., ``Optional[int]`` or ``List[MyClass]``\n* Nominal Types (Python classes), e.g., ``MyClass`` (user-defined), ``torch.tensor`` (built-in)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":89,"to":105}}}}],["624",{"pageContent":"``TSModuleType`` represents ``torch.nn.Module`` and its subclasses. It is treated differently from ``TSType`` because its type schema is inferred partly from the object instance and partly from the class definition.\nAs such, instances of a ``TSModuleType`` may not follow the same static type schema. ``TSModuleType`` cannot be used as a TorchScript type annotation or be composed with ``TSType`` for type safety considerations.\n\nMeta Types\n^^^^^^^^^^\n\nMeta types are so abstract that they are more like type constraints than concrete types.\nCurrently TorchScript defines one meta-type, ``Any``, that represents any TorchScript type.\n\n``Any`` Type\n\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe ``Any`` type represents any TorchScript type. ``Any`` specifies no type constraints, thus there is no type-checking on ``Any``.\nAs such it can be bound to any Python or TorchScript data types (e.g., ``int``, TorchScript ``tuple``, or an arbitrary Python class that is not scripted).\n\n::\n\n    TSMetaType ::= \"Any\"\n\nWhere:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":107,"to":126}}}}],["625",{"pageContent":"::\n\n    TSMetaType ::= \"Any\"\n\nWhere:\n\n* ``Any`` is the Python class name from the typing module. Therefore, to use the ``Any`` type, you must import it from ``typing`` (e.g., ``from typing import Any``).\n* Since ``Any`` can represent any TorchScript type, the set of operators that are allowed to operate on values of this type on ``Any`` is limited.\n\nOperators Supported for ``Any`` Type\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n* Assignment to data of ``Any`` type.\n* Binding to parameter or return of ``Any`` type.\n* ``x is``, ``x is not`` where ``x`` is of ``Any`` type.\n* ``isinstance(x, Type)`` where ``x`` is of ``Any`` type.\n* Data of ``Any`` type is printable.\n* Data of ``List[Any]`` type may be sortable if the data is a list of values of the same type ``T`` and that ``T`` supports comparison operators.\n\n**Compared to Python**","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":126,"to":145}}}}],["626",{"pageContent":"**Compared to Python**\n\n\n``Any`` is the least constrained type in the TorchScript type system. In that sense, it is quite similar to the\n``Object`` class in Python. However, ``Any`` only supports a subset of the operators and methods that are supported by ``Object``.\n\nDesign Notes\n\"\"\"\"\"\"\"\"\"\"\"\"\n\nWhen we script a PyTorch module, we may encounter data that is not involved in the execution of the script. Nevertheless, it has to be described\nby a type schema. It is not only cumbersome to describe static types for unused data (in the context of the script), but also may lead to unnecessary\nscripting failures. ``Any`` is introduced to describe the type of the data where precise static types are not necessary for compilation.\n\n**Example 1**\n\nThis example illustrates how ``Any`` can be used to allow the second element of the tuple parameter to be of any type. This is possible\nbecause ``x[1]`` is not involved in any computation that requires knowing its precise type.\n\n.. testcode::\n\n    import torch","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":145,"to":165}}}}],["627",{"pageContent":".. testcode::\n\n    import torch\n\n    from typing import Tuple\n    from typing import Any\n\n    @torch.jit.export\n    def inc_first_element(x: Tuple[int, Any]):\n        return (x[0]+1, x[1])\n\n    m = torch.jit.script(inc_first_element)\n    print(m((1,2.0)))\n    print(m((1,(100,200))))\n\nThe example above produces the following output:\n\n.. testoutput::\n\n    (2, 2.0)\n    (2, (100, 200))\n\nThe second element of the tuple is of ``Any`` type, thus can bind to multiple types.\nFor example, ``(1, 2.0)`` binds a float type to ``Any`` as in ``Tuple[int, Any]``,\nwhereas ``(1, (100, 200))`` binds a tuple to ``Any`` in the second invocation.\n\n\n**Example 2**\n\nThis example illustrates how we can use ``isinstance`` to dynamically check the type of the data that is annotated as ``Any`` type:\n\n.. testcode::\n\n    import torch\n    from typing import Any\n\n    def f(a:Any):\n        print(a)\n        return (isinstance(a, torch.Tensor))\n\n    ones = torch.ones([2])\n    m = torch.jit.script(f)\n    print(m(ones))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":165,"to":207}}}}],["628",{"pageContent":"import torch\n    from typing import Any\n\n    def f(a:Any):\n        print(a)\n        return (isinstance(a, torch.Tensor))\n\n    ones = torch.ones([2])\n    m = torch.jit.script(f)\n    print(m(ones))\n\nThe example above produces the following output:\n\n.. testoutput::\n\n     1\n     1\n    [ CPUFloatType{2} ]\n    True\n\nPrimitive Types\n^^^^^^^^^^^^^^^\n\nPrimitive TorchScript types are types that represent a single type of value and go with a single pre-defined\ntype name.\n\n::\n\n    TSPrimitiveType ::= \"int\" | \"float\" | \"double\" | \"complex\" | \"bool\" | \"str\" | \"None\"\n\nStructural Types\n^^^^^^^^^^^^^^^^\n\nStructural types are types that are structurally defined without a user-defined name (unlike nominal types),\nsuch as ``Future[int]``. Structural types are composable with any ``TSType``.\n\n::\n\n    TSStructuralType ::=  TSTuple | TSNamedTuple | TSList | TSDict |\n                        TSOptional | TSUnion | TSFuture | TSRRef | TSAwait","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":207,"to":246}}}}],["629",{"pageContent":"::\n\n    TSStructuralType ::=  TSTuple | TSNamedTuple | TSList | TSDict |\n                        TSOptional | TSUnion | TSFuture | TSRRef | TSAwait\n\n    TSTuple          ::= \"Tuple\" \"[\" (TSType \",\")* TSType \"]\"\n    TSNamedTuple     ::= \"namedtuple\" \"(\" (TSType \",\")* TSType \")\"\n    TSList           ::= \"List\" \"[\" TSType \"]\"\n    TSOptional       ::= \"Optional\" \"[\" TSType \"]\"\n    TSUnion          ::= \"Union\" \"[\" (TSType \",\")* TSType \"]\"\n    TSFuture         ::= \"Future\" \"[\" TSType \"]\"\n    TSRRef           ::= \"RRef\" \"[\" TSType \"]\"\n    TSAwait          ::= \"Await\" \"[\" TSType \"]\"\n    TSDict           ::= \"Dict\" \"[\" KeyType \",\" TSType \"]\"\n    KeyType          ::= \"str\" | \"int\" | \"float\" | \"bool\" | TensorType | \"Any\"\n\nWhere:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":246,"to":262}}}}],["630",{"pageContent":"Where:\n\n* ``Tuple``, ``List``, ``Optional``, ``Union``, ``Future``, ``Dict`` represent Python type class names that are defined in the module ``typing``. To use these type names, you must import them from ``typing`` (e.g., ``from typing import Tuple``).\n* ``namedtuple`` represents the Python class ``collections.namedtuple`` or ``typing.NamedTuple``.\n* ``Future`` and ``RRef`` represent the Python classes ``torch.futures`` and ``torch.distributed.rpc``.\n* ``Await`` represent the Python class ``torch._awaits._Await``\n\n**Compared to Python**\n\nApart from being composable with TorchScript types, these TorchScript structural types often support a common subset of the operators and methods of their Python counterparts.\n\n**Example 1**\n\nThis example uses ``typing.NamedTuple`` syntax to define a tuple:\n\n.. testcode::\n\n    import torch\n    from typing import NamedTuple\n    from typing import Tuple\n\n    class MyTuple(NamedTuple):\n        first: int\n        second: int","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":262,"to":285}}}}],["631",{"pageContent":".. testcode::\n\n    import torch\n    from typing import NamedTuple\n    from typing import Tuple\n\n    class MyTuple(NamedTuple):\n        first: int\n        second: int\n\n    def inc(x: MyTuple) -> Tuple[int, int]:\n        return (x.first+1, x.second+1)\n\n    t = MyTuple(first=1, second=2)\n    scripted_inc = torch.jit.script(inc)\n    print(\"TorchScript:\", scripted_inc(t))\n\nThe example above produces the following output:\n\n.. testoutput::\n\n    TorchScript: (2, 3)\n\n**Example 2**\n\nThis example uses ``collections.namedtuple`` syntax to define a tuple:\n\n.. testcode::\n\n    import torch\n    from typing import NamedTuple\n    from typing import Tuple\n    from collections import namedtuple\n\n    _AnnotatedNamedTuple = NamedTuple('_NamedTupleAnnotated', [('first', int), ('second', int)])\n    _UnannotatedNamedTuple = namedtuple('_NamedTupleAnnotated', ['first', 'second'])\n\n    def inc(x: _AnnotatedNamedTuple) -> Tuple[int, int]:\n        return (x.first+1, x.second+1)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":285,"to":323}}}}],["632",{"pageContent":"def inc(x: _AnnotatedNamedTuple) -> Tuple[int, int]:\n        return (x.first+1, x.second+1)\n\n    m = torch.jit.script(inc)\n    print(inc(_UnannotatedNamedTuple(1,2)))\n\nThe example above produces the following output:\n\n.. testoutput::\n\n    (2, 3)\n\n**Example 3**\n\nThis example illustrates a common mistake of annotating structural types, i.e., not importing the composite type\nclasses from the ``typing`` module:\n\n::\n\n    import torch\n\n    # ERROR: Tuple not recognized because not imported from typing\n    @torch.jit.export\n    def inc(x: Tuple[int, int]):\n        return (x[0]+1, x[1]+1)\n\n    m = torch.jit.script(inc)\n    print(m((1,2)))\n\nRunning the above code yields the following scripting error:\n\n::\n\n    File \"test-tuple.py\", line 5, in <module>\n        def inc(x: Tuple[int, int]):\n    NameError: name 'Tuple' is not defined\n\nThe remedy is to add the line ``from typing import Tuple`` to the beginning of the code.\n\nNominal Types\n^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":323,"to":363}}}}],["633",{"pageContent":"The remedy is to add the line ``from typing import Tuple`` to the beginning of the code.\n\nNominal Types\n^^^^^^^^^^^^^\n\nNominal TorchScript types are Python classes. These types are called nominal because they are declared with a custom\nname and are compared using class names. Nominal classes are further classified into the following categories:\n\n::\n\n    TSNominalType ::= TSBuiltinClasses | TSCustomClass | TSEnum\n\nAmong them, ``TSCustomClass`` and ``TSEnum`` must be compilable to TorchScript Intermediate Representation (IR). This is enforced by the type-checker.\n\nBuilt-in Class\n^^^^^^^^^^^^^^\n\nBuilt-in nominal types are Python classes whose semantics are built into the TorchScript system (e.g., tensor types).\nTorchScript defines the semantics of these built-in nominal types, and often supports only a subset of the methods or\nattributes of its Python class definition.\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":363,"to":384}}}}],["634",{"pageContent":"::\n\n    TSBuiltinClass ::= TSTensor | \"torch.device\" | \"torch.Stream\" | \"torch.dtype\" |\n                       \"torch.nn.ModuleList\" | \"torch.nn.ModuleDict\" | ...\n    TSTensor       ::= \"torch.Tensor\" | \"common.SubTensor\" | \"common.SubWithTorchFunction\" |\n                       \"torch.nn.parameter.Parameter\" | and subclasses of torch.Tensor\n\n\nSpecial Note on torch.nn.ModuleList and torch.nn.ModuleDict\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nAlthough ``torch.nn.ModuleList`` and ``torch.nn.ModuleDict`` are defined as a list and dictionary in Python,\nthey behave more like tuples in TorchScript:\n\n* In TorchScript, instances of ``torch.nn.ModuleList``  or ``torch.nn.ModuleDict`` are immutable.\n* Code that iterates over ``torch.nn.ModuleList`` or ``torch.nn.ModuleDict`` is completely unrolled so that elements of ``torch.nn.ModuleList`` or keys of ``torch.nn.ModuleDict`` can be of different subclasses of ``torch.nn.Module``.\n\n**Example**","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":384,"to":401}}}}],["635",{"pageContent":"**Example**\n\nThe following example highlights the use of a few built-in Torchscript classes (``torch.*``):\n\n::\n\n    import torch\n\n    @torch.jit.script\n    class A:\n        def __init__(self):\n            self.x = torch.rand(3)\n\n        def f(self, y: torch.device):\n            return self.x.to(device=y)\n\n    def g():\n        a = A()\n        return a.f(torch.device(\"cpu\"))\n\n    script_g = torch.jit.script(g)\n    print(script_g.graph)\n\nCustom Class\n^^^^^^^^^^^^\n\nUnlike built-in classes, semantics of custom classes are user-defined and the entire class definition must be compilable to TorchScript IR and subject to TorchScript type-checking rules.\n\n::\n\n    TSClassDef ::= [ \"@torch.jit.script\" ]\n                     \"class\" ClassName [ \"(object)\" ]  \":\"\n                        MethodDefinition |\n                    [ \"@torch.jit.ignore\" ] | [ \"@torch.jit.unused\" ]\n                        MethodDefinition\n\nWhere:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":401,"to":437}}}}],["636",{"pageContent":"Where:\n\n* Classes must be new-style classes. Python 3 supports only new-style classes. In Python 2.x, a new-style class is specified by subclassing from the object.\n* Instance data attributes are statically typed, and instance attributes must be declared by assignments inside the ``__init__()`` method.\n* Method overloading is not supported (i.e., you cannot have multiple methods with the same method name).\n* ``MethodDefinition`` must be compilable to TorchScript IR and adhere to TorchScript’s type-checking rules, (i.e., all methods must be valid TorchScript functions and class attribute definitions must be valid TorchScript statements).\n* ``torch.jit.ignore`` and ``torch.jit.unused`` can be used to ignore the method or function that is not fully torchscriptable or should be ignored by the compiler.\n\n**Compared to Python**\n\n\nTorchScript custom classes are quite limited compared to their Python counterpart. Torchscript custom classes:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":437,"to":448}}}}],["637",{"pageContent":"**Compared to Python**\n\n\nTorchScript custom classes are quite limited compared to their Python counterpart. Torchscript custom classes:\n\n* Do not support class attributes.\n* Do not support subclassing except for subclassing an interface type or object.\n* Do not support method overloading.\n* Must initialize all its instance attributes in  ``__init__()``; this is because TorchScript constructs a static schema of the class by inferring attribute types in ``__init__()``.\n* Must contain only methods that satisfy TorchScript type-checking rules and are compilable to TorchScript IRs.\n\n**Example 1**\n\nPython classes can be used in TorchScript if they are annotated with ``@torch.jit.script``, similar to how a TorchScript function would be declared:\n\n::\n\n    @torch.jit.script\n    class MyClass:\n        def __init__(self, x: int):\n            self.x = x\n\n        def inc(self, val: int):\n            self.x += val\n\n\n**Example 2**","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":448,"to":474}}}}],["638",{"pageContent":"::\n\n    @torch.jit.script\n    class MyClass:\n        def __init__(self, x: int):\n            self.x = x\n\n        def inc(self, val: int):\n            self.x += val\n\n\n**Example 2**\n\nA TorchScript custom class type must \"declare\" all its instance attributes by assignments in ``__init__()``. If an instance attribute is not defined in ``__init__()`` but accessed in other methods of the class, the class cannot be compiled as a TorchScript class, as shown in the following example:\n\n::\n\n    import torch\n\n    @torch.jit.script\n    class foo:\n        def __init__(self):\n            self.y = 1\n\n    # ERROR: self.x is not defined in __init__\n    def assign_x(self):\n        self.x = torch.rand(2, 3)\n\nThe class will fail to compile and issue the following error:\n\n::\n\n    RuntimeError:\n    Tried to set nonexistent attribute: x. Did you forget to initialize it in __init__()?:\n    def assign_x(self):\n        self.x = torch.rand(2, 3)\n        ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n\n**Example 3**","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":474,"to":512}}}}],["639",{"pageContent":"**Example 3**\n\nIn this example, a TorchScript custom class defines a class variable name, which is not allowed:\n\n::\n\n    import torch\n\n    @torch.jit.script\n    class MyClass(object):\n        name = \"MyClass\"\n        def __init__(self, x: int):\n            self.x = x\n\n    def fn(a: MyClass):\n        return a.name\n\nIt leads to the following compile-time error:\n\n::\n\n    RuntimeError:\n    '__torch__.MyClass' object has no attribute or method 'name'. Did you forget to initialize an attribute in __init__()?:\n        File \"test-class2.py\", line 10\n    def fn(a: MyClass):\n        return a.name\n            ~~~~~~ <--- HERE\n\nEnum Type\n^^^^^^^^^\n\nLike custom classes, semantics of the enum type are user-defined and the entire class definition must be compilable to TorchScript IR and adhere to TorchScript type-checking rules.\n\n::\n\n    TSEnumDef ::= \"class\" Identifier \"(enum.Enum | TSEnumType)\" \":\"\n                   ( MemberIdentifier \"=\" Value )+\n                   ( MethodDefinition )*\n\nWhere:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":512,"to":551}}}}],["640",{"pageContent":"::\n\n    TSEnumDef ::= \"class\" Identifier \"(enum.Enum | TSEnumType)\" \":\"\n                   ( MemberIdentifier \"=\" Value )+\n                   ( MethodDefinition )*\n\nWhere:\n\n* Value must be a TorchScript literal of type ``int``, ``float``, or ``str``, and must be of the same TorchScript type.\n* ``TSEnumType`` is the name of a TorchScript enumerated type. Similar to Python enum, TorchScript allows restricted ``Enum`` subclassing, that is, subclassing an enumerated is allowed only if it does not define any members.\n\n**Compared to Python**\n\n\n* TorchScript supports only ``enum.Enum``. It does not support other variations such as ``enum.IntEnum``, ``enum.Flag``, ``enum.IntFlag``, and ``enum.auto``.\n* Values of TorchScript enum members must be of the same type and can only be ``int``, ``float``, or ``str`` types, whereas Python enum members can be of any type.\n* Enums containing methods are ignored in TorchScript.\n\n**Example 1**\n\nThe following example defines the class ``Color`` as an ``Enum`` type:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":551,"to":573}}}}],["641",{"pageContent":"**Example 1**\n\nThe following example defines the class ``Color`` as an ``Enum`` type:\n\n::\n\n    import torch\n    from enum import Enum\n\n    class Color(Enum):\n        RED = 1\n        GREEN = 2\n\n    def enum_fn(x: Color, y: Color) -> bool:\n        if x == Color.RED:\n            return True\n        return x == y\n\n    m = torch.jit.script(enum_fn)\n\n    print(\"Eager: \", enum_fn(Color.RED, Color.GREEN))\n    print(\"TorchScript: \", m(Color.RED, Color.GREEN))\n\n**Example 2**\n\nThe following example shows the case of restricted enum subclassing, where ``BaseColor`` does not define any member, thus can be subclassed by ``Color``:\n\n::\n\n    import torch\n    from enum import Enum\n\n    class BaseColor(Enum):\n        def foo(self):\n            pass\n\n    class Color(BaseColor):\n        RED = 1\n        GREEN = 2\n\n    def enum_fn(x: Color, y: Color) -> bool:\n        if x == Color.RED:\n            return True\n        return x == y\n\n    m = torch.jit.script(enum_fn)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":573,"to":618}}}}],["642",{"pageContent":"def enum_fn(x: Color, y: Color) -> bool:\n        if x == Color.RED:\n            return True\n        return x == y\n\n    m = torch.jit.script(enum_fn)\n\n    print(\"TorchScript: \", m(Color.RED, Color.GREEN))\n    print(\"Eager: \", enum_fn(Color.RED, Color.GREEN))\n\nTorchScript Module Class\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n``TSModuleType`` is a special class type that is inferred from object instances that are created outside TorchScript. ``TSModuleType`` is named by the Python class of the object instance. The ``__init__()`` method of the Python class is not considered a TorchScript method, so it does not have to comply with TorchScript’s type-checking rules.\n\nThe type schema of a module instance class is constructed directly from an instance object (created outside the scope of TorchScript) rather than inferred from ``__init__()`` like custom classes. It is possible that two objects of the same instance class type follow two different type schemas.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":618,"to":633}}}}],["643",{"pageContent":"In this sense, ``TSModuleType`` is not really a static type. Therefore, for type safety considerations, ``TSModuleType`` cannot be used in a TorchScript type annotation or be composed with ``TSType``.\n\nModule Instance Class\n^^^^^^^^^^^^^^^^^^^^^\n\nTorchScript module type represents the type schema of a user-defined PyTorch module instance.  When scripting a PyTorch module, the module object is always created outside TorchScript (i.e., passed in as parameter to ``forward``). The Python module class is treated as a module instance class, so the ``__init__()`` method of the Python module class is not subject to the type-checking rules of TorchScript.\n\n::\n\n    TSModuleType ::= \"class\" Identifier \"(torch.nn.Module)\" \":\"\n                        ClassBodyDefinition\n\nWhere:\n\n* ``forward()`` and other methods decorated with ``@torch.jit.export`` must be compilable to TorchScript IR and subject to TorchScript’s type-checking rules.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":635,"to":649}}}}],["644",{"pageContent":"Where:\n\n* ``forward()`` and other methods decorated with ``@torch.jit.export`` must be compilable to TorchScript IR and subject to TorchScript’s type-checking rules.\n\nUnlike custom classes, only the forward method and other methods decorated with ``@torch.jit.export``  of the module type need to be compilable. Most notably, ``__init__()`` is not considered a TorchScript method. Consequently, module type constructors cannot be invoked within the scope of TorchScript. Instead, TorchScript module objects are always constructed outside and passed into ``torch.jit.script(ModuleObj)``.\n\n**Example 1**\n\nThis example illustrates a few features of module types:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":649,"to":657}}}}],["645",{"pageContent":"*  The ``TestModule`` instance is created outside the scope of TorchScript (i.e., before invoking ``torch.jit.script``).\n* ``__init__()`` is not considered a TorchScript method, therefore, it does not have to be annotated and can contain arbitrary Python code. In addition, the ``__init__()`` method of an instance class cannot be invoked in TorchScript code. Because ``TestModule`` instances are instantiated in Python, in this example, ``TestModule(2.0)`` and ``TestModule(2)`` create two instances with different types for its data attributes. ``self.x`` is of type ``float`` for ``TestModule(2.0)``, whereas ``self.y`` is of type ``int`` for ``TestModule(2.0)``.\n* TorchScript automatically compiles other methods (e.g., ``mul()``) invoked by methods annotated via ``@torch.jit.export`` or ``forward()`` methods.\n* Entry-points to a TorchScript program are either ``forward()`` of a module type, functions annotated as ``torch.jit.script``, or methods annotated as ``torch.jit.export``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":659,"to":662}}}}],["646",{"pageContent":".. testcode::\n\n    import torch\n\n    class TestModule(torch.nn.Module):\n        def __init__(self, v):\n            super().__init__()\n            self.x = v\n\n        def forward(self, inc: int):\n            return self.x + inc\n\n    m = torch.jit.script(TestModule(1))\n    print(f\"First instance: {m(3)}\")\n\n    m = torch.jit.script(TestModule(torch.ones([5])))\n    print(f\"Second instance: {m(3)}\")\n\nThe example above produces the following output:\n\n.. testoutput::\n\n    First instance: 4\n    Second instance: tensor([4., 4., 4., 4., 4.])\n\n**Example 2**\n\nThe following example shows an incorrect usage of module type. Specifically, this example invokes the constructor of ``TestModule`` inside the scope of TorchScript:\n\n.. testcode::\n\n    import torch\n\n    class TestModule(torch.nn.Module):\n        def __init__(self, v):\n            super().__init__()\n            self.x = v\n\n        def forward(self, x: int):\n            return self.x + x\n\n    class MyModel:\n        def __init__(self, v: int):\n            self.val = v","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":664,"to":707}}}}],["647",{"pageContent":"def forward(self, x: int):\n            return self.x + x\n\n    class MyModel:\n        def __init__(self, v: int):\n            self.val = v\n\n        @torch.jit.export\n        def doSomething(self, val: int) -> int:\n            # error: should not invoke the constructor of module type\n            myModel = TestModule(self.val)\n            return myModel(val)\n\n    # m = torch.jit.script(MyModel(2)) # Results in below RuntimeError\n    # RuntimeError: Could not get name of python class object\n\n.. _type_annotation:\n\n\nType Annotation\n~~~~~~~~~~~~~~~\nSince TorchScript is statically typed, programmers need to annotate types at *strategic points* of TorchScript code so that every local variable or\ninstance data attribute has a static type, and every function and method has a statically typed signature.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":707,"to":729}}}}],["648",{"pageContent":"When to Annotate Types\n^^^^^^^^^^^^^^^^^^^^^^\nIn general, type annotations are only needed in places where static types cannot be automatically inferred (e.g., parameters or sometimes return types to\nmethods or functions). Types of local variables and data attributes are often automatically inferred from their assignment statements. Sometimes an inferred type\nmay be too restrictive, e.g., ``x`` being inferred as ``NoneType`` through assignment ``x = None``, whereas ``x`` is actually used as an ``Optional``. In such\ncases, type annotations may be needed to overwrite auto inference, e.g., ``x: Optional[int] = None``. Note that it is always safe to type annotate a local variable\nor data attribute even if its type can be automatically inferred. The annotated type must be congruent with TorchScript’s type-checking.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":731,"to":737}}}}],["649",{"pageContent":"When a parameter, local variable, or data attribute is not type annotated and its type cannot be automatically inferred, TorchScript assumes it to be a\ndefault type of ``TensorType``, ``List[TensorType]``, or ``Dict[str, TensorType]``.\n\nAnnotate Function Signature\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nSince a parameter may not be automatically inferred from the body of the function (including both functions and methods), they need to be type annotated. Otherwise, they assume the default type ``TensorType``.\n\nTorchScript supports two styles for method and function signature type annotation:\n\n* **Python3-style** annotates types directly on the signature. As such, it allows individual parameters to be left unannotated (whose type will be the default type of ``TensorType``), or allows the return type to be left unannotated (whose type will be automatically inferred).\n\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":739,"to":751}}}}],["650",{"pageContent":"::\n\n    Python3Annotation ::= \"def\" Identifier [ \"(\" ParamAnnot* \")\" ] [ReturnAnnot] \":\"\n                                FuncOrMethodBody\n    ParamAnnot        ::= Identifier [ \":\" TSType ] \",\"\n    ReturnAnnot       ::= \"->\" TSType\n\nNote that when using Python3 style, the type ``self`` is automatically inferred and should not be annotated.\n\n* **Mypy style** annotates types as a comment right below the function/method declaration. In the Mypy style, since parameter names do not appear in the annotation, all parameters have to be annotated.\n\n\n::\n\n    MyPyAnnotation ::= \"# type:\" \"(\" ParamAnnot* \")\" [ ReturnAnnot ]\n    ParamAnnot     ::= TSType \",\"\n    ReturnAnnot    ::= \"->\" TSType\n\n**Example 1**\n\nIn this example:\n\n* ``a`` is not annotated and assumes the default type of ``TensorType``.\n* ``b`` is annotated as type ``int``.\n* The return type is not annotated and is automatically inferred as type ``TensorType`` (based on the type of the value being returned).\n\n::\n\n    import torch","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":751,"to":779}}}}],["651",{"pageContent":"::\n\n    import torch\n\n    def f(a, b: int):\n        return a+b\n\n    m = torch.jit.script(f)\n    print(\"TorchScript:\", m(torch.ones([6]), 100))\n\n**Example 2**\n\nThe following example uses Mypy style annotation. Note that parameters or return values must be annotated even if some of\nthem assume the default type.\n\n::\n\n    import torch\n\n    def f(a, b):\n        # type: (torch.Tensor, int) → torch.Tensor\n        return a+b\n\n    m = torch.jit.script(f)\n    print(\"TorchScript:\", m(torch.ones([6]), 100))\n\n\nAnnotate Variables and Data Attributes\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIn general, types of data attributes (including class and instance data attributes) and local variables can be automatically inferred from assignment statements.\nSometimes, however, if a variable or attribute is associated with values of different types (e.g., as ``None`` or ``TensorType``), then they may need to be explicitly\ntype annotated as a *wider* type such as ``Optional[int]`` or ``Any``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":779,"to":810}}}}],["652",{"pageContent":"Local Variables\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nLocal variables can be annotated according to Python3 typing module annotation rules, i.e.,\n\n::\n\n    LocalVarAnnotation ::= Identifier [\":\" TSType] \"=\" Expr\n\nIn general, types of local variables can be automatically inferred. In some cases, however, you may need to annotate a multi-type for local variables\nthat may be associated with different concrete types. Typical multi-types include ``Optional[T]`` and ``Any``.\n\n**Example**\n\n::\n\n    import torch\n\n    def f(a, setVal: bool):\n        value: Optional[torch.Tensor] = None\n        if setVal:\n            value = a\n        return value\n\n    ones = torch.ones([6])\n    m = torch.jit.script(f)\n    print(\"TorchScript:\", m(ones, True), m(ones, False))\n\nInstance Data Attributes\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nFor ``ModuleType`` classes, instance data attributes can be annotated according to Python3 typing module annotation rules. Instance data attributes can be annotated (optionally) as final\nvia ``Final``.\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":812,"to":844}}}}],["653",{"pageContent":"::\n\n    \"class\" ClassIdentifier \"(torch.nn.Module):\"\n    InstanceAttrIdentifier \":\" [\"Final(\"] TSType [\")\"]\n    ...\n\nWhere:\n\n* ``InstanceAttrIdentifier`` is the name of an instance attribute.\n* ``Final`` indicates that the attribute cannot be re-assigned outside of ``__init__`` or overridden in subclasses.\n\n**Example**\n\n::\n\n    import torch\n\n    class MyModule(torch.nn.Module):\n        offset_: int\n\n    def __init__(self, offset):\n        self.offset_ = offset\n\n    ...\n\n\n\nType Annotation APIs\n^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":844,"to":872}}}}],["654",{"pageContent":"**Example**\n\n::\n\n    import torch\n\n    class MyModule(torch.nn.Module):\n        offset_: int\n\n    def __init__(self, offset):\n        self.offset_ = offset\n\n    ...\n\n\n\nType Annotation APIs\n^^^^^^^^^^^^^^^^^^^^\n\n``torch.jit.annotate(T, expr)``\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThis API annotates type ``T`` to an expression ``expr``. This is often used when the default type of an expression is not the type intended by the programmer.\nFor instance, an empty list (dictionary) has the default type of ``List[TensorType]`` (``Dict[TensorType, TensorType]``), but sometimes it may be used to initialize\na list of some other types. Another common use case is for annotating the return type of ``tensor.tolist()``. Note, however, that it cannot be used to annotate\nthe type of a module attribute in `__init__`; ``torch.jit.Attribute`` should be used for this instead.\n\n**Example**","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":872,"to":898}}}}],["655",{"pageContent":"**Example**\n\nIn this example, ``[]`` is declared as a list of integers via ``torch.jit.annotate`` (instead of assuming ``[]`` to be the default type of ``List[TensorType]``):\n\n::\n\n    import torch\n    from typing import List\n\n    def g(l: List[int], val: int):\n        l.append(val)\n        return l\n\n    def f(val: int):\n        l = g(torch.jit.annotate(List[int], []), val)\n        return l\n\n    m = torch.jit.script(f)\n    print(\"Eager:\", f(3))\n    print(\"TorchScript:\", m(3))\n\n\nSee :meth:`torch.jit.annotate` for more information.\n\n\nType Annotation Appendix\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nTorchScript Type System Definition\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    TSAllType       ::= TSType | TSModuleType\n    TSType          ::= TSMetaType | TSPrimitiveType | TSStructuralType | TSNominalType\n\n    TSMetaType      ::= \"Any\"\n    TSPrimitiveType ::= \"int\" | \"float\" | \"double\" | \"complex\" | \"bool\" | \"str\" | \"None\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":898,"to":935}}}}],["656",{"pageContent":"TSMetaType      ::= \"Any\"\n    TSPrimitiveType ::= \"int\" | \"float\" | \"double\" | \"complex\" | \"bool\" | \"str\" | \"None\"\n\n    TSStructualType ::=  TSTuple | TSNamedTuple | TSList | TSDict | TSOptional |\n                         TSUnion | TSFuture | TSRRef | TSAwait\n    TSTuple         ::= \"Tuple\" \"[\" (TSType \",\")* TSType \"]\"\n    TSNamedTuple    ::= \"namedtuple\" \"(\" (TSType \",\")* TSType \")\"\n    TSList          ::= \"List\" \"[\" TSType \"]\"\n    TSOptional      ::= \"Optional\" \"[\" TSType \"]\"\n    TSUnion         ::= \"Union\" \"[\" (TSType \",\")* TSType \"]\"\n    TSFuture        ::= \"Future\" \"[\" TSType \"]\"\n    TSRRef          ::= \"RRef\" \"[\" TSType \"]\"\n    TSAwait         ::= \"Await\" \"[\" TSType \"]\"\n    TSDict          ::= \"Dict\" \"[\" KeyType \",\" TSType \"]\"\n    KeyType         ::= \"str\" | \"int\" | \"float\" | \"bool\" | TensorType | \"Any\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":935,"to":949}}}}],["657",{"pageContent":"TSNominalType   ::= TSBuiltinClasses | TSCustomClass | TSEnum\n    TSBuiltinClass  ::= TSTensor | \"torch.device\" | \"torch.stream\"|\n                        \"torch.dtype\" | \"torch.nn.ModuleList\" |\n                        \"torch.nn.ModuleDict\" | ...\n    TSTensor        ::= \"torch.tensor\" and subclasses\n\nUnsupported Typing Constructs\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nTorchScript does not support all features and types of the Python3 `typing <https://docs.python.org/3/library/typing.html#module-typing>`_ module.\nAny functionality from the `typing <https://docs.python.org/3/library/typing.html#module-typing>`_ module that is not explicitly specified in this\ndocumentation is unsupported. The following table summarizes ``typing`` constructs that are either unsupported or supported with restrictions in TorchScript.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":951,"to":961}}}}],["658",{"pageContent":"=============================  ================\n Item                           Description\n-----------------------------  ----------------\n``typing.Any``                  In development\n``typing.NoReturn``             Not supported\n``typing.Callable``             Not supported\n``typing.Literal``              Not supported\n``typing.ClassVar``             Not supported\n``typing.Final``                Supported for module attributes, class attribute, and annotations, but not for functions.\n``typing.AnyStr``               Not supported\n``typing.overload``             In development\nType aliases                    Not supported\nNominal typing                  In development\nStructural typing               Not supported\nNewType                         Not supported\nGenerics                        Not supported\n=============================  ================\n\n\n.. _expressions:\n\n\nExpressions\n~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":963,"to":986}}}}],["659",{"pageContent":".. _expressions:\n\n\nExpressions\n~~~~~~~~~~~\n\nThe following section describes the grammar of expressions that are supported in TorchScript.\nIt is modeled after `the expressions chapter of the Python language reference <https://docs.python.org/3/reference/expressions.html>`_.\n\nArithmetic Conversions\n^^^^^^^^^^^^^^^^^^^^^^\nThere are a number of implicit type conversions that are performed in TorchScript:\n\n\n* A ``Tensor`` with a ``float`` or ``int`` data type can be implicitly converted to an instance of ``FloatType`` or ``IntType`` provided that it has a size of 0, does not have ``require_grad`` set to ``True``, and will not require narrowing.\n* Instances of ``StringType`` can be implicitly converted to ``DeviceType``.\n* The implicit conversion rules from the two bullet points above can be applied to instances of ``TupleType`` to produce instances of ``ListType`` with the appropriate contained type.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":986,"to":1002}}}}],["660",{"pageContent":"Explicit conversions can be invoked using the ``float``, ``int``, ``bool``, and ``str`` built-in functions\nthat accept primitive data types as arguments and can accept user-defined types if they implement\n``__bool__``, ``__str__``, etc.\n\n\nAtoms\n^^^^^\nAtoms are the most basic elements of expressions.\n\n::\n\n    atom      ::=  identifier | literal | enclosure\n    enclosure ::=  parenth_form | list_display | dict_display\n\nIdentifiers\n\"\"\"\"\"\"\"\"\"\"\"\nThe rules that dictate what is a legal identifier in TorchScript are the same as\ntheir `Python counterparts <https://docs.python.org/3/reference/lexical_analysis.html#identifiers>`_.\n\nLiterals\n\"\"\"\"\"\"\"\"\n\n::\n\n    literal ::=  stringliteral | integer | floatnumber","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1005,"to":1029}}}}],["661",{"pageContent":"Literals\n\"\"\"\"\"\"\"\"\n\n::\n\n    literal ::=  stringliteral | integer | floatnumber\n\nEvaluation of a literal yields an object of the appropriate type with the specific value\n(with approximations applied as necessary for floats). Literals are immutable, and multiple evaluations\nof identical literals may obtain the same object or distinct objects with the same value.\n`stringliteral <https://docs.python.org/3/reference/lexical_analysis.html#string-and-bytes-literals>`_,\n`integer <https://docs.python.org/3/reference/lexical_analysis.html#integer-literals>`_, and\n`floatnumber <https://docs.python.org/3/reference/lexical_analysis.html#floating-point-literals>`_\nare defined in the same way as their Python counterparts.\n\nParenthesized Forms\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    parenth_form ::=  '(' [expression_list] ')'","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1029,"to":1049}}}}],["662",{"pageContent":"Parenthesized Forms\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    parenth_form ::=  '(' [expression_list] ')'\n\nA parenthesized expression list yields whatever the expression list yields. If the list contains at least one\ncomma, it yields a ``Tuple``; otherwise, it yields the single expression inside the expression list. An empty\npair of parentheses yields an empty ``Tuple`` object (``Tuple[]``).\n\nList and Dictionary Displays\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    list_comprehension ::=  expression comp_for\n    comp_for           ::=  'for' target_list 'in' or_expr\n    list_display       ::=  '[' [expression_list | list_comprehension] ']'\n    dict_display       ::=  '{' [key_datum_list | dict_comprehension] '}'\n    key_datum_list     ::=  key_datum (',' key_datum)*\n    key_datum          ::=  expression ':' expression\n    dict_comprehension ::=  key_datum comp_for","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1049,"to":1071}}}}],["663",{"pageContent":"Lists and dicts can be constructed by either listing the container contents explicitly or by providing\ninstructions on how to compute them via a set of looping instructions (i.e. a *comprehension*). A comprehension\nis semantically equivalent to using a for loop and appending to an ongoing list.\nComprehensions implicitly create their own scope to make sure that the items of the target list do not leak into the\nenclosing scope. In the case that container items are explicitly listed, the expressions in the expression list\nare evaluated left-to-right. If a key is repeated in a ``dict_display`` that has a ``key_datum_list``, the\nresultant dictionary uses the value from the rightmost datum in the list that uses the repeated key.\n\nPrimaries\n^^^^^^^^^\n\n::\n\n    primary ::=  atom | attributeref | subscription | slicing | call\n\n\nAttribute References\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    attributeref ::=  primary '.' identifier","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1073,"to":1094}}}}],["664",{"pageContent":"Primaries\n^^^^^^^^^\n\n::\n\n    primary ::=  atom | attributeref | subscription | slicing | call\n\n\nAttribute References\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    attributeref ::=  primary '.' identifier\n\n\nThe ``primary`` must evaluate to an object of a type that supports attribute references that have an attribute named\n``identifier``.\n\nSubscriptions\n\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    subscription ::=  primary '[' expression_list ']'\n\n\nThe ``primary`` must evaluate to an object that supports subscription.\n\n* If the primary is a ``List``, ``Tuple``, or ``str``, the expression list must evaluate to an integer or slice.\n* If the primary is a ``Dict``, the expression list must evaluate to an object of the same type as the key type of the ``Dict``.\n* If the primary is a ``ModuleList``, the expression list must be an ``integer`` literal.\n* If the primary is a ``ModuleDict``, the expression must be a ``stringliteral``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1094,"to":1126}}}}],["665",{"pageContent":"Slicings\n\"\"\"\"\"\"\"\"\nA slicing selects a range of items in a ``str``, ``Tuple``, ``List``, or ``Tensor``. Slicings may be used as\nexpressions or targets in assignment or ``del`` statements.\n\n::\n\n    slicing      ::=  primary '[' slice_list ']'\n    slice_list   ::=  slice_item (',' slice_item)* [',']\n    slice_item   ::=  expression | proper_slice\n    proper_slice ::=  [expression] ':' [expression] [':' [expression] ]\n\nSlicings with more than one slice item in their slice lists can only be used with primaries that evaluate to an\nobject of type ``Tensor``.\n\n\nCalls\n\"\"\"\"\"\n\n::\n\n    call          ::=  primary '(' argument_list ')'\n    argument_list ::=  args [',' kwargs] | kwargs\n    args          ::=  [arg (',' arg)*]\n    kwargs        ::=  [kwarg (',' kwarg)*]\n    kwarg         ::=  arg '=' expression\n    arg           ::=  identifier\n\n\nThe ``primary`` must desugar or evaluate to a callable object. All argument expressions are evaluated\nbefore the call is attempted.\n\nPower Operator\n^^^^^^^^^^^^^^\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1129,"to":1164}}}}],["666",{"pageContent":"The ``primary`` must desugar or evaluate to a callable object. All argument expressions are evaluated\nbefore the call is attempted.\n\nPower Operator\n^^^^^^^^^^^^^^\n\n::\n\n    power ::=  primary ['**' u_expr]\n\n\nThe power operator has the same semantics as the built-in pow function (not supported); it computes its\nleft argument raised to the power of its right argument. It binds more tightly than unary operators on the\nleft, but less tightly than unary operators on the right; i.e. ``-2 ** -3 == -(2 ** (-3))``.  The left and right\noperands can be ``int``, ``float`` or ``Tensor``. Scalars are broadcast in the case of scalar-tensor/tensor-scalar\nexponentiation operations, and tensor-tensor exponentiation is done elementwise without any broadcasting.\n\nUnary and Arithmetic Bitwise Operations\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    u_expr ::=  power | '-' power | '~' power","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1164,"to":1186}}}}],["667",{"pageContent":"Unary and Arithmetic Bitwise Operations\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    u_expr ::=  power | '-' power | '~' power\n\nThe unary ``-`` operator yields the negation of its argument. The unary ``~`` operator yields the bitwise inversion\nof its argument. ``-`` can be used with ``int``, ``float``, and ``Tensor`` of ``int`` and ``float``.\n``~`` can only be used with ``int`` and ``Tensor`` of ``int``.\n\nBinary Arithmetic Operations\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    m_expr ::=  u_expr | m_expr '*' u_expr | m_expr '@' m_expr | m_expr '//' u_expr | m_expr '/' u_expr | m_expr '%' u_expr\n    a_expr ::=  m_expr | a_expr '+' m_expr | a_expr '-' m_expr","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1186,"to":1203}}}}],["668",{"pageContent":"::\n\n    m_expr ::=  u_expr | m_expr '*' u_expr | m_expr '@' m_expr | m_expr '//' u_expr | m_expr '/' u_expr | m_expr '%' u_expr\n    a_expr ::=  m_expr | a_expr '+' m_expr | a_expr '-' m_expr\n\nThe binary arithmetic operators can operate on ``Tensor``, ``int``, and ``float``. For tensor-tensor ops, both arguments must\nhave the same shape. For scalar-tensor or tensor-scalar ops, the scalar is usually broadcast to the size of the\ntensor. Division ops can only accept scalars as their right-hand side argument, and do not support broadcasting.\nThe ``@`` operator is for matrix multiplication and only operates on ``Tensor`` arguments. The multiplication operator\n(``*``) can be used with a list and integer in order to get a result that is the original list repeated a certain\nnumber of times.\n\nShifting Operations\n^^^^^^^^^^^^^^^^^^^\n\n::\n\n    shift_expr ::=  a_expr | shift_expr ( '<<' | '>>' ) a_expr","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1203,"to":1220}}}}],["669",{"pageContent":"Shifting Operations\n^^^^^^^^^^^^^^^^^^^\n\n::\n\n    shift_expr ::=  a_expr | shift_expr ( '<<' | '>>' ) a_expr\n\n\nThese operators accept two ``int`` arguments, two ``Tensor`` arguments, or a ``Tensor`` argument and an ``int`` or\n``float`` argument. In all cases, a right shift by ``n`` is defined as floor division by ``pow(2, n)``, and a left shift\nby ``n`` is defined as multiplication by ``pow(2, n)``. When both arguments are ``Tensors``, they must have the same\nshape. When one is a scalar and the other is a ``Tensor``, the scalar is logically broadcast to match the size of\nthe ``Tensor``.\n\nBinary Bitwise Operations\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    and_expr ::=  shift_expr | and_expr '&' shift_expr\n    xor_expr ::=  and_expr | xor_expr '^' and_expr\n    or_expr  ::=  xor_expr | or_expr '|' xor_expr","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1220,"to":1241}}}}],["670",{"pageContent":"::\n\n    and_expr ::=  shift_expr | and_expr '&' shift_expr\n    xor_expr ::=  and_expr | xor_expr '^' and_expr\n    or_expr  ::=  xor_expr | or_expr '|' xor_expr\n\n\nThe ``&`` operator computes the bitwise AND of its arguments, the ``^`` the bitwise XOR, and the ``|`` the bitwise OR.\nBoth operands must be ``int`` or ``Tensor``, or the left operand must be ``Tensor`` and the right operand must be\n``int``. When both operands are ``Tensor``, they must have the same shape. When the right operand is ``int``, and\nthe left operand is ``Tensor``, the right operand is logically broadcast to match the shape of the ``Tensor``.\n\nComparisons\n^^^^^^^^^^^\n\n::\n\n    comparison    ::=  or_expr (comp_operator or_expr)*\n    comp_operator ::=  '<' | '>' | '==' | '>=' | '<=' | '!=' | 'is' ['not'] | ['not'] 'in'","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1241,"to":1259}}}}],["671",{"pageContent":"Comparisons\n^^^^^^^^^^^\n\n::\n\n    comparison    ::=  or_expr (comp_operator or_expr)*\n    comp_operator ::=  '<' | '>' | '==' | '>=' | '<=' | '!=' | 'is' ['not'] | ['not'] 'in'\n\nA comparison yields a boolean value (``True`` or ``False``), or if one of the operands is a ``Tensor``, a boolean\n``Tensor``. Comparisons can be chained arbitrarily as long as they do not yield boolean ``Tensors`` that have more\nthan one element. ``a op1 b op2 c ...`` is equivalent to ``a op1 b and b op2 c and ...``.\n\nValue Comparisons\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThe operators ``<``, ``>``, ``==``, ``>=``, ``<=``, and ``!=`` compare the values of two objects. The two objects generally need to be of\nthe same type, unless there is an implicit type conversion available between the objects. User-defined types can\nbe compared if rich comparison methods (e.g., ``__lt__``) are defined on them. Built-in type comparison works like\nPython:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1259,"to":1276}}}}],["672",{"pageContent":"* Numbers are compared mathematically.\n* Strings are compared lexicographically.\n* ``lists``, ``tuples``, and ``dicts`` can be compared only to other ``lists``, ``tuples``, and ``dicts`` of the same type and are compared using the comparison operator of corresponding elements.\n\nMembership Test Operations\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThe operators ``in`` and ``not in`` test for membership. ``x in s`` evaluates to ``True`` if ``x`` is a member of ``s`` and ``False`` otherwise.\n``x not in s`` is equivalent to ``not x in s``. This operator is supported for ``lists``, ``dicts``, and ``tuples``, and can be used with\nuser-defined types if they implement the ``__contains__`` method.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1278,"to":1286}}}}],["673",{"pageContent":"Identity Comparisons\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nFor all types except ``int``, ``double``, ``bool``, and ``torch.device``, operators ``is`` and ``is not`` test for the object’s identity;\n``x is y`` is ``True`` if and only if ``x`` and ``y`` are the same object. For all other types, ``is`` is equivalent to\ncomparing them using ``==``. ``x is not y`` yields the inverse of ``x is y``.\n\nBoolean Operations\n^^^^^^^^^^^^^^^^^^\n\n::\n\n    or_test  ::=  and_test | or_test 'or' and_test\n    and_test ::=  not_test | and_test 'and' not_test\n    not_test ::=  'bool' '(' or_expr ')' | comparison | 'not' not_test","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1288,"to":1301}}}}],["674",{"pageContent":"::\n\n    or_test  ::=  and_test | or_test 'or' and_test\n    and_test ::=  not_test | and_test 'and' not_test\n    not_test ::=  'bool' '(' or_expr ')' | comparison | 'not' not_test\n\nUser-defined objects can customize their conversion to ``bool`` by implementing a ``__bool__`` method. The operator ``not``\nyields ``True`` if its operand is false, ``False`` otherwise. The expression ``x`` and ``y`` first evaluates ``x``; if it is ``False``, its\nvalue (``False``) is returned; otherwise, ``y`` is evaluated and its value is returned (``False`` or ``True``). The expression ``x`` or ``y``\nfirst evaluates ``x``; if it is ``True``, its value (``True``) is returned; otherwise, ``y`` is evaluated and its value is returned\n(``False`` or ``True``).\n\nConditional Expressions\n^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n   conditional_expression ::=  or_expr ['if' or_test 'else' conditional_expression]\n    expression            ::=  conditional_expression","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1301,"to":1319}}}}],["675",{"pageContent":"Conditional Expressions\n^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n   conditional_expression ::=  or_expr ['if' or_test 'else' conditional_expression]\n    expression            ::=  conditional_expression\n\nThe expression ``x if c else y`` first evaluates the condition ``c`` rather than x. If ``c`` is ``True``, ``x`` is\nevaluated and its value is returned; otherwise, ``y`` is evaluated and its value is returned. As with if-statements,\n``x`` and ``y`` must evaluate to a value of the same type.\n\nExpression Lists\n^^^^^^^^^^^^^^^^\n\n::\n\n    expression_list ::=  expression (',' expression)* [',']\n    starred_item    ::=  '*' primary\n\nA starred item can only appear on the left-hand side of an assignment statement, e.g., ``a, *b, c = ...``.\n\n.. statements:\n\nSimple Statements\n~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1319,"to":1344}}}}],["676",{"pageContent":"A starred item can only appear on the left-hand side of an assignment statement, e.g., ``a, *b, c = ...``.\n\n.. statements:\n\nSimple Statements\n~~~~~~~~~~~~~~~~~\n\nThe following section describes the syntax of simple statements that are supported in TorchScript.\nIt is modeled after `the simple statements chapter of the Python language reference <https://docs.python.org/3/reference/simple_stmts.html>`_.\n\nExpression Statements\n^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    expression_stmt    ::=  starred_expression\n    starred_expression ::=  expression | (starred_item \",\")* [starred_item]\n    starred_item       ::=  assignment_expression | \"*\" or_expr\n\nAssignment Statements\n^^^^^^^^^^^^^^^^^^^^^^\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1344,"to":1366}}}}],["677",{"pageContent":"Assignment Statements\n^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    assignment_stmt ::=  (target_list \"=\")+ (starred_expression)\n    target_list     ::=  target (\",\" target)* [\",\"]\n    target          ::=  identifier\n                         | \"(\" [target_list] \")\"\n                         | \"[\" [target_list] \"]\"\n                         | attributeref\n                         | subscription\n                         | slicing\n                         | \"*\" target\n\nAugmented Assignment Statements\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    augmented_assignment_stmt ::= augtarget augop (expression_list)\n    augtarget                 ::= identifier | attributeref | subscription\n    augop                     ::= \"+=\" | \"-=\" | \"*=\" | \"/=\" | \"//=\" | \"%=\" |\n                                  \"**=\"| \">>=\" | \"<<=\" | \"&=\" | \"^=\" | \"|=\"\n\n\nAnnotated Assignment Statements\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n::\n\n    annotated_assignment_stmt ::= augtarget \":\" expression\n                                  [\"=\" (starred_expression)]","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1366,"to":1397}}}}],["678",{"pageContent":"Annotated Assignment Statements\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n::\n\n    annotated_assignment_stmt ::= augtarget \":\" expression\n                                  [\"=\" (starred_expression)]\n\nThe ``raise`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    raise_stmt ::=  \"raise\" [expression [\"from\" expression]]\n\nRaise statements in TorchScript do not support ``try\\except\\finally``.\n\nThe ``assert`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    assert_stmt ::=  \"assert\" expression [\",\" expression]\n\nAssert statements in TorchScript do not support ``try\\except\\finally``.\n\nThe ``return`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    return_stmt ::=  \"return\" [expression_list]\n\nReturn statements in TorchScript do not support ``try\\except\\finally``.\n\nThe ``del`` Statement\n^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    del_stmt ::=  \"del\" target_list\n\nThe ``pass`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    pass_stmt ::= \"pass\"\n\nThe ``print`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1397,"to":1448}}}}],["679",{"pageContent":"::\n\n    del_stmt ::=  \"del\" target_list\n\nThe ``pass`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    pass_stmt ::= \"pass\"\n\nThe ``print`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    print_stmt ::= \"print\" \"(\" expression  [, expression] [.format{expression_list}] \")\"\n\nThe ``break`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    break_stmt ::= \"break\"\n\nThe ``continue`` Statement:\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    continue_stmt ::= \"continue\"\n\nCompound Statements\n~~~~~~~~~~~~~~~~~~~\n\nThe following section describes the syntax of compound statements that are supported in TorchScript.\nThe section also highlights how Torchscript differs from regular Python statements.\nIt is modeled after `the compound statements chapter of the Python language reference <https://docs.python.org/3/reference/compound_stmts.html>`_.\n\nThe ``if`` Statement\n^^^^^^^^^^^^^^^^^^^^^\n\nTorchscript supports both basic ``if/else`` and ternary ``if/else``.\n\nBasic ``if/else`` Statement\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1448,"to":1495}}}}],["680",{"pageContent":"The ``if`` Statement\n^^^^^^^^^^^^^^^^^^^^^\n\nTorchscript supports both basic ``if/else`` and ternary ``if/else``.\n\nBasic ``if/else`` Statement\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    if_stmt ::= \"if\" assignment_expression \":\" suite\n                (\"elif\" assignment_expression \":\" suite)\n                [\"else\" \":\" suite]\n\n``elif`` statements can repeat for an arbitrary number of times, but it needs to be before ``else`` statement.\n\nTernary ``if/else`` Statement\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    if_stmt ::= return [expression_list] \"if\" assignment_expression \"else\" [expression_list]\n\n**Example 1**\n\nA ``tensor`` with 1 dimension is promoted to ``bool``:\n\n.. testcode::\n\n    import torch\n\n    @torch.jit.script\n    def fn(x: torch.Tensor):\n        if x: # The tensor gets promoted to bool\n            return True\n        return False\n    print(fn(torch.rand(1)))\n\nThe example above produces the following output:\n\n.. testoutput::\n\n    True\n\n**Example 2**\n\nA ``tensor`` with multi dimensions are not promoted to ``bool``:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1495,"to":1543}}}}],["681",{"pageContent":"The example above produces the following output:\n\n.. testoutput::\n\n    True\n\n**Example 2**\n\nA ``tensor`` with multi dimensions are not promoted to ``bool``:\n\n::\n\n    import torch\n\n    # Multi dimensional Tensors error out.\n\n    @torch.jit.script\n    def fn():\n        if torch.rand(2):\n            print(\"Tensor is available\")\n\n        if torch.rand(4,5,6):\n            print(\"Tensor is available\")\n\n    print(fn())\n\nRunning the above code yields the following ``RuntimeError``.\n\n::\n\n    RuntimeError: The following operation failed in the TorchScript interpreter.\n    Traceback of TorchScript (most recent call last):\n    @torch.jit.script\n    def fn():\n        if torch.rand(2):\n           ~~~~~~~~~~~~ <--- HERE\n            print(\"Tensor is available\")\n    RuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\nIf a conditional variable is annotated as ``final``, either the true or false branch is evaluated depending on the evaluation of the conditional variable.\n\n**Example 3**","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1543,"to":1584}}}}],["682",{"pageContent":"If a conditional variable is annotated as ``final``, either the true or false branch is evaluated depending on the evaluation of the conditional variable.\n\n**Example 3**\n\nIn this example, only the True branch is evaluated, since ``a`` is annotated as ``final`` and set to ``True``:\n\n::\n\n    import torch\n\n    a : torch.jit.final[Bool] = True\n\n    if a:\n        return torch.empty(2,3)\n    else:\n        return []\n\n\nThe ``while`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    while_stmt ::=  \"while\" assignment_expression \":\" suite\n\n`while...else` statements are not supported in Torchscript. It results in a ``RuntimeError``.\n\nThe ``for-in`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    for_stmt ::=  \"for\" target_list \"in\" expression_list \":\" suite\n                  [\"else\" \":\" suite]\n\n``for...else`` statements are not supported in Torchscript. It results in a ``RuntimeError``.\n\n**Example 1**","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1584,"to":1621}}}}],["683",{"pageContent":"``for...else`` statements are not supported in Torchscript. It results in a ``RuntimeError``.\n\n**Example 1**\n\nFor loops on tuples: these unroll the loop, generating a body for each member of the tuple. The body must type-check correctly for each member.\n\n.. testcode::\n\n    import torch\n    from typing import Tuple\n\n    @torch.jit.script\n    def fn():\n        tup = (3, torch.ones(4))\n        for x in tup:\n            print(x)\n\n    fn()\n\nThe example above produces the following output:\n\n.. testoutput::\n\n    3\n     1\n     1\n     1\n     1\n    [ CPUFloatType{4} ]\n\n\n**Example 2**\n\nFor loops on lists: for loops over a ``nn.ModuleList`` will unroll the body of the loop at compile time, with each member of the module list.\n\n::\n\n    class SubModule(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.weight = nn.Parameter(torch.randn(2))\n\n        def forward(self, input):\n            return self.weight + input","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1621,"to":1664}}}}],["684",{"pageContent":"def forward(self, input):\n            return self.weight + input\n\n    class MyModule(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.mods = torch.nn.ModuleList([SubModule() for i in range(10)])\n\n        def forward(self, v):\n            for module in self.mods:\n                v = module(v)\n            return v\n\n    model = torch.jit.script(MyModule())\n\nThe ``with`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^\nThe ``with`` statement is used to wrap the execution of a block with methods defined by a context manager.\n\n::\n\n    with_stmt ::=  \"with\" with_item (\",\" with_item) \":\" suite\n    with_item ::=  expression [\"as\" target]","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1664,"to":1686}}}}],["685",{"pageContent":"::\n\n    with_stmt ::=  \"with\" with_item (\",\" with_item) \":\" suite\n    with_item ::=  expression [\"as\" target]\n\n* If a target was included in the ``with`` statement, the return value from the context manager’s ``__enter__()`` is assigned to it. Unlike python, if an exception caused the suite to be exited, its type, value, and traceback are not passed as arguments to ``__exit__()``. Three ``None`` arguments are supplied.\n* ``try``, ``except``, and ``finally`` statements are not supported inside ``with`` blocks.\n*  Exceptions raised within ``with`` block cannot be suppressed.\n\nThe ``tuple`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    tuple_stmt ::= tuple([iterables])\n\n* Iterable types in TorchScript include ``Tensors``, ``lists``, ``tuples``, ``dictionaries``, ``strings``, ``torch.nn.ModuleList``, and ``torch.nn.ModuleDict``.\n* You cannot convert a List to Tuple by using this built-in function.\n\nUnpacking all outputs into a tuple is covered by:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1686,"to":1707}}}}],["686",{"pageContent":"Unpacking all outputs into a tuple is covered by:\n\n::\n\n    abc = func() # Function that returns a tuple\n    a,b = func()\n\nThe ``getattr`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    getattr_stmt ::= getattr(object, name[, default])\n\n* Attribute name must be a literal string.\n* Module type object is not supported (e.g., torch._C).\n* Custom class object is not supported (e.g., torch.classes.*).\n\nThe ``hasattr`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    hasattr_stmt ::= hasattr(object, name)\n\n* Attribute name must be a literal string.\n* Module type object is not supported (e.g., torch._C).\n* Custom class object is not supported (e.g., torch.classes.*).\n\nThe ``zip`` Statement\n^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    zip_stmt ::= zip(iterable1, iterable2)\n\n* Arguments must be iterables.\n* Two iterables of same outer container type but different length are supported.\n\n**Example 1**\n\nBoth the iterables must be of the same container type:\n\n.. testcode::\n\n    a = [1, 2] # List\n    b = [2, 3, 4] # List\n    zip(a, b) # works","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1707,"to":1754}}}}],["687",{"pageContent":"**Example 1**\n\nBoth the iterables must be of the same container type:\n\n.. testcode::\n\n    a = [1, 2] # List\n    b = [2, 3, 4] # List\n    zip(a, b) # works\n\n**Example 2**\n\nThis example fails because the iterables are of different container types:\n\n::\n\n    a = (1, 2) # Tuple\n    b = [2, 3, 4] # List\n    zip(a, b) # Runtime error\n\nRunning the above code yields the following ``RuntimeError``.\n\n::\n\n    RuntimeError: Can not iterate over a module list or\n        tuple with a value that does not have a statically determinable length.\n\n**Example 3**\n\nTwo iterables of the same container Type but different data type is supported:\n\n.. testcode::\n\n    a = [1.3, 2.4]\n    b = [2, 3, 4]\n    zip(a, b) # Works\n\nIterable types in TorchScript include ``Tensors``, ``lists``, ``tuples``, ``dictionaries``, ``strings``, ``torch.nn.ModuleList``, and ``torch.nn.ModuleDict``.\n\nThe ``enumerate`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    enumerate_stmt ::= enumerate([iterable])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1754,"to":1798}}}}],["688",{"pageContent":"The ``enumerate`` Statement\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    enumerate_stmt ::= enumerate([iterable])\n\n* Arguments must be iterables.\n* Iterable types in TorchScript include ``Tensors``, ``lists``, ``tuples``, ``dictionaries``, ``strings``, ``torch.nn.ModuleList`` and ``torch.nn.ModuleDict``.\n\n\n.. _python-values-torch-script:\n\nPython Values\n~~~~~~~~~~~~~\n\n.. _python-builtin-functions-values-resolution:\n\nResolution Rules\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nWhen given a Python value, TorchScript attempts to resolve it in the following five different ways:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1798,"to":1818}}}}],["689",{"pageContent":"* Compilable Python Implementation:\n    * When a Python value is backed by a Python implementation that can be compiled by TorchScript, TorchScript compiles and uses the underlying Python implementation.\n    * Example: ``torch.jit.Attribute``\n* Op Python Wrapper:\n    * When a Python value is a wrapper of a native PyTorch op, TorchScript emits the corresponding operator.\n    * Example: ``torch.jit._logging.add_stat_value``\n* Python Object Identity Match:\n    * For a limited set of ``torch.*`` API calls (in the form of Python values) that TorchScript supports, TorchScript attempts to match a Python value against each item in the set.\n    * When matched, TorchScript generates a corresponding ``SugaredValue`` instance that contains lowering logic for these values.\n    * Example: ``torch.jit.isinstance()``\n* Name Match:\n    * For Python built-in functions and constants, TorchScript identifies them by name, and creates a corresponding ``SugaredValue`` instance that implements their functionality.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1820,"to":1831}}}}],["690",{"pageContent":"* Name Match:\n    * For Python built-in functions and constants, TorchScript identifies them by name, and creates a corresponding ``SugaredValue`` instance that implements their functionality.\n    * Example: ``all()``\n* Value Snapshot:\n    * For Python values from unrecognized modules, TorchScript attempts to take a snapshot of the value and converts it to a constant in the graph of the function(s) or method(s) that are being compiled.\n    * Example: ``math.pi``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1831,"to":1836}}}}],["691",{"pageContent":".. _python-builtin-functions-support:\n\nPython Built-in Functions Support\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.. list-table:: TorchScript Support for Python Built-in Functions\n   :widths: 25 25 50\n   :header-rows: 1","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1840,"to":1846}}}}],["692",{"pageContent":"* - Built-in Function\n     - Support Level\n     - Notes\n   * - ``abs()``\n     - Partial\n     - Only supports ``Tensor``/``Int``/``Float`` type inputs. | Doesn't honor ``__abs__`` override.\n   * - ``all()``\n     - Full\n     -\n   * - ``any()``\n     - Full\n     -\n   * - ``ascii()``\n     - None\n     -\n   * - ``bin()``\n     - Partial\n     - Only supports ``Int`` type input.\n   * - ``bool()``\n     - Partial\n     - Only supports ``Tensor``/``Int``/``Float`` type inputs.\n   * - ``breakpoint()``\n     - None\n     -\n   * - ``bytearray()``\n     - None\n     -\n   * - ``bytes()``\n     - None\n     -\n   * - ``callable()``\n     - None\n     -\n   * - ``chr()``\n     - Partial\n     - Only ASCII character set is supported.\n   * - ``classmethod()``\n     - Full\n     -\n   * - ``compile()``\n     - None\n     -\n   * - ``complex()``\n     - None\n     -\n   * - ``delattr()``\n     - None\n     -\n   * - ``dict()``\n     - Full\n     -\n   * - ``dir()``\n     - None\n     -\n   * - ``divmod()``\n     - Full\n     -\n   * - ``enumerate()``\n     - Full\n     -\n   * - ``eval()``\n     - None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1848,"to":1909}}}}],["693",{"pageContent":"- None\n     -\n   * - ``dict()``\n     - Full\n     -\n   * - ``dir()``\n     - None\n     -\n   * - ``divmod()``\n     - Full\n     -\n   * - ``enumerate()``\n     - Full\n     -\n   * - ``eval()``\n     - None\n     -\n   * - ``exec()``\n     - None\n     -\n   * - ``filter()``\n     - None\n     -\n   * - ``float()``\n     - Partial\n     - Doesn't honor ``__index__`` override.\n   * - ``format()``\n     - Partial\n     - Manual index specification not supported. | Format type modifier not supported.\n   * - ``frozenset()``\n     - None\n     -\n   * - ``getattr()``\n     - Partial\n     - Attribute name must be string literal.\n   * - ``globals()``\n     - None\n     -\n   * - ``hasattr()``\n     - Partial\n     - Attribute name must be string literal.\n   * - ``hash()``\n     - Full\n     - ``Tensor``'s hash is based on identity not numeric value.\n   * - ``hex()``\n     - Partial\n     - Only supports ``Int`` type input.\n   * - ``id()``\n     - Full\n     - Only supports ``Int`` type input.\n   * - ``input()``\n     - None\n     -\n   * - ``int()``\n     - Partial","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1909,"to":1963}}}}],["694",{"pageContent":"- Partial\n     - Only supports ``Int`` type input.\n   * - ``id()``\n     - Full\n     - Only supports ``Int`` type input.\n   * - ``input()``\n     - None\n     -\n   * - ``int()``\n     - Partial\n     - ``base`` argument not supported. | Doesn't honor ``__index__`` override.\n   * - ``isinstance()``\n     - Full\n     - ``torch.jit.isintance`` provides better support when checking against container types like ``Dict[str, int]``.\n   * - ``issubclass()``\n     - None\n     -\n   * - ``iter()``\n     - None\n     -\n   * - ``len()``\n     - Full\n     -\n   * - ``list()``\n     - Full\n     -\n   * - ``ord()``\n     - Partial\n     - Only ASCII character set is supported.\n   * - ``pow()``\n     - Full\n     -\n   * - ``print()``\n     - Partial\n     - ``separate``, ``end`` and ``file`` arguments are not supported.\n   * - ``property()``\n     - None\n     -\n   * - ``range()``\n     - Full\n     -\n   * - ``repr()``\n     - None\n     -\n   * - ``reversed()``\n     - None\n     -\n   * - ``round()``\n     - Partial\n     - ``ndigits`` argument is not supported.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":1963,"to":2012}}}}],["695",{"pageContent":"-\n   * - ``range()``\n     - Full\n     -\n   * - ``repr()``\n     - None\n     -\n   * - ``reversed()``\n     - None\n     -\n   * - ``round()``\n     - Partial\n     - ``ndigits`` argument is not supported.\n   * - ``set()``\n     - None\n     -\n   * - ``setattr()``\n     - None\n     -\n   * - ``slice()``\n     - Full\n     -\n   * - ``sorted()``\n     - Partial\n     - ``key`` argument is not supported.\n   * - ``staticmethod()``\n     - Full\n     -\n   * - ``str()``\n     - Partial\n     - ``encoding`` and ``errors`` arguments are not supported.\n   * - ``sum()``\n     - Full\n     -\n   * - ``super()``\n     - Partial\n     - It can only be used in ``nn.Module``'s ``__init__`` method.\n   * - ``type()``\n     - None\n     -\n   * - ``vars()``\n     - None\n     -\n   * - ``zip()``\n     - Full\n     -\n   * - ``__import__()``\n     - None\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":2012,"to":2060}}}}],["696",{"pageContent":".. _python-builtin-values-support:\n\nPython Built-in Values Support\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.. list-table:: TorchScript Support for Python Built-in Values\n   :widths: 25 25 50\n   :header-rows: 1\n\n   * - Built-in Value\n     - Support Level\n     - Notes\n   * - ``False``\n     - Full\n     -\n   * - ``True``\n     - Full\n     -\n   * - ``None``\n     - Full\n     -\n   * - ``NotImplemented``\n     - None\n     -\n   * - ``Ellipsis``\n     - Full\n     -\n\n\n.. _torch_apis_in_torchscript:\n\ntorch.* APIs\n~~~~~~~~~~~~\n\n.. _torch_apis_in_torchscript_rpc:\n\nRemote Procedure Calls\n^^^^^^^^^^^^^^^^^^^^^^\n\nTorchScript supports a subset of RPC APIs that supports running a function on\na specified remote worker instead of locally.\n\nSpecifically, following APIs are fully supported:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":2062,"to":2103}}}}],["697",{"pageContent":"TorchScript supports a subset of RPC APIs that supports running a function on\na specified remote worker instead of locally.\n\nSpecifically, following APIs are fully supported:\n\n- ``torch.distributed.rpc.rpc_sync()``\n    - ``rpc_sync()`` makes a blocking RPC call to run a function on a remote worker. RPC messages are sent and received in parallel to execution of Python code.\n    - More details about its usage and examples can be found in :meth:`~torch.distributed.rpc.rpc_sync`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":2103,"to":2110}}}}],["698",{"pageContent":"- ``torch.distributed.rpc.rpc_async()``\n    - ``rpc_async()`` makes a non-blocking RPC call to run a function on a remote worker. RPC messages are sent and received in parallel to execution of Python code.\n    - More details about its usage and examples can be found in :meth:`~torch.distributed.rpc.rpc_async`.\n- ``torch.distributed.rpc.remote()``\n    - ``remote.()`` executes a remote call on a worker and gets a Remote Reference ``RRef`` as the return value.\n    - More details about its usage and examples can be found in :meth:`~torch.distributed.rpc.remote`.\n\n.. _torch_apis_in_torchscript_async:\n\nAsynchronous Execution\n^^^^^^^^^^^^^^^^^^^^^^\n\nTorchScript enables you to create asynchronous computation tasks to make better use\nof computation resources. This is done via supporting a list of APIs that are\nonly usable within TorchScript:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":2112,"to":2126}}}}],["699",{"pageContent":"TorchScript enables you to create asynchronous computation tasks to make better use\nof computation resources. This is done via supporting a list of APIs that are\nonly usable within TorchScript:\n\n- ``torch.jit.fork()``\n    - Creates an asynchronous task executing func and a reference to the value of the result of this execution. Fork will return immediately.\n    - Synonymous to ``torch.jit._fork()``, which is only kept for backward compatibility reasons.\n    - More details about its usage and examples can be found in :meth:`~torch.jit.fork`.\n- ``torch.jit.wait()``\n    - Forces completion of a ``torch.jit.Future[T]`` asynchronous task, returning the result of the task.\n    - Synonymous to ``torch.jit._wait()``, which is only kept for backward compatibility reasons.\n    - More details about its usage and examples can be found in :meth:`~torch.jit.wait`.\n\n\n.. _torch_apis_in_torchscript_annotation:\n\nType Annotations\n^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":2126,"to":2143}}}}],["700",{"pageContent":".. _torch_apis_in_torchscript_annotation:\n\nType Annotations\n^^^^^^^^^^^^^^^^\n\nTorchScript is statically-typed. It provides and supports a set of utilities to help annotate variables and attributes:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":2143,"to":2148}}}}],["701",{"pageContent":"- ``torch.jit.annotate()``\n    - Provides a type hint to TorchScript where Python 3 style type hints do not work well.\n    - One common example is to annotate type for expressions like ``[]``. ``[]`` is treated as ``List[torch.Tensor]`` by default. When a different type is needed, you can use this code to hint TorchScript: ``torch.jit.annotate(List[int], [])``.\n    - More details can be found in :meth:`~torch.jit.annotate`\n- ``torch.jit.Attribute``\n    - Common use cases include providing type hint for ``torch.nn.Module`` attributes. Because their ``__init__`` methods are not parsed by TorchScript, ``torch.jit.Attribute`` should be used instead of ``torch.jit.annotate`` in the module's ``__init__`` methods.\n    - More details can be found in :meth:`~torch.jit.Attribute`\n- ``torch.jit.Final``\n    - An alias for Python's ``typing.Final``. ``torch.jit.Final`` is kept only for backward compatibility reasons.\n\n\n.. _torch_apis_in_torchscript_meta_programming:\n\nMeta Programming\n^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":2150,"to":2164}}}}],["702",{"pageContent":".. _torch_apis_in_torchscript_meta_programming:\n\nMeta Programming\n^^^^^^^^^^^^^^^^\n\nTorchScript provides a set of utilities to facilitate meta programming:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":2164,"to":2169}}}}],["703",{"pageContent":"- ``torch.jit.is_scripting()``\n    - Returns a boolean value indicating whether the current program is compiled by ``torch.jit.script`` or not.\n    - When used in an ``assert`` or an ``if`` statement, the scope or branch where ``torch.jit.is_scripting()`` evaluates to ``False`` is not compiled.\n    - Its value can be evaluated statically at compile time, thus commonly used in ``if`` statements to stop TorchScript from compiling one of the branches.\n    - More details and examples can be found in :meth:`~torch.jit.is_scripting`\n- ``torch.jit.is_tracing()``\n    - Returns a boolean value indicating whether the current program is traced by ``torch.jit.trace`` / ``torch.jit.trace_module`` or not.\n    - More details can be found in :meth:`~torch.jit.is_tracing`\n- ``@torch.jit.ignore``\n    - This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.\n    - This allows you to leave code in your model that is not yet TorchScript compatible.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":2171,"to":2181}}}}],["704",{"pageContent":"- This allows you to leave code in your model that is not yet TorchScript compatible.\n    - If a function decorated by ``@torch.jit.ignore`` is called from TorchScript, ignored functions will dispatch the call to the Python interpreter.\n    - Models with ignored functions cannot be exported.\n    - More details and examples can be found in :meth:`~torch.jit.ignore`\n- ``@torch.jit.unused``\n    - This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.\n    - This allows you to leave code in your model that is not yet TorchScript compatible and still export your model.\n    - If a function decorated by ``@torch.jit.unused`` is called from TorchScript, a runtime error will be raised.\n    - More details and examples can be found in :meth:`~torch.jit.unused`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":2181,"to":2189}}}}],["705",{"pageContent":".. _torch_apis_in_torchscript_type_refinement:\n\nType Refinement\n^^^^^^^^^^^^^^^\n\n- ``torch.jit.isinstance()``\n    - Returns a boolean indicating whether a variable is of the specified type.\n    - More details about its usage and examples can be found in :meth:`~torch.jit.isinstance`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_language_reference_v2.rst","loc":{"lines":{"from":2191,"to":2198}}}}],["706",{"pageContent":".. _python-language-reference:\n\nPython Language Reference Coverage\n==================================\n\nThis is a 1:1 mapping of the features listed in https://docs.python.org/3/reference/ and their\nsupport in TorchScript. The categorizations are as follows:\n\n\n.. list-table::\n   :header-rows: 1","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":1,"to":11}}}}],["707",{"pageContent":"* - Section\n     - Status\n     - Note\n   * - `1. Introduction <https://docs.python.org/3/reference/introduction.html>`_\n     - Not Relevant\n     -\n   * - `1.1. Alternate Implementations <https://docs.python.org/3/reference/introduction.html#alternate-implementations>`_\n     - Not Relevant\n     -\n   * - `1.2. Notation <https://docs.python.org/3/reference/introduction.html#notation>`_\n     - Not Relevant\n     -\n   * - `2. Lexical analysis <https://docs.python.org/3/reference/lexical_analysis.html#>`_\n     - Not Relevant\n     -\n   * - `2.1. Line structure <https://docs.python.org/3/reference/lexical_analysis.html#line-structure>`_\n     - Not Relevant\n     -\n   * - `2.1.1. Logical lines <https://docs.python.org/3/reference/lexical_analysis.html#logical-lines>`_\n     - Not Relevant\n     -\n   * - `2.1.2. Physical lines <https://docs.python.org/3/reference/lexical_analysis.html#physical-lines>`_\n     - Supported\n     -\n   * - `2.1.3. Comments <https://docs.python.org/3/reference/lexical_analysis.html#comments>`_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":13,"to":37}}}}],["708",{"pageContent":"- Supported\n     -\n   * - `2.1.3. Comments <https://docs.python.org/3/reference/lexical_analysis.html#comments>`_\n     - Supported\n     -\n   * - `2.1.4. Encoding declarations <https://docs.python.org/3/reference/lexical_analysis.html#encoding-declarations>`_\n     - Not Supported\n     - TorchScript explicitly don't support unicode\n   * - `2.1.5. Explicit line joining <https://docs.python.org/3/reference/lexical_analysis.html#explicit-line-joining>`_\n     - Supported\n     -\n   * - `2.1.6. Implicit line joining <https://docs.python.org/3/reference/lexical_analysis.html#implicit-line-joining>`_\n     - Supported\n     -\n   * - `2.1.7. Blank lines <https://docs.python.org/3/reference/lexical_analysis.html#blank-lines>`_\n     - Supported\n     -\n   * - `2.1.8. Indentation <https://docs.python.org/3/reference/lexical_analysis.html#indentation>`_\n     - Supported\n     -\n   * - `2.1.9. Whitespace between tokens <https://docs.python.org/3/reference/lexical_analysis.html#whitespace-between-tokens>`_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":37,"to":57}}}}],["709",{"pageContent":"- Supported\n     -\n   * - `2.1.9. Whitespace between tokens <https://docs.python.org/3/reference/lexical_analysis.html#whitespace-between-tokens>`_\n     - Not Relevant\n     -\n   * - `2.2. Other tokens <https://docs.python.org/3/reference/lexical_analysis.html#other-tokens>`_\n     - Not Relevant\n     -\n   * - `2.3. Identifiers and keywords <https://docs.python.org/3/reference/lexical_analysis.html#identifiers>`_\n     - Supported\n     -\n   * - `2.3.1. Keywords <https://docs.python.org/3/reference/lexical_analysis.html#keywords>`_\n     - Supported\n     -\n   * - `2.3.2. Reserved classes of identifiers <https://docs.python.org/3/reference/lexical_analysis.html#reserved-classes-of-identifiers>`_\n     - Supported\n     -\n   * - `2.4. Literals <https://docs.python.org/3/reference/lexical_analysis.html#literals>`_\n     - Not Relevant\n     -\n   * - `2.4.1. String and Bytes literals <https://docs.python.org/3/reference/lexical_analysis.html#string-and-bytes-literals>`_\n     - Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":57,"to":79}}}}],["710",{"pageContent":"- Not Relevant\n     -\n   * - `2.4.1. String and Bytes literals <https://docs.python.org/3/reference/lexical_analysis.html#string-and-bytes-literals>`_\n     - Supported\n     -\n   * - `2.4.2. String literal concatenation <https://docs.python.org/3/reference/lexical_analysis.html#string-literal-concatenation>`_\n     - Supported\n     -\n   * - `2.4.3. Formatted string literals <https://docs.python.org/3/reference/lexical_analysis.html#formatted-string-literals>`_\n     - Partially Supported\n     -\n   * - `2.4.4. Numeric literals <https://docs.python.org/3/reference/lexical_analysis.html#numeric-literals>`_\n     - Supported\n     -\n   * - `2.4.5. Integer literals <https://docs.python.org/3/reference/lexical_analysis.html#integer-literals>`_\n     - Supported\n     -\n   * - `2.4.6. Floating point literals <https://docs.python.org/3/reference/lexical_analysis.html#floating-point-literals>`_\n     - Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":79,"to":98}}}}],["711",{"pageContent":"- Supported\n     -\n   * - `2.4.6. Floating point literals <https://docs.python.org/3/reference/lexical_analysis.html#floating-point-literals>`_\n     - Supported\n     -\n   * - `2.4.7. Imaginary literals <https://docs.python.org/3/reference/lexical_analysis.html#imaginary-literals>`_\n     - Not Supported\n     -\n   * - `2.5. Operators <https://docs.python.org/3/reference/lexical_analysis.html#operators>`_\n     - Partially Supported\n     - Not supported: ``<<``, ``>>``, ``:=``\n   * - `2.6. Delimiters <https://docs.python.org/3/reference/lexical_analysis.html#delimiters>`_\n     - Partially Supported\n     - Not supported: ``**=``, ``<<=``, ``>>=``, ``%=``, ``^=``, ``@=``, ``&=``, ``//=``, ``%`` operator for some types (e.g. ``str``\\ )\n   * - `3. Data model <https://docs.python.org/3/reference/datamodel.html#>`_\n     - Not Relevant\n     -\n   * - `3.1. Objects, values and types <https://docs.python.org/3/reference/datamodel.html#objects-values-and-types>`_\n     - Not Relevant\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":98,"to":117}}}}],["712",{"pageContent":"- Not Relevant\n     -\n   * - `3.1. Objects, values and types <https://docs.python.org/3/reference/datamodel.html#objects-values-and-types>`_\n     - Not Relevant\n     -\n   * - `3.2. The standard type hierarchy <https://docs.python.org/3/reference/datamodel.html#the-standard-type-hierarchy>`_\n     - Partially Supported\n     - Not supported: NotImplemented, Ellipsis, numbers.Complex, bytes, byte arrays, sets, frozen sets, generators, coroutines, async generators, modules, I/O objects, internal objects, slice objects ( though slicing is supported), classmethod\n   * - `3.3. Special method names <https://docs.python.org/3/reference/datamodel.html#special-method-names>`_\n     - Supported\n     -\n   * - `3.3.1. Basic customization <https://docs.python.org/3/reference/datamodel.html#basic-customization>`_\n     - Partially Supported\n     - Not supported: ``__new__`` , ``__del__`` , ``__bytes__`` , ``__format__`` , ``__hash__`` ,","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":117,"to":130}}}}],["713",{"pageContent":"- Partially Supported\n     - Not supported: ``__new__`` , ``__del__`` , ``__bytes__`` , ``__format__`` , ``__hash__`` ,\n   * - `3.3.2. Customizing attribute access <https://docs.python.org/3/reference/datamodel.html#customizing-attribute-access>`_\n     - Not Supported\n     -\n   * - `3.3.2.1. Customizing module attribute access <https://docs.python.org/3/reference/datamodel.html#customizing-module-attribute-access>`_\n     - Not Supported\n     -\n   * - `3.3.2.2. Implementing Descriptors <https://docs.python.org/3/reference/datamodel.html#implementing-descriptors>`_\n     - Not Supported\n     -\n   * - `3.3.2.3. Invoking Descriptors <https://docs.python.org/3/reference/datamodel.html#invoking-descriptors>`_\n     - Not Supported\n     -\n   * - `3.3.2.4. __slots__ <https://docs.python.org/3/reference/datamodel.html#slots>`_\n     - Not Supported\n     -\n   * - `3.3.2.4.1. Notes on using __slots__ <https://docs.python.org/3/reference/datamodel.html#notes-on-using-slots>`_\n     - Not Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":130,"to":149}}}}],["714",{"pageContent":"- Not Supported\n     -\n   * - `3.3.2.4.1. Notes on using __slots__ <https://docs.python.org/3/reference/datamodel.html#notes-on-using-slots>`_\n     - Not Supported\n     -\n   * - `3.3.3. Customizing class creation <https://docs.python.org/3/reference/datamodel.html#customizing-class-creation>`_\n     - Not Supported\n     -\n   * - `3.3.3.1. Metaclasses <https://docs.python.org/3/reference/datamodel.html#metaclasses>`_\n     - Not Supported\n     -\n   * - `3.3.3.2. Resolving MRO entries <https://docs.python.org/3/reference/datamodel.html#resolving-mro-entries>`_\n     - Not Supported\n     - ``super()`` is not supported\n   * - `3.3.3.3. Determining the appropriate metaclass <https://docs.python.org/3/reference/datamodel.html#determining-the-appropriate-metaclass>`_\n     - Not relevant\n     -\n   * - `3.3.3.4. Preparing the class namespace <https://docs.python.org/3/reference/datamodel.html#preparing-the-class-namespace>`_\n     - Not relevant\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":149,"to":168}}}}],["715",{"pageContent":"- Not relevant\n     -\n   * - `3.3.3.4. Preparing the class namespace <https://docs.python.org/3/reference/datamodel.html#preparing-the-class-namespace>`_\n     - Not relevant\n     -\n   * - `3.3.3.5. Executing the class body <https://docs.python.org/3/reference/datamodel.html#executing-the-class-body>`_\n     - Not relevant\n     -\n   * - `3.3.3.6. Creating the class object <https://docs.python.org/3/reference/datamodel.html#creating-the-class-object>`_\n     - Not relevant\n     -\n   * - `3.3.3.7. Uses for metaclasses <https://docs.python.org/3/reference/datamodel.html#uses-for-metaclasses>`_\n     - Not relevant\n     -\n   * - `3.3.4. Customizing instance and subclass checks <https://docs.python.org/3/reference/datamodel.html#customizing-instance-and-subclass-checks>`_\n     - Not Supported\n     -\n   * - `3.3.5. Emulating generic types <https://docs.python.org/3/reference/datamodel.html#emulating-generic-types>`_\n     - Not Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":168,"to":187}}}}],["716",{"pageContent":"- Not Supported\n     -\n   * - `3.3.5. Emulating generic types <https://docs.python.org/3/reference/datamodel.html#emulating-generic-types>`_\n     - Not Supported\n     -\n   * - `3.3.6. Emulating callable objects <https://docs.python.org/3/reference/datamodel.html#emulating-callable-objects>`_\n     - Supported\n     -\n   * - `3.3.7. Emulating container types <https://docs.python.org/3/reference/datamodel.html#emulating-container-types>`_\n     - Partially Supported\n     - Some magic methods not supported (e.g. ``__iter__`` )\n   * - `3.3.8. Emulating numeric types <https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types>`_\n     - Partially Supported\n     - Magic methods with swapped operands not supported (``__r*__``)\n   * - `3.3.9. With Statement Context Managers <https://docs.python.org/3/reference/datamodel.html#with-statement-context-managers>`_\n     - Not Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":187,"to":203}}}}],["717",{"pageContent":"* - `3.3.9. With Statement Context Managers <https://docs.python.org/3/reference/datamodel.html#with-statement-context-managers>`_\n     - Not Supported\n     -\n   * - `3.3.10. Special method lookup <https://docs.python.org/3/reference/datamodel.html#special-method-lookup>`_\n     - Not relevant\n     -\n   * - `3.4. Coroutines <https://docs.python.org/3/reference/datamodel.html#coroutines>`_\n     - Not Supported\n     -\n   * - `3.4.1. Awaitable Objects <https://docs.python.org/3/reference/datamodel.html#awaitable-objects>`_\n     - Not Supported\n     -\n   * - `3.4.2. Coroutine Objects <https://docs.python.org/3/reference/datamodel.html#coroutine-objects>`_\n     - Not Supported\n     -\n   * - `3.4.3. Asynchronous Iterators <https://docs.python.org/3/reference/datamodel.html#asynchronous-iterators>`_\n     - Not Supported\n     -\n   * - `3.4.4. Asynchronous Context Managers <https://docs.python.org/3/reference/datamodel.html#asynchronous-context-managers>`_\n     - Not Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":203,"to":223}}}}],["718",{"pageContent":"- Not Supported\n     -\n   * - `3.4.4. Asynchronous Context Managers <https://docs.python.org/3/reference/datamodel.html#asynchronous-context-managers>`_\n     - Not Supported\n     -\n   * - `4. Execution model <https://docs.python.org/3/reference/executionmodel.html#>`_\n     - Not Relevant\n     -\n   * - `4.1. Structure of a program <https://docs.python.org/3/reference/executionmodel.html#structure-of-a-program>`_\n     - Not Relevant\n     -\n   * - `4.2. Naming and binding <https://docs.python.org/3/reference/executionmodel.html#naming-and-binding>`_\n     - Not Relevant\n     - Names are bound at compile time in TorchScript\n   * - `4.2.1. Binding of names <https://docs.python.org/3/reference/executionmodel.html#binding-of-names>`_\n     - Not Relevant\n     - See ``global`` and ``nonlocal`` statements section\n   * - `4.2.2. Resolution of names <https://docs.python.org/3/reference/executionmodel.html#resolution-of-names>`_\n     - Not Relevant\n     - See ``global`` and ``nonlocal`` statements section","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":223,"to":242}}}}],["719",{"pageContent":"* - `4.2.2. Resolution of names <https://docs.python.org/3/reference/executionmodel.html#resolution-of-names>`_\n     - Not Relevant\n     - See ``global`` and ``nonlocal`` statements section\n   * - `4.2.3. Builtins and restricted execution <https://docs.python.org/3/reference/executionmodel.html#builtins-and-restricted-execution>`_\n     - Not Relevant\n     -\n   * - `4.2.4. Interaction with dynamic features <https://docs.python.org/3/reference/executionmodel.html#interaction-with-dynamic-features>`_\n     - Not Supported\n     - Python values cannot be captured\n   * - `4.3. Exceptions <https://docs.python.org/3/reference/executionmodel.html#exceptions>`_\n     - Partially Supported\n     - See ``try`` and ``raise`` statement section\n   * - `5. The import system <https://docs.python.org/3/reference/import.html>`_\n     - Not Relevant\n     -\n   * - `6. Expressions <https://docs.python.org/3/reference/expressions.html#>`_\n     - Not Relevant\n     - See expressions section","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":242,"to":259}}}}],["720",{"pageContent":"- Not Relevant\n     -\n   * - `6. Expressions <https://docs.python.org/3/reference/expressions.html#>`_\n     - Not Relevant\n     - See expressions section\n   * - `6.1. Arithmetic conversions <https://docs.python.org/3/reference/expressions.html#arithmetic-conversions>`_\n     - Supported\n     -\n   * - `6.2. Atoms <https://docs.python.org/3/reference/expressions.html#atoms>`_\n     - Not Relevant\n     -\n   * - `6.2.1. Identifiers (Names) <https://docs.python.org/3/reference/expressions.html#atom-identifiers>`_\n     - Supported\n     -\n   * - `6.2.2. Literals <https://docs.python.org/3/reference/expressions.html#literals>`_\n     - Partially Supported\n     - ``bytesliteral``\\ , ``imagnumber`` not supported\n   * - `6.2.3. Parenthesized forms <https://docs.python.org/3/reference/expressions.html#parenthesized-forms>`_\n     - Supported\n     -\n   * - `6.2.4. Displays for lists, sets and dictionaries <https://docs.python.org/3/reference/expressions.html#displays-for-lists-sets-and-dictionaries>`_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":259,"to":279}}}}],["721",{"pageContent":"- Supported\n     -\n   * - `6.2.4. Displays for lists, sets and dictionaries <https://docs.python.org/3/reference/expressions.html#displays-for-lists-sets-and-dictionaries>`_\n     - Partially Supported\n     - Not supported: comprehension ifs, async iterators\n   * - `6.2.5. List displays <https://docs.python.org/3/reference/expressions.html#list-displays>`_\n     - Supported\n     -\n   * - `6.2.6. Set displays <https://docs.python.org/3/reference/expressions.html#set-displays>`_\n     - Not Supported\n     -\n   * - `6.2.7. Dictionary displays <https://docs.python.org/3/reference/expressions.html#dictionary-displays>`_\n     - Supported\n     - dict() constructor with kwargs doesn't work, dict comprehensions, dictionary unpacking\n   * - `6.2.8. Generator expressions <https://docs.python.org/3/reference/expressions.html#generator-expressions>`_\n     - Not Supported\n     -\n   * - `6.2.9. Yield expressions <https://docs.python.org/3/reference/expressions.html#yield-expressions>`_\n     - Not Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":279,"to":298}}}}],["722",{"pageContent":"- Not Supported\n     -\n   * - `6.2.9. Yield expressions <https://docs.python.org/3/reference/expressions.html#yield-expressions>`_\n     - Not Supported\n     -\n   * - `6.2.9.1. Generator-iterator methods <https://docs.python.org/3/reference/expressions.html#generator-iterator-methods>`_\n     - Not Supported\n     -\n   * - `6.2.9.2. Examples <https://docs.python.org/3/reference/expressions.html#examples>`_\n     - Not Supported\n     -\n   * - `6.2.9.3. Asynchronous generator functions <https://docs.python.org/3/reference/expressions.html#asynchronous-generator-functions>`_\n     - Not Supported\n     -\n   * - `6.2.9.4. Asynchronous generator-iterator methods <https://docs.python.org/3/reference/expressions.html#asynchronous-generator-iterator-methods>`_\n     - Not Supported\n     -\n   * - `6.3. Primaries <https://docs.python.org/3/reference/expressions.html#primaries>`_\n     - Supported\n     -\n   * - `6.3.1. Attribute references <https://docs.python.org/3/reference/expressions.html#attribute-references>`_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":298,"to":318}}}}],["723",{"pageContent":"- Supported\n     -\n   * - `6.3.1. Attribute references <https://docs.python.org/3/reference/expressions.html#attribute-references>`_\n     - Supported\n     -\n   * - `6.3.2. Subscriptions <https://docs.python.org/3/reference/expressions.html#subscriptions>`_\n     - Supported\n     -\n   * - `6.3.3. Slicings <https://docs.python.org/3/reference/expressions.html#slicings>`_\n     - Partially Supported\n     - Tuple slicing with stride is not supported\n   * - `6.3.4. Calls <https://docs.python.org/3/reference/expressions.html#calls>`_\n     - Partially Supported\n     - Args unpack / kwargs unpack is not supported\n   * - `6.4. Await expression <https://docs.python.org/3/reference/expressions.html#await-expression>`_\n     - Not Supported\n     -\n   * - `6.5. The power operator <https://docs.python.org/3/reference/expressions.html#the-power-operator>`_\n     - Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":318,"to":337}}}}],["724",{"pageContent":"- Not Supported\n     -\n   * - `6.5. The power operator <https://docs.python.org/3/reference/expressions.html#the-power-operator>`_\n     - Supported\n     -\n   * - `6.6. Unary arithmetic and bitwise operations <https://docs.python.org/3/reference/expressions.html#unary-arithmetic-and-bitwise-operations>`_\n     - Partially Supported\n     - Some bitwise operators are not implemented for primitive types (e.g. ``~x`` where ``x`` is an ``int`` is not currently supported)\n   * - `6.7. Binary arithmetic operations <https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations>`_\n     - Partially Supported\n     - See delimiters section\n   * - `6.8. Shifting operations <https://docs.python.org/3/reference/expressions.html#shifting-operations>`_\n     - Not Supported\n     -\n   * - `6.9. Binary bitwise operations <https://docs.python.org/3/reference/expressions.html#binary-bitwise-operations>`_\n     - Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":337,"to":353}}}}],["725",{"pageContent":"- Not Supported\n     -\n   * - `6.9. Binary bitwise operations <https://docs.python.org/3/reference/expressions.html#binary-bitwise-operations>`_\n     - Supported\n     -\n   * - `6.10. Comparisons <https://docs.python.org/3/reference/expressions.html#comparisons>`_\n     - Supported\n     -\n   * - `6.10.1. Value comparisons <https://docs.python.org/3/reference/expressions.html#value-comparisons>`_\n     - Partially Supported\n     - Dictionary equality checks are not currently supported\n   * - `6.10.2. Membership test operations <https://docs.python.org/3/reference/expressions.html#membership-test-operations>`_\n     - Partially Supported\n     - Not supported for TorchScript classes\n   * - `6.10.3. Identity comparisons <https://docs.python.org/3/reference/expressions.html#is-not>`_\n     - Supported\n     -\n   * - `6.11. Boolean operations <https://docs.python.org/3/reference/expressions.html#boolean-operations>`_\n     - Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":353,"to":372}}}}],["726",{"pageContent":"- Supported\n     -\n   * - `6.11. Boolean operations <https://docs.python.org/3/reference/expressions.html#boolean-operations>`_\n     - Supported\n     -\n   * - `6.12. Conditional expressions <https://docs.python.org/3/reference/expressions.html#conditional-expressions>`_\n     - Supported\n     -\n   * - `6.13. Lambdas <https://docs.python.org/3/reference/expressions.html#lambda>`_\n     - Not Supported\n     -\n   * - `6.14. Expression lists <https://docs.python.org/3/reference/expressions.html#expression-lists>`_\n     - Partially Supported\n     - Iterable unpacking not supported\n   * - `6.15. Evaluation order <https://docs.python.org/3/reference/expressions.html#evaluation-order>`_\n     - Supported\n     -\n   * - `6.16. Operator precedence <https://docs.python.org/3/reference/expressions.html#operator-precedence>`_\n     - Supported\n     -\n   * - `7. Simple statements <https://docs.python.org/3/reference/simple_stmts.html#>`_\n     - Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":372,"to":394}}}}],["727",{"pageContent":"- Supported\n     -\n   * - `7. Simple statements <https://docs.python.org/3/reference/simple_stmts.html#>`_\n     - Supported\n     -\n   * - `7.1. Expression statements <https://docs.python.org/3/reference/simple_stmts.html#expression-statements>`_\n     - Supported\n     -\n   * - `7.2. Assignment statements <https://docs.python.org/3/reference/simple_stmts.html#assignment-statements>`_\n     - Supported\n     -\n   * - `7.2.1. Augmented assignment statements <https://docs.python.org/3/reference/simple_stmts.html#augmented-assignment-statements>`_\n     - Partially Supported\n     - See delimiters section\n   * - `7.2.2. Annotated assignment statements <https://docs.python.org/3/reference/simple_stmts.html#annotated-assignment-statements>`_\n     - Supported\n     -\n   * - `7.3. The assert statement <https://docs.python.org/3/reference/simple_stmts.html#the-assert-statement>`_\n     - Partially Supported\n     - Exception message is not customizable","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":394,"to":413}}}}],["728",{"pageContent":"-\n   * - `7.3. The assert statement <https://docs.python.org/3/reference/simple_stmts.html#the-assert-statement>`_\n     - Partially Supported\n     - Exception message is not customizable\n   * - `7.4. The pass statement <https://docs.python.org/3/reference/simple_stmts.html#the-pass-statement>`_\n     - Supported\n     -\n   * - `7.5. The del statement <https://docs.python.org/3/reference/simple_stmts.html#the-del-statement>`_\n     - Not Supported\n     -\n   * - `7.6. The return statement <https://docs.python.org/3/reference/simple_stmts.html#the-return-statement>`_\n     - Supported\n     - Some other features of returning (e.g. behavior with try..finally) are unsupported\n   * - `7.7. The yield statement <https://docs.python.org/3/reference/simple_stmts.html#the-yield-statement>`_\n     - Not Supported\n     -\n   * - `7.8. The raise statement <https://docs.python.org/3/reference/simple_stmts.html#the-raise-statement>`_\n     - Partially Supported\n     - Exception message is not customizable","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":413,"to":431}}}}],["729",{"pageContent":"-\n   * - `7.8. The raise statement <https://docs.python.org/3/reference/simple_stmts.html#the-raise-statement>`_\n     - Partially Supported\n     - Exception message is not customizable\n   * - `7.9. The break statement <https://docs.python.org/3/reference/simple_stmts.html#the-break-statement>`_\n     - Supported\n     - Some other features of returning (e.g. behavior with try..finally) are unsupported\n   * - `7.10. The continue statement <https://docs.python.org/3/reference/simple_stmts.html#the-continue-statement>`_\n     - Supported\n     - Some other features of returning (e.g. behavior with try..finally) are unsupported\n   * - `7.11. The import statement <https://docs.python.org/3/reference/simple_stmts.html#the-import-statement>`_\n     - Not Supported\n     -\n   * - `7.11.1. Future statements <https://docs.python.org/3/reference/simple_stmts.html#future-statements>`_\n     - Not Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":431,"to":446}}}}],["730",{"pageContent":"- Not Supported\n     -\n   * - `7.11.1. Future statements <https://docs.python.org/3/reference/simple_stmts.html#future-statements>`_\n     - Not Supported\n     -\n   * - `7.12. The global statement <https://docs.python.org/3/reference/simple_stmts.html#the-global-statement>`_\n     - Not Supported\n     -\n   * - `7.13. The nonlocal statement <https://docs.python.org/3/reference/simple_stmts.html#the-nonlocal-statement>`_\n     - Not Supported\n     -\n   * - `8. Compound statements <https://docs.python.org/3/reference/compound_stmts.html#>`_\n     - Irrelevant\n     -\n   * - `8.1. The if statement <https://docs.python.org/3/reference/compound_stmts.html#the-if-statement>`_\n     - Supported\n     -\n   * - `8.2. The while statement <https://docs.python.org/3/reference/compound_stmts.html#the-while-statement>`_\n     - Partially Supported\n     - while..else is not supported\n   * - `8.3. The for statement <https://docs.python.org/3/reference/compound_stmts.html#the-for-statement>`_\n     - Partially Supported","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":446,"to":467}}}}],["731",{"pageContent":"- Partially Supported\n     - while..else is not supported\n   * - `8.3. The for statement <https://docs.python.org/3/reference/compound_stmts.html#the-for-statement>`_\n     - Partially Supported\n     - for..else is not supported\n   * - `8.4. The try statement <https://docs.python.org/3/reference/compound_stmts.html#the-try-statement>`_\n     - Not Supported\n     -\n   * - `8.5. The with statement <https://docs.python.org/3/reference/compound_stmts.html#the-with-statement>`_\n     - Partially Supported\n     - ``__exit__`` is always called with ``exc_type``, ``exc_value``, and ``traceback`` set to None, even if an exception was raised, and ``__exit__``'s return value is ignored.\n   * - `8.6. Function definitions <https://docs.python.org/3/reference/compound_stmts.html#function-definitions>`_\n     - Not Supported\n     -\n   * - `8.7. Class definitions <https://docs.python.org/3/reference/compound_stmts.html#class-definitions>`_\n     - Not Supported\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":467,"to":483}}}}],["732",{"pageContent":"- Not Supported\n     -\n   * - `8.7. Class definitions <https://docs.python.org/3/reference/compound_stmts.html#class-definitions>`_\n     - Not Supported\n     -\n   * - `8.8. Coroutines <https://docs.python.org/3/reference/compound_stmts.html#coroutines>`_\n     - Not Supported\n     -\n   * - `8.8.1. Coroutine function definition <https://docs.python.org/3/reference/compound_stmts.html#coroutine-function-definition>`_\n     - Not Supported\n     -\n   * - `8.8.2. The async for statement <https://docs.python.org/3/reference/compound_stmts.html#the-async-for-statement>`_\n     - Not Supported\n     -\n   * - `8.8.3. The async with statement <https://docs.python.org/3/reference/compound_stmts.html#the-async-with-statement>`_\n     - Not Supported\n     -\n   * - `9. Top-level components <https://docs.python.org/3/reference/toplevel_components.html#>`_\n     - Not Relevant\n     -\n   * - `9.1. Complete Python programs <https://docs.python.org/3/reference/toplevel_components.html#complete-python-programs>`_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":483,"to":503}}}}],["733",{"pageContent":"- Not Relevant\n     -\n   * - `9.1. Complete Python programs <https://docs.python.org/3/reference/toplevel_components.html#complete-python-programs>`_\n     - Not Relevant\n     -\n   * - `9.2. File input <https://docs.python.org/3/reference/toplevel_components.html#file-input>`_\n     - Not Relevant\n     -\n   * - `9.3. Interactive input <https://docs.python.org/3/reference/toplevel_components.html#interactive-input>`_\n     - Not Relevant\n     -\n   * - `9.4. Expression input <https://docs.python.org/3/reference/toplevel_components.html#expression-input>`_\n     - Not Relevant\n     -","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_python_reference.rst","loc":{"lines":{"from":503,"to":516}}}}],["734",{"pageContent":".. _jit_unsupported:\n\nTorchScript Unsupported PyTorch Constructs\n============================================\n\nTorch and Tensor Unsupported Attributes\n------------------------------------------\n\n\nTorchScript supports most methods defined on ``torch`` and ``torch.Tensor``, but we do not have full coverage.\nHere are specific known ops and categories of ops which have diverging behavior between\nPython and TorchScript. If you encounter something else that is not supported please\nfile a GitHub issue. Deprecated ops are not listed below.\n\n\n\n.. automodule:: torch.jit.unsupported_tensor_ops\n\n\nFunctions Not Correctly Bound on Torch\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe following functions will fail if used in TorchScript, either because they\nare not bound on `torch` or because Python expects a different schema than\nTorchScript.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_unsupported.rst","loc":{"lines":{"from":1,"to":25}}}}],["735",{"pageContent":"The following functions will fail if used in TorchScript, either because they\nare not bound on `torch` or because Python expects a different schema than\nTorchScript.\n\n  * :func:`torch.tensordot`\n  * :func:`torch.nn.init.calculate_gain`\n  * :func:`torch.nn.init.eye_`\n  * :func:`torch.nn.init.dirac_`\n  * :func:`torch.nn.init.kaiming_normal_`\n  * :func:`torch.nn.init.orthogonal_`\n  * :func:`torch.nn.init.sparse`\n\n\nOps With Divergent Schemas Between Torch & Python\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe following categories of ops have divergent schemas:\n\nFunctions which construct tensors from non-tensor inputs do not support the `requires_grad`\nargument, except for `torch.tensor`. This covers the following ops:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_unsupported.rst","loc":{"lines":{"from":25,"to":44}}}}],["736",{"pageContent":"Functions which construct tensors from non-tensor inputs do not support the `requires_grad`\nargument, except for `torch.tensor`. This covers the following ops:\n\n  * :func:`torch.norm`\n  * :func:`torch.bartlett_window`\n  * :func:`torch.blackman_window`\n  * :func:`torch.empty`\n  * :func:`torch.empty_like`\n  * :func:`torch.empty_strided`\n  * :func:`torch.eye`\n  * :func:`torch.full`\n  * :func:`torch.full_like`\n  * :func:`torch.hamming_window`\n  * :func:`torch.hann_window`\n  * :func:`torch.linspace`\n  * :func:`torch.logspace`\n  * :func:`torch.normal`\n  * :func:`torch.ones`\n  * :func:`torch.rand`\n  * :func:`torch.rand_like`\n  * :func:`torch.randint_like`\n  * :func:`torch.randn`\n  * :func:`torch.randn_like`\n  * :func:`torch.randperm`\n  * :func:`torch.tril_indices`\n  * :func:`torch.triu_indices`\n  * :func:`torch.vander`\n  * :func:`torch.zeros`\n  * :func:`torch.zeros_like`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_unsupported.rst","loc":{"lines":{"from":44,"to":72}}}}],["737",{"pageContent":"The following functions require `dtype`, `layout`, `device` as parameters in TorchScript,\nbut these parameters are optional in Python.\n\n  * :func:`torch.randint`\n  * :func:`torch.sparse_coo_tensor`\n  * :meth:`~torch.Tensor.to`\n\n\nPyTorch Unsupported Modules and Classes\n------------------------------------------\n\nTorchScript cannot currently compile a number of other commonly used PyTorch\nconstructs. Below are listed the modules that TorchScript does not support, and\nan incomplete list of PyTorch classes that are not supported. For unsupported modules\nwe suggest using :meth:`torch.jit.trace`.\n\n  * :class:`torch.nn.RNN`\n  * :class:`torch.nn.AdaptiveLogSoftmaxWithLoss`\n  * :class:`torch.autograd.Function`\n  * :class:`torch.autograd.enable_grad`\n  * :class:`torch.Generator`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_unsupported.rst","loc":{"lines":{"from":74,"to":94}}}}],["738",{"pageContent":"JIT Utils - torch.utils.jit\n==================================================\n\n.. automodule:: torch.utils.jit","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/jit_utils.rst","loc":{"lines":{"from":1,"to":4}}}}],["739",{"pageContent":"torch.library\n===================================\n\nPython operator registration API provides capabilities for extending PyTorch's core library\nof operators with user defined operators. Currently, this can be done in two ways:\n\n#. Creating new libraries\n\n   * Lets you to register **new operators** and kernels for various backends and functionalities by specifying the appropriate dispatch keys. For example,\n\n      * Consider registering a new operator ``add`` in your newly created namespace ``foo``. You can access this operator using the ``torch.ops`` API and calling into by calling ``torch.ops.foo.add``. You can also access specific registered overloads by calling ``torch.ops.foo.add.{overload_name}``.\n\n      * If you registered a new kernel for the ``CUDA`` dispatch key for this operator, then your custom defined function will be called for CUDA tensor inputs.\n\n   * This can be done by creating Library class objects of ``\"DEF\"`` kind.\n\n#. Extending existing C++ libraries (e.g., aten)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/library.rst","loc":{"lines":{"from":1,"to":17}}}}],["740",{"pageContent":"* This can be done by creating Library class objects of ``\"DEF\"`` kind.\n\n#. Extending existing C++ libraries (e.g., aten)\n\n   * Lets you register kernels for **existing operators** corresponding to various backends and functionalities by specifying the appropriate dispatch keys.\n\n   * This may come in handy to fill up spotty operator support for a feature implemented through a dispatch key. For example.,\n\n      * You can add operator support for Meta Tensors (by registering function to the ``Meta`` dispatch key).\n\n   * This can be done by creating Library class objects of ``\"IMPL\"`` kind.\n\nA tutorial that walks you through some examples on how to use this API is available on `Google Colab <https://colab.research.google.com/drive/1RRhSfk7So3Cn02itzLWE9K4Fam-8U011?usp=sharing>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/library.rst","loc":{"lines":{"from":17,"to":29}}}}],["741",{"pageContent":"A tutorial that walks you through some examples on how to use this API is available on `Google Colab <https://colab.research.google.com/drive/1RRhSfk7So3Cn02itzLWE9K4Fam-8U011?usp=sharing>`_.\n\n.. warning::\n  Dispatcher is a complicated PyTorch concept and having a sound understanding of Dispatcher is crucial\n  to be able to do anything advanced with this API. `This blog post <http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/>`_\n  is a good starting point to learn about Dispatcher.\n\n.. currentmodule:: torch.library\n\n.. autoclass:: torch.library.Library\n  :members:\n\nWe have also added some function decorators to make it convenient to register functions for operators:\n\n* :func:`torch.library.impl`\n* :func:`torch.library.define`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/library.rst","loc":{"lines":{"from":29,"to":44}}}}],["742",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\ntorch.linalg\n============\n\nCommon linear algebra operations.\n\nSee :ref:`Linear Algebra Stability` for some common numerical edge-cases.\n\n.. automodule:: torch.linalg\n.. currentmodule:: torch.linalg\n\nMatrix Properties\n-----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    norm\n    vector_norm\n    matrix_norm\n    diagonal\n    det\n    slogdet\n    cond\n    matrix_rank\n\nDecompositions\n--------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    cholesky\n    qr\n    lu\n    lu_factor\n    eig\n    eigvals\n    eigh\n    eigvalsh\n    svd\n    svdvals\n\n.. _linalg solvers:\n\nSolvers\n-------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    solve\n    solve_triangular\n    lu_solve\n    lstsq\n\n.. _linalg inverses:\n\nInverses\n--------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    inv\n    pinv\n\nMatrix Functions\n----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    matrix_exp\n    matrix_power","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/linalg.rst","loc":{"lines":{"from":1,"to":82}}}}],["743",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    inv\n    pinv\n\nMatrix Functions\n----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    matrix_exp\n    matrix_power\n\nMatrix Products\n---------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    cross\n    matmul\n    vecdot\n    multi_dot\n    householder_product\n\nTensor Operations\n-----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    tensorinv\n    tensorsolve\n\nMisc\n----\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    vander\n\nExperimental Functions\n----------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    cholesky_ex\n    inv_ex\n    solve_ex\n    lu_factor_ex\n    ldl_factor\n    ldl_factor_ex\n    ldl_solve","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/linalg.rst","loc":{"lines":{"from":82,"to":143}}}}],["744",{"pageContent":".. automodule:: torch.masked\n.. automodule:: torch.masked.maskedtensor\n\n.. currentmodule:: torch\n\n.. _masked-docs:\n\ntorch.masked\n============\n\nIntroduction\n++++++++++++\n\nMotivation\n----------\n\n.. warning::\n\n  The PyTorch API of masked tensors is in the prototype stage and may or may not change in the future.\n\nMaskedTensor serves as an extension to :class:`torch.Tensor` that provides the user with the ability to:\n\n* use any masked semantics (e.g. variable length tensors, nan* operators, etc.)\n* differentiate between 0 and NaN gradients\n* various sparse applications (see tutorial below)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":1,"to":25}}}}],["745",{"pageContent":"* use any masked semantics (e.g. variable length tensors, nan* operators, etc.)\n* differentiate between 0 and NaN gradients\n* various sparse applications (see tutorial below)\n\n\"Specified\" and \"unspecified\" have a long history in PyTorch without formal semantics and certainly without\nconsistency; indeed, MaskedTensor was born out of a build up of issues that the vanilla :class:`torch.Tensor`\nclass could not properly address. Thus, a primary goal of MaskedTensor is to become the source of truth for\nsaid \"specified\" and \"unspecified\" values in PyTorch where they are a first class citizen instead of an afterthought.\nIn turn, this should further unlock `sparsity's <https://pytorch.org/docs/stable/sparse.html>`_ potential,\nenable safer and more consistent operators, and provide a smoother and more intuitive experience\nfor users and developers alike.\n\nWhat is a MaskedTensor?\n-----------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":25,"to":38}}}}],["746",{"pageContent":"What is a MaskedTensor?\n-----------------------\n\nA MaskedTensor is a tensor subclass that consists of 1) an input (data), and 2) a mask. The mask tells us\nwhich entries from the input should be included or ignored.\n\nBy way of example, suppose that we wanted to mask out all values that are equal to 0 (represented by the gray)\nand take the max:\n\n.. image:: _static/img/masked/tensor_comparison.jpg\n      :scale: 50%\n\nOn top is the vanilla tensor example while the bottom is MaskedTensor where all the 0's are masked out.\nThis clearly yields a different result depending on whether we have the mask, but this flexible structure\nallows the user to systematically ignore any elements they'd like during computation.\n\nThere are already a number of existing tutorials that we've written to help users onboard, such as:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":38,"to":54}}}}],["747",{"pageContent":"There are already a number of existing tutorials that we've written to help users onboard, such as:\n\n-  `Overview - the place to start for new users, discusses how to use MaskedTensors and why they're useful`_\n-  `Sparsity - MaskedTensor supports sparse COO and CSR data and mask Tensors`_\n-  `Adagrad sparse semantics - a practical example of how MaskedTensor can simplify sparse semantics and implementations`_\n-  `Advanced semantics - discussion on why certain decisions were made (e.g. requiring masks to match for binary/reduction operations),\n   differences with NumPy's MaskedArray, and reduction semantics`_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":54,"to":60}}}}],["748",{"pageContent":".. _Overview - the place to start for new users, discusses how to use MaskedTensors and why they're useful: https://pytorch.org/tutorials/prototype/maskedtensor_overview\n.. _Sparsity - MaskedTensor supports sparse COO and CSR data and mask Tensors: https://pytorch.org/tutorials/prototype/maskedtensor_sparsity\n.. _Adagrad sparse semantics - a practical example of how MaskedTensor can simplify sparse semantics and implementations: https://pytorch.org/tutorials/prototype/maskedtensor_adagrad\n.. _Advanced semantics - discussion on why certain decisions were made (e.g. requiring masks to match for binary/reduction operations), differences with NumPy's MaskedArray, and reduction semantics: https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics\n\nSupported Operators\n+++++++++++++++++++\n\nUnary Operators\n---------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":62,"to":71}}}}],["749",{"pageContent":"Supported Operators\n+++++++++++++++++++\n\nUnary Operators\n---------------\n\nUnary operators are operators that only contain only a single input.\nApplying them to MaskedTensors is relatively straightforward: if the data is masked out at a given index,\nwe apply the operator, otherwise we'll continue to mask out the data.\n\nThe available unary operators are:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":71,"to":85}}}}],["750",{"pageContent":"The available unary operators are:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    abs\n    absolute\n    acos\n    arccos\n    acosh\n    arccosh\n    angle\n    asin\n    arcsin\n    asinh\n    arcsinh\n    atan\n    arctan\n    atanh\n    arctanh\n    bitwise_not\n    ceil\n    clamp\n    clip\n    conj_physical\n    cos\n    cosh\n    deg2rad\n    digamma\n    erf\n    erfc\n    erfinv\n    exp\n    exp2\n    expm1\n    fix\n    floor\n    frac\n    lgamma\n    log\n    log10\n    log1p\n    log2\n    logit\n    i0\n    isnan\n    nan_to_num\n    neg\n    negative\n    positive\n    pow\n    rad2deg\n    reciprocal\n    round\n    rsqrt\n    sigmoid\n    sign\n    sgn\n    signbit\n    sin\n    sinc\n    sinh\n    sqrt\n    square\n    tan\n    tanh\n    trunc\n\nThe available inplace unary operators are all of the above **except**:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    angle\n    positive\n    signbit\n    isnan\n\nBinary Operators\n----------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":85,"to":166}}}}],["751",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    angle\n    positive\n    signbit\n    isnan\n\nBinary Operators\n----------------\n\nAs you may have seen in the tutorial, :class:`MaskedTensor` also has binary operations implemented with the caveat\nthat the masks in the two MaskedTensors must match or else an error will be raised. As noted in the error, if you\nneed support for a particular operator or have proposed semantics for how they should behave instead, please open\nan issue on GitHub. For now, we have decided to go with the most conservative implementation to ensure that users\nknow exactly what is going on and are being intentional about their decisions with masked semantics.\n\nThe available binary operators are:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":166,"to":188}}}}],["752",{"pageContent":"The available binary operators are:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    add\n    atan2\n    arctan2\n    bitwise_and\n    bitwise_or\n    bitwise_xor\n    bitwise_left_shift\n    bitwise_right_shift\n    div\n    divide\n    floor_divide\n    fmod\n    logaddexp\n    logaddexp2\n    mul\n    multiply\n    nextafter\n    remainder\n    sub\n    subtract\n    true_divide\n    eq\n    ne\n    le\n    ge\n    greater\n    greater_equal\n    gt\n    less_equal\n    lt\n    less\n    maximum\n    minimum\n    fmax\n    fmin\n    not_equal\n\nThe available inplace binary operators are all of the above **except**:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    logaddexp\n    logaddexp2\n    equal\n    fmin\n    minimum\n    fmax\n\nReductions\n----------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":188,"to":245}}}}],["753",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    logaddexp\n    logaddexp2\n    equal\n    fmin\n    minimum\n    fmax\n\nReductions\n----------\n\nThe following reductions are available (with autograd support). For more information, the\n`Overview <https://pytorch.org/tutorials/prototype/maskedtensor_overview.html>`_ tutorial\ndetails some examples of reductions, while the\n`Advanced semantics <https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics.html>`_ tutorial\nhas some further in-depth discussions about how we decided on certain reduction semantics.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    sum\n    mean\n    amin\n    amax\n    argmin\n    argmax\n    prod\n    all\n    norm\n    var\n    std\n\nView and select functions\n-------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":245,"to":282}}}}],["754",{"pageContent":"sum\n    mean\n    amin\n    amax\n    argmin\n    argmax\n    prod\n    all\n    norm\n    var\n    std\n\nView and select functions\n-------------------------\n\nWe've included a number of view and select functions as well; intuitively, these operators will apply to\nboth the data and the mask and then wrap the result in a :class:`MaskedTensor`. For a quick example,\nconsider :func:`select`:\n\n    >>> data = torch.arange(12, dtype=torch.float).reshape(3, 4)\n    >>> data\n    tensor([[ 0.,  1.,  2.,  3.],\n            [ 4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11.]])\n    >>> mask = torch.tensor([[True, False, False, True], [False, True, False, False], [True, True, True, True]])\n    >>> mt = masked_tensor(data, mask)\n    >>> data.select(0, 1)\n    tensor([4., 5., 6., 7.])\n    >>> mask.select(0, 1)\n    tensor([False,  True, False, False])\n    >>> mt.select(0, 1)\n    MaskedTensor(\n      [      --,   5.0000,       --,       --]\n    )\n\nThe following ops are currently supported:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":282,"to":317}}}}],["755",{"pageContent":"The following ops are currently supported:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    atleast_1d\n    broadcast_tensors\n    broadcast_to\n    cat\n    chunk\n    column_stack\n    dsplit\n    flatten\n    hsplit\n    hstack\n    kron\n    meshgrid\n    narrow\n    ravel\n    select\n    split\n    t\n    transpose\n    vsplit\n    vstack\n    Tensor.expand\n    Tensor.expand_as\n    Tensor.reshape\n    Tensor.reshape_as\n    Tensor.view","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/masked.rst","loc":{"lines":{"from":317,"to":347}}}}],["756",{"pageContent":"torch.utils.mobile_optimizer\n===================================\n\n.. warning::\n    This API is in beta and may change in the near future.\n\nTorch mobile supports ``torch.utils.mobile_optimizer.optimize_for_mobile`` utility to run a list of optimization pass with modules in eval mode.\nThe method takes the following parameters: a torch.jit.ScriptModule object, a blocklisting optimization set, a preserved method list, and a backend.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/mobile_optimizer.rst","loc":{"lines":{"from":1,"to":8}}}}],["757",{"pageContent":"For CPU Backend, by default, if optimization blocklist is None or empty, ``optimize_for_mobile`` will run the following optimizations:\n    - **Conv2D + BatchNorm fusion** (blocklisting option `mobile_optimizer.MobileOptimizerType.CONV_BN_FUSION`):  This optimization pass folds ``Conv2d-BatchNorm2d`` into ``Conv2d`` in ``forward`` method of this module and all its submodules. The weight and bias of the ``Conv2d`` are correspondingly updated.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/mobile_optimizer.rst","loc":{"lines":{"from":10,"to":11}}}}],["758",{"pageContent":"- **Insert and Fold prepacked ops** (blocklisting option `mobile_optimizer.MobileOptimizerType.INSERT_FOLD_PREPACK_OPS`): This optimization pass rewrites the graph to replace 2D convolutions and linear ops with their prepacked counterparts. Prepacked ops are stateful ops in that, they require some state to be created, such as weight prepacking and use this state, i.e. prepacked weights, during op execution. XNNPACK is one such backend that provides prepacked ops, with kernels optimized for mobile platforms (such as ARM CPUs). Prepacking of weight enables efficient memory access and thus faster kernel execution. At the moment ``optimize_for_mobile`` pass rewrites the graph to replace ``Conv2D/Linear`` with 1) op that pre-packs weight for XNNPACK conv2d/linear ops and 2) op that takes pre-packed weight and activation as input and generates output activations. Since 1 needs to be done only once, we fold the weight pre-packing such that it is done only once at model load time. This pass of the ``optimize_for_mobile`` does 1 and 2 and then folds, i.e. removes, weight pre-packing ops.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/mobile_optimizer.rst","loc":{"lines":{"from":12,"to":12}}}}],["759",{"pageContent":"- **ReLU/Hardtanh fusion**: XNNPACK ops support fusion of clamping. That is clamping of output activation is done as part of the kernel, including for 2D convolution and linear op kernels. Thus clamping effectively comes for free. Thus any op that can be expressed as clamping op, such as ``ReLU`` or ``hardtanh``, can be fused with previous ``Conv2D`` or ``linear`` op in XNNPACK. This pass rewrites graph by finding ``ReLU/hardtanh`` ops that follow XNNPACK ``Conv2D/linear`` ops, written by the previous pass, and fuses them together.\n    - **Dropout removal** (blocklisting option `mobile_optimizer.MobileOptimizerType.REMOVE_DROPOUT`): This optimization pass removes ``dropout`` and ``dropout_`` nodes from this module when training is false.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/mobile_optimizer.rst","loc":{"lines":{"from":13,"to":14}}}}],["760",{"pageContent":"- **Conv packed params hoisting** (blocklisting option `mobile_optimizer.MobileOptimizerType.HOIST_CONV_PACKED_PARAMS`): This optimization pass moves convolution packed params to the root module, so that the convolution structs can be deleted. This decreases model size without impacting numerics.\n    - **Add/ReLU fusion** (blocklisting option `mobile_optimizer.MobileOptimizerType.FUSE_ADD_RELU`): This pass finds instances of ``relu`` ops that follow ``add`` ops and fuses them into a single ``add_relu``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/mobile_optimizer.rst","loc":{"lines":{"from":15,"to":16}}}}],["761",{"pageContent":"for Vulkan Backend, by default, if optimization blocklist is None or empty, ``optimize_for_mobile`` will run the following optimization:\n    - **Automatic GPU Transfer** (blocklisting option `mobile_optimizer.MobileOptimizerType.VULKAN_AUTOMATIC_GPU_TRANSFER`): This optimization pass rewrites the graph so that moving input and output data to and from the GPU becomes part of the model.\n\n``optimize_for_mobile`` will also invoke freeze_module pass which only preserves ``forward`` method. If you have other method to that needed to be preserved, add them into the preserved method list and pass into the method.\n\n\n.. currentmodule:: torch.utils.mobile_optimizer\n.. autofunction:: optimize_for_mobile","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/mobile_optimizer.rst","loc":{"lines":{"from":18,"to":25}}}}],["762",{"pageContent":"torch.utils.model_zoo\n===================================\n\nMoved to `torch.hub`.\n\n.. automodule:: torch.utils.model_zoo\n.. autofunction:: load_url","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/model_zoo.rst","loc":{"lines":{"from":1,"to":7}}}}],["763",{"pageContent":"torch.monitor\n=============\n\n.. warning::\n\n    This module is a prototype release, and its interfaces and functionality may\n    change without warning in future PyTorch releases.\n\n``torch.monitor`` provides an interface for logging events and counters from\nPyTorch.\n\nThe stat interfaces are designed to be used for tracking high level metrics that\nare periodically logged out to be used for monitoring system performance. Since\nthe stats aggregate with a specific window size you can log to them from\ncritical loops with minimal performance impact.\n\nFor more infrequent events or values such as loss, accuracy, usage tracking the\nevent interface can be directly used.\n\nEvent handlers can be registered to handle the events and pass them to an\nexternal event sink.\n\nAPI Reference\n-------------\n\n.. automodule:: torch.monitor\n\n.. autoclass:: torch.monitor.Aggregation\n    :members:\n\n.. autoclass:: torch.monitor.Stat\n    :members:\n    :special-members: __init__\n\n.. autoclass:: torch.monitor.data_value_t\n    :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/monitor.rst","loc":{"lines":{"from":1,"to":36}}}}],["764",{"pageContent":".. autoclass:: torch.monitor.Aggregation\n    :members:\n\n.. autoclass:: torch.monitor.Stat\n    :members:\n    :special-members: __init__\n\n.. autoclass:: torch.monitor.data_value_t\n    :members:\n\n.. autoclass:: torch.monitor.Event\n    :members:\n    :special-members: __init__\n\n.. autoclass:: torch.monitor.EventHandlerHandle\n    :members:\n\n.. autofunction:: torch.monitor.log_event\n\n.. autofunction:: torch.monitor.register_event_handler\n\n.. autofunction:: torch.monitor.unregister_event_handler\n\n.. autoclass:: torch.monitor.TensorboardEventHandler\n    :members:\n    :special-members: __init__","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/monitor.rst","loc":{"lines":{"from":36,"to":61}}}}],["765",{"pageContent":"torch.mps\n===================================\n.. automodule:: torch.mps\n.. currentmodule:: torch.mps\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    synchronize\n    get_rng_state\n    set_rng_state\n    manual_seed\n    seed\n    empty_cache\n    set_per_process_memory_fraction\n    current_allocated_memory\n    driver_allocated_memory","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/mps.rst","loc":{"lines":{"from":1,"to":18}}}}],["766",{"pageContent":":orphan:\n\n.. _multiprocessing-doc:\n\nMultiprocessing package - torch.multiprocessing\n===============================================\n\n.. automodule:: torch.multiprocessing\n.. currentmodule:: torch.multiprocessing\n\n.. warning::\n\n    If the main process exits abruptly (e.g. because of an incoming signal),\n    Python's ``multiprocessing`` sometimes fails to clean up its children.\n    It's a known caveat, so if you're seeing any resource leaks after\n    interrupting the interpreter, it probably means that this has just happened\n    to you.\n\nStrategy management\n-------------------\n\n.. autofunction:: get_all_sharing_strategies\n.. autofunction:: get_sharing_strategy\n.. autofunction:: set_sharing_strategy\n\n\n.. _multiprocessing-cuda-sharing-details:\n\nSharing CUDA tensors\n--------------------\n\nSharing CUDA tensors between processes is supported only in Python 3, using\na ``spawn`` or ``forkserver`` start methods.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/multiprocessing.rst","loc":{"lines":{"from":1,"to":33}}}}],["767",{"pageContent":"Sharing CUDA tensors\n--------------------\n\nSharing CUDA tensors between processes is supported only in Python 3, using\na ``spawn`` or ``forkserver`` start methods.\n\n\nUnlike CPU tensors, the sending process is required to keep the original tensor\nas long as the receiving process retains a copy of the tensor. The refcounting is\nimplemented under the hood but requires users to follow the next best practices.\n\n.. warning::\n    If the consumer process dies abnormally to a fatal signal, the shared tensor\n    could be forever kept in memory as long as the sending process is running.\n\n\n1. Release memory ASAP in the consumer.\n\n::\n\n    ## Good\n    x = queue.get()\n    # do somethings with x\n    del x\n\n::\n\n    ## Bad\n    x = queue.get()\n    # do somethings with x\n    # do everything else (producer have to keep x in memory)\n\n2. Keep producer process running until all consumers exits. This will prevent\nthe situation when the producer process releasing memory which is still in use\nby the consumer.\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/multiprocessing.rst","loc":{"lines":{"from":33,"to":69}}}}],["768",{"pageContent":"2. Keep producer process running until all consumers exits. This will prevent\nthe situation when the producer process releasing memory which is still in use\nby the consumer.\n\n::\n\n    ## producer\n    # send tensors, do something\n    event.wait()\n\n\n::\n\n    ## consumer\n    # receive tensors and use them\n    event.set()\n\n3. Don't pass received tensors.\n\n::\n\n    # not going to work\n    x = queue.get()\n    queue_2.put(x)\n\n\n::\n\n    # you need to create a process-local copy\n    x = queue.get()\n    x_clone = x.clone()\n    queue_2.put(x_clone)\n\n\n::\n\n    # putting and getting from the same queue in the same process will likely end up with segfault\n    queue.put(tensor)\n    x = queue.get()\n\n\nSharing strategies\n------------------\n\nThis section provides a brief overview into how different sharing strategies\nwork. Note that it applies only to CPU tensor - CUDA tensors will always use\nthe CUDA API, as that's the only way they can be shared.\n\nFile descriptor - ``file_descriptor``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n.. note::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/multiprocessing.rst","loc":{"lines":{"from":69,"to":121}}}}],["769",{"pageContent":"File descriptor - ``file_descriptor``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n.. note::\n\n    This is the default strategy (except for macOS and OS X where it's not\n    supported).\n\nThis strategy will use file descriptors as shared memory handles. Whenever a\nstorage is moved to shared memory, a file descriptor obtained from ``shm_open``\nis cached with the object, and when it's going to be sent to other processes,\nthe file descriptor will be transferred (e.g. via UNIX sockets) to it. The\nreceiver will also cache the file descriptor and ``mmap`` it, to obtain a shared\nview onto the storage data.\n\nNote that if there will be a lot of tensors shared, this strategy will keep a\nlarge number of file descriptors open most of the time. If your system has low\nlimits for the number of open file descriptors, and you can't raise them, you\nshould use the ``file_system`` strategy.\n\nFile system - ``file_system``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/multiprocessing.rst","loc":{"lines":{"from":121,"to":143}}}}],["770",{"pageContent":"File system - ``file_system``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThis strategy will use file names given to ``shm_open`` to identify the shared\nmemory regions. This has a benefit of not requiring the implementation to cache\nthe file descriptors obtained from it, but at the same time is prone to shared\nmemory leaks. The file can't be deleted right after its creation, because other\nprocesses need to access it to open their views. If the processes fatally\ncrash, or are killed, and don't call the storage destructors, the files will\nremain in the system. This is very serious, because they keep using up the\nmemory until the system is restarted, or they're freed manually.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/multiprocessing.rst","loc":{"lines":{"from":143,"to":153}}}}],["771",{"pageContent":"To counter the problem of shared memory file leaks, :mod:`torch.multiprocessing`\nwill spawn a daemon named ``torch_shm_manager`` that will isolate itself from\nthe current process group, and will keep track of all shared memory allocations.\nOnce all processes connected to it exit, it will wait a moment to ensure there\nwill be no new connections, and will iterate over all shared memory files\nallocated by the group. If it finds that any of them still exist, they will be\ndeallocated. We've tested this method and it proved to be robust to various\nfailures. Still, if your system has high enough limits, and ``file_descriptor``\nis a supported strategy, we do not recommend switching to this one.\n\nSpawning subprocesses\n---------------------\n\n.. note::\n\n   Available for Python >= 3.4.\n\n   This depends on the ``spawn`` start method in Python's\n   ``multiprocessing`` package.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/multiprocessing.rst","loc":{"lines":{"from":155,"to":173}}}}],["772",{"pageContent":"Spawning subprocesses\n---------------------\n\n.. note::\n\n   Available for Python >= 3.4.\n\n   This depends on the ``spawn`` start method in Python's\n   ``multiprocessing`` package.\n\nSpawning a number of subprocesses to perform some function can be done\nby creating ``Process`` instances and calling ``join`` to wait for\ntheir completion. This approach works fine when dealing with a single\nsubprocess but presents potential issues when dealing with multiple\nprocesses.\n\nNamely, joining processes sequentially implies they will terminate\nsequentially. If they don't, and the first process does not terminate,\nthe process termination will go unnoticed. Also, there are no native\nfacilities for error propagation.\n\nThe ``spawn`` function below addresses these concerns and takes care\nof error propagation, out of order termination, and will actively\nterminate processes upon detecting an error in one of them.\n\n.. autofunction:: spawn\n\n.. class:: SpawnContext","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/multiprocessing.rst","loc":{"lines":{"from":173,"to":200}}}}],["773",{"pageContent":".. autofunction:: spawn\n\n.. class:: SpawnContext\n\n   Returned by :func:`~spawn` when called with ``join=False``.\n\n   .. automethod:: join","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/multiprocessing.rst","loc":{"lines":{"from":200,"to":206}}}}],["774",{"pageContent":".. currentmodule:: torch\n\n.. _name_inference_reference-doc:\n\nNamed Tensors operator coverage\n===============================\n\nPlease read :ref:`named_tensors-doc` first for an introduction to named tensors.\n\nThis document is a reference for *name inference*, a process that defines how\nnamed tensors:\n\n1. use names to provide additional automatic runtime correctness checks\n2. propagate names from input tensors to output tensors\n\nBelow is a list of all operations that are supported with named tensors\nand their associated name inference rules.\n\nIf you don't see an operation listed here, but it would help your use case, please\n`search if an issue has already been filed <https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22>`_ and if not, `file one <https://github.com/pytorch/pytorch/issues/new/choose>`_.\n\n.. warning::\n    The named tensor API is experimental and subject to change.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":1,"to":23}}}}],["775",{"pageContent":".. warning::\n    The named tensor API is experimental and subject to change.\n\n.. csv-table:: Supported Operations\n   :header: API, Name inference rule\n   :widths: 20, 20","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":23,"to":28}}}}],["776",{"pageContent":"\":meth:`Tensor.abs`, :func:`torch.abs`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.abs_`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.acos`, :func:`torch.acos`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.acos_`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.add`, :func:`torch.add`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.add_`,:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.addmm`, :func:`torch.addmm`\",:ref:`contracts_away_dims-doc`\n   :meth:`Tensor.addmm_`,:ref:`contracts_away_dims-doc`\n   \":meth:`Tensor.addmv`, :func:`torch.addmv`\",:ref:`contracts_away_dims-doc`\n   :meth:`Tensor.addmv_`,:ref:`contracts_away_dims-doc`\n   :meth:`Tensor.align_as`,See documentation\n   :meth:`Tensor.align_to`,See documentation\n   \":meth:`Tensor.all`, :func:`torch.all`\",None\n   \":meth:`Tensor.any`, :func:`torch.any`\",None\n   \":meth:`Tensor.asin`, :func:`torch.asin`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.asin_`,:ref:`keeps_input_names-doc`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":30,"to":45}}}}],["777",{"pageContent":"\":meth:`Tensor.any`, :func:`torch.any`\",None\n   \":meth:`Tensor.asin`, :func:`torch.asin`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.asin_`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.atan`, :func:`torch.atan`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.atan2`, :func:`torch.atan2`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.atan2_`,:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.atan_`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.bernoulli`, :func:`torch.bernoulli`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.bernoulli_`,None\n   :meth:`Tensor.bfloat16`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.bitwise_not`, :func:`torch.bitwise_not`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.bitwise_not_`,None\n   \":meth:`Tensor.bmm`, :func:`torch.bmm`\",:ref:`contracts_away_dims-doc`\n   :meth:`Tensor.bool`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.byte`,:ref:`keeps_input_names-doc`\n   :func:`torch.cat`,:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.cauchy_`,None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":45,"to":61}}}}],["778",{"pageContent":":meth:`Tensor.bool`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.byte`,:ref:`keeps_input_names-doc`\n   :func:`torch.cat`,:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.cauchy_`,None\n   \":meth:`Tensor.ceil`, :func:`torch.ceil`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.ceil_`,None\n   :meth:`Tensor.char`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.chunk`, :func:`torch.chunk`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.clamp`, :func:`torch.clamp`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.clamp_`,None\n   :meth:`Tensor.copy_`,:ref:`out_function_semantics-doc`\n   \":meth:`Tensor.cos`, :func:`torch.cos`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.cos_`,None\n   \":meth:`Tensor.cosh`, :func:`torch.cosh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.cosh_`,None\n   \":meth:`Tensor.acosh`, :func:`torch.acosh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.acosh_`,None\n   :meth:`Tensor.cpu`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.cuda`,:ref:`keeps_input_names-doc`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":61,"to":79}}}}],["779",{"pageContent":":meth:`Tensor.acosh_`,None\n   :meth:`Tensor.cpu`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.cuda`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.cumprod`, :func:`torch.cumprod`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.cumsum`, :func:`torch.cumsum`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.data_ptr`,None\n   \":meth:`Tensor.deg2rad`, :func:`torch.deg2rad`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.deg2rad_`,None\n   \":meth:`Tensor.detach`, :func:`torch.detach`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.detach_`,None\n   \":attr:`Tensor.device`, :func:`torch.device`\",None\n   \":meth:`Tensor.digamma`, :func:`torch.digamma`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.digamma_`,None\n   :meth:`Tensor.dim`,None\n   \":meth:`Tensor.div`, :func:`torch.div`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.div_`,:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.dot`, :func:`torch.dot`\",None\n   :meth:`Tensor.double`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.element_size`,None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":79,"to":97}}}}],["780",{"pageContent":":meth:`Tensor.div_`,:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.dot`, :func:`torch.dot`\",None\n   :meth:`Tensor.double`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.element_size`,None\n   :func:`torch.empty`,:ref:`factory-doc`\n   :func:`torch.empty_like`,:ref:`factory-doc`\n   \":meth:`Tensor.eq`, :func:`torch.eq`\",:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.erf`, :func:`torch.erf`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.erf_`,None\n   \":meth:`Tensor.erfc`, :func:`torch.erfc`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.erfc_`,None\n   \":meth:`Tensor.erfinv`, :func:`torch.erfinv`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.erfinv_`,None\n   \":meth:`Tensor.exp`, :func:`torch.exp`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.exp_`,None\n   :meth:`Tensor.expand`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.expm1`, :func:`torch.expm1`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.expm1_`,None\n   :meth:`Tensor.exponential_`,None\n   :meth:`Tensor.fill_`,None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":97,"to":116}}}}],["781",{"pageContent":"\":meth:`Tensor.expm1`, :func:`torch.expm1`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.expm1_`,None\n   :meth:`Tensor.exponential_`,None\n   :meth:`Tensor.fill_`,None\n   \":meth:`Tensor.flatten`, :func:`torch.flatten`\",See documentation\n   :meth:`Tensor.float`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.floor`, :func:`torch.floor`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.floor_`,None\n   \":meth:`Tensor.frac`, :func:`torch.frac`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.frac_`,None\n   \":meth:`Tensor.ge`, :func:`torch.ge`\",:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.get_device`, :func:`torch.get_device`\",None\n   :attr:`Tensor.grad`,None\n   \":meth:`Tensor.gt`, :func:`torch.gt`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.half`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.has_names`,See documentation\n   \":meth:`Tensor.index_fill`, :func:`torch.index_fill`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.index_fill_`,None\n   :meth:`Tensor.int`,:ref:`keeps_input_names-doc`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":116,"to":134}}}}],["782",{"pageContent":"\":meth:`Tensor.index_fill`, :func:`torch.index_fill`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.index_fill_`,None\n   :meth:`Tensor.int`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.is_contiguous`,None\n   :attr:`Tensor.is_cuda`,None\n   \":meth:`Tensor.is_floating_point`, :func:`torch.is_floating_point`\",None\n   :attr:`Tensor.is_leaf`,None\n   :meth:`Tensor.is_pinned`,None\n   :meth:`Tensor.is_shared`,None\n   \":meth:`Tensor.is_signed`, :func:`torch.is_signed`\",None\n   :attr:`Tensor.is_sparse`,None\n   :attr:`Tensor.is_sparse_csr`,None\n   :func:`torch.is_tensor`,None\n   :meth:`Tensor.item`,None\n   :attr:`Tensor.itemsize`,None\n   \":meth:`Tensor.kthvalue`, :func:`torch.kthvalue`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.le`, :func:`torch.le`\",:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.log`, :func:`torch.log`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.log10`, :func:`torch.log10`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.log10_`,None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":134,"to":153}}}}],["783",{"pageContent":"\":meth:`Tensor.log`, :func:`torch.log`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.log10`, :func:`torch.log10`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.log10_`,None\n   \":meth:`Tensor.log1p`, :func:`torch.log1p`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.log1p_`,None\n   \":meth:`Tensor.log2`, :func:`torch.log2`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.log2_`,None\n   :meth:`Tensor.log_`,None\n   :meth:`Tensor.log_normal_`,None\n   \":meth:`Tensor.logical_not`, :func:`torch.logical_not`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.logical_not_`,None\n   \":meth:`Tensor.logsumexp`, :func:`torch.logsumexp`\",:ref:`removes_dimensions-doc`\n   :meth:`Tensor.long`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.lt`, :func:`torch.lt`\",:ref:`unifies_names_from_inputs-doc`\n   :func:`torch.manual_seed`,None\n   \":meth:`Tensor.masked_fill`, :func:`torch.masked_fill`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.masked_fill_`,None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":153,"to":169}}}}],["784",{"pageContent":":func:`torch.manual_seed`,None\n   \":meth:`Tensor.masked_fill`, :func:`torch.masked_fill`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.masked_fill_`,None\n   \":meth:`Tensor.masked_select`, :func:`torch.masked_select`\",Aligns mask up to input and then unifies_names_from_input_tensors\n   \":meth:`Tensor.matmul`, :func:`torch.matmul`\",:ref:`contracts_away_dims-doc`\n   \":meth:`Tensor.mean`, :func:`torch.mean`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.median`, :func:`torch.median`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.nanmedian`, :func:`torch.nanmedian`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.mm`, :func:`torch.mm`\",:ref:`contracts_away_dims-doc`\n   \":meth:`Tensor.mode`, :func:`torch.mode`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.mul`, :func:`torch.mul`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.mul_`,:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.mv`, :func:`torch.mv`\",:ref:`contracts_away_dims-doc`\n   :attr:`Tensor.names`,See documentation","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":169,"to":182}}}}],["785",{"pageContent":":meth:`Tensor.mul_`,:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.mv`, :func:`torch.mv`\",:ref:`contracts_away_dims-doc`\n   :attr:`Tensor.names`,See documentation\n   \":meth:`Tensor.narrow`, :func:`torch.narrow`\",:ref:`keeps_input_names-doc`\n   :attr:`Tensor.nbytes`,None\n   :attr:`Tensor.ndim`,None\n   :meth:`Tensor.ndimension`,None\n   \":meth:`Tensor.ne`, :func:`torch.ne`\",:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.neg`, :func:`torch.neg`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.neg_`,None\n   :func:`torch.normal`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.normal_`,None\n   \":meth:`Tensor.numel`, :func:`torch.numel`\",None\n   :func:`torch.ones`,:ref:`factory-doc`\n   \":meth:`Tensor.pow`, :func:`torch.pow`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.pow_`,None\n   \":meth:`Tensor.prod`, :func:`torch.prod`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.rad2deg`, :func:`torch.rad2deg`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.rad2deg_`,None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":182,"to":200}}}}],["786",{"pageContent":"\":meth:`Tensor.prod`, :func:`torch.prod`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.rad2deg`, :func:`torch.rad2deg`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.rad2deg_`,None\n   :func:`torch.rand`,:ref:`factory-doc`\n   :func:`torch.rand`,:ref:`factory-doc`\n   :func:`torch.randn`,:ref:`factory-doc`\n   :func:`torch.randn`,:ref:`factory-doc`\n   :meth:`Tensor.random_`,None\n   \":meth:`Tensor.reciprocal`, :func:`torch.reciprocal`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.reciprocal_`,None\n   :meth:`Tensor.refine_names`,See documentation\n   :meth:`Tensor.register_hook`,None\n   :meth:`Tensor.rename`,See documentation\n   :meth:`Tensor.rename_`,See documentation\n   :attr:`Tensor.requires_grad`,None\n   :meth:`Tensor.requires_grad_`,None\n   :meth:`Tensor.resize_`,Only allow resizes that do not change shape\n   :meth:`Tensor.resize_as_`,Only allow resizes that do not change shape\n   \":meth:`Tensor.round`, :func:`torch.round`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.round_`,None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":200,"to":219}}}}],["787",{"pageContent":":meth:`Tensor.resize_as_`,Only allow resizes that do not change shape\n   \":meth:`Tensor.round`, :func:`torch.round`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.round_`,None\n   \":meth:`Tensor.rsqrt`, :func:`torch.rsqrt`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.rsqrt_`,None\n   \":meth:`Tensor.select`, :func:`torch.select`\",:ref:`removes_dimensions-doc`\n   :meth:`Tensor.short`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.sigmoid`, :func:`torch.sigmoid`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sigmoid_`,None\n   \":meth:`Tensor.sign`, :func:`torch.sign`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sign_`,None\n   \":meth:`Tensor.sgn`, :func:`torch.sgn`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sgn_`,None\n   \":meth:`Tensor.sin`, :func:`torch.sin`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sin_`,None\n   \":meth:`Tensor.sinh`, :func:`torch.sinh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sinh_`,None\n   \":meth:`Tensor.asinh`, :func:`torch.asinh`\",:ref:`keeps_input_names-doc`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":219,"to":236}}}}],["788",{"pageContent":"\":meth:`Tensor.sinh`, :func:`torch.sinh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sinh_`,None\n   \":meth:`Tensor.asinh`, :func:`torch.asinh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.asinh_`,None\n   :meth:`Tensor.size`,None\n   \":meth:`Tensor.softmax`, :func:`torch.softmax`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.split`, :func:`torch.split`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.sqrt`, :func:`torch.sqrt`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sqrt_`,None\n   \":meth:`Tensor.squeeze`, :func:`torch.squeeze`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.std`, :func:`torch.std`\",:ref:`removes_dimensions-doc`\n   :func:`torch.std_mean`,:ref:`removes_dimensions-doc`\n   :meth:`Tensor.stride`,None\n   \":meth:`Tensor.sub`, :func:`torch.sub`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.sub_`,:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.sum`, :func:`torch.sum`\",:ref:`removes_dimensions-doc`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":236,"to":251}}}}],["789",{"pageContent":":meth:`Tensor.sub_`,:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.sum`, :func:`torch.sum`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.tan`, :func:`torch.tan`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.tan_`,None\n   \":meth:`Tensor.tanh`, :func:`torch.tanh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.tanh_`,None\n   \":meth:`Tensor.atanh`, :func:`torch.atanh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.atanh_`,None\n   :func:`torch.tensor`,:ref:`factory-doc`\n   :meth:`Tensor.to`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.topk`, :func:`torch.topk`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.transpose`, :func:`torch.transpose`\",:ref:`permutes_dimensions-doc`\n   \":meth:`Tensor.trunc`, :func:`torch.trunc`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.trunc_`,None\n   :meth:`Tensor.type`,None\n   :meth:`Tensor.type_as`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.unbind`, :func:`torch.unbind`\",:ref:`removes_dimensions-doc`\n   :meth:`Tensor.unflatten`,See documentation","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":251,"to":268}}}}],["790",{"pageContent":":meth:`Tensor.type_as`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.unbind`, :func:`torch.unbind`\",:ref:`removes_dimensions-doc`\n   :meth:`Tensor.unflatten`,See documentation\n   :meth:`Tensor.uniform_`,None\n   \":meth:`Tensor.var`, :func:`torch.var`\",:ref:`removes_dimensions-doc`\n   :func:`torch.var_mean`,:ref:`removes_dimensions-doc`\n   :meth:`Tensor.zero_`,None\n   :func:`torch.zeros`,:ref:`factory-doc`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":268,"to":275}}}}],["791",{"pageContent":".. _keeps_input_names-doc:\n\nKeeps input names\n^^^^^^^^^^^^^^^^^\n\nAll pointwise unary functions follow this rule as well as some other unary functions.\n\n- Check names: None\n- Propagate names: input tensor's names are propagated to the output.\n\n::\n\n    >>> x = torch.randn(3, 3, names=('N', 'C'))\n    >>> x.abs().names\n    ('N', 'C')\n\n.. _removes_dimensions-doc:\n\nRemoves dimensions\n^^^^^^^^^^^^^^^^^^\n\nAll reduction ops like :meth:`~Tensor.sum` remove dimensions by reducing\nover the desired dimensions. Other operations like :meth:`~Tensor.select` and\n:meth:`~Tensor.squeeze` remove dimensions.\n\nWherever one can pass an integer dimension index to an operator, one can also pass\na dimension name. Functions that take lists of dimension indices can also take in a\nlist of dimension names.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":278,"to":305}}}}],["792",{"pageContent":"Wherever one can pass an integer dimension index to an operator, one can also pass\na dimension name. Functions that take lists of dimension indices can also take in a\nlist of dimension names.\n\n- Check names: If :attr:`dim` or :attr:`dims` is passed in as a list of names,\n  check that those names exist in :attr:`self`.\n- Propagate names: If the dimensions of the input tensor specified by :attr:`dim`\n  or :attr:`dims` are not present in the output tensor, then the corresponding names\n  of those dimensions do not appear in ``output.names``.\n\n::\n\n    >>> x = torch.randn(1, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n    >>> x.squeeze('N').names\n    ('C', 'H', 'W')\n\n    >>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n    >>> x.sum(['N', 'C']).names\n    ('H', 'W')\n\n    # Reduction ops with keepdim=True don't actually remove dimensions.\n    >>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n    >>> x.sum(['N', 'C'], keepdim=True).names\n    ('N', 'C', 'H', 'W')","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":305,"to":328}}}}],["793",{"pageContent":".. _unifies_names_from_inputs-doc:\n\nUnifies names from inputs\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAll binary arithmetic ops follow this rule. Operations that broadcast still\nbroadcast positionally from the right to preserve compatibility with unnamed\ntensors. To perform explicit broadcasting by names, use :meth:`Tensor.align_as`.\n\n- Check names: All names must match positionally from the right. i.e., in\n  ``tensor + other``, ``match(tensor.names[i], other.names[i])`` must be true for all\n  ``i`` in ``(-min(tensor.dim(), other.dim()) + 1, -1]``.\n- Check names: Furthermore, all named dimensions must be aligned from the right.\n  During matching, if we match a named dimension ``A`` with an unnamed dimension\n  ``None``, then ``A`` must not appear in the tensor with the unnamed dimension.\n- Propagate names: unify pairs of names from the right from both tensors to\n  produce output names.\n\nFor example,\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":331,"to":351}}}}],["794",{"pageContent":"For example,\n\n::\n\n    # tensor: Tensor[   N, None]\n    # other:  Tensor[None,    C]\n    >>> tensor = torch.randn(3, 3, names=('N', None))\n    >>> other = torch.randn(3, 3, names=(None, 'C'))\n    >>> (tensor + other).names\n    ('N', 'C')\n\nCheck names:\n\n- ``match(tensor.names[-1], other.names[-1])`` is ``True``\n- ``match(tensor.names[-2], tensor.names[-2])`` is ``True``\n- Because we matched ``None`` in :attr:`tensor` with ``'C'``,\n  check to make sure ``'C'`` doesn't exist in :attr:`tensor` (it does not).\n- Check to make sure ``'N'`` doesn't exists in :attr:`other` (it does not).\n\nFinally, the output names are computed with\n``[unify('N', None), unify(None, 'C')] = ['N', 'C']``\n\nMore examples::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":351,"to":373}}}}],["795",{"pageContent":"Finally, the output names are computed with\n``[unify('N', None), unify(None, 'C')] = ['N', 'C']``\n\nMore examples::\n\n    # Dimensions don't match from the right:\n    # tensor: Tensor[N, C]\n    # other:  Tensor[   N]\n    >>> tensor = torch.randn(3, 3, names=('N', 'C'))\n    >>> other = torch.randn(3, names=('N',))\n    >>> (tensor + other).names\n    RuntimeError: Error when attempting to broadcast dims ['N', 'C'] and dims\n    ['N']: dim 'C' and dim 'N' are at the same position from the right but do\n    not match.\n\n    # Dimensions aren't aligned when matching tensor.names[-1] and other.names[-1]:\n    # tensor: Tensor[N, None]\n    # other:  Tensor[      N]\n    >>> tensor = torch.randn(3, 3, names=('N', None))\n    >>> other = torch.randn(3, names=('N',))\n    >>> (tensor + other).names\n    RuntimeError: Misaligned dims when attempting to broadcast dims ['N'] and\n    dims ['N', None]: dim 'N' appears in a different position from the right\n    across both lists.\n\n.. note::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":373,"to":398}}}}],["796",{"pageContent":".. note::\n\n    In both of the last examples, it is possible to align the tensors by names\n    and then perform the addition. Use :meth:`Tensor.align_as` to align\n    tensors by name or :meth:`Tensor.align_to` to align tensors to a custom\n    dimension ordering.\n\n.. _permutes_dimensions-doc:\n\nPermutes dimensions\n^^^^^^^^^^^^^^^^^^^\n\nSome operations, like :meth:`Tensor.t()`, permute the order of dimensions. Dimension names\nare attached to individual dimensions so they get permuted as well.\n\nIf the operator takes in positional index :attr:`dim`, it is also able to take a dimension\nname as :attr:`dim`.\n\n- Check names: If :attr:`dim` is passed as a name, check that it exists in the tensor.\n- Propagate names: Permute dimension names in the same way as the dimensions that are\n  being permuted.\n\n::\n\n    >>> x = torch.randn(3, 3, names=('N', 'C'))\n    >>> x.transpose('N', 'C').names\n    ('C', 'N')\n\n.. _contracts_away_dims-doc:\n\nContracts away dims\n^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":398,"to":429}}}}],["797",{"pageContent":"::\n\n    >>> x = torch.randn(3, 3, names=('N', 'C'))\n    >>> x.transpose('N', 'C').names\n    ('C', 'N')\n\n.. _contracts_away_dims-doc:\n\nContracts away dims\n^^^^^^^^^^^^^^^^^^^\n\nMatrix multiply functions follow some variant of this. Let's go through\n:func:`torch.mm` first and then generalize the rule for batch matrix multiplication.\n\nFor ``torch.mm(tensor, other)``:\n\n- Check names: None\n- Propagate names: result names are ``(tensor.names[-2], other.names[-1])``.\n\n::\n\n    >>> x = torch.randn(3, 3, names=('N', 'D'))\n    >>> y = torch.randn(3, 3, names=('in', 'out'))\n    >>> x.mm(y).names\n    ('N', 'out')\n\nInherently, a matrix multiplication performs a dot product over two dimensions,\ncollapsing them. When two tensors are matrix-multiplied, the contracted dimensions\ndisappear and do not show up in the output tensor.\n\n:func:`torch.mv`, :func:`torch.dot` work in a similar way: name inference does not\ncheck input names and removes the dimensions that are involved in the dot product:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":429,"to":462}}}}],["798",{"pageContent":":func:`torch.mv`, :func:`torch.dot` work in a similar way: name inference does not\ncheck input names and removes the dimensions that are involved in the dot product:\n\n::\n\n    >>> x = torch.randn(3, 3, names=('N', 'D'))\n    >>> y = torch.randn(3, names=('something',))\n    >>> x.mv(y).names\n    ('N',)\n\nNow, let's take a look at ``torch.matmul(tensor, other)``. Assume that ``tensor.dim() >= 2``\nand ``other.dim() >= 2``.\n\n- Check names: Check that the batch dimensions of the inputs are aligned and broadcastable.\n  See :ref:`unifies_names_from_inputs-doc` for what it means for the inputs to be aligned.\n- Propagate names: result names are obtained by unifying the batch dimensions and removing\n  the contracted dimensions:\n  ``unify(tensor.names[:-2], other.names[:-2]) + (tensor.names[-2], other.names[-1])``.\n\nExamples::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":462,"to":481}}}}],["799",{"pageContent":"Examples::\n\n    # Batch matrix multiply of matrices Tensor['C', 'D'] and Tensor['E', 'F'].\n    # 'A', 'B' are batch dimensions.\n    >>> x = torch.randn(3, 3, 3, 3, names=('A', 'B', 'C', 'D'))\n    >>> y = torch.randn(3, 3, 3, names=('B', 'E', 'F'))\n    >>> torch.matmul(x, y).names\n    ('A', 'B', 'C', 'F')\n\n\nFinally, there are fused ``add`` versions of many matmul functions. i.e., :func:`addmm`\nand :func:`addmv`. These are treated as composing name inference for i.e. :func:`mm` and\nname inference for :func:`add`.\n\n.. _factory-doc:\n\nFactory functions\n^^^^^^^^^^^^^^^^^\n\n\nFactory functions now take a new :attr:`names` argument that associates a name\nwith each dimension.\n\n::\n\n    >>> torch.zeros(2, 3, names=('N', 'C'))\n    tensor([[0., 0., 0.],\n            [0., 0., 0.]], names=('N', 'C'))\n\n.. _out_function_semantics-doc:\n\nout function and in-place variants\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nA tensor specified as an ``out=`` tensor has the following behavior:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":481,"to":515}}}}],["800",{"pageContent":".. _out_function_semantics-doc:\n\nout function and in-place variants\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nA tensor specified as an ``out=`` tensor has the following behavior:\n\n- If it has no named dimensions, then the names computed from the operation\n  get propagated to it.\n- If it has any named dimensions, then the names computed from the operation\n  must be exactly equal to the existing names. Otherwise, the operation errors.\n\nAll in-place methods modify inputs to have names equal to the computed names\nfrom name inference. For example:\n\n::\n\n    >>> x = torch.randn(3, 3)\n    >>> y = torch.randn(3, 3, names=('N', 'C'))\n    >>> x.names\n    (None, None)\n\n    >>> x += y\n    >>> x.names\n    ('N', 'C')","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/name_inference.rst","loc":{"lines":{"from":515,"to":539}}}}],["801",{"pageContent":".. currentmodule:: torch\n\n.. _named_tensors-doc:\n\nNamed Tensors\n=============\n\nNamed Tensors allow users to give explicit names to tensor dimensions.\nIn most cases, operations that take dimension parameters will accept\ndimension names, avoiding the need to track dimensions by position.\nIn addition, named tensors use names to automatically check that APIs\nare being used correctly at runtime, providing extra safety. Names can\nalso be used to rearrange dimensions, for example, to support\n\"broadcasting by name\" rather than \"broadcasting by position\".\n\n\n.. warning::\n    The named tensor API is a prototype feature and subject to change.\n\nCreating named tensors\n----------------------\n\nFactory functions now take a new :attr:`names` argument that associates a name\nwith each dimension.\n\n::\n\n    >>> torch.zeros(2, 3, names=('N', 'C'))\n    tensor([[0., 0., 0.],\n            [0., 0., 0.]], names=('N', 'C'))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":1,"to":30}}}}],["802",{"pageContent":"::\n\n    >>> torch.zeros(2, 3, names=('N', 'C'))\n    tensor([[0., 0., 0.],\n            [0., 0., 0.]], names=('N', 'C'))\n\nNamed dimensions, like regular Tensor dimensions, are ordered.\n``tensor.names[i]`` is the name of dimension ``i`` of ``tensor``.\n\nThe following factory functions support named tensors:\n\n- :func:`torch.empty`\n- :func:`torch.rand`\n- :func:`torch.randn`\n- :func:`torch.ones`\n- :func:`torch.tensor`\n- :func:`torch.zeros`\n\nNamed dimensions\n----------------\n\nSee :attr:`~Tensor.names` for restrictions on tensor names.\n\nUse :attr:`~Tensor.names` to access the dimension names of a tensor and\n:meth:`~Tensor.rename` to rename named dimensions.\n\n::\n\n    >>> imgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W'))\n    >>> imgs.names\n    ('N', 'C', 'H', 'W')\n\n    >>> renamed_imgs = imgs.rename(H='height', W='width')\n    >>> renamed_imgs.names\n    ('N', 'C', 'height', 'width)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":30,"to":64}}}}],["803",{"pageContent":">>> renamed_imgs = imgs.rename(H='height', W='width')\n    >>> renamed_imgs.names\n    ('N', 'C', 'height', 'width)\n\n\nNamed tensors can coexist with unnamed tensors; named tensors are instances of\n:class:`torch.Tensor`. Unnamed tensors have ``None``-named dimensions. Named\ntensors do not require all dimensions to be named.\n\n::\n\n    >>> imgs = torch.randn(1, 2, 2, 3 , names=(None, 'C', 'H', 'W'))\n    >>> imgs.names\n    (None, 'C', 'H', 'W')\n\nName propagation semantics\n--------------------------\n\nNamed tensors use names to automatically check that APIs are being called\ncorrectly at runtime. This occurs in a process called *name inference*.\nMore formally, name inference consists of the following two steps:\n\n- **Check names**: an operator may perform automatic checks at runtime that\n  check that certain dimension names must match.\n- **Propagate names**: name inference propagates names to output tensors.\n\nAll operations that support named tensors propagate names.\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":64,"to":92}}}}],["804",{"pageContent":"All operations that support named tensors propagate names.\n\n::\n\n    >>> x = torch.randn(3, 3, names=('N', 'C'))\n    >>> x.abs().names\n    ('N', 'C')\n\n\n.. _match_semantics-doc:\n\nmatch semantics\n^^^^^^^^^^^^^^^\n\nTwo names *match* if they are equal (string equality) or if at least one is ``None``.\nNones are essentially a special \"wildcard\" name.\n\n``unify(A, B)`` determines which of the names ``A`` and ``B`` to propagate to the outputs.\nIt returns the more *specific* of the two names, if they match. If the names do not match,\nthen it errors.\n\n.. note::\n    In practice, when working with named tensors, one should avoid having unnamed\n    dimensions because their handling can be complicated. It is recommended to lift\n    all unnamed dimensions to be named dimensions by using :meth:`~Tensor.refine_names`.\n\n\nBasic name inference rules\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nLet's see how ``match`` and ``unify`` are used in name inference in the case of\nadding two one-dim tensors with no broadcasting.\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":92,"to":125}}}}],["805",{"pageContent":"Basic name inference rules\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nLet's see how ``match`` and ``unify`` are used in name inference in the case of\nadding two one-dim tensors with no broadcasting.\n\n::\n\n    x = torch.randn(3, names=('X',))\n    y = torch.randn(3)\n    z = torch.randn(3, names=('Z',))\n\n**Check names**: check that the names of the two tensors *match*.\n\nFor the following examples:\n\n::\n\n    >>> # x + y  # match('X', None) is True\n    >>> # x + z  # match('X', 'Z') is False\n    >>> # x + x  # match('X', 'X') is True\n\n    >>> x + z\n    Error when attempting to broadcast dims ['X'] and dims ['Z']: dim 'X' and dim 'Z' are at the same position from the right but do not match.\n\n**Propagate names**: *unify* the names to select which one to propagate.\nIn the case of ``x + y``, ``unify('X', None) = 'X'`` because ``'X'`` is more\nspecific than ``None``.\n\n::\n\n    >>> (x + y).names\n    ('X',)\n    >>> (x + x).names\n    ('X',)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":125,"to":159}}}}],["806",{"pageContent":"::\n\n    >>> (x + y).names\n    ('X',)\n    >>> (x + x).names\n    ('X',)\n\nFor a comprehensive list of name inference rules, see :ref:`name_inference_reference-doc`.\nHere are two common operations that may be useful to go over:\n\n- Binary arithmetic ops: :ref:`unifies_names_from_inputs-doc`\n- Matrix multiplication ops: :ref:`contracts_away_dims-doc`\n\nExplicit alignment by names\n---------------------------\n\nUse :meth:`~Tensor.align_as` or :meth:`~Tensor.align_to` to align tensor dimensions\nby name to a specified ordering. This is useful for performing \"broadcasting by names\".\n\n::\n\n    # This function is agnostic to the dimension ordering of `input`,\n    # as long as it has a `C` dimension somewhere.\n    def scale_channels(input, scale):\n        scale = scale.refine_names('C')\n        return input * scale.align_as(input)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":159,"to":184}}}}],["807",{"pageContent":">>> num_channels = 3\n    >>> scale = torch.randn(num_channels, names=('C',))\n    >>> imgs = torch.rand(3, 3, 3, num_channels, names=('N', 'H', 'W', 'C'))\n    >>> more_imgs = torch.rand(3, num_channels, 3, 3, names=('N', 'C', 'H', 'W'))\n    >>> videos = torch.randn(3, num_channels, 3, 3, 3, names=('N', 'C', 'H', 'W', 'D')\n\n    >>> scale_channels(imgs, scale)\n    >>> scale_channels(more_imgs, scale)\n    >>> scale_channels(videos, scale)\n\nManipulating dimensions\n-----------------------\n\nUse :meth:`~Tensor.align_to` to permute large amounts of dimensions without\nmentioning all of them as in required by :meth:`~Tensor.permute`.\n\n::\n\n    >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n    >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n\n    # Move the F (dim 5) and E dimension (dim 4) to the front while keeping\n    # the rest in the same order\n    >>> tensor.permute(5, 4, 0, 1, 2, 3)\n    >>> named_tensor.align_to('F', 'E', ...)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":186,"to":210}}}}],["808",{"pageContent":"# Move the F (dim 5) and E dimension (dim 4) to the front while keeping\n    # the rest in the same order\n    >>> tensor.permute(5, 4, 0, 1, 2, 3)\n    >>> named_tensor.align_to('F', 'E', ...)\n\nUse :meth:`~Tensor.flatten` and :meth:`~Tensor.unflatten` to flatten and unflatten\ndimensions, respectively. These methods are more verbose than :meth:`~Tensor.view`\nand :meth:`~Tensor.reshape`, but have more semantic meaning to someone reading the code.\n\n::\n\n    >>> imgs = torch.randn(32, 3, 128, 128)\n    >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n\n    >>> flat_imgs = imgs.view(32, -1)\n    >>> named_flat_imgs = named_imgs.flatten(['C', 'H', 'W'], 'features')\n    >>> named_flat_imgs.names\n    ('N', 'features')\n\n    >>> unflattened_imgs = imgs.view(32, 3, 128, 128)\n    >>> unflattened_named_imgs = named_flat_imgs.unflatten(\n            'features', [('C', 3), ('H', 128), ('W', 128)])\n\n.. _named_tensors_autograd-doc:\n\nAutograd support\n----------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":210,"to":236}}}}],["809",{"pageContent":".. _named_tensors_autograd-doc:\n\nAutograd support\n----------------\n\nAutograd currently supports named tensors in a limited manner: autograd ignores\nnames on all tensors. Gradient computation is still correct but we lose the\nsafety that names give us.\n\n::\n\n    >>> x = torch.randn(3, names=('D',))\n    >>> weight = torch.randn(3, names=('D',), requires_grad=True)\n    >>> loss = (x - weight).abs()\n    >>> grad_loss = torch.randn(3)\n    >>> loss.backward(grad_loss)\n    >>> weight.grad  # Unnamed for now. Will be named in the future\n    tensor([-1.8107, -0.6357,  0.0783])\n\n    >>> weight.grad.zero_()\n    >>> grad_loss = grad_loss.refine_names('C')\n    >>> loss = (x - weight).abs()\n    # Ideally we'd check that the names of loss and grad_loss match but we don't yet.\n    >>> loss.backward(grad_loss)\n    >>> weight.grad\n    tensor([-1.8107, -0.6357,  0.0783])\n\nCurrently supported operations and subsystems\n---------------------------------------------\n\nOperators\n^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":236,"to":267}}}}],["810",{"pageContent":"Currently supported operations and subsystems\n---------------------------------------------\n\nOperators\n^^^^^^^^^\n\nSee :ref:`name_inference_reference-doc` for a full list of the supported torch and\ntensor operations. We do not yet support the following that is not covered by the link:\n\n- indexing, advanced indexing.\n\nFor ``torch.nn.functional`` operators, we support the following:\n\n- :func:`torch.nn.functional.relu`\n- :func:`torch.nn.functional.softmax`\n- :func:`torch.nn.functional.log_softmax`\n- :func:`torch.nn.functional.tanh`\n- :func:`torch.nn.functional.sigmoid`\n- :func:`torch.nn.functional.dropout`\n\nSubsystems\n^^^^^^^^^^\n\nAutograd is supported, see :ref:`named_tensors_autograd-doc`.\nBecause gradients are currently unnamed, optimizers may work but are untested.\n\nNN modules are currently unsupported. This can lead to the following when calling\nmodules with named tensor inputs:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":267,"to":294}}}}],["811",{"pageContent":"NN modules are currently unsupported. This can lead to the following when calling\nmodules with named tensor inputs:\n\n- NN module parameters are unnamed, so outputs may be partially named.\n- NN module forward passes have code that don't support named tensors and will\n  error out appropriately.\n\nWe also do not support the following subsystems, though some may work out\nof the box:\n\n- distributions\n- serialization (:func:`torch.load`, :func:`torch.save`)\n- multiprocessing\n- JIT\n- distributed\n- ONNX\n\nIf any of these would help your use case, please\n`search if an issue has already been filed <https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22>`_\nand if not, `file one <https://github.com/pytorch/pytorch/issues/new/choose>`_.\n\nNamed tensor API reference\n--------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":294,"to":316}}}}],["812",{"pageContent":"Named tensor API reference\n--------------------------\n\nIn this section please find the documentation for named tensor specific APIs.\nFor a comprehensive reference for how names are propagated through other PyTorch\noperators, see :ref:`name_inference_reference-doc`.\n\n.. class:: Tensor()\n   :noindex:\n\n   .. autoattribute:: names\n   .. automethod:: rename\n   .. automethod:: rename_\n   .. automethod:: refine_names\n\n   .. automethod:: align_as\n   .. automethod:: align_to\n\n   .. py:method:: flatten(dims, out_dim) -> Tensor\n      :noindex:\n\n      Flattens :attr:`dims` into a single dimension with name :attr:`out_dim`.\n\n      All of `dims` must be consecutive in order in the :attr:`self` tensor,\n      but not necessary contiguous in memory.\n\n      Examples::\n\n          >>> imgs = torch.randn(32, 3, 128, 128, names=('N', 'C', 'H', 'W'))\n          >>> flat_imgs = imgs.flatten(['C', 'H', 'W'], 'features')\n          >>> flat_imgs.names, flat_imgs.shape\n          (('N', 'features'), torch.Size([32, 49152]))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":316,"to":347}}}}],["813",{"pageContent":".. warning::\n          The named tensor API is experimental and subject to change.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/named_tensor.rst","loc":{"lines":{"from":349,"to":350}}}}],["814",{"pageContent":"torch.nested\n============\n\n.. automodule:: torch.nested\n\nIntroduction\n++++++++++++\n\n.. warning::\n\n  The PyTorch API of nested tensors is in prototype stage and will change in the near future.\n\nNestedTensor allows the user to pack a list of Tensors into a single, efficient datastructure.\n\nThe only constraint on the input Tensors is that their dimension must match.\n\nThis enables more efficient metadata representations and access to purpose built kernels.\n\nOne application of NestedTensors is to express sequential data in various domains.\nWhile the conventional approach is to pad variable length sequences, NestedTensor\nenables users to bypass padding. The API for calling operations on a nested tensor is no different\nfrom that of a regular ``torch.Tensor``, which should allow seamless integration with existing models,\nwith the main difference being :ref:`construction of the inputs <construction>`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":1,"to":23}}}}],["815",{"pageContent":"As this is a prototype feature, the :ref:`operations supported <supported operations>` are still\nlimited. However, we welcome issues, feature requests and contributions. More information on contributing can be found\n`in this Readme <https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/nested/README.md>`_.\n\n.. _construction:\n\nConstruction\n++++++++++++\n\nConstruction is straightforward and involves passing a list of Tensors to the ``torch.nested.nested_tensor``\nconstructor.\n\n>>> a, b = torch.arange(3), torch.arange(5) + 3\n>>> a\ntensor([0, 1, 2])\n>>> b\ntensor([3, 4, 5, 6, 7])\n>>> nt = torch.nested.nested_tensor([a, b])\n>>> nt\nnested_tensor([\n  tensor([0, 1, 2]),\n    tensor([3, 4, 5, 6, 7])\n    ])\n\nData type, device and whether gradients are required can be chosen via the usual keyword arguments.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":25,"to":49}}}}],["816",{"pageContent":"Data type, device and whether gradients are required can be chosen via the usual keyword arguments.\n\n>>> nt = torch.nested.nested_tensor([a, b], dtype=torch.float32, device=\"cuda\", requires_grad=True)\n>>> nt\nnested_tensor([\n  tensor([0., 1., 2.], device='cuda:0', requires_grad=True),\n  tensor([3., 4., 5., 6., 7.], device='cuda:0', requires_grad=True)\n], device='cuda:0', requires_grad=True)\n\nIn the vein of ``torch.as_tensor``, ``torch.nested.as_nested_tensor`` can be used to preserve autograd\nhistory from the tensors passed to the constructor. For more information, refer to the section on\n:ref:`constructor functions`.\n\nIn order to form a valid NestedTensor all the passed Tensors need to match in dimension, but none of the other attributes need to.\n\n>>> a = torch.randn(3, 50, 70) # image 1\n>>> b = torch.randn(3, 128, 64) # image 2\n>>> nt = torch.nested.nested_tensor([a, b], dtype=torch.float32)\n>>> nt.dim()\n4\n\nIf one of the dimensions doesn't match, the constructor throws an error.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":49,"to":70}}}}],["817",{"pageContent":"If one of the dimensions doesn't match, the constructor throws an error.\n\n>>> a = torch.randn(50, 128) # text 1\n>>> b = torch.randn(3, 128, 64) # image 2\n>>> nt = torch.nested.nested_tensor([a, b], dtype=torch.float32)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: All Tensors given to nested_tensor must have the same dimension. Found dimension 3 for Tensor at index 1 and dimension 2 for Tensor at index 0.\n\nNote that the passed Tensors are being copied into a contiguous piece of memory. The resulting\nNestedTensor allocates new memory to store them and does not keep a reference.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":70,"to":80}}}}],["818",{"pageContent":"Note that the passed Tensors are being copied into a contiguous piece of memory. The resulting\nNestedTensor allocates new memory to store them and does not keep a reference.\n\nAt this moment we only support one level of nesting, i.e. a simple, flat list of Tensors. In the future\nwe can add support for multiple levels of nesting, such as a list that consists entirely of lists of Tensors.\nNote that for this extension it is important to maintain an even level of nesting across entries so that the resulting NestedTensor\nhas a well defined dimension. If you have a need for this feature, please feel encouraged to open a feature request so that\nwe can track it and plan accordingly.\n\nsize\n+++++++++++++++++++++++++\n\nEven though a NestedTensor does not support ``.size()`` (or ``.shape``), it supports ``.size(i)`` if dimension i is regular.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":80,"to":92}}}}],["819",{"pageContent":"size\n+++++++++++++++++++++++++\n\nEven though a NestedTensor does not support ``.size()`` (or ``.shape``), it supports ``.size(i)`` if dimension i is regular.\n\n>>> a = torch.randn(50, 128) # text 1\n>>> b = torch.randn(32, 128) # text 2\n>>> nt = torch.nested.nested_tensor([a, b], dtype=torch.float32)\n>>> nt.size(0)\n2\n>>> nt.size(1)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: Given dimension 1 is irregular and does not have a size.\n>>> nt.size(2)\n128\n\nIf all dimensions are regular, the NestedTensor is intended to be semantically indistinguishable from a regular ``torch.Tensor``.\n\n>>> a = torch.randn(20, 128) # text 1\n>>> nt = torch.nested.nested_tensor([a, a], dtype=torch.float32)\n>>> nt.size(0)\n2\n>>> nt.size(1)\n20\n>>> nt.size(2)\n128\n>>> torch.stack(nt.unbind()).size()\ntorch.Size([2, 20, 128])\n>>> torch.stack([a, a]).size()\ntorch.Size([2, 20, 128])\n>>> torch.equal(torch.stack(nt.unbind()), torch.stack([a, a]))\nTrue","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":92,"to":124}}}}],["820",{"pageContent":"In the future we might make it easier to detect this condition and convert seamlessly.\n\nPlease open a feature request if you have a need for this (or any other related feature for that matter).\n\nunbind\n+++++++++++++++++++++++++\n\n``unbind`` allows you to retrieve a view of the constituents.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":126,"to":133}}}}],["821",{"pageContent":">>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(3, 4)\n>>> nt = torch.nested.nested_tensor([a, b], dtype=torch.float32)\n>>> nt\nnested_tensor([\n  tensor([[ 1.2286, -1.2343, -1.4842],\n          [-0.7827,  0.6745,  0.0658]]),\n  tensor([[-1.1247, -0.4078, -1.0633,  0.8083],\n          [-0.2871, -0.2980,  0.5559,  1.9885],\n          [ 0.4074,  2.4855,  0.0733,  0.8285]])\n])\n>>> nt.unbind()\n(tensor([[ 1.2286, -1.2343, -1.4842],\n        [-0.7827,  0.6745,  0.0658]]), tensor([[-1.1247, -0.4078, -1.0633,  0.8083],\n        [-0.2871, -0.2980,  0.5559,  1.9885],\n        [ 0.4074,  2.4855,  0.0733,  0.8285]]))\n>>> nt.unbind()[0] is not a\nTrue\n>>> nt.unbind()[0].mul_(3)\ntensor([[ 3.6858, -3.7030, -4.4525],\n        [-2.3481,  2.0236,  0.1975]])\n>>> nt\nnested_tensor([\n  tensor([[ 3.6858, -3.7030, -4.4525],\n          [-2.3481,  2.0236,  0.1975]]),\n  tensor([[-1.1247, -0.4078, -1.0633,  0.8083],\n          [-0.2871, -0.2980,  0.5559,  1.9885],\n          [ 0.4074,  2.4855,  0.0733,  0.8285]])\n])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":135,"to":164}}}}],["822",{"pageContent":"Note that ``nt.unbind()[0]`` is not a copy, but rather a slice of the underlying memory, which represents the first entry or constituent of the NestedTensor.\n\n.. _constructor functions:\n\nNested tensor constructor and conversion functions\n++++++++++++++++++++++++++++++++++++++++++++++++++\n\nThe following functions are related to nested tensors:\n\n.. currentmodule:: torch.nested\n\n.. autofunction:: nested_tensor\n.. autofunction:: as_nested_tensor\n.. autofunction:: to_padded_tensor\n\n.. _supported operations:\n\nSupported operations\n++++++++++++++++++++++++++\n\nIn this section, we summarize the operations that are currently supported on\nNestedTensor and any constraints they have.\n\n.. csv-table::\n   :header: \"PyTorch operation\",  \"Constraints\"\n   :widths: 30, 55\n   :delim: ;","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":166,"to":192}}}}],["823",{"pageContent":":func:`torch.matmul`;  \"Supports matrix multiplication between two (>= 3d) nested tensors where\n   the last two dimensions are matrix dimensions and the leading (batch) dimensions have the same size\n   (i.e. no broadcasting support for batch dimensions yet).\"\n   :func:`torch.bmm`; \"Supports batch matrix multiplication of two 3-d nested tensors.\"\n   :func:`torch.nn.Linear`;  \"Supports 3-d nested input and a dense 2-d weight matrix.\"\n   :func:`torch.nn.functional.softmax`; \"Supports softmax along all dims except dim=0.\"\n   :func:`torch.nn.Dropout`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.Tensor.masked_fill`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.relu`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.gelu`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.silu`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.abs`; \"Behavior is the same as on regular tensors.\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":194,"to":205}}}}],["824",{"pageContent":":func:`torch.silu`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.abs`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.sgn`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.logical_not`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.neg`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.sub`; \"Supports elementwise subtraction of two nested tensors.\"\n   :func:`torch.add`; \"Supports elementwise addition of two nested tensors. Supports addition of a scalar to a nested tensor.\"\n   :func:`torch.mul`; \"Supports elementwise multiplication of two nested tensors. Supports multiplication of a nested tensor by a scalar.\"\n   :func:`torch.select`; \"Supports selecting along all dimensions.\"\n   :func:`torch.clone`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.detach`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.unbind`; \"Supports unbinding along ``dim=0`` only.\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":205,"to":216}}}}],["825",{"pageContent":":func:`torch.detach`; \"Behavior is the same as on regular tensors.\"\n   :func:`torch.unbind`; \"Supports unbinding along ``dim=0`` only.\"\n   :func:`torch.reshape`; \"Supports reshaping with size of ``dim=0`` preserved (i.e. number of tensors nested cannot be changed).\n   Unlike regular tensors, a size of ``-1`` here means that the existing size is inherited.\n   In particular, the only valid size for a irregular dimension is ``-1``.\n   Size inference is not implemented yet and hence for new dimensions the size cannot be ``-1``.\"\n   :func:`torch.Tensor.reshape_as`; \"Similar constraint as for ``reshape``.\"\n   :func:`torch.transpose`; \"Supports transposing of all dims except ``dim=0``.\"\n   :func:`torch.Tensor.view`; \"Rules for the new shape are similar to that of ``reshape``.\"\n   :func:`torch.empty_like`; \"Behavior is analogous to that of regular tensors; returns a new empty nested tensor (i.e. with uninitialized values) matching the nested structure of the input.\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":216,"to":225}}}}],["826",{"pageContent":":func:`torch.empty_like`; \"Behavior is analogous to that of regular tensors; returns a new empty nested tensor (i.e. with uninitialized values) matching the nested structure of the input.\"\n   :func:`torch.randn_like`; \"Behavior is analogous to that of regular tensors; returns a new nested tensor with values randomly initialized according to a standard normal distribution matching the nested structure of the input.\"\n   :func:`torch.zeros_like`; \"Behavior is analogous to that of regular tensors; returns a new nested tensor with all zero values matching the nested structure of the input.\"\n   :func:`torch.nn.LayerNorm`; \"The ``normalized_shape`` argument is restricted to not extend into the irregular dimensions of the NestedTensor.\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nested.rst","loc":{"lines":{"from":225,"to":228}}}}],["827",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\ntorch.nn.functional\n===================\n\n.. currentmodule:: torch.nn.functional\n\nConvolution functions\n----------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    conv1d\n    conv2d\n    conv3d\n    conv_transpose1d\n    conv_transpose2d\n    conv_transpose3d\n    unfold\n    fold\n\nPooling functions\n----------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    avg_pool1d\n    avg_pool2d\n    avg_pool3d\n    max_pool1d\n    max_pool2d\n    max_pool3d\n    max_unpool1d\n    max_unpool2d\n    max_unpool3d\n    lp_pool1d\n    lp_pool2d\n    adaptive_max_pool1d\n    adaptive_max_pool2d\n    adaptive_max_pool3d\n    adaptive_avg_pool1d\n    adaptive_avg_pool2d\n    adaptive_avg_pool3d\n    fractional_max_pool2d\n    fractional_max_pool3d\n\nAttention Mechanisms\n-------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    scaled_dot_product_attention","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.functional.rst","loc":{"lines":{"from":1,"to":59}}}}],["828",{"pageContent":"Attention Mechanisms\n-------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    scaled_dot_product_attention\n\nNon-linear activation functions\n-------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    threshold\n    threshold_\n    relu\n    relu_\n    hardtanh\n    hardtanh_\n    hardswish\n    relu6\n    elu\n    elu_\n    selu\n    celu\n    leaky_relu\n    leaky_relu_\n    prelu\n    rrelu\n    rrelu_\n    glu\n    gelu\n    logsigmoid\n    hardshrink\n    tanhshrink\n    softsign\n    softplus\n    softmin\n    softmax\n    softshrink\n    gumbel_softmax\n    log_softmax\n    tanh\n    sigmoid\n    hardsigmoid\n    silu\n    mish\n    batch_norm\n    group_norm\n    instance_norm\n    layer_norm\n    local_response_norm\n    normalize\n\n.. _Link 1: https://arxiv.org/abs/1611.00712\n.. _Link 2: https://arxiv.org/abs/1611.01144\n\nLinear functions\n----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    linear\n    bilinear","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.functional.rst","loc":{"lines":{"from":59,"to":127}}}}],["829",{"pageContent":"Linear functions\n----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    linear\n    bilinear\n\nDropout functions\n-----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    dropout\n    alpha_dropout\n    feature_alpha_dropout\n    dropout1d\n    dropout2d\n    dropout3d\n\nSparse functions\n----------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    embedding\n    embedding_bag\n    one_hot\n\nDistance functions\n----------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    pairwise_distance\n    cosine_similarity\n    pdist\n\n\nLoss functions\n--------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.functional.rst","loc":{"lines":{"from":127,"to":179}}}}],["830",{"pageContent":"pairwise_distance\n    cosine_similarity\n    pdist\n\n\nLoss functions\n--------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    binary_cross_entropy\n    binary_cross_entropy_with_logits\n    poisson_nll_loss\n    cosine_embedding_loss\n    cross_entropy\n    ctc_loss\n    gaussian_nll_loss\n    hinge_embedding_loss\n    kl_div\n    l1_loss\n    mse_loss\n    margin_ranking_loss\n    multilabel_margin_loss\n    multilabel_soft_margin_loss\n    multi_margin_loss\n    nll_loss\n    huber_loss\n    smooth_l1_loss\n    soft_margin_loss\n    triplet_margin_loss\n    triplet_margin_with_distance_loss\n\nVision functions\n----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    pixel_shuffle\n    pixel_unshuffle\n    pad\n    interpolate\n    upsample\n    upsample_nearest\n    upsample_bilinear\n    grid_sample\n    affine_grid\n\nDataParallel functions (multi-GPU, distributed)\n-----------------------------------------------\n\n:hidden:`data_parallel`\n~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.functional.rst","loc":{"lines":{"from":179,"to":234}}}}],["831",{"pageContent":"DataParallel functions (multi-GPU, distributed)\n-----------------------------------------------\n\n:hidden:`data_parallel`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    torch.nn.parallel.data_parallel","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.functional.rst","loc":{"lines":{"from":234,"to":244}}}}],["832",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\n.. _nn-init-doc:\n\ntorch.nn.init\n=============\n\n.. warning::\n    All the functions in this module are intended to be used to initialize neural network\n    parameters, so they all run in :func:`torch.no_grad` mode and will not be taken into\n    account by autograd.\n\n.. currentmodule:: torch.nn.init\n.. autofunction:: calculate_gain\n.. autofunction:: uniform_\n.. autofunction:: normal_\n.. autofunction:: constant_\n.. autofunction:: ones_\n.. autofunction:: zeros_\n.. autofunction:: eye_\n.. autofunction:: dirac_\n.. autofunction:: xavier_uniform_\n.. autofunction:: xavier_normal_\n.. autofunction:: kaiming_uniform_\n.. autofunction:: kaiming_normal_\n.. autofunction:: trunc_normal_\n.. autofunction:: orthogonal_\n.. autofunction:: sparse_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.init.rst","loc":{"lines":{"from":1,"to":29}}}}],["833",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\ntorch.nn\n===================================\n.. automodule:: torch.nn\n.. automodule:: torch.nn.modules\n\nThese are the basic building blocks for graphs:\n\n.. contents:: torch.nn\n    :depth: 2\n    :local:\n    :backlinks: top\n\n\n.. currentmodule:: torch.nn\n\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    ~parameter.Parameter\n    ~parameter.UninitializedParameter\n    ~parameter.UninitializedBuffer\n\nContainers\n----------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Module\n    Sequential\n    ModuleList\n    ModuleDict\n    ParameterList\n    ParameterDict\n\nGlobal Hooks For Module\n\n.. currentmodule:: torch.nn.modules.module\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":1,"to":49}}}}],["834",{"pageContent":"Global Hooks For Module\n\n.. currentmodule:: torch.nn.modules.module\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    register_module_forward_pre_hook\n    register_module_forward_hook\n    register_module_backward_hook\n    register_module_full_backward_pre_hook\n    register_module_full_backward_hook\n    register_module_buffer_registration_hook\n    register_module_module_registration_hook\n    register_module_parameter_registration_hook\n\n.. currentmodule:: torch\n\nConvolution Layers\n----------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.Conv1d\n    nn.Conv2d\n    nn.Conv3d\n    nn.ConvTranspose1d\n    nn.ConvTranspose2d\n    nn.ConvTranspose3d\n    nn.LazyConv1d\n    nn.LazyConv2d\n    nn.LazyConv3d\n    nn.LazyConvTranspose1d\n    nn.LazyConvTranspose2d\n    nn.LazyConvTranspose3d\n    nn.Unfold\n    nn.Fold\n\nPooling layers\n----------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":49,"to":91}}}}],["835",{"pageContent":"Pooling layers\n----------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.MaxPool1d\n    nn.MaxPool2d\n    nn.MaxPool3d\n    nn.MaxUnpool1d\n    nn.MaxUnpool2d\n    nn.MaxUnpool3d\n    nn.AvgPool1d\n    nn.AvgPool2d\n    nn.AvgPool3d\n    nn.FractionalMaxPool2d\n    nn.FractionalMaxPool3d\n    nn.LPPool1d\n    nn.LPPool2d\n    nn.AdaptiveMaxPool1d\n    nn.AdaptiveMaxPool2d\n    nn.AdaptiveMaxPool3d\n    nn.AdaptiveAvgPool1d\n    nn.AdaptiveAvgPool2d\n    nn.AdaptiveAvgPool3d\n\nPadding Layers\n--------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.ReflectionPad1d\n    nn.ReflectionPad2d\n    nn.ReflectionPad3d\n    nn.ReplicationPad1d\n    nn.ReplicationPad2d\n    nn.ReplicationPad3d\n    nn.ZeroPad1d\n    nn.ZeroPad2d\n    nn.ZeroPad3d\n    nn.ConstantPad1d\n    nn.ConstantPad2d\n    nn.ConstantPad3d","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":91,"to":138}}}}],["836",{"pageContent":"Non-linear Activations (weighted sum, nonlinearity)\n---------------------------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.ELU\n    nn.Hardshrink\n    nn.Hardsigmoid\n    nn.Hardtanh\n    nn.Hardswish\n    nn.LeakyReLU\n    nn.LogSigmoid\n    nn.MultiheadAttention\n    nn.PReLU\n    nn.ReLU\n    nn.ReLU6\n    nn.RReLU\n    nn.SELU\n    nn.CELU\n    nn.GELU\n    nn.Sigmoid\n    nn.SiLU\n    nn.Mish\n    nn.Softplus\n    nn.Softshrink\n    nn.Softsign\n    nn.Tanh\n    nn.Tanhshrink\n    nn.Threshold\n    nn.GLU\n\nNon-linear Activations (other)\n------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.Softmin\n    nn.Softmax\n    nn.Softmax2d\n    nn.LogSoftmax\n    nn.AdaptiveLogSoftmaxWithLoss\n\nNormalization Layers\n----------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":140,"to":194}}}}],["837",{"pageContent":"Normalization Layers\n----------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.BatchNorm1d\n    nn.BatchNorm2d\n    nn.BatchNorm3d\n    nn.LazyBatchNorm1d\n    nn.LazyBatchNorm2d\n    nn.LazyBatchNorm3d\n    nn.GroupNorm\n    nn.SyncBatchNorm\n    nn.InstanceNorm1d\n    nn.InstanceNorm2d\n    nn.InstanceNorm3d\n    nn.LazyInstanceNorm1d\n    nn.LazyInstanceNorm2d\n    nn.LazyInstanceNorm3d\n    nn.LayerNorm\n    nn.LocalResponseNorm\n\nRecurrent Layers\n----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.RNNBase\n    nn.RNN\n    nn.LSTM\n    nn.GRU\n    nn.RNNCell\n    nn.LSTMCell\n    nn.GRUCell\n\nTransformer Layers\n----------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.Transformer\n    nn.TransformerEncoder\n    nn.TransformerDecoder\n    nn.TransformerEncoderLayer\n    nn.TransformerDecoderLayer","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":194,"to":247}}}}],["838",{"pageContent":"nn.Transformer\n    nn.TransformerEncoder\n    nn.TransformerDecoder\n    nn.TransformerEncoderLayer\n    nn.TransformerDecoderLayer\n\nLinear Layers\n----------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.Identity\n    nn.Linear\n    nn.Bilinear\n    nn.LazyLinear\n\nDropout Layers\n--------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.Dropout\n    nn.Dropout1d\n    nn.Dropout2d\n    nn.Dropout3d\n    nn.AlphaDropout\n    nn.FeatureAlphaDropout\n\nSparse Layers\n-------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.Embedding\n    nn.EmbeddingBag\n\nDistance Functions\n------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.CosineSimilarity\n    nn.PairwiseDistance\n\nLoss Functions\n--------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":247,"to":304}}}}],["839",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.CosineSimilarity\n    nn.PairwiseDistance\n\nLoss Functions\n--------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.L1Loss\n    nn.MSELoss\n    nn.CrossEntropyLoss\n    nn.CTCLoss\n    nn.NLLLoss\n    nn.PoissonNLLLoss\n    nn.GaussianNLLLoss\n    nn.KLDivLoss\n    nn.BCELoss\n    nn.BCEWithLogitsLoss\n    nn.MarginRankingLoss\n    nn.HingeEmbeddingLoss\n    nn.MultiLabelMarginLoss\n    nn.HuberLoss\n    nn.SmoothL1Loss\n    nn.SoftMarginLoss\n    nn.MultiLabelSoftMarginLoss\n    nn.CosineEmbeddingLoss\n    nn.MultiMarginLoss\n    nn.TripletMarginLoss\n    nn.TripletMarginWithDistanceLoss\n\nVision Layers\n----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.PixelShuffle\n    nn.PixelUnshuffle\n    nn.Upsample\n    nn.UpsamplingNearest2d\n    nn.UpsamplingBilinear2d\n\nShuffle Layers\n----------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":304,"to":357}}}}],["840",{"pageContent":"nn.PixelShuffle\n    nn.PixelUnshuffle\n    nn.Upsample\n    nn.UpsamplingNearest2d\n    nn.UpsamplingBilinear2d\n\nShuffle Layers\n----------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.ChannelShuffle\n\nDataParallel Layers (multi-GPU, distributed)\n--------------------------------------------\n.. automodule:: torch.nn.parallel\n.. currentmodule:: torch\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.DataParallel\n    nn.parallel.DistributedDataParallel\n\nUtilities\n---------\n.. automodule:: torch.nn.utils\n\nFrom the ``torch.nn.utils`` module\n\n.. currentmodule:: torch.nn.utils\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    clip_grad_norm_\n    clip_grad_value_\n    parameters_to_vector\n    vector_to_parameters\n    prune.BasePruningMethod\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":357,"to":405}}}}],["841",{"pageContent":"clip_grad_norm_\n    clip_grad_value_\n    parameters_to_vector\n    vector_to_parameters\n    prune.BasePruningMethod\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    prune.PruningContainer\n    prune.Identity\n    prune.RandomUnstructured\n    prune.L1Unstructured\n    prune.RandomStructured\n    prune.LnStructured\n    prune.CustomFromMask\n    prune.identity\n    prune.random_unstructured\n    prune.l1_unstructured\n    prune.random_structured\n    prune.ln_structured\n    prune.global_unstructured\n    prune.custom_from_mask\n    prune.remove\n    prune.is_pruned\n    weight_norm\n    remove_weight_norm\n    spectral_norm\n    remove_spectral_norm\n    skip_init\n\nParametrizations implemented using the new parametrization functionality\nin :func:`torch.nn.utils.parameterize.register_parametrization`.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    parametrizations.orthogonal\n    parametrizations.spectral_norm","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":405,"to":445}}}}],["842",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    parametrizations.orthogonal\n    parametrizations.spectral_norm\n\nUtility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See the\n`Parametrizations tutorial <https://pytorch.org/tutorials/intermediate/parametrizations.html>`_\nfor more information on how to implement your own parametrizations.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    parametrize.register_parametrization\n    parametrize.remove_parametrizations\n    parametrize.cached\n    parametrize.is_parametrized\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    parametrize.ParametrizationList\n\nUtility functions to calls a given Module in a stateless manner.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":445,"to":476}}}}],["843",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    parametrize.ParametrizationList\n\nUtility functions to calls a given Module in a stateless manner.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    stateless.functional_call\n\nUtility functions in other modules\n\n.. currentmodule:: torch\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    nn.utils.rnn.PackedSequence\n    nn.utils.rnn.pack_padded_sequence\n    nn.utils.rnn.pad_packed_sequence\n    nn.utils.rnn.pad_sequence\n    nn.utils.rnn.pack_sequence\n    nn.utils.rnn.unpack_sequence\n    nn.utils.rnn.unpad_sequence\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.Flatten\n    nn.Unflatten\n\nQuantized Functions\n--------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":476,"to":515}}}}],["844",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.Flatten\n    nn.Unflatten\n\nQuantized Functions\n--------------------\n\nQuantization refers to techniques for performing computations and storing tensors at lower bitwidths than\nfloating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the :ref:`quantization-doc` documentation.\n\nLazy Modules Initialization\n---------------------------\n\n.. currentmodule:: torch\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    nn.modules.lazy.LazyModuleMixin\n\n\n.. This module is kept only for backward compatibility\n.. py:module:: torch.nn.backends\n.. py:module:: torch.nn.utils.stateless","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/nn.rst","loc":{"lines":{"from":515,"to":543}}}}],["845",{"pageContent":".. _amp-examples:\n\nCUDA Automatic Mixed Precision examples\n=======================================\n\n.. currentmodule:: torch.cuda.amp\n\nOrdinarily, \"automatic mixed precision training\" means training with\n:class:`torch.autocast` and :class:`torch.cuda.amp.GradScaler` together.\n\nInstances of :class:`torch.autocast` enable autocasting for chosen regions.\nAutocasting automatically chooses the precision for GPU operations to improve performance\nwhile maintaining accuracy.\n\nInstances of :class:`torch.cuda.amp.GradScaler` help perform the steps of\ngradient scaling conveniently.  Gradient scaling improves convergence for networks with ``float16``\ngradients by minimizing gradient underflow, as explained :ref:`here<gradient-scaling>`.\n\n:class:`torch.autocast` and :class:`torch.cuda.amp.GradScaler` are modular.\nIn the samples below, each is used as its individual documentation suggests.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":1,"to":20}}}}],["846",{"pageContent":":class:`torch.autocast` and :class:`torch.cuda.amp.GradScaler` are modular.\nIn the samples below, each is used as its individual documentation suggests.\n\n(Samples here are illustrative.  See the\n`Automatic Mixed Precision recipe <https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html>`_\nfor a runnable walkthrough.)\n\n.. contents:: :local:\n\nTypical Mixed Precision Training\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    # Creates model and optimizer in default precision\n    model = Net().cuda()\n    optimizer = optim.SGD(model.parameters(), ...)\n\n    # Creates a GradScaler once at the beginning of training.\n    scaler = GradScaler()\n\n    for epoch in epochs:\n        for input, target in data:\n            optimizer.zero_grad()\n\n            # Runs the forward pass with autocasting.\n            with autocast(device_type='cuda', dtype=torch.float16):\n                output = model(input)\n                loss = loss_fn(output, target)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":20,"to":48}}}}],["847",{"pageContent":"# Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n            # Backward passes under autocast are not recommended.\n            # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n            scaler.scale(loss).backward()\n\n            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n            # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n            # otherwise, optimizer.step() is skipped.\n            scaler.step(optimizer)\n\n            # Updates the scale for next iteration.\n            scaler.update()\n\n.. _working-with-unscaled-gradients:\n\nWorking with Unscaled Gradients\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":50,"to":66}}}}],["848",{"pageContent":"# Updates the scale for next iteration.\n            scaler.update()\n\n.. _working-with-unscaled-gradients:\n\nWorking with Unscaled Gradients\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAll gradients produced by ``scaler.scale(loss).backward()`` are scaled.  If you wish to modify or inspect\nthe parameters' ``.grad`` attributes between ``backward()`` and ``scaler.step(optimizer)``,  you should\nunscale them first.  For example, gradient clipping manipulates a set of gradients such that their global norm\n(see :func:`torch.nn.utils.clip_grad_norm_`) or maximum magnitude (see :func:`torch.nn.utils.clip_grad_value_`)\nis :math:`<=` some user-imposed threshold.  If you attempted to clip *without* unscaling, the gradients' norm/maximum\nmagnitude would also be scaled, so your requested threshold (which was meant to be the threshold for *unscaled*\ngradients) would be invalid.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":66,"to":80}}}}],["849",{"pageContent":"``scaler.unscale_(optimizer)`` unscales gradients held by ``optimizer``'s assigned parameters.\nIf your model or models contain other parameters that were assigned to another optimizer\n(say ``optimizer2``), you may call ``scaler.unscale_(optimizer2)`` separately to unscale those\nparameters' gradients as well.\n\nGradient clipping\n-----------------\n\nCalling ``scaler.unscale_(optimizer)`` before clipping enables you to clip unscaled gradients as usual::\n\n    scaler = GradScaler()\n\n    for epoch in epochs:\n        for input, target in data:\n            optimizer.zero_grad()\n            with autocast(device_type='cuda', dtype=torch.float16):\n                output = model(input)\n                loss = loss_fn(output, target)\n            scaler.scale(loss).backward()\n\n            # Unscales the gradients of optimizer's assigned params in-place\n            scaler.unscale_(optimizer)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":82,"to":103}}}}],["850",{"pageContent":"# Unscales the gradients of optimizer's assigned params in-place\n            scaler.unscale_(optimizer)\n\n            # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n\n            # optimizer's gradients are already unscaled, so scaler.step does not unscale them,\n            # although it still skips optimizer.step() if the gradients contain infs or NaNs.\n            scaler.step(optimizer)\n\n            # Updates the scale for next iteration.\n            scaler.update()\n\n``scaler`` records that ``scaler.unscale_(optimizer)`` was already called for this optimizer\nthis iteration, so ``scaler.step(optimizer)`` knows not to redundantly unscale gradients before\n(internally) calling ``optimizer.step()``.\n\n.. currentmodule:: torch.cuda.amp.GradScaler","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":103,"to":120}}}}],["851",{"pageContent":".. currentmodule:: torch.cuda.amp.GradScaler\n\n.. warning::\n    :meth:`unscale_<unscale_>` should only be called once per optimizer per :meth:`step<step>` call,\n    and only after all gradients for that optimizer's assigned parameters have been accumulated.\n    Calling :meth:`unscale_<unscale_>` twice for a given optimizer between each :meth:`step<step>` triggers a RuntimeError.\n\n\nWorking with Scaled Gradients\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nGradient accumulation\n---------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":120,"to":132}}}}],["852",{"pageContent":"Working with Scaled Gradients\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nGradient accumulation\n---------------------\n\nGradient accumulation adds gradients over an effective batch of size ``batch_per_iter * iters_to_accumulate``\n(``* num_procs`` if distributed).  The scale should be calibrated for the effective batch, which means inf/NaN checking,\nstep skipping if inf/NaN grads are found, and scale updates should occur at effective-batch granularity.\nAlso, grads should remain scaled, and the scale factor should remain constant, while grads for a given effective\nbatch are accumulated.  If grads are unscaled (or the scale factor changes) before accumulation is complete,\nthe next backward pass will add scaled grads to unscaled grads (or grads scaled by a different factor)\nafter which it's impossible to recover the accumulated unscaled grads :meth:`step<step>` must apply.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":132,"to":144}}}}],["853",{"pageContent":"Therefore, if you want to :meth:`unscale_<unscale_>` grads (e.g., to allow clipping unscaled grads),\ncall :meth:`unscale_<unscale_>` just before :meth:`step<step>`, after all (scaled) grads for the upcoming\n:meth:`step<step>` have been accumulated.  Also, only call :meth:`update<update>` at the end of iterations\nwhere you called :meth:`step<step>` for a full effective batch::\n\n    scaler = GradScaler()\n\n    for epoch in epochs:\n        for i, (input, target) in enumerate(data):\n            with autocast(device_type='cuda', dtype=torch.float16):\n                output = model(input)\n                loss = loss_fn(output, target)\n                loss = loss / iters_to_accumulate\n\n            # Accumulates scaled gradients.\n            scaler.scale(loss).backward()\n\n            if (i + 1) % iters_to_accumulate == 0:\n                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":146,"to":164}}}}],["854",{"pageContent":"if (i + 1) % iters_to_accumulate == 0:\n                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n.. currentmodule:: torch.cuda.amp\n\nGradient penalty\n----------------\n\nA gradient penalty implementation commonly creates gradients using\n:func:`torch.autograd.grad`, combines them to create the penalty value,\nand adds the penalty value to the loss.\n\nHere's an ordinary example of an L2 penalty without gradient scaling or autocasting::\n\n    for epoch in epochs:\n        for input, target in data:\n            optimizer.zero_grad()\n            output = model(input)\n            loss = loss_fn(output, target)\n\n            # Creates gradients\n            grad_params = torch.autograd.grad(outputs=loss,\n                                              inputs=model.parameters(),\n                                              create_graph=True)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":164,"to":191}}}}],["855",{"pageContent":"# Computes the penalty term and adds it to the loss\n            grad_norm = 0\n            for grad in grad_params:\n                grad_norm += grad.pow(2).sum()\n            grad_norm = grad_norm.sqrt()\n            loss = loss + grad_norm\n\n            loss.backward()\n\n            # clip gradients here, if desired\n\n            optimizer.step()\n\nTo implement a gradient penalty *with* gradient scaling, the ``outputs`` Tensor(s)\npassed to :func:`torch.autograd.grad` should be scaled.  The resulting gradients\nwill therefore be scaled, and should be unscaled before being combined to create the\npenalty value.\n\nAlso, the penalty term computation is part of the forward pass, and therefore should be\ninside an :class:`autocast` context.\n\nHere's how that looks for the same L2 penalty::\n\n    scaler = GradScaler()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":193,"to":216}}}}],["856",{"pageContent":"Also, the penalty term computation is part of the forward pass, and therefore should be\ninside an :class:`autocast` context.\n\nHere's how that looks for the same L2 penalty::\n\n    scaler = GradScaler()\n\n    for epoch in epochs:\n        for input, target in data:\n            optimizer.zero_grad()\n            with autocast(device_type='cuda', dtype=torch.float16):\n                output = model(input)\n                loss = loss_fn(output, target)\n\n            # Scales the loss for autograd.grad's backward pass, producing scaled_grad_params\n            scaled_grad_params = torch.autograd.grad(outputs=scaler.scale(loss),\n                                                     inputs=model.parameters(),\n                                                     create_graph=True)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":216,"to":233}}}}],["857",{"pageContent":"# Creates unscaled grad_params before computing the penalty. scaled_grad_params are\n            # not owned by any optimizer, so ordinary division is used instead of scaler.unscale_:\n            inv_scale = 1./scaler.get_scale()\n            grad_params = [p * inv_scale for p in scaled_grad_params]\n\n            # Computes the penalty term and adds it to the loss\n            with autocast(device_type='cuda', dtype=torch.float16):\n                grad_norm = 0\n                for grad in grad_params:\n                    grad_norm += grad.pow(2).sum()\n                grad_norm = grad_norm.sqrt()\n                loss = loss + grad_norm\n\n            # Applies scaling to the backward call as usual.\n            # Accumulates leaf gradients that are correctly scaled.\n            scaler.scale(loss).backward()\n\n            # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":235,"to":252}}}}],["858",{"pageContent":"# may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n            # step() and update() proceed as usual.\n            scaler.step(optimizer)\n            scaler.update()\n\n\nWorking with Multiple Models, Losses, and Optimizers\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. currentmodule:: torch.cuda.amp.GradScaler\n\nIf your network has multiple losses, you must call :meth:`scaler.scale<scale>` on each of them individually.\nIf your network has multiple optimizers, you may call :meth:`scaler.unscale_<unscale_>` on any of them individually,\nand you must call :meth:`scaler.step<step>` on each of them individually.\n\nHowever, :meth:`scaler.update<update>` should only be called once,\nafter all optimizers used this iteration have been stepped::\n\n    scaler = torch.cuda.amp.GradScaler()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":252,"to":271}}}}],["859",{"pageContent":"However, :meth:`scaler.update<update>` should only be called once,\nafter all optimizers used this iteration have been stepped::\n\n    scaler = torch.cuda.amp.GradScaler()\n\n    for epoch in epochs:\n        for input, target in data:\n            optimizer0.zero_grad()\n            optimizer1.zero_grad()\n            with autocast(device_type='cuda', dtype=torch.float16):\n                output0 = model0(input)\n                output1 = model1(input)\n                loss0 = loss_fn(2 * output0 + 3 * output1, target)\n                loss1 = loss_fn(3 * output0 - 5 * output1, target)\n\n            # (retain_graph here is unrelated to amp, it's present because in this\n            # example, both backward() calls share some sections of graph.)\n            scaler.scale(loss0).backward(retain_graph=True)\n            scaler.scale(loss1).backward()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":271,"to":289}}}}],["860",{"pageContent":"# You can choose which optimizers receive explicit unscaling, if you\n            # want to inspect or modify the gradients of the params they own.\n            scaler.unscale_(optimizer0)\n\n            scaler.step(optimizer0)\n            scaler.step(optimizer1)\n\n            scaler.update()\n\nEach optimizer checks its gradients for infs/NaNs and makes an independent decision\nwhether or not to skip the step.  This may result in one optimizer skipping the step\nwhile the other one does not.  Since step skipping occurs rarely (every several hundred iterations)\nthis should not impede convergence.  If you observe poor convergence after adding gradient scaling\nto a multiple-optimizer model, please report a bug.\n\n.. currentmodule:: torch.cuda.amp\n\n.. _amp-multigpu:\n\nWorking with Multiple GPUs\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe issues described here only affect :class:`autocast`.  :class:`GradScaler`\\ 's usage is unchanged.\n\n.. _amp-dataparallel:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":291,"to":315}}}}],["861",{"pageContent":".. _amp-multigpu:\n\nWorking with Multiple GPUs\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe issues described here only affect :class:`autocast`.  :class:`GradScaler`\\ 's usage is unchanged.\n\n.. _amp-dataparallel:\n\nDataParallel in a single process\n--------------------------------\n\nEven if :class:`torch.nn.DataParallel` spawns threads to run the forward pass on each device.\nThe autocast state is propagated in each one and the following will work::\n\n    model = MyModel()\n    dp_model = nn.DataParallel(model)\n\n    # Sets autocast in the main thread\n    with autocast(device_type='cuda', dtype=torch.float16):\n        # dp_model's internal threads will autocast.\n        output = dp_model(input)\n        # loss_fn also autocast\n        loss = loss_fn(output)\n\nDistributedDataParallel, one GPU per process\n--------------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":315,"to":341}}}}],["862",{"pageContent":"DistributedDataParallel, one GPU per process\n--------------------------------------------\n\n:class:`torch.nn.parallel.DistributedDataParallel`'s documentation recommends one GPU per process for best\nperformance.  In this case, ``DistributedDataParallel`` does not spawn threads internally,\nso usages of :class:`autocast` and :class:`GradScaler` are not affected.\n\nDistributedDataParallel, multiple GPUs per process\n--------------------------------------------------\n\nHere :class:`torch.nn.parallel.DistributedDataParallel` may spawn a side thread to run the forward pass on each\ndevice, like :class:`torch.nn.DataParallel`.  :ref:`The fix is the same<amp-dataparallel>`:\napply autocast as part of your model's ``forward`` method to ensure it's enabled in side threads.\n\n.. _amp-custom-examples:\n\nAutocast and Custom Autograd Functions\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":341,"to":358}}}}],["863",{"pageContent":".. _amp-custom-examples:\n\nAutocast and Custom Autograd Functions\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf your network uses :ref:`custom autograd functions<extending-autograd>`\n(subclasses of :class:`torch.autograd.Function`), changes are required for\nautocast compatibility if any function\n\n* takes multiple floating-point Tensor inputs,\n* wraps any autocastable op (see the :ref:`Autocast Op Reference<autocast-op-reference>`), or\n* requires a particular ``dtype`` (for example, if it wraps\n  `CUDA extensions <https://pytorch.org/tutorials/advanced/cpp_extension.html>`_\n  that were only compiled for ``dtype``).\n\nIn all cases, if you're importing the function and can't alter its definition, a safe fallback\nis to disable autocast and force execution in ``float32`` ( or ``dtype``) at any points of use where errors occur::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":358,"to":374}}}}],["864",{"pageContent":"with autocast(device_type='cuda', dtype=torch.float16):\n        ...\n        with autocast(device_type='cuda', dtype=torch.float16, enabled=False):\n            output = imported_function(input1.float(), input2.float())\n\nIf you're the function's author (or can alter its definition) a better solution is to use the\n:func:`torch.cuda.amp.custom_fwd` and :func:`torch.cuda.amp.custom_bwd` decorators as shown in\nthe relevant case below.\n\nFunctions with multiple inputs or autocastable ops\n--------------------------------------------------\n\nApply :func:`custom_fwd<custom_fwd>` and :func:`custom_bwd<custom_bwd>` (with no arguments) to ``forward`` and\n``backward`` respectively.  These ensure ``forward`` executes with the current autocast state and ``backward``\nexecutes with the same autocast state as ``forward`` (which can prevent type mismatch errors)::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":376,"to":390}}}}],["865",{"pageContent":"class MyMM(torch.autograd.Function):\n        @staticmethod\n        @custom_fwd\n        def forward(ctx, a, b):\n            ctx.save_for_backward(a, b)\n            return a.mm(b)\n        @staticmethod\n        @custom_bwd\n        def backward(ctx, grad):\n            a, b = ctx.saved_tensors\n            return grad.mm(b.t()), a.t().mm(grad)\n\nNow ``MyMM`` can be invoked anywhere, without disabling autocast or manually casting inputs::\n\n    mymm = MyMM.apply\n\n    with autocast(device_type='cuda', dtype=torch.float16):\n        output = mymm(input1, input2)\n\nFunctions that need a particular ``dtype``\n------------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":392,"to":412}}}}],["866",{"pageContent":"with autocast(device_type='cuda', dtype=torch.float16):\n        output = mymm(input1, input2)\n\nFunctions that need a particular ``dtype``\n------------------------------------------\n\nConsider a custom function that requires ``torch.float32`` inputs.\nApply :func:`custom_fwd(cast_inputs=torch.float32)<custom_fwd>` to ``forward``\nand :func:`custom_bwd<custom_bwd>` (with no arguments) to ``backward``.\nIf ``forward`` runs in an autocast-enabled region, the decorators cast floating-point CUDA Tensor\ninputs to ``float32``, and locally disable autocast during ``forward`` and ``backward``::\n\n    class MyFloat32Func(torch.autograd.Function):\n        @staticmethod\n        @custom_fwd(cast_inputs=torch.float32)\n        def forward(ctx, input):\n            ctx.save_for_backward(input)\n            ...\n            return fwd_output\n        @staticmethod\n        @custom_bwd\n        def backward(ctx, grad):\n            ...","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":412,"to":434}}}}],["867",{"pageContent":"Now ``MyFloat32Func`` can be invoked anywhere, without manually disabling autocast or casting inputs::\n\n    func = MyFloat32Func.apply\n\n    with autocast(device_type='cuda', dtype=torch.float16):\n        # func will run in float32, regardless of the surrounding autocast state\n        output = func(input)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/amp_examples.rst","loc":{"lines":{"from":436,"to":442}}}}],["868",{"pageContent":".. _autograd-mechanics:\n\nAutograd mechanics\n==================\n\nThis note will present an overview of how autograd works and records the\noperations. It's not strictly necessary to understand all this, but we recommend\ngetting familiar with it, as it will help you write more efficient, cleaner\nprograms, and can aid you in debugging.\n\n.. _how-autograd-encodes-history:\n\nHow autograd encodes the history\n--------------------------------\n\nAutograd is a reverse automatic differentiation system.  Conceptually,\nautograd records a graph recording all of the operations that created\nthe data as you execute operations, giving you a directed acyclic graph\nwhose leaves are the input tensors and roots are the output tensors.\nBy tracing this graph from roots to leaves, you can automatically\ncompute the gradients using the chain rule.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":1,"to":21}}}}],["869",{"pageContent":"Internally, autograd represents this graph as a graph of\n:class:`Function` objects (really expressions), which can be\n:meth:`~torch.autograd.Function.apply` ed to compute the result of\nevaluating the graph.  When computing the forward pass, autograd\nsimultaneously performs the requested computations and builds up a graph\nrepresenting the function that computes the gradient (the ``.grad_fn``\nattribute of each :class:`torch.Tensor` is an entry point into this graph).\nWhen the forward pass is completed, we evaluate this graph in the\nbackwards pass to compute the gradients.\n\nAn important thing to note is that the graph is recreated from scratch at every\niteration, and this is exactly what allows for using arbitrary Python control\nflow statements, that can change the overall shape and size of the graph at\nevery iteration. You don't have to encode all possible paths before you\nlaunch the training - what you run is what you differentiate.\n\n.. _saved-tensors-doc:\n\nSaved tensors\n^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":23,"to":42}}}}],["870",{"pageContent":".. _saved-tensors-doc:\n\nSaved tensors\n^^^^^^^^^^^^^\n\nSome operations need intermediary results to be saved during the forward pass\nin order to execute the backward pass. For example, the function\n:math:`x\\mapsto x^2` saves the input :math:`x` to compute the gradient.\n\nWhen defining a custom Python :class:`~torch.autograd.Function`, you can use\n:func:`~torch.autograd.function._ContextMethodMixin.save_for_backward` to save\ntensors during the forward pass and\n:attr:`~torch.autograd.function.Function.saved_tensors` to retrieve them\nduring the backward pass. See :doc:`/notes/extending` for more information.\n\nFor operations that PyTorch defines (e.g. :func:`torch.pow`), tensors are\nautomatically saved as needed. You can explore (for educational or debugging\npurposes) which tensors are saved by a certain ``grad_fn`` by looking for its\nattributes starting with the prefix ``_saved``.\n\n.. code::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":42,"to":62}}}}],["871",{"pageContent":".. code::\n\n    x = torch.randn(5, requires_grad=True)\n    y = x.pow(2)\n    print(x.equal(y.grad_fn._saved_self))  # True\n    print(x is y.grad_fn._saved_self)  # True\n\n\nIn the previous code, ``y.grad_fn._saved_self`` refers to the same Tensor object as `x`.\nBut that may not always be the case. For instance:\n\n.. code::\n\n    x = torch.randn(5, requires_grad=True)\n    y = x.exp()\n    print(y.equal(y.grad_fn._saved_result))  # True\n    print(y is y.grad_fn._saved_result)  # False\n\n\nUnder the hood, to prevent reference cycles, PyTorch has *packed* the tensor\nupon saving and *unpacked* it into a different tensor for reading. Here, the\ntensor you get from accessing ``y.grad_fn._saved_result`` is a different tensor\nobject than ``y`` (but they still share the same storage).\n\nWhether a tensor will be packed into a different tensor object depends on\nwhether it is an output of its own `grad_fn`, which is an implementation detail\nsubject to change and that users should not rely on.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":62,"to":88}}}}],["872",{"pageContent":"You can control how PyTorch does packing / unpacking with :ref:`saved-tensors-hooks-doc`.\n\n\n.. _non-differentiable-func-grad:\n\nGradients for non-differentiable functions\n------------------------------------------\n\nThe gradient computation using Automatic Differentiation is only valid when each elementary function being used is differentiable.\nUnfortunately many of the functions we use in practice do not have this property (``relu`` or ``sqrt`` at ``0``, for example).\nTo try and reduce the impact of functions that are non-differentiable, we define the gradients of the elementary operations by applying the following rules in order:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":90,"to":100}}}}],["873",{"pageContent":"#. If the function is differentiable and thus a gradient exists at the current point, use it.\n#. If the function is convex (at least locally), use the sub-gradient of minimum norm (it is the steepest descent direction).\n#. If the function is concave (at least locally), use the super-gradient of minimum norm (consider `-f(x)` and apply the previous point).\n#. If the function is defined, define the gradient at the current point by continuity (note that ``inf`` is possible here, for example for ``sqrt(0)``). If multiple values are possible, pick one arbitrarily.\n#. If the function is not defined (``sqrt(-1)``, ``log(-1)`` or most functions when the input is ``NaN``, for example) then the value used as the gradient is arbitrary (we might also raise an error but that is not guaranteed). Most functions will use ``NaN`` as the gradient, but for performance reasons, some functions will use other values (``log(-1)``, for example).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":102,"to":106}}}}],["874",{"pageContent":"#. If the function is not a deterministic mapping (i.e. it is not a `mathematical function`_), it will be marked as non-differentiable. This will make it error out in the backward if used on tensors that require grad outside of a ``no_grad`` environment.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":107,"to":107}}}}],["875",{"pageContent":".. _mathematical function: https://en.wikipedia.org/wiki/Function_(mathematics)\n\n.. _locally-disable-grad-doc:\n\nLocally disabling gradient computation\n--------------------------------------\n\nThere are several mechanisms available from Python to locally disable gradient\ncomputation:\n\nTo disable gradients across entire blocks of code, there are context managers\nlike no-grad mode and inference mode.\nFor more fine-grained exclusion of subgraphs from gradient computation,\nthere is setting the ``requires_grad`` field of a tensor.\n\nBelow, in addition to discussing the mechanisms above, we also describe\nevaluation mode (:meth:`nn.Module.eval()`), a method that is not used\nto disable gradient computation but, because of its name, is often mixed up with the three.\n\nSetting ``requires_grad``\n^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":109,"to":129}}}}],["876",{"pageContent":"Setting ``requires_grad``\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n:attr:`requires_grad` is a flag, defaulting to false *unless wrapped\nin a* ``nn.Parameter``, that allows for fine-grained exclusion of\nsubgraphs from gradient computation. It takes effect in both the\nforward and backward passes:\n\nDuring the forward pass, an operation is only recorded in the backward graph if\nat least one of its input tensors require grad.\nDuring the backward pass (``.backward()``), only leaf tensors with\n``requires_grad=True`` will have gradients accumulated into their ``.grad``\nfields.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":129,"to":141}}}}],["877",{"pageContent":"It is important to note that even though every tensor has this flag,\n*setting* it only makes sense for leaf tensors (tensors that do not have a\n``grad_fn``, e.g., a ``nn.Module``'s parameters).\nNon-leaf tensors (tensors that do have ``grad_fn``) are tensors that have a\nbackward graph associated with them. Thus their gradients will be needed\nas an intermediary result to compute the gradient for a leaf tensor that\nrequires grad. From this definition, it is clear that all non-leaf tensors\nwill automatically have ``require_grad=True``.\n\nSetting ``requires_grad`` should be the main way you control which parts\nof the model are part of the gradient computation, for example, if you need to\nfreeze parts of your pretrained model during model fine-tuning.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":143,"to":154}}}}],["878",{"pageContent":"To freeze parts of your model, simply apply ``.requires_grad_(False)`` to\nthe parameters that you don't want updated. And as described above,\nsince computations that use these parameters as inputs would not be recorded in\nthe forward pass, they won't have their ``.grad`` fields updated in the backward\npass because they won't be part of the backward graph in the first place, as\ndesired.\n\nBecause this is such a common pattern, ``requires_grad`` can also be set at\nthe module level with :meth:`nn.Module.requires_grad_()`.\nWhen applied to a module, ``.requires_grad_()`` takes effect on all\nof the module's parameters (which have ``requires_grad=True`` by default).\n\nGrad Modes\n^^^^^^^^^^\n\nApart from setting ``requires_grad`` there are also three grad modes that can\nbe selected from Python that can affect how computations in PyTorch are\nprocessed by autograd internally: default mode (grad mode), no-grad mode,\nand inference mode, all of which can be togglable via context managers and\ndecorators.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":156,"to":175}}}}],["879",{"pageContent":"Default Mode (Grad Mode)\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe \"default mode\" is the mode we are implicitly in when no other modes like\nno-grad and inference mode are enabled. To be contrasted with\n\"no-grad mode\" the default mode is also sometimes called \"grad mode\".\n\nThe most important thing to know about the default mode is that it is the only\nmode in which ``requires_grad`` takes effect. ``requires_grad`` is always overridden\nto be ``False`` in both the two other modes.\n\nNo-grad Mode\n^^^^^^^^^^^^\n\nComputations in no-grad mode behave as if none of the inputs require grad.\nIn other words, computations in no-grad mode are never recorded in the backward graph\neven if there are inputs that have ``require_grad=True``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":177,"to":193}}}}],["880",{"pageContent":"Enable no-grad mode when you need to perform operations that should not be\nrecorded by autograd, but you’d still like to use the outputs of these\ncomputations in grad mode later. This context manager makes it convenient to\ndisable gradients for a block of code or function without\nhaving to temporarily set tensors to have ``requires_grad=False``, and then\nback to ``True``.\n\nFor example, no-grad mode might be useful when writing an optimizer: when\nperforming the training update you’d like to update parameters\nin-place without the update being recorded by autograd.\nYou also intend to use the updated parameters for computations in\ngrad mode in the next forward pass.\n\nThe implementations in :ref:`nn-init-doc` also\nrely on no-grad mode when initializing the parameters as to avoid\nautograd tracking when updating the initialized parameters in-place.\n\nInference Mode\n^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":195,"to":213}}}}],["881",{"pageContent":"Inference Mode\n^^^^^^^^^^^^^^\n\nInference mode is the extreme version of no-grad mode. Just like in no-grad\nmode, computations in inference mode are not recorded in the backward graph, but\nenabling inference mode will allow PyTorch to speed up your model even more.\nThis better runtime comes with a drawback: tensors created in inference mode\nwill not be able to be used in computations to be recorded by autograd after\nexiting inference mode.\n\nEnable inference mode when you are performing computations that don’t need\nto be recorded in the backward graph, AND you don’t plan on using the tensors\ncreated in inference mode in any computation that is to be recorded by autograd later.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":213,"to":225}}}}],["882",{"pageContent":"It is recommended that you try out inference mode in the parts of your code\nthat do not require autograd tracking (e.g., data processing and model evaluation).\nIf it works out of the box\nfor your use case it’s a free performance win. If you run into errors after\nenabling inference mode, check that you are not using tensors created in\ninference mode in computations that are recorded by autograd after exiting inference\nmode. If you cannot avoid such use in your case, you can always switch back\nto no-grad mode.\n\nFor details on inference mode please see\n`Inference Mode <https://pytorch.org/cppdocs/notes/inference_mode.html>`_.\n\nFor implementation details of inference mode see\n`RFC-0011-InferenceMode <https://github.com/pytorch/rfcs/pull/17>`_.\n\nEvaluation Mode (``nn.Module.eval()``)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nEvaluation mode is not a mechanism to locally disable gradient computation.\nIt is included here anyway because it is sometimes confused to be such a mechanism.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":227,"to":246}}}}],["883",{"pageContent":"Evaluation mode is not a mechanism to locally disable gradient computation.\nIt is included here anyway because it is sometimes confused to be such a mechanism.\n\nFunctionally, ``module.eval()`` (or equivalently ``module.train(False)``) are completely\northogonal to no-grad mode and inference mode. How ``model.eval()`` affects\nyour model depends entirely on the specific modules used in your model and\nwhether they define any training-mode specific behavior.\n\nYou are responsible for calling ``model.eval()`` and ``model.train()`` if your\nmodel relies on modules such as :class:`torch.nn.Dropout` and\n:class:`torch.nn.BatchNorm2d` that may behave\ndifferently depending on training mode, for example, to avoid updating your\nBatchNorm running statistics on validation data.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":246,"to":258}}}}],["884",{"pageContent":"It is recommended that you always use ``model.train()`` when\ntraining and ``model.eval()`` when evaluating your model (validation/testing) even\nif you aren’t sure your model has training-mode specific behavior, because a\nmodule you are using might be updated to behave differently in training and\neval modes.\n\nIn-place operations with autograd\n---------------------------------\n\nSupporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd's aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nlower memory usage by any significant amount. Unless you're operating\nunder heavy memory pressure, you might never need to use them.\n\nThere are two main reasons that limit the applicability of in-place operations:\n\n1. In-place operations can potentially overwrite values required to compute\n   gradients.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":260,"to":278}}}}],["885",{"pageContent":"There are two main reasons that limit the applicability of in-place operations:\n\n1. In-place operations can potentially overwrite values required to compute\n   gradients.\n\n2. Every in-place operation requires the implementation to rewrite the\n   computational graph. Out-of-place versions simply allocate new objects and\n   keep references to the old graph, while in-place operations, require\n   changing the creator of all inputs to the :class:`Function` representing\n   this operation. This can be tricky, especially if there are many Tensors\n   that reference the same storage (e.g. created by indexing or transposing),\n   and in-place functions will raise an error if the storage of\n   modified inputs is referenced by any other :class:`Tensor`.\n\nIn-place correctness checks\n^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":278,"to":293}}}}],["886",{"pageContent":"In-place correctness checks\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nEvery tensor keeps a version counter, that is incremented every time it is\nmarked dirty in any operation. When a Function saves any tensors for backward,\na version counter of their containing Tensor is saved as well. Once you access\n``self.saved_tensors`` it is checked, and if it is greater than the saved value\nan error is raised. This ensures that if you're using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.\n\nMultithreaded Autograd\n----------------------\n\nThe autograd engine is responsible for running all the backward operations\nnecessary to compute the backward pass. This section will describe all the details\nthat can help you make the best use of it in a multithreaded environment. (This is\nrelevant only for PyTorch 1.6+ as the behavior in previous version was different.)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":293,"to":310}}}}],["887",{"pageContent":"User could train their model with multithreading code (e.g. Hogwild training), and\ndoes not block on the concurrent backward computations, example code could be:\n\n.. code::\n\n    # Define a train function to be used in different threads\n    def train_fn():\n        x = torch.ones(5, 5, requires_grad=True)\n        # forward\n        y = (x + 3) * (x + 4) * 0.5\n        # backward\n        y.sum().backward()\n        # potential optimizer update\n\n\n    # User write their own threading code to drive the train_fn\n    threads = []\n    for _ in range(10):\n        p = threading.Thread(target=train_fn, args=())\n        p.start()\n        threads.append(p)\n\n    for p in threads:\n        p.join()\n\n\nNote that some behaviors that user should be aware of:\n\nConcurrency on CPU\n^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":312,"to":341}}}}],["888",{"pageContent":"for p in threads:\n        p.join()\n\n\nNote that some behaviors that user should be aware of:\n\nConcurrency on CPU\n^^^^^^^^^^^^^^^^^^\n\nWhen you run ``backward()`` or ``grad()`` via python or C++ API in multiple\nthreads on CPU, you are expecting to see extra concurrency instead of\nserializing all the backward calls in a specific order during execution\n(behavior before PyTorch 1.6).\n\nNon-determinism\n^^^^^^^^^^^^^^^\n\nIf you are calling ``backward()`` from multiple threads concurrently and have\nshared inputs (i.e. Hogwild CPU training), then non-determinism should be expected.\nThis can occur because parameters are automatically shared across threads,\nas such, multiple threads may access and try to accumulate the same ``.grad``\nattribute during gradient accumulation. This is technically not safe, and\nit might result in race condition and the result might be invalid to use.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":341,"to":363}}}}],["889",{"pageContent":"Users developing multithreaded models featuring shared parameters should have the\nthreading model in mind and should understand the issues described above.\n\nThe functional API :func:`torch.autograd.grad` may be used to calculate the\ngradients instead of ``backward()`` to avoid non-determinism.\n\nGraph retaining\n^^^^^^^^^^^^^^^\n\nIf part of the autograd graph is shared between threads, i.e. run first\npart of forward single thread, then run second part in multiple threads,\nthen the first part of graph is shared. In this case different threads\nexecute ``grad()`` or ``backward()`` on the same graph might have issue of\ndestroying the graph on the fly of one thread, and the other thread will\ncrash in this case. Autograd will error out to the user similar to what call\n``backward()`` twice with out ``retain_graph=True``, and let the user know\nthey should use ``retain_graph=True``.\n\nThread Safety on Autograd Node\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":365,"to":384}}}}],["890",{"pageContent":"Thread Safety on Autograd Node\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSince Autograd allows the caller thread to drive its backward execution for\npotential parallelism, it's important that we ensure thread safety on CPU with\nparallel ``backward()`` calls that share part/whole of the GraphTask.\n\nCustom Python ``autograd.Function``\\s are automatically thread safe because of GIL.\nFor built-in C++ Autograd Nodes (e.g. AccumulateGrad, CopySlices) and custom\n``autograd::Function``\\s, the Autograd Engine uses thread mutex locking to ensure\nthread safety on autograd Nodes that might have state write/read.\n\nNo thread safety on C++ hooks\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAutograd relies on the user to write thread safe C++ hooks. If you want the hook\nto be correctly applied in multithreading environment, you will need to write\nproper thread locking code to ensure the hooks are thread safe.\n\n.. _complex_autograd-doc:\n\nAutograd for Complex Numbers\n----------------------------\n\nThe short version:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":384,"to":408}}}}],["891",{"pageContent":"The short version:\n\n- When you use PyTorch to differentiate any function :math:`f(z)` with complex domain and/or codomain,\n  the gradients are computed under the assumption that the function is a part of a larger real-valued\n  loss function :math:`g(input)=L`. The gradient computed is :math:`\\frac{\\partial L}{\\partial z^*}`\n  (note the conjugation of z), the negative of which is precisely the direction of steepest descent\n  used in Gradient Descent algorithm. Thus, all the existing optimizers work out of\n  the box with complex parameters.\n- This convention matches TensorFlow's convention for complex\n  differentiation, but is different from JAX (which computes\n  :math:`\\frac{\\partial L}{\\partial z}`).\n- If you have a real-to-real function which internally uses complex\n  operations, the convention here doesn't matter: you will always get\n  the same result that you would have gotten if it had been implemented\n  with only real operations.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":408,"to":422}}}}],["892",{"pageContent":"If you are curious about the mathematical details, or want to know how\nto define complex derivatives in PyTorch, read on.\n\nWhat are complex derivatives?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe mathematical definition of complex-differentiability takes the\nlimit definition of a derivative and generalizes it to operate on\ncomplex numbers. Consider a function :math:`f: ℂ → ℂ`,\n\n    .. math::\n        f(z=x+yj) = u(x, y) + v(x, y)j\n\nwhere :math:`u` and :math:`v` are two variable real valued functions\nand :math:`j` is the imaginary unit.\n\nUsing the derivative definition, we can write:\n\n    .. math::\n        f'(z) = \\lim_{h \\to 0, h \\in C} \\frac{f(z+h) - f(z)}{h}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":424,"to":443}}}}],["893",{"pageContent":"Using the derivative definition, we can write:\n\n    .. math::\n        f'(z) = \\lim_{h \\to 0, h \\in C} \\frac{f(z+h) - f(z)}{h}\n\nIn order for this limit to exist, not only must :math:`u` and :math:`v` must be\nreal differentiable, but :math:`f` must also satisfy the Cauchy-Riemann `equations\n<https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations>`_.  In\nother words: the limit computed for real and imaginary steps (:math:`h`)\nmust be equal. This is a more restrictive condition.\n\nThe complex differentiable functions are commonly known as holomorphic\nfunctions. They are well behaved, have all the nice properties that\nyou've seen from real differentiable functions, but are practically of no\nuse in the optimization world. For optimization problems, only real valued objective\nfunctions are used in the research community since complex numbers are not part of any\nordered field and so having complex valued loss does not make much sense.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":443,"to":459}}}}],["894",{"pageContent":"It also turns out that no interesting real-valued objective fulfill the\nCauchy-Riemann equations. So the theory with homomorphic function cannot be\nused for optimization and most people therefore use the Wirtinger calculus.\n\nWirtinger Calculus comes into the picture ...\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSo, we have this great theory of complex differentiability and\nholomorphic functions, and we can’t use any of it at all, because many\nof the commonly used functions are not holomorphic. What’s a poor\nmathematician to do? Well, Wirtinger observed that even if :math:`f(z)`\nisn’t holomorphic, one could rewrite it as a two variable function\n:math:`f(z, z*)` which is always holomorphic. This is because real and\nimaginary of the components of :math:`z` can be expressed in terms of\n:math:`z` and :math:`z^*` as:\n\n    .. math::\n        \\begin{aligned}\n            Re(z) &= \\frac {z + z^*}{2} \\\\\n            Im(z) &= \\frac {z - z^*}{2j}\n        \\end{aligned}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":461,"to":481}}}}],["895",{"pageContent":".. math::\n        \\begin{aligned}\n            Re(z) &= \\frac {z + z^*}{2} \\\\\n            Im(z) &= \\frac {z - z^*}{2j}\n        \\end{aligned}\n\nWirtinger calculus suggests to study :math:`f(z, z^*)` instead, which is\nguaranteed to be holomorphic if :math:`f` was real differentiable (another\nway to think of it is as a change of coordinate system, from :math:`f(x, y)`\nto :math:`f(z, z^*)`.)  This function has partial derivatives\n:math:`\\frac{\\partial }{\\partial z}` and :math:`\\frac{\\partial}{\\partial z^{*}}`.\nWe can use the chain rule to establish a\nrelationship between these partial derivatives and the partial\nderivatives w.r.t., the real and imaginary components of :math:`z`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":481,"to":494}}}}],["896",{"pageContent":".. math::\n        \\begin{aligned}\n            \\frac{\\partial }{\\partial x} &= \\frac{\\partial z}{\\partial x} * \\frac{\\partial }{\\partial z} + \\frac{\\partial z^*}{\\partial x} * \\frac{\\partial }{\\partial z^*} \\\\\n                                         &= \\frac{\\partial }{\\partial z} + \\frac{\\partial }{\\partial z^*}   \\\\\n            \\\\\n            \\frac{\\partial }{\\partial y} &= \\frac{\\partial z}{\\partial y} * \\frac{\\partial }{\\partial z} + \\frac{\\partial z^*}{\\partial y} * \\frac{\\partial }{\\partial z^*} \\\\\n                                         &= 1j * (\\frac{\\partial }{\\partial z} - \\frac{\\partial }{\\partial z^*})\n        \\end{aligned}\n\nFrom the above equations, we get:\n\n    .. math::\n        \\begin{aligned}\n            \\frac{\\partial }{\\partial z} &= 1/2 * (\\frac{\\partial }{\\partial x} - 1j * \\frac{\\partial }{\\partial y})   \\\\\n            \\frac{\\partial }{\\partial z^*} &= 1/2 * (\\frac{\\partial }{\\partial x} + 1j * \\frac{\\partial }{\\partial y})\n        \\end{aligned}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":496,"to":511}}}}],["897",{"pageContent":"which is the classic definition of Wirtinger calculus that you would find on `Wikipedia <https://en.wikipedia.org/wiki/Wirtinger_derivatives>`_.\n\nThere are a lot of beautiful consequences of this change.\n\n- For one, the Cauchy-Riemann equations translate into simply saying that :math:`\\frac{\\partial f}{\\partial z^*} = 0` (that is to say, the function :math:`f` can be written\n  entirely in terms of :math:`z`, without making reference to :math:`z^*`).\n- Another important (and somewhat counterintuitive) result, as we’ll see later, is that when we do optimization on a real-valued loss, the step we should\n  take while making variable update is given by :math:`\\frac{\\partial Loss}{\\partial z^*}` (not :math:`\\frac{\\partial Loss}{\\partial z}`).\n\nFor more reading, check out: https://arxiv.org/pdf/0906.4835.pdf\n\nHow is Wirtinger Calculus useful in optimization?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":513,"to":525}}}}],["898",{"pageContent":"For more reading, check out: https://arxiv.org/pdf/0906.4835.pdf\n\nHow is Wirtinger Calculus useful in optimization?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nResearchers in audio and other fields, more commonly, use gradient\ndescent to optimize real valued loss functions with complex variables.\nTypically, these people treat the real and imaginary values as separate\nchannels that can be updated. For a step size :math:`\\alpha/2` and loss\n:math:`L`, we can write the following equations in :math:`ℝ^2`:\n\n    .. math::\n        \\begin{aligned}\n            x_{n+1} &= x_n - (\\alpha/2) * \\frac{\\partial L}{\\partial x}  \\\\\n            y_{n+1} &= y_n - (\\alpha/2) * \\frac{\\partial L}{\\partial y}\n        \\end{aligned}\n\nHow do these equations translate into complex space :math:`ℂ`?","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":525,"to":542}}}}],["899",{"pageContent":"How do these equations translate into complex space :math:`ℂ`?\n\n    .. math::\n        \\begin{aligned}\n            z_{n+1} &= x_n - (\\alpha/2) * \\frac{\\partial L}{\\partial x} + 1j * (y_n - (\\alpha/2) * \\frac{\\partial L}{\\partial y}) \\\\\n                    &= z_n - \\alpha * 1/2 * (\\frac{\\partial L}{\\partial x} + j \\frac{\\partial L}{\\partial y}) \\\\\n                    &= z_n - \\alpha * \\frac{\\partial L}{\\partial z^*}\n        \\end{aligned}\n\nSomething very interesting has happened: Wirtinger calculus tells us\nthat we can simplify the complex variable update formula above to only\nrefer to the conjugate Wirtinger derivative\n:math:`\\frac{\\partial L}{\\partial z^*}`, giving us exactly the step we take in optimization.\n\nBecause the conjugate Wirtinger derivative gives us exactly the correct step for a real valued loss function, PyTorch gives you this derivative\nwhen you differentiate a function with a real valued loss.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":542,"to":557}}}}],["900",{"pageContent":"How does PyTorch compute the conjugate Wirtinger derivative?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTypically, our derivative formulas take in `grad_output` as an input,\nrepresenting the incoming Vector-Jacobian product that we’ve already\ncomputed, aka, :math:`\\frac{\\partial L}{\\partial s^*}`, where :math:`L`\nis the loss of the entire computation (producing a real loss) and\n:math:`s` is the output of our function. The goal here is to compute\n:math:`\\frac{\\partial L}{\\partial z^*}`, where :math:`z` is the input of\nthe function.  It turns out that in the case of real loss, we can\nget away with *only* calculating :math:`\\frac{\\partial L}{\\partial z^*}`,\neven though the chain rule implies that we also need to\nhave access to :math:`\\frac{\\partial L}{\\partial z^*}`.  If you want\nto skip this derivation, look at the last equation in this section\nand then skip to the next section.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":559,"to":573}}}}],["901",{"pageContent":"Let’s continue working with :math:`f: ℂ → ℂ` defined as\n:math:`f(z) = f(x+yj) = u(x, y) + v(x, y)j`. As discussed above,\nautograd’s gradient convention is centered around optimization for real\nvalued loss functions, so let’s assume :math:`f` is a part of larger\nreal valued loss function :math:`g`. Using chain rule, we can write:\n\n    .. math::\n        \\frac{\\partial L}{\\partial z^*} = \\frac{\\partial L}{\\partial u} * \\frac{\\partial u}{\\partial z^*} + \\frac{\\partial L}{\\partial v} * \\frac{\\partial v}{\\partial z^*}\n        :label: [1]\n\nNow using Wirtinger derivative definition, we can write:\n\n    .. math::\n        \\begin{aligned}\n            \\frac{\\partial L}{\\partial s} = 1/2 * (\\frac{\\partial L}{\\partial u} - \\frac{\\partial L}{\\partial v} j) \\\\\n            \\frac{\\partial L}{\\partial s^*} = 1/2 * (\\frac{\\partial L}{\\partial u} + \\frac{\\partial L}{\\partial v} j)\n        \\end{aligned}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":575,"to":591}}}}],["902",{"pageContent":"It should be noted here that since :math:`u` and :math:`v` are real\nfunctions, and :math:`L` is real by our assumption that :math:`f` is a\npart of a real valued function, we have:\n\n    .. math::\n        (\\frac{\\partial L}{\\partial s})^* = \\frac{\\partial L}{\\partial s^*}\n        :label: [2]\n\ni.e., :math:`\\frac{\\partial L}{\\partial s}` equals to :math:`grad\\_output^*`.\n\nSolving the above equations for :math:`\\frac{\\partial L}{\\partial u}` and :math:`\\frac{\\partial L}{\\partial v}`, we get:\n\n    .. math::\n        \\begin{aligned}\n            \\frac{\\partial L}{\\partial u} = \\frac{\\partial L}{\\partial s} + \\frac{\\partial L}{\\partial s^*} \\\\\n            \\frac{\\partial L}{\\partial v} = -1j * (\\frac{\\partial L}{\\partial s} - \\frac{\\partial L}{\\partial s^*})\n        \\end{aligned}\n        :label: [3]\n\nSubstituting :eq:`[3]` in :eq:`[1]`, we get:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":593,"to":612}}}}],["903",{"pageContent":".. math::\n        \\begin{aligned}\n            \\frac{\\partial L}{\\partial z^*} &= (\\frac{\\partial L}{\\partial s} + \\frac{\\partial L}{\\partial s^*}) * \\frac{\\partial u}{\\partial z^*} - 1j * (\\frac{\\partial L}{\\partial s} - \\frac{\\partial L}{\\partial s^*}) * \\frac{\\partial v}{\\partial z^*}  \\\\\n                                            &= \\frac{\\partial L}{\\partial s} * (\\frac{\\partial u}{\\partial z^*} + \\frac{\\partial v}{\\partial z^*} j) + \\frac{\\partial L}{\\partial s^*} * (\\frac{\\partial u}{\\partial z^*} - \\frac{\\partial v}{\\partial z^*} j)  \\\\\n                                            &= \\frac{\\partial L}{\\partial s^*} * \\frac{\\partial (u + vj)}{\\partial z^*} + \\frac{\\partial L}{\\partial s} * \\frac{\\partial (u + vj)^*}{\\partial z^*}  \\\\\n                                            &= \\frac{\\partial L}{\\partial s} * \\frac{\\partial s}{\\partial z^*} + \\frac{\\partial L}{\\partial s^*} * \\frac{\\partial s^*}{\\partial z^*}    \\\\\n        \\end{aligned}\n\nUsing :eq:`[2]`, we get:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":614,"to":622}}}}],["904",{"pageContent":"Using :eq:`[2]`, we get:\n\n    .. math::\n        \\begin{aligned}\n            \\frac{\\partial L}{\\partial z^*} &= (\\frac{\\partial L}{\\partial s^*})^* * \\frac{\\partial s}{\\partial z^*} + \\frac{\\partial L}{\\partial s^*} * (\\frac{\\partial s}{\\partial z})^*  \\\\\n                                            &= \\boxed{ (grad\\_output)^* * \\frac{\\partial s}{\\partial z^*} + grad\\_output * {(\\frac{\\partial s}{\\partial z})}^* }       \\\\\n        \\end{aligned}\n        :label: [4]\n\nThis last equation is the important one for writing your own gradients,\nas it decomposes our derivative formula into a simpler one that is easy\nto compute by hand.\n\nHow can I write my own derivative formula for a complex function?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":622,"to":636}}}}],["905",{"pageContent":"How can I write my own derivative formula for a complex function?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe above boxed equation gives us the general formula for all\nderivatives on complex functions.  However, we still need to\ncompute :math:`\\frac{\\partial s}{\\partial z}` and :math:`\\frac{\\partial s}{\\partial z^*}`.\nThere are two ways you could do this:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":636,"to":642}}}}],["906",{"pageContent":"- The first way is to just use the definition of Wirtinger derivatives directly and calculate :math:`\\frac{\\partial s}{\\partial z}` and :math:`\\frac{\\partial s}{\\partial z^*}` by\n      using :math:`\\frac{\\partial s}{\\partial x}` and :math:`\\frac{\\partial s}{\\partial y}`\n      (which you can compute in the normal way).\n    - The second way is to use the change of variables trick and rewrite :math:`f(z)` as a two variable function :math:`f(z, z^*)`, and compute\n      the conjugate Wirtinger derivatives by treating :math:`z` and :math:`z^*` as independent variables. This is often easier; for example, if the function in question is holomorphic, only :math:`z` will be used (and :math:`\\frac{\\partial s}{\\partial z^*}` will be zero).\n\nLet's consider the function :math:`f(z = x + yj) = c * z = c * (x+yj)` as an example, where :math:`c \\in ℝ`.\n\nUsing the first way to compute the Wirtinger derivatives, we have.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":644,"to":652}}}}],["907",{"pageContent":"Let's consider the function :math:`f(z = x + yj) = c * z = c * (x+yj)` as an example, where :math:`c \\in ℝ`.\n\nUsing the first way to compute the Wirtinger derivatives, we have.\n\n.. math::\n    \\begin{aligned}\n        \\frac{\\partial s}{\\partial z} &= 1/2 * (\\frac{\\partial s}{\\partial x} - \\frac{\\partial s}{\\partial y} j) \\\\\n                                      &= 1/2 * (c - (c * 1j) * 1j)  \\\\\n                                      &= c                          \\\\\n        \\\\\n        \\\\\n        \\frac{\\partial s}{\\partial z^*} &= 1/2 * (\\frac{\\partial s}{\\partial x} + \\frac{\\partial s}{\\partial y} j) \\\\\n                                        &= 1/2 * (c + (c * 1j) * 1j)  \\\\\n                                        &= 0                          \\\\\n    \\end{aligned}\n\nUsing :eq:`[4]`, and `grad\\_output = 1.0` (which is the default grad output value used when :func:`backward` is called on a scalar output in PyTorch), we get:\n\n    .. math::\n        \\frac{\\partial L}{\\partial z^*} = 1 * 0 + 1 * c = c","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":652,"to":671}}}}],["908",{"pageContent":".. math::\n        \\frac{\\partial L}{\\partial z^*} = 1 * 0 + 1 * c = c\n\nUsing the second way to compute Wirtinger derivatives, we directly get:\n\n    .. math::\n        \\begin{aligned}\n           \\frac{\\partial s}{\\partial z} &= \\frac{\\partial (c*z)}{\\partial z}       \\\\\n                                         &= c                                       \\\\\n            \\frac{\\partial s}{\\partial z^*} &= \\frac{\\partial (c*z)}{\\partial z^*}       \\\\\n                                         &= 0\n        \\end{aligned}\n\nAnd using :eq:`[4]` again, we get :math:`\\frac{\\partial L}{\\partial z^*} = c`. As you can see, the second way involves lesser calculations, and comes\nin more handy for faster calculations.\n\nWhat about cross-domain functions?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSome functions map from complex inputs to real outputs, or vice versa.\nThese functions form a special case of :eq:`[4]`, which we can derive using the\nchain rule:\n\n    - For :math:`f: ℂ → ℝ`, we get:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":671,"to":694}}}}],["909",{"pageContent":"Some functions map from complex inputs to real outputs, or vice versa.\nThese functions form a special case of :eq:`[4]`, which we can derive using the\nchain rule:\n\n    - For :math:`f: ℂ → ℝ`, we get:\n\n        .. math::\n            \\frac{\\partial L}{\\partial z^*} = 2 * grad\\_output * \\frac{\\partial s}{\\partial z^{*}}\n\n    - For :math:`f: ℝ → ℂ`, we get:\n\n        .. math::\n            \\frac{\\partial L}{\\partial z^*} = 2 * Re(grad\\_out^* * \\frac{\\partial s}{\\partial z^{*}})\n\n.. _saved-tensors-hooks-doc:\n\nHooks for saved tensors\n-----------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":694,"to":711}}}}],["910",{"pageContent":".. math::\n            \\frac{\\partial L}{\\partial z^*} = 2 * Re(grad\\_out^* * \\frac{\\partial s}{\\partial z^{*}})\n\n.. _saved-tensors-hooks-doc:\n\nHooks for saved tensors\n-----------------------\n\nYou can control :ref:`how saved tensors are packed / unpacked\n<saved-tensors-doc>` by defining a pair of ``pack_hook`` / ``unpack_hook``\nhooks.  The ``pack_hook`` function should take a tensor as its single argument\nbut can return any python object (e.g. another tensor, a tuple, or even a\nstring containing a filename). The ``unpack_hook`` function takes as its single\nargument the output of ``pack_hook`` and should return a tensor to be used in\nthe backward pass. The tensor returned by ``unpack_hook`` only needs to have\nthe same content as the tensor passed as input to ``pack_hook``. In particular,\nany autograd-related metadata can be ignored as they will be overwritten during\nunpacking.\n\nAn example of such pair is:\n\n.. code::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":711,"to":732}}}}],["911",{"pageContent":"An example of such pair is:\n\n.. code::\n\n    class SelfDeletingTempFile():\n        def __init__(self):\n            self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n\n        def __del__(self):\n            os.remove(self.name)\n\n    def pack_hook(tensor):\n        temp_file = SelfDeletingTempFile()\n        torch.save(tensor, temp_file.name)\n        return temp_file\n\n    def unpack_hook(temp_file):\n        return torch.load(temp_file.name)\n\nNotice that the ``unpack_hook`` should not delete the temporary file because it\nmight be called multiple times: the temporary file should be alive for as long\nas the returned `SelfDeletingTempFile` object is alive.  In the above example,\nwe prevent leaking the temporary file by closing it when it is no longer needed\n(on deletion of the `SelfDeletingTempFile` object).\n\n.. note::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":732,"to":757}}}}],["912",{"pageContent":".. note::\n\n    We guarantee that ``pack_hook`` will only be called once but ``unpack_hook`` can\n    be called as many times as the backward pass requires it and we expect it to\n    return the same data each time.\n\n.. warning::\n\n    Performing inplace operations on the input of any of the functions is forbidden\n    as they may lead to unexpected side-effects. PyTorch will throw an error if the\n    input to a pack hook is modified inplace but does not catch the case where the\n    input to an unpack hook is modified inplace.\n\n\nRegistering hooks for a saved tensor\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nYou can register a pair of hooks on a saved tensor by calling the\n:meth:`~torch.autograd.SavedTensor.register_hooks` method on a\n:class:`SavedTensor` object. Those objects are exposed as attributes of a\n``grad_fn`` and start with the ``_raw_saved_`` prefix.\n\n.. code::\n\n    x = torch.randn(5, requires_grad=True)\n    y = x.pow(2)\n    y.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":757,"to":783}}}}],["913",{"pageContent":".. code::\n\n    x = torch.randn(5, requires_grad=True)\n    y = x.pow(2)\n    y.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)\n\nThe ``pack_hook`` method is called as soon as the pair is registered.\nThe ``unpack_hook`` method is called each time the saved tensor needs to be\naccessed, either by means of ``y.grad_fn._saved_self`` or during the backward\npass.\n\n.. warning::\n\n    If you maintain a reference to a :class:`SavedTensor` after the saved\n    tensors have been released (i.e. after backward has been called), calling\n    its :meth:`~torch.autograd.SavedTensor.register_hooks` is forbidden.\n    PyTorch will throw an error most of the time but it may fail\n    to do so in some cases and undefined behavior may arise.\n\nRegistering default hooks for saved tensors\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":783,"to":803}}}}],["914",{"pageContent":"Registering default hooks for saved tensors\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAlternatively, you can use the context-manager\n:class:`~torch.autograd.graph.saved_tensors_hooks` to register a pair of\nhooks which will be applied to *all* saved tensors that are created in\nthat context.\n\nExample:\n\n.. code::\n\n    # Only save on disk tensors that have size >= 1000\n    SAVE_ON_DISK_THRESHOLD = 1000\n\n    def pack_hook(x):\n        if x.numel() < SAVE_ON_DISK_THRESHOLD:\n            return x\n        temp_file = SelfDeletingTempFile()\n        torch.save(tensor, temp_file.name)\n        return temp_file\n\n    def unpack_hook(tensor_or_sctf):\n        if isinstance(tensor_or_sctf, torch.Tensor):\n            return tensor_or_sctf\n        return torch.load(tensor_or_sctf.name)\n\n    class Model(nn.Module):\n        def forward(self, x):\n            with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n              # ... compute output\n              output = x\n            return output","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":803,"to":835}}}}],["915",{"pageContent":"model = Model()\n    net = nn.DataParallel(model)\n\n\n\nThe hooks defined with this context manager are thread-local.\nHence, the following code will not produce the desired effects because the hooks do not go\nthrough `DataParallel`.\n\n.. code::\n\n      # Example what NOT to do\n\n      net = nn.DataParallel(model)\n      with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n          output = net(input)\n\n\nNote that using those hooks disables all the optimization in place to reduce\nTensor object creation. For example:\n\n.. code::\n\n    with torch.autograd.graph.saved_tensors_hooks(lambda x: x, lambda x: x):\n        x = torch.randn(5, requires_grad=True)\n        y = x * x\n\nWithout the hooks, ``x``, ``y.grad_fn._saved_self`` and\n``y.grad_fn._saved_other`` all refer to the same tensor object.\nWith the hooks, PyTorch will pack and unpack `x` into two new tensor objects\nthat share the same storage with the original `x` (no copy performed).\n\n.. _backward-hooks-execution:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":837,"to":869}}}}],["916",{"pageContent":".. _backward-hooks-execution:\n\nBackward Hooks execution\n------------------------\n\nThis section will discuss when different hooks fire or don't fire.\nThen it will discuss the order in which they are fired.\nThe hooks that will be covered are: hooks registered to Tensor via\n:meth:`torch.tensor.register_hook`,\npost-hooks registered to Node via :meth:`torch.autograd.graph.Node.register_hook`, and\npre-hooks registered to Node via :meth:`torch.autograd.graph.Node.register_prehook`.\n\nWhether a particular hook will be fired\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nHooks registered to a Tensor via :meth:`torch.tensor.register_hook`\nare executed when gradients are being computed for that Tensor. (Note that this does not require\nthe Tensor's grad_fn to be executed. For example, if the Tensor is passed\nas part of the ``inputs`` argument to :func:`torch.autograd.grad`,\nthe Tensor's grad_fn may not be executed, but the hook register to that Tensor will always be executed.)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":869,"to":888}}}}],["917",{"pageContent":"Hooks registered to :class:`torch.autograd.graph.Node` using\n:meth:`torch.autograd.graph.Node.register_hook` or\n:meth:`torch.autograd.graph.Node.register_prehook` are only fired if\nthe Node it was registered to is executed.\n\nWhether a particular Node is executed may depend on whether the backward pass was called with\n:func:`torch.autograd.grad` or :func:`torch.autograd.backward`.\nSpecifically, you should be aware of these differences when you register a hook on a\nNode corresponding to a Tensor that you are passing to :func:`torch.autograd.grad` or\n:func:`torch.autograd.backward` as part of the ``inputs`` argument.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":890,"to":899}}}}],["918",{"pageContent":"If you are using :func:`torch.autograd.backward`, all of the above mentioned hooks will be executed,\nwhether or not you specified the ``inputs`` argument. This is because `.backward()` executes all\nNodes, even if they correspond to a Tensor specified as an input.\n(Note that the execution of this additional Node corresponding to Tensors passed as  ``inputs``\nis usually unnecessary, but done anyway. This behavior is subject to change;\nyou should not depend on it.)\n\nOn the other hand, if you are using :func:`torch.autograd.grad`, the backward hooks registered\nto Nodes that correspond to the Tensors passed to ``input`` may not be executed, because\nthose Nodes will not be executed unless there is another input that depends on the gradient\nresult of this Node.\n\nThe order in which the different hooks are fired\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":901,"to":914}}}}],["919",{"pageContent":"The order in which the different hooks are fired\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe order in which things happen are:\n1. hooks registered to Tensor are executed\n2. pre-hook registered to Node are executed (if Node is executed).\n3. The ``.grad`` field is updated for Tensors that retain_grad\n4. Node is executed (subject to rules above)\n5. post-hook registered to Node are executed (if Node is executed)\n\nIf multiple hooks of the same type are registered on the same Tensor or Node\nthey are executed in the order in which they are registered.\nHooks that are executed later can observe the modifications to the gradient made by\nearlier hooks.\n\nSpecial hooks\n^^^^^^^^^^^^^\n\n:func:`torch.autograd.graph.register_multi_grad_hook` is implemented using hooks registered\nto Tensors. Each individual Tensor hook is fired following the Tensor hook ordering\ndefined above and the registered multi-grad hook is called when the last Tensor gradient\nis computed.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":914,"to":935}}}}],["920",{"pageContent":":meth:`torch.nn.modules.module.register_module_full_backward_hook` is implemented using hooks\nregistered to Node. As the forward is computed, hooks are registered to grad_fn corresponding\nto the inputs and outputs of the module. Because a module may take multiple inputs and return\nmultiple outputs, a dummy custom autograd Function is first applied to the inputs of the module\nbefore forward and the outputs of the module before the output of forward is returned to ensure\nthat those Tensors share a single grad_fn, which we can then attach our hooks to.\n\nBehavior of Tensor hooks when Tensor is modified in-place\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nUsually hooks registered to a Tensor receive the gradient of the outputs with respect to that\nTensor, where the value of the Tensor is taken to be its value at the time backward is computed.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":937,"to":948}}}}],["921",{"pageContent":"Usually hooks registered to a Tensor receive the gradient of the outputs with respect to that\nTensor, where the value of the Tensor is taken to be its value at the time backward is computed.\n\nHowever, if you register hooks to a Tensor, and then modify that Tensor in-place, hooks\nregistered before in-place modification similarly receive gradients of the outputs with\nrespect to the Tensor, but the value of the Tensor is taken to be its value before\nin-place modification.\n\nIf you prefer the behavior in the former case,\nyou should register them to the Tensor after all in-place modifications to it have been made.\nFor example:\n\n.. code::\n\n    t = torch.tensor(1., requires_grad=True).sin()\n    t.cos_()\n    t.register_hook(fn)\n    t.backward()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":948,"to":965}}}}],["922",{"pageContent":".. code::\n\n    t = torch.tensor(1., requires_grad=True).sin()\n    t.cos_()\n    t.register_hook(fn)\n    t.backward()\n\nFurthermore, it can be helpful to know that under the hood,\nwhen hooks are registered to a Tensor, they actually become permanently bound to the grad_fn\nof that Tensor, so if that Tensor is then modified in-place,\neven though the Tensor now has a new grad_fn, hooks registered before it was\nmodified in-place will continue to be associated with the old grad_fn, e.g. they will\nfire when that Tensor's old grad_fn is reached in the graph by the autograd engine.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/autograd.rst","loc":{"lines":{"from":965,"to":977}}}}],["923",{"pageContent":".. _broadcasting-semantics:\n\nBroadcasting semantics\n======================\n\nMany PyTorch operations support NumPy's broadcasting semantics.\nSee https://numpy.org/doc/stable/user/basics.broadcasting.html for details.\n\nIn short, if a PyTorch operation supports broadcast, then its Tensor arguments can be\nautomatically expanded to be of equal sizes (without making copies of the data).\n\nGeneral semantics\n-----------------\nTwo tensors are \"broadcastable\" if the following rules hold:\n\n- Each tensor has at least one dimension.\n- When iterating over the dimension sizes, starting at the trailing dimension,\n  the dimension sizes must either be equal, one of them is 1, or one of them\n  does not exist.\n\nFor Example::\n\n    >>> x=torch.empty(5,7,3)\n    >>> y=torch.empty(5,7,3)\n    # same shapes are always broadcastable (i.e. the above rules always hold)\n\n    >>> x=torch.empty((0,))\n    >>> y=torch.empty(2,2)\n    # x and y are not broadcastable, because x does not have at least 1 dimension","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/broadcasting.rst","loc":{"lines":{"from":1,"to":29}}}}],["924",{"pageContent":">>> x=torch.empty((0,))\n    >>> y=torch.empty(2,2)\n    # x and y are not broadcastable, because x does not have at least 1 dimension\n\n    # can line up trailing dimensions\n    >>> x=torch.empty(5,3,4,1)\n    >>> y=torch.empty(  3,1,1)\n    # x and y are broadcastable.\n    # 1st trailing dimension: both have size 1\n    # 2nd trailing dimension: y has size 1\n    # 3rd trailing dimension: x size == y size\n    # 4th trailing dimension: y dimension doesn't exist\n\n    # but:\n    >>> x=torch.empty(5,2,4,1)\n    >>> y=torch.empty(  3,1,1)\n    # x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3\n\nIf two tensors :attr:`x`, :attr:`y` are \"broadcastable\", the resulting tensor size\nis calculated as follows:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/broadcasting.rst","loc":{"lines":{"from":29,"to":48}}}}],["925",{"pageContent":"If two tensors :attr:`x`, :attr:`y` are \"broadcastable\", the resulting tensor size\nis calculated as follows:\n\n- If the number of dimensions of :attr:`x` and :attr:`y` are not equal, prepend 1\n  to the dimensions of the tensor with fewer dimensions to make them equal length.\n- Then, for each dimension size, the resulting dimension size is the max of the sizes of\n  :attr:`x` and :attr:`y` along that dimension.\n\nFor Example::\n\n    # can line up trailing dimensions to make reading easier\n    >>> x=torch.empty(5,1,4,1)\n    >>> y=torch.empty(  3,1,1)\n    >>> (x+y).size()\n    torch.Size([5, 3, 4, 1])\n\n    # but not necessary:\n    >>> x=torch.empty(1)\n    >>> y=torch.empty(3,1,7)\n    >>> (x+y).size()\n    torch.Size([3, 1, 7])\n\n    >>> x=torch.empty(5,2,4,1)\n    >>> y=torch.empty(3,1,1)\n    >>> (x+y).size()\n    RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/broadcasting.rst","loc":{"lines":{"from":48,"to":73}}}}],["926",{"pageContent":">>> x=torch.empty(5,2,4,1)\n    >>> y=torch.empty(3,1,1)\n    >>> (x+y).size()\n    RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\nIn-place semantics\n------------------\nOne complication is that in-place operations do not allow the in-place tensor to change shape\nas a result of the broadcast.\n\nFor Example::\n\n    >>> x=torch.empty(5,3,4,1)\n    >>> y=torch.empty(3,1,1)\n    >>> (x.add_(y)).size()\n    torch.Size([5, 3, 4, 1])\n\n    # but:\n    >>> x=torch.empty(1,3,1)\n    >>> y=torch.empty(3,1,7)\n    >>> (x.add_(y)).size()\n    RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/broadcasting.rst","loc":{"lines":{"from":73,"to":94}}}}],["927",{"pageContent":"Backwards compatibility\n-----------------------\nPrior versions of PyTorch allowed certain pointwise functions to execute on tensors with different shapes,\nas long as the number of elements in each tensor was equal.  The pointwise operation would then be carried\nout by viewing each tensor as 1-dimensional.  PyTorch now supports broadcasting and the \"1-dimensional\"\npointwise behavior is considered deprecated and will generate a Python warning in cases where tensors are\nnot broadcastable, but have the same number of elements.\n\nNote that the introduction of broadcasting can cause backwards incompatible changes in the case where\ntwo tensors do not have the same shape, but are broadcastable and have the same number of elements.\nFor Example::\n\n    >>> torch.add(torch.ones(4,1), torch.randn(4))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/broadcasting.rst","loc":{"lines":{"from":96,"to":108}}}}],["928",{"pageContent":">>> torch.add(torch.ones(4,1), torch.randn(4))\n\nwould previously produce a Tensor with size: torch.Size([4,1]), but now produces a Tensor with size: torch.Size([4,4]).\nIn order to help identify cases in your code where backwards incompatibilities introduced by broadcasting may exist,\nyou may set `torch.utils.backcompat.broadcast_warning.enabled` to `True`, which will generate a python warning\nin such cases.\n\nFor Example::\n\n    >>> torch.utils.backcompat.broadcast_warning.enabled=True\n    >>> torch.add(torch.ones(4,1), torch.ones(4))\n    __main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.\n    Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/broadcasting.rst","loc":{"lines":{"from":108,"to":120}}}}],["929",{"pageContent":".. _cpu-threading-torchscript-inference:\n\nCPU threading and TorchScript inference\n=================================================\n\nPyTorch allows using multiple CPU threads during TorchScript model inference.\nThe following figure shows different levels of parallelism one would find in a\ntypical application:\n\n.. image:: cpu_threading_torchscript_inference.svg\n   :width: 75%\n\nOne or more inference threads execute a model's forward pass on the given inputs.\nEach inference thread invokes a JIT interpreter that executes the ops\nof a model inline, one by one. A model can utilize a ``fork`` TorchScript\nprimitive to launch an asynchronous task. Forking several operations at once\nresults in a task that is executed in parallel. The ``fork`` operator returns a\n``Future`` object which can be used to synchronize on later, for example:\n\n.. code-block:: python\n\n    @torch.jit.script\n    def compute_z(x):\n        return torch.mm(x, self.w_z)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":1,"to":24}}}}],["930",{"pageContent":".. code-block:: python\n\n    @torch.jit.script\n    def compute_z(x):\n        return torch.mm(x, self.w_z)\n\n    @torch.jit.script\n    def forward(x):\n        # launch compute_z asynchronously:\n        fut = torch.jit._fork(compute_z, x)\n        # execute the next operation in parallel to compute_z:\n        y = torch.mm(x, self.w_y)\n        # wait for the result of compute_z:\n        z = torch.jit._wait(fut)\n        return y + z\n\n\nPyTorch uses a single thread pool for the inter-op parallelism, this thread pool\nis shared by all inference tasks that are forked within the application process.\n\nIn addition to the inter-op parallelism, PyTorch can also utilize multiple threads\nwithin the ops (`intra-op parallelism`). This can be useful in many cases,\nincluding element-wise ops on large tensors, convolutions, GEMMs, embedding\nlookups and others.\n\n\nBuild options\n-------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":24,"to":51}}}}],["931",{"pageContent":"Build options\n-------------\n\nPyTorch uses an internal ATen library to implement ops. In addition to that,\nPyTorch can also be built with support of external libraries, such as MKL_ and MKL-DNN_,\nto speed up computations on CPU.\n\nATen, MKL and MKL-DNN support intra-op parallelism and depend on the\nfollowing parallelization libraries to implement it:\n\n* OpenMP_ - a standard (and a library, usually shipped with a compiler), widely used in external libraries;\n* TBB_ - a newer parallelization library optimized for task-based parallelism and concurrent environments.\n\nOpenMP historically has been used by a large number of libraries. It is known\nfor a relative ease of use and support for loop-based parallelism and other primitives.\n\nTBB is used to a lesser extent in external libraries, but, at the same time,\nis optimized for the concurrent environments. PyTorch's TBB backend guarantees that\nthere's a separate, single, per-process intra-op thread pool used by all of the\nops running in the application.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":51,"to":70}}}}],["932",{"pageContent":"Depending of the use case, one might find one or another parallelization\nlibrary a better choice in their application.\n\nPyTorch allows selecting of the parallelization backend used by ATen and other\nlibraries at the build time with the following build options:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":72,"to":76}}}}],["933",{"pageContent":"+------------+------------------------+-----------------------------+----------------------------------------+\n| Library    | Build Option           | Values                      | Notes                                  |\n+============+========================+=============================+========================================+\n| ATen       | ``ATEN_THREADING``     | ``OMP`` (default), ``TBB``  |                                        |\n+------------+------------------------+-----------------------------+----------------------------------------+\n| MKL        | ``MKL_THREADING``      | (same)                      | To enable MKL use ``BLAS=MKL``         |\n+------------+------------------------+-----------------------------+----------------------------------------+\n| MKL-DNN    | ``MKLDNN_CPU_RUNTIME`` | (same)                      | To enable MKL-DNN use ``USE_MKLDNN=1`` |\n+------------+------------------------+-----------------------------+----------------------------------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":78,"to":86}}}}],["934",{"pageContent":"It is recommended not to mix OpenMP and TBB within one build.\n\nAny of the ``TBB`` values above require ``USE_TBB=1`` build setting (default: OFF).\nA separate setting ``USE_OPENMP=1`` (default: ON) is required for OpenMP parallelism.\n\nRuntime API\n-----------\n\nThe following API is used to control thread settings:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":88,"to":96}}}}],["935",{"pageContent":"+------------------------+-----------------------------------------------------------+---------------------------------------------------------+\n| Type of parallelism    | Settings                                                  | Notes                                                   |\n+========================+===========================================================+=========================================================+\n| Inter-op parallelism   | ``at::set_num_interop_threads``,                          | Default number of threads: number of CPU cores.         |\n|                        | ``at::get_num_interop_threads`` (C++)                     |                                                         |\n|                        |                                                           |                                                         |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":98,"to":103}}}}],["936",{"pageContent":"|                        |                                                           |                                                         |\n|                        | ``set_num_interop_threads``,                              |                                                         |\n|                        | ``get_num_interop_threads`` (Python, :mod:`torch` module) |                                                         |\n+------------------------+-----------------------------------------------------------+                                                         |\n| Intra-op parallelism   | ``at::set_num_threads``,                                  |                                                         |\n|                        | ``at::get_num_threads`` (C++)                             |                                                         |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":103,"to":108}}}}],["937",{"pageContent":"|                        | ``at::get_num_threads`` (C++)                             |                                                         |\n|                        | ``set_num_threads``,                                      |                                                         |\n|                        | ``get_num_threads`` (Python, :mod:`torch` module)         |                                                         |\n|                        |                                                           |                                                         |\n|                        | Environment variables:                                    |                                                         |\n|                        | ``OMP_NUM_THREADS`` and ``MKL_NUM_THREADS``               |                                                         |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":108,"to":113}}}}],["938",{"pageContent":"|                        | ``OMP_NUM_THREADS`` and ``MKL_NUM_THREADS``               |                                                         |\n+------------------------+-----------------------------------------------------------+---------------------------------------------------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":113,"to":114}}}}],["939",{"pageContent":"For the intra-op parallelism settings, ``at::set_num_threads``, ``torch.set_num_threads`` always take precedence\nover environment variables, ``MKL_NUM_THREADS`` variable takes precedence over ``OMP_NUM_THREADS``.\n\nTuning the number of threads\n----------------------------\n\nThe following simple script shows how a runtime of matrix multiplication changes with the number of threads:\n\n.. code-block:: python\n\n    import timeit\n    runtimes = []\n    threads = [1] + [t for t in range(2, 49, 2)]\n    for t in threads:\n        torch.set_num_threads(t)\n        r = timeit.timeit(setup = \"import torch; x = torch.randn(1024, 1024); y = torch.randn(1024, 1024)\", stmt=\"torch.mm(x, y)\", number=100)\n        runtimes.append(r)\n    # ... plotting (threads, runtimes) ...\n\nRunning the script on a system with 24 physical CPU cores (Xeon E5-2680, MKL and OpenMP based build) results in the following runtimes:\n\n.. image:: cpu_threading_runtimes.svg\n   :width: 75%","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":116,"to":138}}}}],["940",{"pageContent":"Running the script on a system with 24 physical CPU cores (Xeon E5-2680, MKL and OpenMP based build) results in the following runtimes:\n\n.. image:: cpu_threading_runtimes.svg\n   :width: 75%\n\nThe following considerations should be taken into account when tuning the number of intra- and inter-op threads:\n\n* When choosing the number of threads one needs to avoid `oversubscription` (using too many threads, leads to performance degradation). For example, in an application that uses a large application thread pool or heavily relies on\n  inter-op parallelism, one might find disabling intra-op parallelism as a possible option (i.e. by calling ``set_num_threads(1)``);","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":138,"to":146}}}}],["941",{"pageContent":"* In a typical application one might encounter a trade off between `latency` (time spent on processing an inference request) and `throughput` (amount of work done per unit of time). Tuning the number of threads can be a useful\n  tool to adjust this trade off in one way or another. For example, in latency critical applications one might want to increase the number of intra-op threads to process each request as fast as possible. At the same time, parallel implementations\n  of ops may add an extra overhead that increases amount work done per single request and thus reduces the overall throughput.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":148,"to":150}}}}],["942",{"pageContent":".. warning::\n    OpenMP does not guarantee that a single per-process intra-op thread\n    pool is going to be used in the application. On the contrary, two different application or inter-op\n    threads may use different OpenMP thread pools for intra-op work.\n    This might result in a large number of threads used by the application.\n    Extra care in tuning the number of threads is needed to avoid\n    oversubscription in multi-threaded applications in OpenMP case.\n\n.. note::\n    Pre-built PyTorch releases are compiled with OpenMP support.\n\n.. note::\n    ``parallel_info`` utility prints information about thread settings and can be used for debugging.\n    Similar output can be also obtained in Python with ``torch.__config__.parallel_info()`` call.\n\n.. _OpenMP: https://www.openmp.org/\n.. _TBB: https://github.com/intel/tbb\n.. _MKL: https://software.intel.com/en-us/mkl\n.. _MKL-DNN: https://github.com/intel/mkl-dnn","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cpu_threading_torchscript_inference.rst","loc":{"lines":{"from":152,"to":170}}}}],["943",{"pageContent":".. _cuda-semantics:\n\nCUDA semantics\n==============\n\n:mod:`torch.cuda` is used to set up and run CUDA operations. It keeps track of\nthe currently selected GPU, and all CUDA tensors you allocate will by default be\ncreated on that device. The selected device can be changed with a\n:any:`torch.cuda.device` context manager.\n\nHowever, once a tensor is allocated, you can do operations on it irrespective\nof the selected device, and the results will be always placed on the same\ndevice as the tensor.\n\nCross-GPU operations are not allowed by default, with the exception of\n:meth:`~torch.Tensor.copy_` and other methods with copy-like functionality\nsuch as :meth:`~torch.Tensor.to` and :meth:`~torch.Tensor.cuda`.\nUnless you enable peer-to-peer memory access, any attempts to launch ops on\ntensors spread across different devices will raise an error.\n\nBelow you can find a small example showcasing this::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1,"to":21}}}}],["944",{"pageContent":"Below you can find a small example showcasing this::\n\n    cuda = torch.device('cuda')     # Default CUDA device\n    cuda0 = torch.device('cuda:0')\n    cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\n    x = torch.tensor([1., 2.], device=cuda0)\n    # x.device is device(type='cuda', index=0)\n    y = torch.tensor([1., 2.]).cuda()\n    # y.device is device(type='cuda', index=0)\n\n    with torch.cuda.device(1):\n        # allocates a tensor on GPU 1\n        a = torch.tensor([1., 2.], device=cuda)\n\n        # transfers a tensor from CPU to GPU 1\n        b = torch.tensor([1., 2.]).cuda()\n        # a.device and b.device are device(type='cuda', index=1)\n\n        # You can also use ``Tensor.to`` to transfer a tensor:\n        b2 = torch.tensor([1., 2.]).to(device=cuda)\n        # b.device and b2.device are device(type='cuda', index=1)\n\n        c = a + b\n        # c.device is device(type='cuda', index=1)\n\n        z = x + y\n        # z.device is device(type='cuda', index=0)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":21,"to":48}}}}],["945",{"pageContent":"c = a + b\n        # c.device is device(type='cuda', index=1)\n\n        z = x + y\n        # z.device is device(type='cuda', index=0)\n\n        # even within a context, you can specify the device\n        # (or give a GPU index to the .cuda call)\n        d = torch.randn(2, device=cuda2)\n        e = torch.randn(2).to(cuda2)\n        f = torch.randn(2).cuda(cuda2)\n        # d.device, e.device, and f.device are all device(type='cuda', index=2)\n\n.. _tf32_on_ampere:\n\nTensorFloat-32(TF32) on Ampere devices\n--------------------------------------\n\nStarting in PyTorch 1.7, there is a new flag called `allow_tf32`. This flag\ndefaults to True in PyTorch 1.7 to PyTorch 1.11, and False in PyTorch 1.12 and later.\nThis flag controls whether PyTorch is allowed to use the TensorFloat32 (TF32) tensor cores,\navailable on new NVIDIA GPUs since Ampere, internally to compute matmul (matrix multiplies\nand batched matrix multiplies) and convolutions.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":48,"to":70}}}}],["946",{"pageContent":"TF32 tensor cores are designed to achieve better performance on matmul and convolutions on\n`torch.float32` tensors by rounding input data to have 10 bits of mantissa, and accumulating\nresults with FP32 precision, maintaining FP32 dynamic range.\n\nmatmuls and convolutions are controlled separately, and their corresponding flags can be accessed at:\n\n.. code:: python\n\n  # The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n  # in PyTorch 1.12 and later.\n  torch.backends.cuda.matmul.allow_tf32 = True\n\n  # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n  torch.backends.cudnn.allow_tf32 = True\n\nNote that besides matmuls and convolutions themselves, functions and nn modules that internally uses\nmatmuls or convolutions are also affected. These include `nn.Linear`, `nn.Conv*`, cdist, tensordot,\naffine grid and grid sample, adaptive log softmax, GRU and LSTM.\n\nTo get an idea of the precision and speed, see the example code below:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":72,"to":91}}}}],["947",{"pageContent":"To get an idea of the precision and speed, see the example code below:\n\n.. code:: python\n\n  a_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')\n  b_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')\n  ab_full = a_full @ b_full\n  mean = ab_full.abs().mean()  # 80.7277\n\n  a = a_full.float()\n  b = b_full.float()\n\n  # Do matmul at TF32 mode.\n  torch.backends.cuda.matmul.allow_tf32 = True\n  ab_tf32 = a @ b  # takes 0.016s on GA100\n  error = (ab_tf32 - ab_full).abs().max()  # 0.1747\n  relative_error = error / mean  # 0.0022\n\n  # Do matmul with TF32 disabled.\n  torch.backends.cuda.matmul.allow_tf32 = False\n  ab_fp32 = a @ b  # takes 0.11s on GA100\n  error = (ab_fp32 - ab_full).abs().max()  # 0.0031\n  relative_error = error / mean  # 0.000039","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":91,"to":113}}}}],["948",{"pageContent":"From the above example, we can see that with TF32 enabled, the speed is ~7x faster, relative error\ncompared to double precision is approximately 2 orders of magnitude larger.  If full FP32 precision\nis needed, users can disable TF32 by:\n\n.. code:: python\n\n  torch.backends.cuda.matmul.allow_tf32 = False\n  torch.backends.cudnn.allow_tf32 = False\n\nTo toggle the TF32 flags off in C++, you can do\n\n.. code:: C++\n\n  at::globalContext().setAllowTF32CuBLAS(false);\n  at::globalContext().setAllowTF32CuDNN(false);\n\nFor more information about TF32, see:\n\n- `TensorFloat-32`_\n- `CUDA 11`_\n- `Ampere architecture`_\n\n.. _TensorFloat-32: https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/\n.. _CUDA 11: https://devblogs.nvidia.com/cuda-11-features-revealed/\n.. _Ampere architecture: https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/\n\n.. _fp16reducedprecision:\n\nReduced Precision Reduction in FP16 GEMMs\n-----------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":115,"to":144}}}}],["949",{"pageContent":".. _fp16reducedprecision:\n\nReduced Precision Reduction in FP16 GEMMs\n-----------------------------------------\n\nfp16 GEMMs are potentially done with some intermediate reduced precision reductions (e.g., in fp16 rather than fp32). These selective reductions in precision can allow for higher performance on certain workloads (particularly those with a large `k` dimension) and GPU architectures at the cost of numerical precision and potential for overflow.\n\nSome example benchmark data on V100:\n\n.. code::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":144,"to":153}}}}],["950",{"pageContent":"[--------------------------- bench_gemm_transformer --------------------------]\n        [  m ,  k  ,  n  ]    |  allow_fp16_reduc=True  |  allow_fp16_reduc=False\n  1 threads: --------------------------------------------------------------------\n        [4096, 4048, 4096]    |           1634.6        |           1639.8\n        [4096, 4056, 4096]    |           1670.8        |           1661.9\n        [4096, 4080, 4096]    |           1664.2        |           1658.3\n        [4096, 4096, 4096]    |           1639.4        |           1651.0\n        [4096, 4104, 4096]    |           1677.4        |           1674.9\n        [4096, 4128, 4096]    |           1655.7        |           1646.0\n        [4096, 4144, 4096]    |           1796.8        |           2519.6\n        [4096, 5096, 4096]    |           2094.6        |           3190.0\n        [4096, 5104, 4096]    |           2144.0        |           2663.5\n        [4096, 5112, 4096]    |           2149.1        |           2766.9","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":155,"to":167}}}}],["951",{"pageContent":"[4096, 5104, 4096]    |           2144.0        |           2663.5\n        [4096, 5112, 4096]    |           2149.1        |           2766.9\n        [4096, 5120, 4096]    |           2142.8        |           2631.0\n        [4096, 9728, 4096]    |           3875.1        |           5779.8\n        [4096, 16384, 4096]   |           6182.9        |           9656.5\n  (times in microseconds).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":167,"to":172}}}}],["952",{"pageContent":"If full precision reductions are needed, users can disable reduced precision reductions in fp16 GEMMs with:\n\n.. code:: python\n\n  torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\nTo toggle the reduced precision reduction flags in C++, one can do\n\n.. code:: C++\n\n  at::globalContext().setAllowFP16ReductionCuBLAS(false);\n\n.. _bf16reducedprecision:\n\nReduced Precision Reduction in BF16 GEMMs\n-----------------------------------------\n\nA similar flag (as above) exists for BFloat16 GEMMs. Note that this switch is\nset to `False` by default for BF16 as we have observed numerical instability in\nPyTorch CI tests (e.g., test/test_matmul_cuda.py).\n\nIf reduced precision reductions are not desired, users can disable reduced\nprecision reductions in bf16 GEMMs with:\n\n.. code:: python\n\n  torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n\nTo toggle the reduced precision reduction flags in C++, one can do\n\n.. code:: C++\n\n  at::globalContext().setAllowBF16ReductionCuBLAS(true);","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":174,"to":206}}}}],["953",{"pageContent":"To toggle the reduced precision reduction flags in C++, one can do\n\n.. code:: C++\n\n  at::globalContext().setAllowBF16ReductionCuBLAS(true);\n\nAsynchronous execution\n----------------------\n\nBy default, GPU operations are asynchronous.  When you call a function that\nuses the GPU, the operations are *enqueued* to the particular device, but not\nnecessarily executed until later.  This allows us to execute more computations\nin parallel, including operations on CPU or other GPUs.\n\nIn general, the effect of asynchronous computation is invisible to the caller,\nbecause (1) each device executes operations in the order they are queued, and\n(2) PyTorch automatically performs necessary synchronization when copying data\nbetween CPU and GPU or between two GPUs.  Hence, computation will proceed as if\nevery operation was executed synchronously.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":206,"to":224}}}}],["954",{"pageContent":"You can force synchronous computation by setting environment variable\n``CUDA_LAUNCH_BLOCKING=1``.  This can be handy when an error occurs on the GPU.\n(With asynchronous execution, such an error isn't reported until after the\noperation is actually executed, so the stack trace does not show where it was\nrequested.)\n\nA consequence of the asynchronous computation is that time measurements without\nsynchronizations are not accurate. To get precise measurements, one should either\ncall :func:`torch.cuda.synchronize()` before measuring, or use :class:`torch.cuda.Event`\nto record times as following::\n\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n    start_event.record()\n\n    # Run some things here\n\n    end_event.record()\n    torch.cuda.synchronize()  # Wait for the events to be recorded!\n    elapsed_time_ms = start_event.elapsed_time(end_event)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":226,"to":245}}}}],["955",{"pageContent":"# Run some things here\n\n    end_event.record()\n    torch.cuda.synchronize()  # Wait for the events to be recorded!\n    elapsed_time_ms = start_event.elapsed_time(end_event)\n\nAs an exception, several functions such as :meth:`~torch.Tensor.to` and\n:meth:`~torch.Tensor.copy_` admit an explicit :attr:`non_blocking` argument,\nwhich lets the caller bypass synchronization when it is unnecessary.\nAnother exception is CUDA streams, explained below.\n\nCUDA streams\n^^^^^^^^^^^^\n\nA `CUDA stream`_ is a linear sequence of execution that belongs to a specific\ndevice.  You normally do not need to create one explicitly: by default, each\ndevice uses its own \"default\" stream.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":245,"to":261}}}}],["956",{"pageContent":"A `CUDA stream`_ is a linear sequence of execution that belongs to a specific\ndevice.  You normally do not need to create one explicitly: by default, each\ndevice uses its own \"default\" stream.\n\nOperations inside each stream are serialized in the order they are created,\nbut operations from different streams can execute concurrently in any\nrelative order, unless explicit synchronization functions (such as\n:meth:`~torch.cuda.synchronize` or :meth:`~torch.cuda.Stream.wait_stream`) are\nused.  For example, the following code is incorrect::\n\n    cuda = torch.device('cuda')\n    s = torch.cuda.Stream()  # Create a new stream.\n    A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\n    with torch.cuda.stream(s):\n        # sum() may start execution before normal_() finishes!\n        B = torch.sum(A)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":261,"to":276}}}}],["957",{"pageContent":"When the \"current stream\" is the default stream, PyTorch automatically performs\nnecessary synchronization when data is moved around, as explained above.\nHowever, when using non-default streams, it is the user's responsibility to\nensure proper synchronization.\n\n.. _bwd-cuda-stream-semantics:\n\nStream semantics of backward passes\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nEach backward CUDA op runs on the same stream that was used for its corresponding forward op.\nIf your forward pass runs independent ops in parallel on different streams,\nthis helps the backward pass exploit that same parallelism.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":278,"to":290}}}}],["958",{"pageContent":"The stream semantics of a backward call with respect to surrounding ops are the same\nas for any other call. The backward pass inserts internal syncs to ensure this even when\nbackward ops run on multiple streams as described in the previous paragraph.\nMore concretely, when calling\n:func:`autograd.backward<torch.autograd.backward>`,\n:func:`autograd.grad<torch.autograd.grad>`, or\n:meth:`tensor.backward<torch.Tensor.backward>`,\nand optionally supplying CUDA tensor(s) as the  initial gradient(s) (e.g.,\n:func:`autograd.backward(..., grad_tensors=initial_grads)<torch.autograd.backward>`,\n:func:`autograd.grad(..., grad_outputs=initial_grads)<torch.autograd.grad>`, or\n:meth:`tensor.backward(..., gradient=initial_grad)<torch.Tensor.backward>`),\nthe acts of\n\n1. optionally populating initial gradient(s),\n2. invoking the backward pass, and\n3. using the gradients\n\nhave the same stream-semantics relationship as any group of ops::\n\n    s = torch.cuda.Stream()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":292,"to":311}}}}],["959",{"pageContent":"1. optionally populating initial gradient(s),\n2. invoking the backward pass, and\n3. using the gradients\n\nhave the same stream-semantics relationship as any group of ops::\n\n    s = torch.cuda.Stream()\n\n    # Safe, grads are used in the same stream context as backward()\n    with torch.cuda.stream(s):\n        loss.backward()\n        use grads\n\n    # Unsafe\n    with torch.cuda.stream(s):\n        loss.backward()\n    use grads\n\n    # Safe, with synchronization\n    with torch.cuda.stream(s):\n        loss.backward()\n    torch.cuda.current_stream().wait_stream(s)\n    use grads\n\n    # Safe, populating initial grad and invoking backward are in the same stream context\n    with torch.cuda.stream(s):\n        loss.backward(gradient=torch.ones_like(loss))\n\n    # Unsafe, populating initial_grad and invoking backward are in different stream contexts,\n    # without synchronization\n    initial_grad = torch.ones_like(loss)\n    with torch.cuda.stream(s):\n        loss.backward(gradient=initial_grad)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":311,"to":343}}}}],["960",{"pageContent":"# Safe, with synchronization\n    initial_grad = torch.ones_like(loss)\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        initial_grad.record_stream(s)\n        loss.backward(gradient=initial_grad)\n\nBC note: Using grads on the default stream\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn prior versions of PyTorch (1.9 and earlier), the autograd engine always synced\nthe default stream with all backward ops, so the following pattern::\n\n    with torch.cuda.stream(s):\n        loss.backward()\n    use grads\n\nwas safe as long as ``use grads`` happened on the default stream.\nIn present PyTorch, that pattern is no longer safe. If ``backward()``\nand ``use grads`` are in different stream contexts, you must sync the streams::\n\n    with torch.cuda.stream(s):\n        loss.backward()\n    torch.cuda.current_stream().wait_stream(s)\n    use grads\n\neven if ``use grads`` is on the default stream.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":345,"to":371}}}}],["961",{"pageContent":"with torch.cuda.stream(s):\n        loss.backward()\n    torch.cuda.current_stream().wait_stream(s)\n    use grads\n\neven if ``use grads`` is on the default stream.\n\n.. _CUDA stream: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams\n\n.. _cuda-memory-management:\n\nMemory management\n-----------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":371,"to":383}}}}],["962",{"pageContent":".. _CUDA stream: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams\n\n.. _cuda-memory-management:\n\nMemory management\n-----------------\n\nPyTorch uses a caching memory allocator to speed up memory allocations. This\nallows fast memory deallocation without device synchronizations. However, the\nunused memory managed by the allocator will still show as if used in\n``nvidia-smi``. You can use :meth:`~torch.cuda.memory_allocated` and\n:meth:`~torch.cuda.max_memory_allocated` to monitor memory occupied by\ntensors, and use :meth:`~torch.cuda.memory_reserved` and\n:meth:`~torch.cuda.max_memory_reserved` to monitor the total amount of memory\nmanaged by the caching allocator. Calling :meth:`~torch.cuda.empty_cache`\nreleases all **unused** cached memory from PyTorch so that those can be used\nby other GPU applications. However, the occupied GPU memory by tensors will not\nbe freed so it can not increase the amount of GPU memory available for PyTorch.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":383,"to":400}}}}],["963",{"pageContent":"For more advanced users, we offer more comprehensive memory benchmarking via\n:meth:`~torch.cuda.memory_stats`. We also offer the capability to capture a\ncomplete snapshot of the memory allocator state via\n:meth:`~torch.cuda.memory_snapshot`, which can help you understand the\nunderlying allocation patterns produced by your code.\n\n.. _cuda-memory-envvars:\n\nEnvironment variables\n^^^^^^^^^^^^^^^^^^^^^\n\nUse of a caching allocator can interfere with memory checking tools such as\n``cuda-memcheck``.  To debug memory errors using ``cuda-memcheck``, set\n``PYTORCH_NO_CUDA_MEMORY_CACHING=1`` in your environment to disable caching.\n\nThe behavior of the caching allocator can be controlled via the environment variable\n``PYTORCH_CUDA_ALLOC_CONF``.\nThe format is ``PYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>...``\nAvailable options:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":402,"to":420}}}}],["964",{"pageContent":"* ``backend`` allows selecting the underlying allocator implementation.\n  Currently, valid options are ``native``, which uses PyTorch's native\n  implementation, and ``cudaMallocAsync``, which uses\n  `CUDA's built-in asynchronous allocator`_.\n  ``cudaMallocAsync`` requires CUDA 11.4 or newer. The default is ``native``.\n  ``backend`` applies to all devices used by the process, and can't be\n  specified on a per-device basis.\n* ``max_split_size_mb`` prevents the native allocator\n  from splitting blocks larger than this size (in MB). This can reduce\n  fragmentation and may allow some borderline workloads to complete without\n  running out of memory. Performance cost can range from 'zero' to 'substantial'\n  depending on allocation patterns.  Default value is unlimited, i.e. all blocks\n  can be split. The\n  :meth:`~torch.cuda.memory_stats` and\n  :meth:`~torch.cuda.memory_summary` methods are useful for tuning.  This\n  option should be used as a last resort for a workload that is aborting","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":422,"to":437}}}}],["965",{"pageContent":":meth:`~torch.cuda.memory_stats` and\n  :meth:`~torch.cuda.memory_summary` methods are useful for tuning.  This\n  option should be used as a last resort for a workload that is aborting\n  due to 'out of memory' and showing a large amount of inactive split blocks.\n  ``max_split_size_mb`` is only meaningful with ``backend:native``.\n  With ``backend:cudaMallocAsync``, ``max_split_size_mb`` is ignored.\n* ``roundup_power2_divisions`` helps with rounding the requested allocation\n  size to nearest power-2 division and making better use of the blocks. In\n  the native CUDACachingAllocator, the sizes are rounded up in multiple\n  of blocks size of 512, so this works fine for smaller sizes. However, this\n  can be inefficient for large near-by allocations as each will go to different\n  size of blocks and re-use of those blocks are minimized. This might create\n  lots of unused blocks and will waste GPU memory capacity. This option enables","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":437,"to":449}}}}],["966",{"pageContent":"size of blocks and re-use of those blocks are minimized. This might create\n  lots of unused blocks and will waste GPU memory capacity. This option enables\n  the rounding of allocation size to nearest power-2 division. For example, if\n  we need to round-up size of 1200 and if number of divisions is 4,\n  the size 1200 lies between 1024 and 2048 and if we do 4 divisions between\n  them, the values are 1024, 1280, 1536, and 1792. So, allocation size of 1200\n  will be rounded to 1280 as the nearest ceiling of power-2 division.\n  Specify a single value to apply for all allocation sizes or specify an\n  array of key value pairs to set power-2 division individually for each\n  power of two interval. For example to set 1 division for all allocations\n  under 256MB, 2 division for allocations between 256MB and 512MB, 4 divisions\n  for allocations between 512MB and 1GB and 8 divisions for any larger allocations,\n  set the knob value to: [256:1,512:2,1024:4,>:8].","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":449,"to":461}}}}],["967",{"pageContent":"for allocations between 512MB and 1GB and 8 divisions for any larger allocations,\n  set the knob value to: [256:1,512:2,1024:4,>:8].\n  ``roundup_power2_divisions`` is only meaningful with ``backend:native``.\n  With ``backend:cudaMallocAsync``, ``roundup_power2_divisions`` is ignored.\n* ``roundup_bypass_threshold_mb`` bypass rounding the requested allocation size,\n  for allocation requests larger than the threshold value (in MB). This can help\n  reduce the memory footprint when making large allocations that are expected to\n  be persistent or have a large lifetime.\n  ``roundup_bypass_threshold_mb`` is only meaningful with ``backend:native``.\n  With ``backend:cudaMallocAsync``, ``roundup_bypass_threshold_mb`` is ignored.\n* ``garbage_collection_threshold`` helps actively reclaiming unused GPU memory to\n  avoid triggering expensive sync-and-reclaim-all operation (release_cached_blocks),\n  which can be unfavorable to latency-critical GPU applications (e.g., servers).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":461,"to":473}}}}],["968",{"pageContent":"avoid triggering expensive sync-and-reclaim-all operation (release_cached_blocks),\n  which can be unfavorable to latency-critical GPU applications (e.g., servers).\n  Upon setting this threshold (e.g., 0.8), the allocator will start reclaiming\n  GPU memory blocks if the GPU memory capacity usage exceeds the threshold (i.e.,\n  80% of the total memory allocated to the GPU application). The algorithm prefers\n  to free old & unused blocks first to avoid freeing blocks that are actively being\n  reused. The threshold value should be between greater than 0.0 and less than 1.0.\n  ``garbage_collection_threshold`` is only meaningful with ``backend:native``.\n  With ``backend:cudaMallocAsync``, ``garbage_collection_threshold`` is ignored.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":473,"to":481}}}}],["969",{"pageContent":".. note::\n\n    Some stats reported by the\n    :ref:`CUDA memory management API<cuda-memory-management-api>`\n    are specific to ``backend:native``, and are not meaningful with\n    ``backend:cudaMallocAsync``.\n    See each function's docstring for details.\n\n.. _CUDA's built-in asynchronous allocator:\n    https://developer.nvidia.com/blog/using-cuda-stream-ordered-memory-allocator-part-1/\n\n.. _cuda-memory-custom-allocator:\n\nUsing custom memory allocators for CUDA\n---------------------------------------\n\nIt is possible to define allocators as simple functions in C/C++ and compile\nthem as a shared library, the code below shows a basic allocator that just\ntraces all the memory operations.\n\n.. code:: C++","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":483,"to":503}}}}],["970",{"pageContent":"It is possible to define allocators as simple functions in C/C++ and compile\nthem as a shared library, the code below shows a basic allocator that just\ntraces all the memory operations.\n\n.. code:: C++\n\n   #include <sys/types.h>\n   #include <cuda_runtime_api.h>\n   #include <iostream>\n   // Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC\n   extern \"C\" {\n   void* my_malloc(ssize_t size, int device, cudaStream_t stream) {\n      void *ptr;\n      cudaMalloc(&ptr, size);\n      std::cout<<\"alloc \"<<ptr<<size<<std::endl;\n      return ptr;\n   }\n\n   void my_free(void* ptr, ssize_t size, int device, cudaStream_t stream) {\n      std::cout<<\"free \"<<ptr<< \" \"<<stream<<std::endl;\n      cudaFree(ptr);\n   }\n   }\n\n\nThis can be used in python through the :class:`torch.cuda.memory.CUDAPluggableAllocator`.\nThe user is responsible for supplying the path to the `.so` file and the name\nof the alloc/free functions that match the signatures specified above.\n\n.. code:: python\n\n   import torch","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":503,"to":534}}}}],["971",{"pageContent":".. code:: python\n\n   import torch\n\n   # Load the allocator\n   new_alloc = torch.cuda.memory.CUDAPluggableAllocator(\n       'alloc.so', 'my_malloc', 'my_free')\n   # Swap the current allocator\n   torch.cuda.memory.change_current_allocator(new_alloc)\n   # This will allocate memory in the device using the new allocator\n   b = torch.zeros(10, device='cuda')\n\n\n.. code:: python\n\n   import torch\n\n   # Do an initial memory allocator\n   b = torch.zeros(10, device='cuda')\n   # Load the allocator\n   new_alloc = torch.cuda.memory.CUDAPluggableAllocator(\n       'alloc.so', 'my_malloc', 'my_free')\n   # This will error since the current allocator was already instantiated\n   torch.cuda.memory.change_current_allocator(new_alloc)\n\n\n.. _cufft-plan-cache:\n\ncuFFT plan cache\n----------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":534,"to":563}}}}],["972",{"pageContent":".. _cufft-plan-cache:\n\ncuFFT plan cache\n----------------\n\nFor each CUDA device, an LRU cache of cuFFT plans is used to speed up repeatedly\nrunning FFT methods (e.g., :func:`torch.fft.fft`) on CUDA tensors of same geometry\nwith same configuration. Because some cuFFT plans may allocate GPU memory,\nthese caches have a maximum capacity.\n\nYou may control and query the properties of the cache of current device with\nthe following APIs:\n\n* ``torch.backends.cuda.cufft_plan_cache.max_size`` gives the capacity of the\n  cache (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions).\n  Setting this value directly modifies the capacity.\n\n* ``torch.backends.cuda.cufft_plan_cache.size`` gives the number of plans\n  currently residing in the cache.\n\n* ``torch.backends.cuda.cufft_plan_cache.clear()`` clears the cache.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":563,"to":583}}}}],["973",{"pageContent":"* ``torch.backends.cuda.cufft_plan_cache.size`` gives the number of plans\n  currently residing in the cache.\n\n* ``torch.backends.cuda.cufft_plan_cache.clear()`` clears the cache.\n\nTo control and query plan caches of a non-default device, you can index the\n``torch.backends.cuda.cufft_plan_cache`` object with either a :class:`torch.device`\nobject or a device index, and access one of the above attributes. E.g., to set\nthe capacity of the cache for device ``1``, one can write\n``torch.backends.cuda.cufft_plan_cache[1].max_size = 10``.\n\n.. _cuda-just-in-time-compilation:\n\nJust-in-Time Compilation\n------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":583,"to":597}}}}],["974",{"pageContent":".. _cuda-just-in-time-compilation:\n\nJust-in-Time Compilation\n------------------------\n\nPyTorch just-in-time compiles some operations, like torch.special.zeta, when\nperformed on CUDA tensors. This compilation can be time consuming\n(up to a few seconds depending on your hardware and software)\nand may occur multiple times for a single operator since many PyTorch operators actually\nselect from a variety of kernels, each of which must be compiled once, depending on their input.\nThis compilation occurs once per process, or just once if a kernel cache is used.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":597,"to":607}}}}],["975",{"pageContent":"By default, PyTorch creates a kernel cache in $XDG_CACHE_HOME/torch/kernels if\nXDG_CACHE_HOME is defined and $HOME/.cache/torch/kernels if it's not (except on Windows,\nwhere the kernel cache is not yet supported). The caching behavior can be directly\ncontrolled with two environment variables. If USE_PYTORCH_KERNEL_CACHE is set to 0 then no\ncache will be used, and if PYTORCH_KERNEL_CACHE_PATH is set then that path will be used\nas a kernel cache instead of the default location.\n\nBest practices\n--------------\n\nDevice-agnostic code\n^^^^^^^^^^^^^^^^^^^^\n\nDue to the structure of PyTorch, you may need to explicitly write\ndevice-agnostic (CPU or GPU) code; an example may be creating a new tensor as\nthe initial hidden state of a recurrent neural network.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":609,"to":624}}}}],["976",{"pageContent":"Due to the structure of PyTorch, you may need to explicitly write\ndevice-agnostic (CPU or GPU) code; an example may be creating a new tensor as\nthe initial hidden state of a recurrent neural network.\n\nThe first step is to determine whether the GPU should be used or not. A common\npattern is to use Python's ``argparse`` module to read in user arguments, and\nhave a flag that can be used to disable CUDA, in combination with\n:meth:`~torch.cuda.is_available`. In the following, ``args.device`` results in a\n:class:`torch.device` object that can be used to move tensors to CPU or CUDA.\n\n::\n\n    import argparse\n    import torch","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":624,"to":637}}}}],["977",{"pageContent":"::\n\n    import argparse\n    import torch\n\n    parser = argparse.ArgumentParser(description='PyTorch Example')\n    parser.add_argument('--disable-cuda', action='store_true',\n                        help='Disable CUDA')\n    args = parser.parse_args()\n    args.device = None\n    if not args.disable_cuda and torch.cuda.is_available():\n        args.device = torch.device('cuda')\n    else:\n        args.device = torch.device('cpu')\n\n.. note::\n\n    When assessing the availability of CUDA in a given environment (:meth:`~torch.cuda.is_available`), PyTorch's default\n    behavior is to call the CUDA Runtime API method `cudaGetDeviceCount`_. Because this call in turn initializes the\n    CUDA Driver API (via `cuInit`_) if it is not already initialized, subsequent forks of a process that has run\n    :meth:`~torch.cuda.is_available` will fail with a CUDA initialization error.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":637,"to":657}}}}],["978",{"pageContent":"One can set ``PYTORCH_NVML_BASED_CUDA_CHECK=1`` in your environment before importing PyTorch modules that execute\n    :meth:`~torch.cuda.is_available` (or before executing it directly) in order to direct\n    :meth:`~torch.cuda.is_available` to attempt an NVML-based assessment (`nvmlDeviceGetCount_v2`_). If the\n    NVML-based assessment is successful (i.e. NVML discovery/initialization does not fail),\n    :meth:`~torch.cuda.is_available` calls will not poison subsequent forks.\n\n    If NVML discovery/initialization fails, :meth:`~torch.cuda.is_available` will fallback to the standard CUDA Runtime\n    API assessment and the aforementioned fork constraint will apply.\n\n    Note that the above NVML-based CUDA availability assessment provides a weaker guarantee than the default CUDA\n    Runtime API approach (which requires CUDA initialization to succeed). In some circumstances, the NVML-based check\n    may succeed while later CUDA initialization fails.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":659,"to":670}}}}],["979",{"pageContent":"Now that we have ``args.device``, we can use it to create a Tensor on the\ndesired device.\n\n::\n\n    x = torch.empty((8, 42), device=args.device)\n    net = Network().to(device=args.device)\n\nThis can be used in a number of cases to produce device agnostic code. Below\nis an example when using a dataloader:\n\n::\n\n    cuda0 = torch.device('cuda:0')  # CUDA GPU 0\n    for i, x in enumerate(train_loader):\n        x = x.to(cuda0)\n\nWhen working with multiple GPUs on a system, you can use the\n``CUDA_VISIBLE_DEVICES`` environment flag to manage which GPUs are available to\nPyTorch. As mentioned above, to manually control which GPU a tensor is created\non, the best practice is to use a :any:`torch.cuda.device` context manager.\n\n::\n\n    print(\"Outside device is 0\")  # On device 0 (default in most scenarios)\n    with torch.cuda.device(1):\n        print(\"Inside device is 1\")  # On device 1\n    print(\"Outside device is still 0\")  # On device 0","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":672,"to":699}}}}],["980",{"pageContent":"If you have a tensor and would like to create a new tensor of the same type on\nthe same device, then you can use a ``torch.Tensor.new_*`` method\n(see :class:`torch.Tensor`).\nWhilst the previously mentioned ``torch.*`` factory functions\n(:ref:`tensor-creation-ops`) depend on the current GPU context and\nthe attributes arguments you pass in, ``torch.Tensor.new_*`` methods preserve\nthe device and other attributes of the tensor.\n\nThis is the recommended practice when creating modules in which new\ntensors need to be created internally during the forward pass.\n\n::\n\n    cuda = torch.device('cuda')\n    x_cpu = torch.empty(2)\n    x_gpu = torch.empty(2, device=cuda)\n    x_cpu_long = torch.empty(2, dtype=torch.int64)\n\n    y_cpu = x_cpu.new_full([3, 2], fill_value=0.3)\n    print(y_cpu)\n\n        tensor([[ 0.3000,  0.3000],\n                [ 0.3000,  0.3000],\n                [ 0.3000,  0.3000]])\n\n    y_gpu = x_gpu.new_full([3, 2], fill_value=-5)\n    print(y_gpu)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":701,"to":727}}}}],["981",{"pageContent":"tensor([[ 0.3000,  0.3000],\n                [ 0.3000,  0.3000],\n                [ 0.3000,  0.3000]])\n\n    y_gpu = x_gpu.new_full([3, 2], fill_value=-5)\n    print(y_gpu)\n\n        tensor([[-5.0000, -5.0000],\n                [-5.0000, -5.0000],\n                [-5.0000, -5.0000]], device='cuda:0')\n\n    y_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])\n    print(y_cpu_long)\n\n        tensor([[ 1,  2,  3]])\n\n\nIf you want to create a tensor of the same type and size of another tensor, and\nfill it with either ones or zeros, :meth:`~torch.ones_like` or\n:meth:`~torch.zeros_like` are provided as convenient helper functions (which\nalso preserve :class:`torch.device` and :class:`torch.dtype` of a Tensor).\n\n::\n\n    x_cpu = torch.empty(2, 3)\n    x_gpu = torch.empty(2, 3)\n\n    y_cpu = torch.ones_like(x_cpu)\n    y_gpu = torch.zeros_like(x_gpu)\n\n\n.. _cuda-memory-pinning:\n\nUse pinned memory buffers\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. warning::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":727,"to":763}}}}],["982",{"pageContent":"y_cpu = torch.ones_like(x_cpu)\n    y_gpu = torch.zeros_like(x_gpu)\n\n\n.. _cuda-memory-pinning:\n\nUse pinned memory buffers\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. warning::\n\n    This is an advanced tip. If you overuse pinned memory, it can cause serious\n    problems when running low on RAM, and you should be aware that pinning is\n    often an expensive operation.\n\nHost to GPU copies are much faster when they originate from pinned (page-locked)\nmemory. CPU tensors and storages expose a :meth:`~torch.Tensor.pin_memory`\nmethod, that returns a copy of the object, with data put in a pinned region.\n\nAlso, once you pin a tensor or storage, you can use asynchronous GPU copies.\nJust pass an additional ``non_blocking=True`` argument to a\n:meth:`~torch.Tensor.to` or a :meth:`~torch.Tensor.cuda` call. This can be used\nto overlap data transfers with computation.\n\nYou can make the :class:`~torch.utils.data.DataLoader` return batches placed in\npinned memory by passing ``pin_memory=True`` to its constructor.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":763,"to":788}}}}],["983",{"pageContent":"You can make the :class:`~torch.utils.data.DataLoader` return batches placed in\npinned memory by passing ``pin_memory=True`` to its constructor.\n\n.. _cuda-nn-ddp-instead:\n\nUse nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nMost use cases involving batched inputs and multiple GPUs should default to\nusing :class:`~torch.nn.parallel.DistributedDataParallel` to utilize more\nthan one GPU.\n\nThere are significant caveats to using CUDA models with\n:mod:`~torch.multiprocessing`; unless care is taken to meet the data handling\nrequirements exactly, it is likely that your program will have incorrect or\nundefined behavior.\n\nIt is recommended to use :class:`~torch.nn.parallel.DistributedDataParallel`,\ninstead of :class:`~torch.nn.DataParallel` to do multi-GPU training, even if\nthere is only a single node.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":788,"to":807}}}}],["984",{"pageContent":"It is recommended to use :class:`~torch.nn.parallel.DistributedDataParallel`,\ninstead of :class:`~torch.nn.DataParallel` to do multi-GPU training, even if\nthere is only a single node.\n\nThe difference between :class:`~torch.nn.parallel.DistributedDataParallel` and\n:class:`~torch.nn.DataParallel` is: :class:`~torch.nn.parallel.DistributedDataParallel`\nuses multiprocessing where a process is created for each GPU, while\n:class:`~torch.nn.DataParallel` uses multithreading. By using multiprocessing,\neach GPU has its dedicated process, this avoids the performance overhead caused\nby GIL of Python interpreter.\n\nIf you use :class:`~torch.nn.parallel.DistributedDataParallel`, you could use\n`torch.distributed.launch` utility to launch your program, see :ref:`distributed-launch`.\n\n.. _cudaGetDeviceCount:\n    https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g18808e54893cfcaafefeab31a73cc55f","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":807,"to":822}}}}],["985",{"pageContent":".. _cudaGetDeviceCount:\n    https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g18808e54893cfcaafefeab31a73cc55f\n\n.. _cuInit:\n    https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__INITIALIZE.html#group__CUDA__INITIALIZE_1g0a2f1517e1bd8502c7194c3a8c134bc3\n\n.. _nvmlDeviceGetCount_v2:\n    https://docs.nvidia.com/deploy/nvml-api/group__nvmlDeviceQueries.html#group__nvmlDeviceQueries_1ga93623b195bff04bbe3490ca33c8a42d\n\n.. _cuda-graph-semantics:\n\nCUDA Graphs\n-----------\n\nA CUDA graph is a record of the work (mostly kernels and their arguments) that a\nCUDA stream and its dependent streams perform.\nFor general principles and details on the underlying CUDA API, see\n`Getting Started with CUDA Graphs`_ and the\n`Graphs section`_ of the CUDA C Programming Guide.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":822,"to":840}}}}],["986",{"pageContent":"PyTorch supports the construction of CUDA graphs using `stream capture`_, which puts a\nCUDA stream in *capture mode*. CUDA work issued to a capturing stream doesn't actually\nrun on the GPU. Instead, the work is recorded in a graph.\n\nAfter capture, the graph can be *launched* to run the GPU work as many times as needed.\nEach replay runs the same kernels with the same arguments. For pointer arguments this\nmeans the same memory addresses are used.\nBy filling input memory with new data (e.g., from a new batch) before each replay,\nyou can rerun the same work on new data.\n\nWhy CUDA Graphs?\n^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":842,"to":853}}}}],["987",{"pageContent":"Why CUDA Graphs?\n^^^^^^^^^^^^^^^^\n\nReplaying a graph sacrifices the dynamic flexibility of typical eager execution in exchange for\n**greatly reduced CPU overhead**. A graph's arguments and kernels are fixed, so a graph replay\nskips all layers of argument setup and kernel dispatch, including Python, C++, and CUDA driver\noverheads. Under the hood, a replay submits the entire graph's work to the GPU with\na single call to `cudaGraphLaunch`_.  Kernels in a replay also execute slightly faster\non the GPU, but eliding CPU overhead is the main benefit.\n\nYou should try CUDA graphs if all or part of your network is graph-safe (usually this means\nstatic shapes and static control flow, but see the other :ref:`constraints<capture-constraints>`)\nand you suspect its runtime is at least somewhat CPU-limited.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":853,"to":865}}}}],["988",{"pageContent":".. _Getting Started with CUDA Graphs:\n    https://developer.nvidia.com/blog/cuda-graphs/\n.. _Graphs section:\n    https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs\n.. _stream capture:\n    https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#creating-a-graph-using-stream-capture\n.. _cudaGraphLaunch:\n    https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH_1g1accfe1da0c605a577c22d9751a09597\n\nPyTorch API\n^^^^^^^^^^^\n\n.. warning::\n    This API is in beta and may change in future releases.\n\nPyTorch exposes graphs via a raw :class:`torch.cuda.CUDAGraph` class\nand two convenience wrappers,\n:class:`torch.cuda.graph` and\n:class:`torch.cuda.make_graphed_callables`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":867,"to":885}}}}],["989",{"pageContent":"PyTorch exposes graphs via a raw :class:`torch.cuda.CUDAGraph` class\nand two convenience wrappers,\n:class:`torch.cuda.graph` and\n:class:`torch.cuda.make_graphed_callables`.\n\n:class:`torch.cuda.graph` is a simple, versatile context manager that\ncaptures CUDA work in its context.\nBefore capture, warm up the workload to be captured by running\na few eager iterations. Warmup must occur on a side stream.\nBecause the graph reads from and writes to the same memory addresses in every\nreplay, you must maintain long-lived references to tensors that hold\ninput and output data during capture.\nTo run the graph on new input data, copy new data to the capture's input tensor(s),\nreplay the graph, then read the new output from the capture's output tensor(s).\nExample::\n\n    g = torch.cuda.CUDAGraph()\n\n    # Placeholder input used for capture\n    static_input = torch.empty((5,), device=\"cuda\")","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":885,"to":904}}}}],["990",{"pageContent":"g = torch.cuda.CUDAGraph()\n\n    # Placeholder input used for capture\n    static_input = torch.empty((5,), device=\"cuda\")\n\n    # Warmup before capture\n    s = torch.cuda.Stream()\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        for _ in range(3):\n            static_output = static_input * 2\n    torch.cuda.current_stream().wait_stream(s)\n\n    # Captures the graph\n    # To allow capture, automatically sets a side stream as the current stream in the context\n    with torch.cuda.graph(g):\n        static_output = static_input * 2\n\n    # Fills the graph's input memory with new data to compute on\n    static_input.copy_(torch.full((5,), 3, device=\"cuda\"))\n    g.replay()\n    # static_output holds the results\n    print(static_output)  # full of 3 * 2 = 6\n\n    # Fills the graph's input memory with more data to compute on\n    static_input.copy_(torch.full((5,), 4, device=\"cuda\"))\n    g.replay()\n    print(static_output)  # full of 4 * 2 = 8","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":904,"to":931}}}}],["991",{"pageContent":"# Fills the graph's input memory with more data to compute on\n    static_input.copy_(torch.full((5,), 4, device=\"cuda\"))\n    g.replay()\n    print(static_output)  # full of 4 * 2 = 8\n\nSee\n:ref:`Whole-network capture<whole-network-capture>`,\n:ref:`Usage with torch.cuda.amp<graphs-with-amp>`, and\n:ref:`Usage with multiple streams<multistream-capture>`\nfor realistic and advanced patterns.\n\n:class:`~torch.cuda.make_graphed_callables` is more sophisticated.\n:class:`~torch.cuda.make_graphed_callables` accepts Python functions and\n:class:`torch.nn.Module`\\s. For each passed function or Module,\nit creates separate graphs of the forward-pass and backward-pass work. See\n:ref:`Partial-network capture<partial-network-capture>`.\n\n.. _capture-constraints:\n\nConstraints\n~~~~~~~~~~~\n\nA set of ops is *capturable* if it doesn't violate any of the following constraints.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":931,"to":953}}}}],["992",{"pageContent":".. _capture-constraints:\n\nConstraints\n~~~~~~~~~~~\n\nA set of ops is *capturable* if it doesn't violate any of the following constraints.\n\nConstraints apply to all work in a\n:class:`torch.cuda.graph` context and all work in the forward and backward passes\nof any callable you pass to :func:`torch.cuda.make_graphed_callables`.\n\nViolating any of these will likely cause a runtime error:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":953,"to":964}}}}],["993",{"pageContent":"Violating any of these will likely cause a runtime error:\n\n* Capture must occur on a non-default stream. (This is only a concern if you use the raw\n  :meth:`CUDAGraph.capture_begin<torch.cuda.CUDAGraph.capture_begin>` and\n  :meth:`CUDAGraph.capture_end<torch.cuda.CUDAGraph.capture_end>` calls.\n  :class:`~torch.cuda.graph` and\n  :func:`~torch.cuda.make_graphed_callables` set a side stream for you.)\n* Ops that synchronize the CPU with the GPU (e.g., ``.item()`` calls) are prohibited.\n* CUDA RNG ops are allowed, but must use default generators. For example, explicitly constructing a\n  new :class:`torch.Generator` instance and passing it as the ``generator`` argument to an RNG function\n  is prohibited.\n\nViolating any of these will likely cause silent numerical errors or undefined behavior:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":964,"to":976}}}}],["994",{"pageContent":"Violating any of these will likely cause silent numerical errors or undefined behavior:\n\n* Within a process, only one capture may be underway at a time.\n* No non-captured CUDA work may run in this process (on any thread) while capture is underway.\n* CPU work is not captured. If the captured ops include CPU work, that work will be elided during replay.\n* Every replay reads from and writes to the same (virtual) memory addresses.\n* Dynamic control flow (based on CPU or GPU data) is prohibited.\n* Dynamic shapes are prohibited. The graph assumes every tensor in the captured op sequence\n  has the same size and layout in every replay.\n* Using multiple streams in a capture is allowed, but there are :ref:`restrictions<multistream-capture>`.\n\nNon-constraints\n~~~~~~~~~~~~~~~\n\n* Once captured, the graph may be replayed on any stream.\n\n.. _whole-network-capture:\n\nWhole-network capture\n^^^^^^^^^^^^^^^^^^^^^^\n\nIf your entire network is capturable, you can capture and replay an entire iteration::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":976,"to":997}}}}],["995",{"pageContent":".. _whole-network-capture:\n\nWhole-network capture\n^^^^^^^^^^^^^^^^^^^^^^\n\nIf your entire network is capturable, you can capture and replay an entire iteration::\n\n    N, D_in, H, D_out = 640, 4096, 2048, 1024\n    model = torch.nn.Sequential(torch.nn.Linear(D_in, H),\n                                torch.nn.Dropout(p=0.2),\n                                torch.nn.Linear(H, D_out),\n                                torch.nn.Dropout(p=0.1)).cuda()\n    loss_fn = torch.nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n    # Placeholders used for capture\n    static_input = torch.randn(N, D_in, device='cuda')\n    static_target = torch.randn(N, D_out, device='cuda')","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":997,"to":1014}}}}],["996",{"pageContent":"# Placeholders used for capture\n    static_input = torch.randn(N, D_in, device='cuda')\n    static_target = torch.randn(N, D_out, device='cuda')\n\n    # warmup\n    # Uses static_input and static_target here for convenience,\n    # but in a real setting, because the warmup includes optimizer.step()\n    # you must use a few batches of real data.\n    s = torch.cuda.Stream()\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        for i in range(3):\n            optimizer.zero_grad(set_to_none=True)\n            y_pred = model(static_input)\n            loss = loss_fn(y_pred, static_target)\n            loss.backward()\n            optimizer.step()\n    torch.cuda.current_stream().wait_stream(s)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1014,"to":1031}}}}],["997",{"pageContent":"# capture\n    g = torch.cuda.CUDAGraph()\n    # Sets grads to None before capture, so backward() will create\n    # .grad attributes with allocations from the graph's private pool\n    optimizer.zero_grad(set_to_none=True)\n    with torch.cuda.graph(g):\n        static_y_pred = model(static_input)\n        static_loss = loss_fn(static_y_pred, static_target)\n        static_loss.backward()\n        optimizer.step()\n\n    real_inputs = [torch.rand_like(static_input) for _ in range(10)]\n    real_targets = [torch.rand_like(static_target) for _ in range(10)]","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1033,"to":1045}}}}],["998",{"pageContent":"real_inputs = [torch.rand_like(static_input) for _ in range(10)]\n    real_targets = [torch.rand_like(static_target) for _ in range(10)]\n\n    for data, target in zip(real_inputs, real_targets):\n        # Fills the graph's input memory with new data to compute on\n        static_input.copy_(data)\n        static_target.copy_(target)\n        # replay() includes forward, backward, and step.\n        # You don't even need to call optimizer.zero_grad() between iterations\n        # because the captured backward refills static .grad tensors in place.\n        g.replay()\n        # Params have been updated. static_y_pred, static_loss, and .grad\n        # attributes hold values from computing on this iteration's data.\n\n.. _partial-network-capture:\n\nPartial-network capture\n^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1045,"to":1062}}}}],["999",{"pageContent":".. _partial-network-capture:\n\nPartial-network capture\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf some of your network is unsafe to capture (e.g., due to dynamic control flow,\ndynamic shapes, CPU syncs, or essential CPU-side logic), you can run the unsafe\npart(s) eagerly and use :func:`torch.cuda.make_graphed_callables` to graph only\nthe capture-safe part(s).\n\nBy default, callables returned by :func:`~torch.cuda.make_graphed_callables`\nare autograd-aware, and can be used in the training loop as direct replacements\nfor the functions or :class:`nn.Module<torch.nn.Module>`\\ s you passed.\n\n:func:`~torch.cuda.make_graphed_callables` internally creates\n:class:`~torch.cuda.CUDAGraph` objects, runs warmup iterations, and maintains\nstatic inputs and outputs as needed.  Therefore (unlike with\n:class:`torch.cuda.graph`) you don't need to handle those manually.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1062,"to":1079}}}}],["1000",{"pageContent":"In the following example, data-dependent dynamic control flow means the\nnetwork isn't capturable end-to-end, but\n:func:`~torch.cuda.make_graphed_callables`\nlets us capture and run graph-safe sections as graphs regardless::\n\n    N, D_in, H, D_out = 640, 4096, 2048, 1024\n\n    module1 = torch.nn.Linear(D_in, H).cuda()\n    module2 = torch.nn.Linear(H, D_out).cuda()\n    module3 = torch.nn.Linear(H, D_out).cuda()\n\n    loss_fn = torch.nn.MSELoss()\n    optimizer = torch.optim.SGD(chain(module1.parameters(),\n                                      module2.parameters(),\n                                      module3.parameters()),\n                                lr=0.1)\n\n    # Sample inputs used for capture\n    # requires_grad state of sample inputs must match\n    # requires_grad state of real inputs each callable will see.\n    x = torch.randn(N, D_in, device='cuda')\n    h = torch.randn(N, H, device='cuda', requires_grad=True)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1081,"to":1102}}}}],["1001",{"pageContent":"module1 = torch.cuda.make_graphed_callables(module1, (x,))\n    module2 = torch.cuda.make_graphed_callables(module2, (h,))\n    module3 = torch.cuda.make_graphed_callables(module3, (h,))\n\n    real_inputs = [torch.rand_like(x) for _ in range(10)]\n    real_targets = [torch.randn(N, D_out, device=\"cuda\") for _ in range(10)]\n\n    for data, target in zip(real_inputs, real_targets):\n        optimizer.zero_grad(set_to_none=True)\n\n        tmp = module1(data)  # forward ops run as a graph\n\n        if tmp.sum().item() > 0:\n            tmp = module2(tmp)  # forward ops run as a graph\n        else:\n            tmp = module3(tmp)  # forward ops run as a graph\n\n        loss = loss_fn(tmp, target)\n        # module2's or module3's (whichever was chosen) backward ops,\n        # as well as module1's backward ops, run as graphs\n        loss.backward()\n        optimizer.step()\n\n.. _graphs-with-amp:\n\nUsage with torch.cuda.amp\n^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1104,"to":1130}}}}],["1002",{"pageContent":".. _graphs-with-amp:\n\nUsage with torch.cuda.amp\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nFor typical optimizers, :meth:`GradScaler.step<torch.cuda.amp.GradScaler.step>` syncs\nthe CPU with the GPU, which is prohibited during capture. To avoid errors, either use\n:ref:`partial-network capture<partial-network-capture>`, or (if forward, loss,\nand backward are capture-safe) capture forward, loss, and backward but not the\noptimizer step::\n\n    # warmup\n    # In a real setting, use a few batches of real data.\n    s = torch.cuda.Stream()\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        for i in range(3):\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast():\n                y_pred = model(static_input)\n                loss = loss_fn(y_pred, static_target)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n    torch.cuda.current_stream().wait_stream(s)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1130,"to":1154}}}}],["1003",{"pageContent":"# capture\n    g = torch.cuda.CUDAGraph()\n    optimizer.zero_grad(set_to_none=True)\n    with torch.cuda.graph(g):\n        with torch.cuda.amp.autocast():\n            static_y_pred = model(static_input)\n            static_loss = loss_fn(static_y_pred, static_target)\n        scaler.scale(static_loss).backward()\n        # don't capture scaler.step(optimizer) or scaler.update()\n\n    real_inputs = [torch.rand_like(static_input) for _ in range(10)]\n    real_targets = [torch.rand_like(static_target) for _ in range(10)]\n\n    for data, target in zip(real_inputs, real_targets):\n        static_input.copy_(data)\n        static_target.copy_(target)\n        g.replay()\n        # Runs scaler.step and scaler.update eagerly\n        scaler.step(optimizer)\n        scaler.update()\n\n.. _multistream-capture:\n\nUsage with multiple streams\n^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1156,"to":1180}}}}],["1004",{"pageContent":".. _multistream-capture:\n\nUsage with multiple streams\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nCapture mode automatically propagates to any streams that sync with a capturing stream.\nWithin capture, you may expose parallelism by issuing calls to different streams,\nbut the overall stream dependency DAG must branch out from the\ninitial capturing stream after capture begins and rejoin the initial stream\nbefore capture ends::\n\n    with torch.cuda.graph(g):\n        # at context manager entrance, torch.cuda.current_stream()\n        # is the initial capturing stream\n\n        # INCORRECT (does not branch out from or rejoin initial stream)\n        with torch.cuda.stream(s):\n            cuda_work()\n\n        # CORRECT:\n        # branches out from initial stream\n        s.wait_stream(torch.cuda.current_stream())\n        with torch.cuda.stream(s):\n            cuda_work()\n        # rejoins initial stream before capture ends\n        torch.cuda.current_stream().wait_stream(s)\n\n.. note::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1180,"to":1207}}}}],["1005",{"pageContent":".. note::\n\n    To avoid confusion for power users looking at replays in nsight systems or nvprof:\n    Unlike eager execution, the graph interprets a nontrivial stream DAG in capture\n    as a hint, not a command. During replay, the graph may reorganize independent ops\n    onto different streams or enqueue them in a different order (while respecting your\n    original DAG's overall dependencies).\n\nUsage with DistributedDataParallel\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nNCCL < 2.9.6\n~~~~~~~~~~~~\n\nNCCL versions earlier than 2.9.6 don't allow collectives to be captured.\nYou must use :ref:`partial-network capture<partial-network-capture>`,\nwhich defers allreduces to happen outside graphed sections of backward.\n\nCall :func:`~torch.cuda.make_graphed_callables` on graphable network sections\n*before* wrapping the network with DDP.\n\nNCCL >= 2.9.6\n~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1207,"to":1229}}}}],["1006",{"pageContent":"Call :func:`~torch.cuda.make_graphed_callables` on graphable network sections\n*before* wrapping the network with DDP.\n\nNCCL >= 2.9.6\n~~~~~~~~~~~~~\n\nNCCL versions 2.9.6 or later allow collectives in the graph.\nApproaches that capture an :ref:`entire backward pass<whole-network-capture>`\nare a viable option, but need three setup steps.\n\n1. Disable DDP's internal async error handling::\n\n    os.environ[\"NCCL_ASYNC_ERROR_HANDLING\"] = \"0\"\n    torch.distributed.init_process_group(...)\n\n2. Before full-backward capture, DDP must be constructed in a side-stream context::\n\n    with torch.cuda.stream(s):\n        model = DistributedDataParallel(model)\n\n3. Your warmup must run at least 11 DDP-enabled eager iterations before capture.\n\n.. _graph-memory-management:\n\nGraph memory management\n^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1229,"to":1254}}}}],["1007",{"pageContent":"3. Your warmup must run at least 11 DDP-enabled eager iterations before capture.\n\n.. _graph-memory-management:\n\nGraph memory management\n^^^^^^^^^^^^^^^^^^^^^^^\n\nA captured graph acts on the same virtual addresses every time it replays.\nIf PyTorch frees the memory, a later replay can hit an illegal memory access.\nIf PyTorch reassigns the memory to new tensors, the replay can corrupt the values\nseen by those tensors.  Therefore, the virtual addresses used by the graph must be\nreserved for the graph across replays. The PyTorch caching allocator achieves this\nby detecting when capture is underway and satisfying the capture's allocations\nfrom a graph-private memory pool. The private pool stays alive until its\n:class:`~torch.cuda.CUDAGraph` object and all tensors created during capture\ngo out of scope.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1254,"to":1269}}}}],["1008",{"pageContent":"Private pools are maintained automatically. By default, the allocator creates a\nseparate private pool for each capture. If you capture multiple graphs,\nthis conservative approach ensures graph replays never corrupt each other's values,\nbut sometimes needlessly wastes memory.\n\nSharing memory across captures\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo economize the memory stashed in private pools, :class:`torch.cuda.graph`\nand :func:`torch.cuda.make_graphed_callables` optionally allow different\ncaptures to share the same private pool.\nIt's safe for a set of graphs to share a private pool if you know they'll always\nbe replayed in the same order they were captured,\nand never be replayed concurrently.\n\n:class:`torch.cuda.graph`'s ``pool`` argument is a hint to use a particular private pool,\nand can be used to share memory across graphs as shown::\n\n    g1 = torch.cuda.CUDAGraph()\n    g2 = torch.cuda.CUDAGraph()\n\n    # (create static inputs for g1 and g2, run warmups of their workloads...)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1271,"to":1292}}}}],["1009",{"pageContent":"g1 = torch.cuda.CUDAGraph()\n    g2 = torch.cuda.CUDAGraph()\n\n    # (create static inputs for g1 and g2, run warmups of their workloads...)\n\n    # Captures g1\n    with torch.cuda.graph(g1):\n        static_out_1 = g1_workload(static_in_1)\n\n    # Captures g2, hinting that g2 may share a memory pool with g1\n    with torch.cuda.graph(g2, pool=g1.pool()):\n        static_out_2 = g2_workload(static_in_2)\n\n    static_in_1.copy_(real_data_1)\n    static_in_2.copy_(real_data_2)\n    g1.replay()\n    g2.replay()\n\nWith :func:`torch.cuda.make_graphed_callables`, if you want to graph several\ncallables and you know they'll always run in the same order (and never concurrently)\npass them as a tuple in the same order they'll run in the live workload, and\n:func:`~torch.cuda.make_graphed_callables` will capture their graphs using a shared\nprivate pool.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1292,"to":1314}}}}],["1010",{"pageContent":"If, in the live workload, your callables will run in an order that occasionally changes,\nor if they'll run concurrently, passing them as a tuple to a single invocation of\n:func:`~torch.cuda.make_graphed_callables` is not allowed. Instead, you must call\n:func:`~torch.cuda.make_graphed_callables` separately for each one.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/cuda.rst","loc":{"lines":{"from":1316,"to":1319}}}}],["1011",{"pageContent":".. _ddp:\n\nDistributed Data Parallel\n=========================\n\n.. warning::\n  The implementation of :class:`torch.nn.parallel.DistributedDataParallel`\n  evolves over time. This design note is written based on the state as of v1.4.\n\n\n:class:`torch.nn.parallel.DistributedDataParallel` (DDP) transparently performs\ndistributed data parallel training. This page describes how it works and reveals\nimplementation details.\n\nExample\n^^^^^^^\n\nLet us start with a simple :class:`torch.nn.parallel.DistributedDataParallel`\nexample. This example uses a :class:`torch.nn.Linear` as the local model, wraps\nit with DDP, and then runs one forward pass, one backward pass, and an optimizer\nstep on the DDP model. After that, parameters on the local model will be\nupdated, and all models on different processes should be exactly the same.\n\n.. code::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":1,"to":24}}}}],["1012",{"pageContent":".. code::\n\n    import torch\n    import torch.distributed as dist\n    import torch.multiprocessing as mp\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.nn.parallel import DistributedDataParallel as DDP\n\n\n    def example(rank, world_size):\n        # create default process group\n        dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n        # create local model\n        model = nn.Linear(10, 10).to(rank)\n        # construct DDP model\n        ddp_model = DDP(model, device_ids=[rank])\n        # define loss function and optimizer\n        loss_fn = nn.MSELoss()\n        optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n        # forward pass\n        outputs = ddp_model(torch.randn(20, 10).to(rank))\n        labels = torch.randn(20, 10).to(rank)\n        # backward pass\n        loss_fn(outputs, labels).backward()\n        # update parameters\n        optimizer.step()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":24,"to":51}}}}],["1013",{"pageContent":"def main():\n        world_size = 2\n        mp.spawn(example,\n            args=(world_size,),\n            nprocs=world_size,\n            join=True)\n\n    if __name__==\"__main__\":\n        # Environment variables which need to be\n        # set when using c10d's default \"env\"\n        # initialization mode.\n        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n        os.environ[\"MASTER_PORT\"] = \"29500\"\n        main()\n\nDDP works with TorchDynamo.  When used with TorchDynamo, apply the DDP model wrapper\nbefore compiling the model, such that torchdynamo can apply ``DDPOptimizer``\n(graph-break optimizations) based on DDP bucket sizes.  (See `TorchDynamo DDPOptimizer <./ddp.html#torchdynamo-ddpoptimizer>`_ for more information.)\n\nTorchDynamo support for DDP currently requires setting `static_graph=False`, due to\ninteractions between the graph tracing process and DDP's mechanism for observing operations happening on its module,\nbut this should be fixed ultimately.\n\n.. code::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":53,"to":76}}}}],["1014",{"pageContent":".. code::\n\n        ddp_model = DDP(model, device_ids=[rank])\n        ddp_model = torch.compile(ddp_model)\n\nInternal Design\n^^^^^^^^^^^^^^^\n\nThis section reveals how it works under the hood of\n:class:`torch.nn.parallel.DistributedDataParallel` by diving into details of\nevery step in one iteration.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":76,"to":86}}}}],["1015",{"pageContent":"- **Prerequisite**: DDP relies on c10d ``ProcessGroup`` for communications.\n  Hence, applications must create ``ProcessGroup`` instances before constructing\n  DDP.\n- **Construction**: The DDP constructor takes a reference to the local module,\n  and broadcasts ``state_dict()`` from the process with rank 0 to all other\n  processes in the group to make sure that all model replicas start from the\n  exact same state. Then, each DDP process creates a local ``Reducer``, which\n  later will take care of the gradients synchronization during the backward\n  pass. To improve communication efficiency, the ``Reducer`` organizes parameter\n  gradients into buckets, and reduces one bucket at a time. Bucket size can be\n  configured by setting the `bucket_cap_mb` argument in DDP constructor. The\n  mapping from parameter gradients to buckets is determined at the construction\n  time, based on the bucket size limit and parameter sizes. Model parameters are\n  allocated into buckets in (roughly) the reverse order of","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":88,"to":101}}}}],["1016",{"pageContent":"time, based on the bucket size limit and parameter sizes. Model parameters are\n  allocated into buckets in (roughly) the reverse order of\n  ``Model.parameters()`` from the given model. The reason for using the reverse\n  order is because DDP expects gradients to become ready during the backward\n  pass in approximately that order. The figure below shows an example. Note\n  that, the ``grad0`` and ``grad1`` are in ``bucket1``, and the other two\n  gradients are in ``bucket0``. Of course, this assumption might not always\n  be true, and when that happens it could hurt DDP backward speed as the\n  ``Reducer`` cannot kick off the communication at the earliest possible time.\n  Besides bucketing, the ``Reducer`` also registers autograd hooks during\n  construction, one hook per parameter. These hooks will be triggered during\n  the backward pass when the gradient becomes ready.\n- **Forward Pass**: The DDP takes the input and passes it to the local model,\n  and then analyzes the output from the local model if","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":101,"to":114}}}}],["1017",{"pageContent":"the backward pass when the gradient becomes ready.\n- **Forward Pass**: The DDP takes the input and passes it to the local model,\n  and then analyzes the output from the local model if\n  ``find_unused_parameters`` is set to ``True``. This mode allows running\n  backward on a subgraph of the model, and DDP finds out which parameters are\n  involved in the backward pass by traversing the autograd graph from the model\n  output and marking all unused parameters as ready for reduction. During the\n  backward pass, the ``Reducer`` would only wait for unready parameters, but it\n  would still reduce all buckets. Marking a parameter gradient as ready does not\n  help DDP skip buckets as for now, but it will prevent DDP from waiting for\n  absent gradients forever during the backward pass. Note that traversing the\n  autograd graph introduces extra overheads, so applications should only set\n  ``find_unused_parameters`` to ``True`` when necessary.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":114,"to":126}}}}],["1018",{"pageContent":"autograd graph introduces extra overheads, so applications should only set\n  ``find_unused_parameters`` to ``True`` when necessary.\n- **Backward Pass**: The ``backward()`` function is directly invoked on the loss\n  ``Tensor``, which is out of DDP's control, and DDP uses autograd hooks\n  registered at construction time to trigger gradients synchronizations. When\n  one gradient becomes ready, its corresponding DDP hook on that grad\n  accumulator will fire, and DDP will then mark that parameter gradient as\n  ready for reduction. When gradients in one bucket are all ready, the\n  ``Reducer`` kicks off an asynchronous ``allreduce`` on that bucket to\n  calculate mean of gradients across all processes. When all buckets are ready,\n  the ``Reducer`` will block waiting for all ``allreduce`` operations to finish.\n  When this is done, averaged gradients are written to the ``param.grad`` field\n  of all parameters. So after the backward pass, the `grad` field on the same","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":126,"to":138}}}}],["1019",{"pageContent":"When this is done, averaged gradients are written to the ``param.grad`` field\n  of all parameters. So after the backward pass, the `grad` field on the same\n  corresponding parameter across different DDP processes should be the same.\n- **Optimizer Step**: From the optimizer's perspective, it is optimizing a local\n  model. Model replicas on all DDP processes can keep in sync because they all\n  start from the same state and they have the same averaged gradients in\n  every iteration.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":138,"to":144}}}}],["1020",{"pageContent":".. image:: https://user-images.githubusercontent.com/16999635/72401724-d296d880-371a-11ea-90ab-737f86543df9.png\n    :alt: ddp_grad_sync.png\n    :width: 700 px\n\n.. note::\n  DDP requires ``Reducer`` instances on all processes to invoke ``allreduce``\n  in exactly the same order, which is done by always running ``allreduce``\n  in the bucket index order instead of actual bucket ready order. Mismatched\n  ``allreduce`` order across processes can lead to wrong results or DDP backward\n  hang.\n\nImplementation\n^^^^^^^^^^^^^^\n\nBelow are pointers to the DDP implementation components. The stacked graph shows\nthe structure of the code.\n\nProcessGroup\n------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":147,"to":165}}}}],["1021",{"pageContent":"Implementation\n^^^^^^^^^^^^^^\n\nBelow are pointers to the DDP implementation components. The stacked graph shows\nthe structure of the code.\n\nProcessGroup\n------------\n\n- `ProcessGroup.hpp <https://github.com/pytorch/pytorch/blob/v1.7.0/torch/lib/c10d/ProcessGroup.hpp>`__:\n  contains the abstract API of all process group implementations. The ``c10d``\n  library provides 3 implementations out of the box, namely,\n  `ProcessGroupGloo`, `ProcessGroupNCCL`, and `ProcessGroupMPI`.\n  ``DistributedDataParallel`` uses ``ProcessGroup::broadcast()`` to send\n  model states from the process with rank 0 to others during initialization\n  and ``ProcessGroup::allreduce()`` to sum gradients.\n\n\n- `Store.hpp <https://github.com/pytorch/pytorch/blob/v1.7.0/torch/lib/c10d/Store.hpp>`__:\n  assists the rendezvous service for process group instances to find each other.\n\nDistributedDataParallel\n-----------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":165,"to":187}}}}],["1022",{"pageContent":"DistributedDataParallel\n-----------------------\n\n- `distributed.py <https://github.com/pytorch/pytorch/blob/v1.7.0/torch/nn/parallel/distributed.py>`__:\n  is the Python entry point for DDP. It implements the initialization steps and\n  the ``forward`` function for the ``nn.parallel.DistributedDataParallel``\n  module which call into C++ libraries. Its ``_sync_param`` function performs\n  intra-process parameter synchronization when one DDP process works on multiple\n  devices, and it also broadcasts model buffers from the process with rank 0 to\n  all other processes. The inter-process parameter synchronization happens in\n  ``Reducer.cpp``.\n\n- `comm.h <https://github.com/pytorch/pytorch/blob/v1.7.0/torch/csrc/distributed/c10d/comm.h>`__:\n  implements the coalesced broadcast helper function which is invoked to\n  broadcast model states during initialization and synchronize model buffers\n  before the forward pass.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":187,"to":202}}}}],["1023",{"pageContent":"- `reducer.h <https://github.com/pytorch/pytorch/blob/v1.7.0/torch/csrc/distributed/c10d/reducer.h>`__:\n  provides the core implementation for gradient synchronization in the backward\n  pass. It has three entry point functions:\n\n  * ``Reducer``: The constructor is called in ``distributed.py`` which registers\n    ``Reducer::autograd_hook()`` to gradient accumulators.\n  * ``autograd_hook()`` function will be invoked by the autograd engine when\n    a gradient becomes ready.\n  * ``prepare_for_backward()`` is called at the end of DDP forward pass in\n    ``distributed.py``. It traverses the autograd graph to find unused\n    parameters when ``find_unused_parameters`` is set to ``True`` in DDP\n    constructor.\n\n.. image:: https://user-images.githubusercontent.com/16999635/72313120-4e7c1c80-3658-11ea-9c6d-44336b2daeac.png\n    :alt: ddp_code.png\n    :width: 400 px\n\n\nTorchDynamo DDPOptimizer\n------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":204,"to":223}}}}],["1024",{"pageContent":"TorchDynamo DDPOptimizer\n------------------------\n\nDDP's performance advantage comes from overlapping allreduce collectives with computations during backwards.\nAotAutograd prevents this overlap when used with TorchDynamo for compiling a whole forward and whole backward graph,\nbecause allreduce ops are launched by autograd hooks _after_ the whole optimized backwards computation finishes.\n\nTorchDynamo's DDPOptimizer helps by breaking the forward graph at the logical boundaries of DDP's allreduce buckets\nduring backwards.  Note: the goal is to break the graph during backwards, and the simplest implementation is to\nbreak the forward graphs and then call AotAutograd and compilation on each section.  This allows DDP's allreduce hooks\nto fire in-between sections of backwards, and schedule communications to overlap with compute.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":223,"to":233}}}}],["1025",{"pageContent":"See `this blog post <https://dev-discuss.pytorch.org/t/torchdynamo-update-9-making-ddp-work-with-torchdynamo/860/1>`_ for\na more in-depth explanation and experimental results, or read the docs and code at\n`torch/_dynamo/optimizations/distributed.py <https://github.com/pytorch/pytorch/blob/4908a12542798a3e8641faae6b74f068fdfc6778/torch/_dynamo/optimizations/distributed.py#L56>`_\n\nTo Debug DDPOptimizer, set `torch._dynamo.config.log_level` to DEBUG (for full graph dumps) or INFO\n(for basic info about bucket boundaries).  To disable DDPOptimizer, set `torch._dynamo.config.optimize_ddp=False`.\nDDP and TorchDynamo should still work correctly without DDPOptimizer, but with performance degradation.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/ddp.rst","loc":{"lines":{"from":235,"to":241}}}}],["1026",{"pageContent":".. _func-autograd-function:\n\nExtending torch.func with autograd.Function\n===========================================\n\n.. currentmodule:: torch.autograd\n\nSo you'd like to use :class:`torch.autograd.Function` with the :mod:`torch.func`\ntransforms like :func:`torch.vmap`, :func:`torch.func.grad`, etc.\n\nThere are two main use cases:\n\n- you wish to call code that does not contain PyTorch operations and\n  have it work with function transforms. That is, the :class:`torch.autograd.Function`'s\n  forward/backward/etc calls into functions from other systems like C++, CUDA, numpy.\n- you wish to specify custom gradient rules, like\n  JAX's `custom_vjp/custom_jvp <https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html>`_\n\nPyTorch combines both of these concepts into :class:`torch.autograd.Function`.\n\nBasic Usage\n-----------\n\nThis guide assumes you are familiar with :ref:`extending-autograd`,\nwhich explains how to use :class:`torch.autograd.Function`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":1,"to":25}}}}],["1027",{"pageContent":"Basic Usage\n-----------\n\nThis guide assumes you are familiar with :ref:`extending-autograd`,\nwhich explains how to use :class:`torch.autograd.Function`.\n\n:class:`torch.autograd.Function` can either have a :meth:`~Function.forward` that accepts a ctx object,\nor it can have separate :meth:`~Function.forward` (that does not accept ``ctx``) and a :meth:`~Function.setup_context`\nstaticmethod that modifies the ``ctx`` object.\n\nOnly the latter is supported with function transforms:\n\n- :meth:`~Function.forward` is the code that performs the operation and it should not accept\n  a ``ctx`` object.\n- ``setup_context(ctx, inputs, output)`` is the code where you can\n  call methods on ``ctx``. Here is where you should save Tensors for backward\n  (by calling ``ctx.save_for_backward(*tensors)``), or save non-Tensors\n  (by assigning them to the ``ctx`` object).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":25,"to":42}}}}],["1028",{"pageContent":"Because :meth:`~Function.setup_context` accepts only ``inputs`` and ``output``,\nthe only quantities that can be saved are either objects (such as Tensors) in\nthe inputs or outputs or quantities (like ``Tensor.shape``) derived from them.\nIf you wish to save a non-input intermediate activation from\n:meth:`Function.forward` for backward, then you'll need to return it as an\noutput from :meth:`~Function.forward` so that it gets passed to\n:meth:`~Function.setup_context`.\n\nDepending on the transform,","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":44,"to":52}}}}],["1029",{"pageContent":"Depending on the transform,\n\n- to support reverse-mode AD (:func:`torch.func.grad`, :func:`torch.func.vjp`),\n  the :class:`torch.autograd.Function` needs a :meth:`~Function.backward` staticmethod.\n- to support :func:`torch.vmap`, the :class:`torch.autograd.Function` needs a :meth:`~Function.vmap` staticmethod.\n- to support :func:`torch.func.jvp`, the :class:`torch.autograd.Function` needs a :meth:`~Function.jvp` staticmethod.\n- to support compositions of transforms (like :func:`torch.func.jacrev`,\n  :func:`torch.func.jacfwd`, :func:`torch.func.hessian`) -- you may need multiple\n  of the above.\n\nIn order for the :class:`torch.autograd.Function` to be arbitrarily composable with function\ntransforms, we recommend that all other staticmethods other than :meth:`~Function.forward` and\n:meth:`~Function.setup_context` must be transformable: that is, they must consist of only PyTorch\noperators or call other :class:`torch.autograd.Function` (that may call into C++/CUDA/etc).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":52,"to":65}}}}],["1030",{"pageContent":"Let's go over some examples of common use cases.\n\nExample 1: autograd.Function calls into another system\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nA common case is a :class:`torch.autograd.Function` with both forward() and backward() calling\ninto another system (like C++, CUDA, numpy, triton).\n\n::\n\n    import torch\n    import numpy as np\n\n    def to_numpy(tensor):\n        return tensor.cpu().numpy()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":67,"to":81}}}}],["1031",{"pageContent":"::\n\n    import torch\n    import numpy as np\n\n    def to_numpy(tensor):\n        return tensor.cpu().numpy()\n\n    class NumpySort(torch.autograd.Function):\n        # Note that forward does not take ctx\n        @staticmethod\n        def forward(x, dim):\n            device = x.device\n            x = to_numpy(x)\n            ind = np.argsort(x, axis=dim)\n            ind_inv = np.argsort(ind, axis=dim)\n            result = np.take_along_axis(x, ind, axis=dim)\n            # Any intermediates to be saved in backward must be returned as\n            # outputs.\n            return (\n                # The desired output\n                torch.tensor(result, device=device),\n                # intermediate to save for backward\n                torch.tensor(ind, device=device),\n                # intermediate to save for backward\n                torch.tensor(ind_inv, device=device),\n            )","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":81,"to":107}}}}],["1032",{"pageContent":"# setup_context is responsible for calling methods and/or assigning to\n        # the ctx object. Please do not do additional compute (e.g. add\n        # Tensors together) in setup_context.\n        @staticmethod\n        def setup_context(ctx, inputs, output):\n            x, dim = inputs\n            # Note that output is whatever you returned from forward.\n            # If you returned multiple values, then output is a Tuple of multiple values.\n            # If you returned a single Tensor, then output is a Tensor.\n            # If you returned a Tuple with a single Tensor, then output is a\n            # Tuple with a single Tensor.\n            _, ind, ind_inv = output\n            ctx.mark_non_differentiable(ind, ind_inv)\n            # Tensors must be saved via ctx.save_for_backward. Please do not\n            # assign them directly onto the ctx object.\n            ctx.save_for_backward(ind, ind_inv)\n            # Non-tensors may be saved by assigning them as attributes on the ctx object.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":109,"to":125}}}}],["1033",{"pageContent":"# assign them directly onto the ctx object.\n            ctx.save_for_backward(ind, ind_inv)\n            # Non-tensors may be saved by assigning them as attributes on the ctx object.\n            ctx.dim = dim","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":125,"to":128}}}}],["1034",{"pageContent":"@staticmethod\n        def backward(ctx, grad_output, _0, _1):\n            # For the autograd.Function to be arbitrarily composable with function\n            # transforms, all staticmethod other than forward and setup_context\n            # must be implemented in a \"transformable\" way; that is, they must\n            # only consist of PyTorch operations or autograd.Function.\n            #\n            # For example, this allows us to do double backwards and/or compute\n            # second order gradients.\n            #\n            # We've written the backward pass of NumpySort in terms of another\n            # autograd.Function, NumpyTake.\n            ind, ind_inv = ctx.saved_tensors\n            return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":130,"to":143}}}}],["1035",{"pageContent":"class NumpyTake(torch.autograd.Function):\n        @staticmethod\n        def forward(x, ind, ind_inv, dim):\n            device = x.device\n            x = to_numpy(x)\n            ind = to_numpy(ind)\n            return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n        @staticmethod\n        def setup_context(ctx, inputs, output):\n            x, ind, ind_inv, dim = inputs\n            ctx.save_for_backward(ind, ind_inv)\n            ctx.dim = dim\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            ind, ind_inv = ctx.saved_tensors\n            result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n            return result, None, None, None\n\n\nNow, to make it easier to use ``NumpySort`` (to hide away the intermediates we\nreturned as outputs, as well as allow default args and kwargs), we create a new\nfunction that invokes it::\n\n    def numpy_sort(x, dim=-1):\n        result, _, _ = NumpySort.apply(x, dim)\n        return result","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":145,"to":172}}}}],["1036",{"pageContent":"def numpy_sort(x, dim=-1):\n        result, _, _ = NumpySort.apply(x, dim)\n        return result\n\nAnd here's a sanity check::\n\n    x = torch.randn(2, 3)\n    grad_x = torch.func.grad(lambda x: numpy_sort(x).sum())(x)\n    assert torch.allclose(grad_x, torch.ones_like(x))\n\n\n\nExample 2: autograd.Function specifies custom gradient rules\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAnother common case is an :class:`torch.autograd.Function` that is implemented with PyTorch\noperations. PyTorch is able to compute gradients for PyTorch operations automatically,\nbut perhaps we wish to customize how the gradients are computed. Some reasons why\nwe may want a custom backward different from the one PyTorch gives us are:\n\n- improving numeric stability\n- changing the performance characteristics of the backward\n- changing how edge cases are handled (e.g. nans, inf)\n- modifying the gradient (e.g. gradient clipping)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":172,"to":195}}}}],["1037",{"pageContent":"- improving numeric stability\n- changing the performance characteristics of the backward\n- changing how edge cases are handled (e.g. nans, inf)\n- modifying the gradient (e.g. gradient clipping)\n\nHere's an example of an :class:`torch.autograd.Function` for the function ``y = x ** 3`` where we\nchange the performance characteristics (some computation that would normally happen\nduring the backward pass, computing dx, happens in the forward pass).\n\n::\n\n  class MyCube(torch.autograd.Function):\n      @staticmethod\n      def forward(x):\n          result = x ** 3\n          # In regular PyTorch, if we had just run y = x ** 3, then the backward\n          # pass computes dx = 3 * x ** 2. In this autograd.Function, we've done\n          # that computation here in the forward pass instead.\n          dx = 3 * x ** 2\n          return result, dx\n\n      @staticmethod\n      def setup_context(ctx, inputs, output):\n          x, = inputs\n          result, dx = output\n          ctx.save_for_backward(x, dx)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":195,"to":220}}}}],["1038",{"pageContent":"@staticmethod\n      def setup_context(ctx, inputs, output):\n          x, = inputs\n          result, dx = output\n          ctx.save_for_backward(x, dx)\n\n      @staticmethod\n      def backward(ctx, grad_output, grad_dx):\n          x, dx = ctx.saved_tensors\n          # In order for the autograd.Function to work with higher-order\n          # gradients, we must add the gradient contribution of `dx`.\n          result = grad_output * dx + grad_dx * 6 * x\n          return result\n\nNow, to make it easier to use ``NumpySort`` (and hide away the intermediates we\nreturned as outputs) we create a new function that invokes it::\n\n    def my_cube(x):\n        result, _ = MyCube.apply(x)\n        return result\n\nHere's a sanity check computing the second-order gradients::\n\n    x = torch.randn([])\n    ggx = torch.func.grad(torch.func.grad(my_cube))(x)\n    assert torch.allclose(ggx, 6 * x)\n\nLimitations and gotchas\n^^^^^^^^^^^^^^^^^^^^^^^\n\n.. warning::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":220,"to":250}}}}],["1039",{"pageContent":"x = torch.randn([])\n    ggx = torch.func.grad(torch.func.grad(my_cube))(x)\n    assert torch.allclose(ggx, 6 * x)\n\nLimitations and gotchas\n^^^^^^^^^^^^^^^^^^^^^^^\n\n.. warning::\n\n    Please read these limitations of :class:`torch.autograd.Function` with torch.func transforms\n    carefully. We are not able to catch many of these situations and error out\n    gracefully so they will lead to undefined behavior.\n\nPlease do not capture Tensors that are being transformed over, have\nrequires_grad=True, or are dual tensors, into the methods of the\n:class:`torch.autograd.Function`. The way to be completely safe is to ensure that the only\nTensors being used inside any method of the :class:`torch.autograd.Function` must be directly\npassed as inputs (or via the ctx object) rather than come from outside\nthe :class:`torch.autograd.Function`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":250,"to":268}}}}],["1040",{"pageContent":":class:`torch.autograd.Function` does not handle Tensors in pytrees (arbitrary nested\nPython data structures that may or may not contain Tensors). For\nthose Tensors to be tracked by autograd, they must be passed directly as\nan argument to :class:`torch.autograd.Function`. This is in contrast to\njax.{custom_vjp, custom_jvp}, which do accept pytrees.\n\nPlease only use :meth:`~torch.autograd.function.FunctionCtx.save_for_backward` or\n:meth:`~torch.autograd.function.FunctionCtx.save_for_forward` to save Tensors.\nPlease do not assign Tensors or collections of Tensors directly onto the ctx object -\nthese Tensors will not get tracked\n\n\n:func:`torch.vmap` Support\n--------------------------\n\nTo use an :class:`torch.autograd.Function` with :func:`torch.vmap`, you must either:\n\n- provide a :meth:`~Function.vmap` staticmethod that tells us the behavior of the :class:`torch.autograd.Function`\n  under :func:`torch.vmap`\n- ask us to autogenerate it by setting ``generate_vmap_rule=True``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":270,"to":289}}}}],["1041",{"pageContent":"Automatically generate a vmap rule\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf your :class:`torch.autograd.Function` fulfills the following additional constraints, then we\nare able to generate a vmap rule for it. If it doesn't fulfill the constraints or if you\nwant custom behavior under vmap, please manually define a vmap staticmethod (see next section).\n\n.. warning::\n\n     We are not easily able to check for the following constraints and error\n     out gracefully. Violation of the constraints may lead to undefined\n     behavior.\n\n- The :class:`torch.autograd.Function`'s :meth:`~Function.forward`, :meth:`~Function.backward` (if it exists) and :meth:`~Function.jvp`\n  (if it exists) staticmethods must be transformable via :func:`torch.vmap`. That\n  is, they must consist of only PyTorch operations (as opposed to e.g. NumPy or custom\n  CUDA kernels).\n\nExample::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":291,"to":309}}}}],["1042",{"pageContent":"Example::\n\n    class MyCube(torch.autograd.Function):\n        # Set generate_vmap_rule to True to ask PyTorch to automatically generate\n        # a vmap rule.\n        generate_vmap_rule = True\n\n        @staticmethod\n        def forward(x):\n            result = x ** 3\n            dx = 3 * x ** 2\n            return result, dx\n\n        @staticmethod\n        def setup_context(ctx, inputs, output):\n            x, = inputs\n            result, dx = output\n            ctx.save_for_backward(x, dx)\n\n        @staticmethod\n        def backward(ctx, grad_output, grad_dx):\n            x, dx = ctx.saved_tensors\n            result = grad_output * dx + grad_dx * 6 * x\n            return result\n\n    def my_cube(x):\n        result, dx = MyCube.apply(x)\n        return result\n\n    x = torch.randn(3)\n    result = torch.vmap(my_cube)(x)\n    assert torch.allclose(result, x ** 3)\n\n\nDefining the vmap staticmethod\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":309,"to":344}}}}],["1043",{"pageContent":"x = torch.randn(3)\n    result = torch.vmap(my_cube)(x)\n    assert torch.allclose(result, x ** 3)\n\n\nDefining the vmap staticmethod\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf your :class:`torch.autograd.Function` calls into another system (like NumPy, C++, CUDA, triton),\nthen to get it to work with :func:`torch.vmap` or transforms that use it, you'll\nneed to manually define a :meth:`~Function.vmap` staticmethod.\n\nDepending on what transforms you want to use and your use case, you may not need\nto add a :meth:`~Function.vmap` staticmethod to all of your :class:`torch.autograd.Function`:\n\n- For example, :func:`torch.func.jacrev` performs :func:`~torch.vmap` over the backward pass.\n  So if you're only interested in using :func:`torch.func.jacrev`, only\n  the :meth:`~Function.backward` staticmethod needs to be vmappable.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":344,"to":361}}}}],["1044",{"pageContent":"We do recommend ensuring all of your :class:`torch.autograd.Function` have support for\n:func:`torch.vmap` though, especially if you are writing a third-party library and you want your\n:class:`torch.autograd.Function` to work with all combinations of :func:`torch.func` transforms.\n\nConceptually, the vmap staticmethod is responsible for defining how the :meth:`~Function.forward`\nshould behave under :func:`torch.vmap`. That is, it defines how to transform\nthe :meth:`~Function.forward` to run over inputs with an additional dimension (the dimension\nbeing vmapped over). This is similar to how :func:`torch.vmap` is implemented over\nPyTorch operations: for each operation, we define a vmap rule (sometimes also\nreferred to as a \"batching rule\").\n\nHere's how to define the :meth:`~Function.vmap` staticmethod:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":363,"to":374}}}}],["1045",{"pageContent":"- the signature is ``vmap(info, in_dims: Tuple[Optional[int]], *args)``, where\n  ``*args`` is the same as the args to :meth:`~Function.forward`.\n- The vmap staticmethod is responsible for defining how the :meth:`~Function.forward` should behave\n  under :func:`torch.vmap`. That is, given inputs with an additional dimension\n  (specified by ``in_dims``), how do we compute the batched version of :meth:`~Function.forward`?\n- For each arg in ``args``, ``in_dims`` has a corresponding ``Optional[int]``.\n  It is ``None`` if the arg is not a Tensor or if the arg is not being vmapped over,\n  otherwise, it is an integer specifying what dimension of the Tensor is being vmapped\n  over.\n- ``info`` is a collection of additional metadata that may be helpful:\n  ``info.batch_size`` specifies the size of the dimension being vmapped over, while\n  ``info.randomness`` is the ``randomness`` option that was passed to :func:`torch.vmap`.\n- The return of the vmap staticmethod is a tuple of ``(output, out_dims)``. Similar","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":376,"to":388}}}}],["1046",{"pageContent":"``info.randomness`` is the ``randomness`` option that was passed to :func:`torch.vmap`.\n- The return of the vmap staticmethod is a tuple of ``(output, out_dims)``. Similar\n  to ``in_dims``, ``out_dims`` should be of the same structure as ``output`` and contain\n  one ``out_dim`` per output that specifies if the output has the vmapped\n  dimension and what index it is in.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":388,"to":392}}}}],["1047",{"pageContent":"Example::\n\n    def to_numpy(tensor):\n        return tensor.cpu().numpy()\n\n    class NumpySort(torch.autograd.Function):\n        @staticmethod\n        def forward(x, dim):\n            device = x.device\n            x = to_numpy(x)\n            ind = np.argsort(x, axis=dim)\n            ind_inv = np.argsort(ind, axis=dim)\n            result = np.take_along_axis(x, ind, axis=dim)\n            return (\n                torch.tensor(result, device=device),\n                torch.tensor(ind, device=device),\n                torch.tensor(ind_inv, device=device),\n            )\n\n        @staticmethod\n        def setup_context(ctx, inputs, output):\n            x, dim = inputs\n            _, ind, ind_inv = output\n            ctx.mark_non_differentiable(ind, ind_inv)\n            ctx.save_for_backward(ind, ind_inv)\n            ctx.dim = dim\n\n        @staticmethod\n        def backward(ctx, grad_output, _0, _1):\n            return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":395,"to":424}}}}],["1048",{"pageContent":"@staticmethod\n        def backward(ctx, grad_output, _0, _1):\n            return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n\n        # The signature of the vmap staticmethod is:\n        # vmap(info, in_dims: Tuple[Optional[int]], *args)\n        # where *args is the same as the arguments to `forward`.\n        @staticmethod\n        def vmap(info, in_dims, x, dim):\n            # For every input (x and dim), in_dims stores an Optional[int]\n            # that is:\n            # - None if the input is not being vmapped over or if the input\n            #   is not a Tensor\n            # - an integer if the input is being vmapped over that represents\n            #   the index of the dimension being vmapped over.\n            x_bdim, _ = in_dims","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":424,"to":439}}}}],["1049",{"pageContent":"# A \"vmap rule\" is the logic of how to perform the operation given\n            # inputs with one additional dimension. In NumpySort, x has an\n            # additional dimension (x_bdim). The vmap rule is simply\n            # to call NumpySort again but pass it a different `dim`.\n            x = x.movedim(x_bdim, 0)\n            # Handle negative dims correctly\n            dim = dim if dim >= 0 else dim + x.dim() - 1\n            result = NumpySort.apply(x, dim + 1)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":441,"to":448}}}}],["1050",{"pageContent":"# The vmap rule must return a tuple of two things\n            # 1. the output. Should be the same amount of things\n            #    as returned by the forward().\n            # 2. one Optional[int] for each output specifying if each output\n            # is being vmapped over, and if so, the index of the\n            # dimension being vmapped over.\n            #\n            # NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the\n            # dimension being vmapped over to the front of `x`, that appears at\n            # dimension 0 of all outputs.\n            # The return is (output, out_dims) -- output is a tuple of 3 Tensors\n            # and out_dims is a Tuple of 3 Optional[int]\n            return NumpySort.apply(x, dim + 1), (0, 0, 0)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":450,"to":462}}}}],["1051",{"pageContent":"class NumpyTake(torch.autograd.Function):\n        @staticmethod\n        def forward(x, ind, ind_inv, dim):\n            device = x.device\n            x = to_numpy(x)\n            ind = to_numpy(ind)\n            return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n        @staticmethod\n        def setup_context(ctx, inputs, output):\n            x, ind, ind_inv, dim = inputs\n            ctx.save_for_backward(ind, ind_inv)\n            ctx.dim = dim\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            ind, ind_inv = ctx.saved_tensors\n            result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n            return result, None, None, None\n\n        @staticmethod\n        def vmap(info, in_dims, x, ind, ind_inv, dim):\n            x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":464,"to":486}}}}],["1052",{"pageContent":"@staticmethod\n        def vmap(info, in_dims, x, ind, ind_inv, dim):\n            x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims\n\n            # The strategy is: expand {x, ind, ind_inv} to all have the dimension\n            # being vmapped over.\n            # Then, call back into NumpyTake(expanded_x, expanded_ind, expanded_ind_inv, new_dim).\n\n            # Handle negative dims by wrapping them to be positive\n            logical_dim = x.dim() if x_bdim is None else x_bdim - 1\n            dim = dim if dim >= 0 else dim + logical_dim\n\n            def maybe_expand_bdim_at_front(x, x_bdim):\n                if x_bdim is None:\n                    return x.expand(info.batch_size, *x.shape)\n                return x.movedim(x_bdim, 0)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":486,"to":501}}}}],["1053",{"pageContent":"def maybe_expand_bdim_at_front(x, x_bdim):\n                if x_bdim is None:\n                    return x.expand(info.batch_size, *x.shape)\n                return x.movedim(x_bdim, 0)\n\n            # If the Tensor doesn't have the dimension being vmapped over,\n            # expand it out. Otherwise, move it to the front of the Tensor\n            x = maybe_expand_bdim_at_front(x, x_bdim)\n            ind = maybe_expand_bdim_at_front(ind, ind_bdim)\n            ind_inv = maybe_expand_bdim_at_front(ind_inv, ind_inv_bdim)\n\n            # The return is a tuple (output, out_dims). Since output is a Tensor,\n            # then out_dims is an Optional[int] (instead of being a Tuple).\n            return NumpyTake.apply(x, ind, ind_inv, dim + 1), 0\n\n    def numpy_sort(x, dim=-1):\n        result, _, _ = NumpySort.apply(x, dim)\n        return result\n\n    x = torch.randn(2, 3)\n    result = torch.vmap(numpy_sort)(x)\n    assert torch.allclose(result, numpy_sort(result, 1))\n\n\n.. note::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":501,"to":525}}}}],["1054",{"pageContent":"x = torch.randn(2, 3)\n    result = torch.vmap(numpy_sort)(x)\n    assert torch.allclose(result, numpy_sort(result, 1))\n\n\n.. note::\n\n    The vmap staticmethod should aim to preserve the semantics of the\n    entire :class:`~torch.autograd.Function`. That is, (pseudocode) ``grad(vmap(MyFunc))``\n    should be replaceable with a ``grad(map(MyFunc))``.\n\n    If your autograd.Function has any custom behavior in the backward pass, please\n    keep this in mind.\n\n.. note::\n\n    It is a legitimate use case to write a custom vmap staticmethod for a\n    :class:`~torch.autograd.Function` that PyTorch is able to generate a vmap\n    rule for via ``generate_vmap_rule=True``. You may wish to do this if the\n    generated vmap rule doesn't have the semantics you're looking for.\n\n:func:`torch.func.jvp` Support\n------------------------------\n\nTo support forward-mode AD, a :class:`torch.autograd.Function` must have a :meth:`~Function.jvp` staticmethod.\nPlease see :ref:`forward-ad-autograd-function` for details.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.func.rst","loc":{"lines":{"from":525,"to":550}}}}],["1055",{"pageContent":"Extending PyTorch\n=================\n\nIn this note we'll cover ways of extending :mod:`torch.nn`,\n:mod:`torch.autograd`, :mod:`torch`, and writing custom C extensions utilizing our C\nlibraries.\n\n.. _extending-autograd:\n\nExtending :mod:`torch.autograd`\n-------------------------------\n\n.. currentmodule:: torch.autograd\n\nAdding operations to :mod:`~torch.autograd` requires implementing a new\n:class:`Function` subclass for each operation. Recall that Functions\nare what :mod:`~torch.autograd` uses to encode the operation history and compute\ngradients.\n\nThe first part of this doc is focused on backward mode AD as it is the most widely used\nfeature. A section at the end discusses the extensions for forward mode AD.\n\nWhen to use\n^^^^^^^^^^^\nIn general, implement a custom function if you want to perform computations in your model\nthat are not differentiable or rely on non-PyTorch libraries (e.g., NumPy), but\nstill wish for your operation to chain with other ops and work with the autograd engine.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":1,"to":27}}}}],["1056",{"pageContent":"In some situations, custom functions can also be used to improve performance and\nmemory usage: If you implemented your forward and backward passes using a\n`C++ extension <https://pytorch.org/tutorials/advanced/cpp_extension.html>`_,\nyou can wrap them in :class:`~Function` to interface with the autograd\nengine. If you'd like to reduce the number of buffers saved for the backward pass,\ncustom functions can be used to combine ops together.\n\nWhen not to use\n^^^^^^^^^^^^^^^\nIf you can already write your function in terms of PyTorch's built-in ops, its\nbackward graph is (most likely) already able to be recorded by autograd. In this case, you do\nnot need to implement the backward function yourself. Consider using a plain\nold Python function.\n\nIf you need to maintain state, i.e., trainable parameters, you should (also) use a\ncustom module. See the section below for more information on extending :mod:`torch.nn`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":29,"to":44}}}}],["1057",{"pageContent":"If you need to maintain state, i.e., trainable parameters, you should (also) use a\ncustom module. See the section below for more information on extending :mod:`torch.nn`.\n\nIf you'd like to alter the gradients during the backward pass or perform a side\neffect, consider registering a\n`tensor <https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook>`_ or\n`Module <https://pytorch.org/docs/stable/notes/modules.html#module-hooks>`_ hook.\n\nHow to use\n^^^^^^^^^^\nTake the following steps:\n1. Subclass :class:`~Function` and implement the :meth:`~Function.forward`,\n(optional) :meth:`~Function.setup_context` and\n:meth:`~Function.backward` methods.\n2. Call the proper methods on the `ctx` argument.\n3. Declare whether your function supports\n`double backward <https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html>`_.\n4. Validate whether your gradients are correct using gradcheck.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":44,"to":61}}}}],["1058",{"pageContent":"**Step 1:** After subclassing :class:`Function`, you'll need to define 3 methods:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":63,"to":63}}}}],["1059",{"pageContent":"- :meth:`~Function.forward` is the code that performs the operation. It can take\n  as many arguments as you want, with some of them being optional, if you\n  specify the default values. All kinds of Python objects are accepted here.\n  :class:`Tensor` arguments that track history (i.e., with\n  ``requires_grad=True``) will be converted to ones that don't track history\n  before the call, and their use will be registered in the graph. Note that this\n  logic won't traverse lists/dicts/any other data structures and will only\n  consider tensors that are direct arguments to the call. You can\n  return either a single :class:`Tensor` output, or a :class:`tuple` of\n  tensors if there are multiple outputs. Also, please refer to the\n  docs of :class:`Function` to find descriptions of useful methods that can be\n  called only from :meth:`~Function.forward`.\n- :meth:`~Function.setup_context` (optional). One can either write a \"combined\" :meth:`~Function.forward` that","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":65,"to":77}}}}],["1060",{"pageContent":"called only from :meth:`~Function.forward`.\n- :meth:`~Function.setup_context` (optional). One can either write a \"combined\" :meth:`~Function.forward` that\n  accepts a ``ctx`` object or (as of PyTorch 2.0) a separate :meth:`~Function.forward` that does\n  not accept ``ctx`` and a :meth:`~Function.setup_context` method where the ``ctx`` modification happens.\n  The :meth:`~Function.forward` should have the compute and :meth:`~Function.setup_context` should\n  only be responsible for the ``ctx`` modification (and not have any compute).\n  In general the separate :meth:`~Function.forward` and :meth:`~Function.setup_context` is closer to how\n  PyTorch native operations work and therefore more composable with various PyTorch subsystems.\n  See :ref:`combining-forward-context` for more details.\n- :meth:`~Function.backward` (or :meth:`~Function.vjp`) defines the gradient formula.\n  It will be given as many :class:`Tensor` arguments as there were outputs, with each","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":77,"to":87}}}}],["1061",{"pageContent":"- :meth:`~Function.backward` (or :meth:`~Function.vjp`) defines the gradient formula.\n  It will be given as many :class:`Tensor` arguments as there were outputs, with each\n  of them representing gradient w.r.t. that output. It is important NEVER to modify\n  these in-place. It should return as many tensors as there\n  were inputs, with each of them containing the gradient w.r.t. its\n  corresponding input. If your inputs didn't require gradient\n  (:attr:`~ctx.needs_input_grad` is a tuple of booleans indicating\n  whether each input needs gradient computation), or were non-:class:`Tensor`\n  objects, you can return :class:`python:None`. Also, if you have optional\n  arguments to :meth:`~Function.forward` you can return more gradients than there\n  were inputs, as long as they're all :any:`python:None`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":87,"to":97}}}}],["1062",{"pageContent":"**Step 2:** It is your responsibility to use the functions in ``ctx``\nproperly in order to ensure that the new :class:`Function` works properly with\nthe autograd engine.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":99,"to":101}}}}],["1063",{"pageContent":"- :meth:`~torch.autograd.function.FunctionCtx.save_for_backward` must be\n  used to save any tensors to be used in the backward pass. Non-tensors should\n  be stored directly on `ctx`. If tensors that are neither input nor output\n  are saved for backward your :class:`~Function` may not support double backward\n  (see step 3).\n- :meth:`~torch.autograd.function.FunctionCtx.mark_dirty` must be used to\n  mark any input that is modified inplace by the forward function.\n- :meth:`~torch.autograd.function.FunctionCtx.mark_non_differentiable` must\n  be used to tell the engine if an output is not differentiable. By\n  default all output tensors that are of differentiable type will be set\n  to require gradient. Tensors of non-differentiable type (i.e., integral types)\n  are never marked as requiring gradients.\n- :meth:`~torch.autograd.function.FunctionCtx.set_materialize_grads` can be\n  used to tell the autograd engine to optimize gradient computations in the cases where","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":103,"to":116}}}}],["1064",{"pageContent":"- :meth:`~torch.autograd.function.FunctionCtx.set_materialize_grads` can be\n  used to tell the autograd engine to optimize gradient computations in the cases where\n  the output does not depend on the input by not materializing grad tensors given to backward\n  function. That is, if set to False, None object in python or \"undefined tensor\" (tensor x for\n  which x.defined() is False) in C++ will not be converted to a tensor filled with zeros prior\n  to calling backward, and so your code will need to handle such objects as if they were\n  tensors filled with zeros. The default value of this setting is True.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":116,"to":122}}}}],["1065",{"pageContent":"**Step 3:** If your :class:`~Function` does not support double backward\nyou should explicitly declare this by decorating backward with the\n:func:`~function.once_differentiable`. With this decorator, attempts to\nperform double backward through your function will produce an error.\nSee our double backward tutorial for more information on double backward.\n\n**Step 4:** It is recommended that you use :func:`torch.autograd.gradcheck`\nto check whether your backward function correctly computes gradients of the\nforward by computing the Jacobian matrix using your backward function and\ncomparing the value element-wise with the Jacobian computed numerically using\nfinite-differencing.\n\nExample\n^^^^^^^\n\nBelow you can find code for a ``Linear`` function, with\nadditional comments::\n\n    # Inherit from Function\n    class LinearFunction(Function):","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":124,"to":143}}}}],["1066",{"pageContent":"Example\n^^^^^^^\n\nBelow you can find code for a ``Linear`` function, with\nadditional comments::\n\n    # Inherit from Function\n    class LinearFunction(Function):\n\n        # Note that forward, setup_context, and backward are @staticmethods\n        @staticmethod\n        def forward(input, weight, bias):\n            output = input.mm(weight.t())\n            if bias is not None:\n                output += bias.unsqueeze(0).expand_as(output)\n            return output\n\n        @staticmethod\n        # inputs is a Tuple of all of the inputs passed to forward.\n        # output is the output of the forward().\n        def setup_context(ctx, inputs, output):\n            input, weight, bias = inputs\n            ctx.save_for_backward(input, weight, bias)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":143,"to":165}}}}],["1067",{"pageContent":"# This function has only a single output, so it gets only one gradient\n        @staticmethod\n        def backward(ctx, grad_output):\n            # This is a pattern that is very convenient - at the top of backward\n            # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n            # None. Thanks to the fact that additional trailing Nones are\n            # ignored, the return statement is simple even when the function has\n            # optional inputs.\n            input, weight, bias = ctx.saved_tensors\n            grad_input = grad_weight = grad_bias = None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":167,"to":176}}}}],["1068",{"pageContent":"# These needs_input_grad checks are optional and there only to\n            # improve efficiency. If you want to make your code simpler, you can\n            # skip them. Returning gradients for inputs that don't require it is\n            # not an error.\n            if ctx.needs_input_grad[0]:\n                grad_input = grad_output.mm(weight)\n            if ctx.needs_input_grad[1]:\n                grad_weight = grad_output.t().mm(input)\n            if bias is not None and ctx.needs_input_grad[2]:\n                grad_bias = grad_output.sum(0)\n\n            return grad_input, grad_weight, grad_bias\n\nNow, to make it easier to use these custom ops, we recommend either aliasing\nthem or wrapping them in a function. Wrapping in a function lets us support\ndefault arguments and keyword arguments::\n\n    # Option 1: alias\n    linear = LinearFunction.apply","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":178,"to":196}}}}],["1069",{"pageContent":"# Option 1: alias\n    linear = LinearFunction.apply\n\n    # Option 2: wrap in a function, to support default args and keyword args.\n    def linear(input, weight, bias=None):\n        return LinearFunction.apply(input, weight, bias)\n\nHere, we give an additional example of a function that is parametrized by\nnon-Tensor arguments::\n\n    class MulConstant(Function):\n        @staticmethod\n        def forward(tensor, constant):\n            return tensor * constant\n\n        @staticmethod\n        def setup_context(ctx, inputs, output):\n            # ctx is a context object that can be used to stash information\n            # for backward computation\n            tensor, constant = inputs\n            ctx.constant = constant\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            # We return as many input gradients as there were arguments.\n            # Gradients of non-Tensor arguments to forward must be None.\n            return grad_output * ctx.constant, None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":196,"to":222}}}}],["1070",{"pageContent":"And here, we optimize the above example by calling set_materialize_grads(False)::\n\n    class MulConstant(Function):\n        @staticmethod\n        def forward(tensor, constant):\n            return tensor * constant\n\n        @staticmethod\n        def setup_context(ctx, inputs, output):\n            tensor, constant = inputs\n            ctx.set_materialize_grads(False)\n            ctx.constant = constant\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            # Here we must handle None grad_output tensor. In this case we\n            # can skip unnecessary computations and just return None.\n            if grad_output is None:\n                return None, None\n\n            # We return as many input gradients as there were arguments.\n            # Gradients of non-Tensor arguments to forward must be None.\n            return grad_output * ctx.constant, None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":224,"to":246}}}}],["1071",{"pageContent":"# We return as many input gradients as there were arguments.\n            # Gradients of non-Tensor arguments to forward must be None.\n            return grad_output * ctx.constant, None\n\nIf you need any \"intermediate\" Tensors computed in :meth:`~Function.forward` to be saved,\neither they must be returned as outputs, or combine ``forward`` and :meth:`~Function.setup_context`\n(see :ref:`combining-forward-context`).\nNote that this means if you want gradients to flow through those intermediate values, you\nneed to define the gradient formula for them (see also\n`the double backward tutorial <https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html>`_\n)::\n\n    class MyCube(torch.autograd.Function):\n        @staticmethod\n        def forward(x):\n            # We wish to save dx for backward. In order to do so, it must\n            # be returned as an output.\n            dx = 3 * x ** 2\n            result = x ** 3\n            return result, dx","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":246,"to":265}}}}],["1072",{"pageContent":"@staticmethod\n        def setup_context(ctx, inputs, output):\n            x, = inputs\n            result, dx = output\n            ctx.save_for_backward(x, dx)\n\n        @staticmethod\n        def backward(ctx, grad_output, grad_dx):\n            x, dx = ctx.saved_tensors\n            # In order for the autograd.Function to work with higher-order\n            # gradients, we must add the gradient contribution of `dx`,\n            # which is grad_dx * 6 * x.\n            result = grad_output * dx + grad_dx * 6 * x\n            return result\n\n    # Wrap MyCube in a function so that it is clearer what the output is\n    def my_cube(x):\n        result, dx = MyCube.apply(x)\n        return result","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":267,"to":285}}}}],["1073",{"pageContent":"# Wrap MyCube in a function so that it is clearer what the output is\n    def my_cube(x):\n        result, dx = MyCube.apply(x)\n        return result\n\n.. note::\n    Inputs to ``backward``, i.e., :attr:`grad_output`, can also be tensors that\n    track history. So if ``backward`` is implemented with differentiable\n    operations, (e.g., invocation of another custom\n    :class:`~torch.autograd.Function`), higher order derivatives will work.\n    In this case, the tensors saved with ``save_for_backward`` can also be used\n    in the backward and have gradients flowing back but tensors saved in the ``ctx``\n    won't have gradients flowing back for them.\n    If you need gradients to flow back for a Tensor saved in the ``ctx``, you should\n    make it an output of the custom ``Function`` and save it with ``save_for_backward``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":285,"to":299}}}}],["1074",{"pageContent":"You probably want to check if the backward method you implemented actually\ncomputes the derivatives of your function. It is possible by comparing with\nnumerical approximations using small finite differences::\n\n    from torch.autograd import gradcheck\n\n    # gradcheck takes a tuple of tensors as input, check if your gradient\n    # evaluated with these tensors are close enough to numerical\n    # approximations and returns True if they all verify this condition.\n    input = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\n    test = gradcheck(linear, input, eps=1e-6, atol=1e-4)\n    print(test)\n\nSee :ref:`grad-check` for more details on finite-difference gradient comparisons.\nIf your function is used in higher order derivatives (differentiating the backward pass) you\ncan use the ``gradgradcheck`` function from the same package to check higher order derivatives.\n\n.. _combining-forward-context:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":301,"to":318}}}}],["1075",{"pageContent":".. _combining-forward-context:\n\nCombined or separate :meth:`~Function.forward` and :meth:`~Function.setup_context`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThere are two main ways to define :class:`~Function`. Either:\n\n- define a :meth:`~Function.forward` that combines the forward compute logic with :meth:`~Function.setup_context`\n- (as of PyTorch 2.0) define a separate :meth:`~Function.forward` and :meth:`~Function.setup_context`\n\nWe recommend the second option (separate :meth:`~Function.forward` and :meth:`~Function.setup_context`)\nbecause that is closer to how PyTorch native operations are implemented and it composes\nwith :mod:`torch.func` transforms. However, we plan to support both approaches going forward;\ncombining :meth:`~Function.forward` with :meth:`~Function.setup_context`: leads to more flexibility since\nyou are able to save intermediates without returning them as output.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":318,"to":332}}}}],["1076",{"pageContent":"Please see the previous section for how to define :class:`~Function` with separate\n:meth:`~Function.forward` and :meth:`~Function.setup_context`.\n\nHere is an example of how to define a :class:`Function` with combined :meth:`~Function.forward` and\n:meth:`~Function.setup_context`::\n\n    class LinearFunction(Function):\n        @staticmethod\n        # ctx is the first argument to forward\n        def forward(ctx, input, weight, bias=None):\n            # The forward pass can use ctx.\n            ctx.save_for_backward(input, weight, bias)\n            output = input.mm(weight.t())\n            if bias is not None:\n                output += bias.unsqueeze(0).expand_as(output)\n            return output\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            input, weight, bias = ctx.saved_tensors\n            grad_input = grad_weight = grad_bias = None","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":334,"to":354}}}}],["1077",{"pageContent":"@staticmethod\n        def backward(ctx, grad_output):\n            input, weight, bias = ctx.saved_tensors\n            grad_input = grad_weight = grad_bias = None\n\n            if ctx.needs_input_grad[0]:\n                grad_input = grad_output.mm(weight)\n            if ctx.needs_input_grad[1]:\n                grad_weight = grad_output.t().mm(input)\n            if bias is not None and ctx.needs_input_grad[2]:\n                grad_bias = grad_output.sum(0)\n\n            return grad_input, grad_weight, grad_bias\n\n.. _forward-ad-autograd-function:\n\nForward mode AD\n^^^^^^^^^^^^^^^\n\nOverriding the forward mode AD formula has a very similar API with some different subtleties.\nYou can implement the :meth:`~Function.jvp` function.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":354,"to":374}}}}],["1078",{"pageContent":"Forward mode AD\n^^^^^^^^^^^^^^^\n\nOverriding the forward mode AD formula has a very similar API with some different subtleties.\nYou can implement the :meth:`~Function.jvp` function.\n\nIt will be given as many :class:`Tensor` arguments as there were inputs, with each\nof them representing gradient w.r.t. that input. It should return as many tensors as there\nwere outputs, with each of them containing the gradient w.r.t. its corresponding output.\nThe :meth:`~Function.jvp` will be called just after the :meth:`~Function.forward`\nmethod, before the :meth:`~Function.apply` returns.\n\n:meth:`~Function.jvp` has a few subtle differences with the :meth:`~Function.backward` function:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":374,"to":386}}}}],["1079",{"pageContent":"- You can use the `ctx` to pass any data from the :meth:`~Function.forward` to the :meth:`~Function.jvp` function.\n  If that state will not be needed for the :meth:`~Function.backward`,\n  you can explicitly free it by doing ``del ctx.foo`` at the end of the :meth:`~Function.jvp` function.\n- The implementation of :meth:`~Function.jvp` must be backward differentiable or explicitly check that\n  none of the given forward mode gradient has ``requires_grad`` set.\n- The :meth:`~Function.jvp` function must match the view/inplace behavior of :meth:`~Function.forward`.\n  For example, if the ``i`` th input is modified inplace, then the ``i`` th gradient must be updated inplace.\n  Similarly, if the ``j`` th output is a view of the ``k`` th input. Then the returned ``j`` th output gradient must be\n  a view of the given ``k`` th input gradient.\n- Because the user cannot specify which gradient needs to be computed, the :meth:`~Function.jvp` function should\n  always compute gradients for all the outputs.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":388,"to":398}}}}],["1080",{"pageContent":"- Because the user cannot specify which gradient needs to be computed, the :meth:`~Function.jvp` function should\n  always compute gradients for all the outputs.\n- The forward mode gradients do respect the flag set by :meth:`~torch.autograd.function.FunctionCtx.set_materialize_grads`\n  and you can get `None` input gradients when this is disabled.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":398,"to":401}}}}],["1081",{"pageContent":":mod:`torch.func` transforms and/or :func:`torch.vmap`\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPlease see :ref:`func-autograd-function` for details.\n\n\nExtending :mod:`torch.nn`\n-------------------------\n\n.. currentmodule:: torch.nn\n\n:mod:`~torch.nn` exports two kinds of interfaces - modules and their functional\nversions. You can extend it in both ways, but we recommend using modules for\nall kinds of layers, that hold any parameters or buffers, and recommend using\na functional form parameter-less operations like activation functions, pooling,\netc.\n\nAdding a functional version of an operation is already fully covered in the\nsection above.\n\nAdding a :class:`Module`\n^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":403,"to":424}}}}],["1082",{"pageContent":"Adding a functional version of an operation is already fully covered in the\nsection above.\n\nAdding a :class:`Module`\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nSince :mod:`~torch.nn` heavily utilizes :mod:`~torch.autograd`, adding a new\n:class:`Module` requires implementing a :class:`~torch.autograd.Function`\nthat performs the operation and can compute the gradient. From now on let's\nassume that we want to implement a ``Linear`` module and we have the function\nimplemented as in the listing above. There's very little code required to\nadd this. Now, there are two functions that need to be implemented:\n\n- ``__init__`` (*optional*) - takes in arguments such as kernel sizes, numbers\n  of features, etc. and initializes parameters and buffers.\n- :meth:`~Module.forward` - instantiates a :class:`~torch.autograd.Function` and\n  uses it to perform the operation. It's very similar to a functional wrapper\n  shown above.\n\nThis is how a ``Linear`` module can be implemented::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":424,"to":443}}}}],["1083",{"pageContent":"This is how a ``Linear`` module can be implemented::\n\n    class Linear(nn.Module):\n        def __init__(self, input_features, output_features, bias=True):\n            super().__init__()\n            self.input_features = input_features\n            self.output_features = output_features","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":443,"to":449}}}}],["1084",{"pageContent":"# nn.Parameter is a special kind of Tensor, that will get\n            # automatically registered as Module's parameter once it's assigned\n            # as an attribute. Parameters and buffers need to be registered, or\n            # they won't appear in .parameters() (doesn't apply to buffers), and\n            # won't be converted when e.g. .cuda() is called. You can use\n            # .register_buffer() to register buffers.\n            # nn.Parameters require gradients by default.\n            self.weight = nn.Parameter(torch.empty(output_features, input_features))\n            if bias:\n                self.bias = nn.Parameter(torch.empty(output_features))\n            else:\n                # You should always register all possible parameters, but the\n                # optional ones can be None if you want.\n                self.register_parameter('bias', None)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":451,"to":464}}}}],["1085",{"pageContent":"# Not a very smart way to initialize weights\n            nn.init.uniform_(self.weight, -0.1, 0.1)\n            if self.bias is not None:\n                nn.init.uniform_(self.bias, -0.1, 0.1)\n\n        def forward(self, input):\n            # See the autograd section for explanation of what happens here.\n            return LinearFunction.apply(input, self.weight, self.bias)\n\n        def extra_repr(self):\n            # (Optional)Set the extra information about this module. You can test\n            # it by printing an object of this class.\n            return 'input_features={}, output_features={}, bias={}'.format(\n                self.input_features, self.output_features, self.bias is not None\n            )\n\n.. _extending-torch:\n\nExtending :mod:`torch`\n----------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":466,"to":485}}}}],["1086",{"pageContent":".. _extending-torch:\n\nExtending :mod:`torch`\n----------------------\n\nYou can create custom types that emulate :class:`Tensor` by defining a custom\nclass with methods that match :class:`Tensor`. But what if you want to be able\nto pass these types to functions like :func:`torch.add` in the top-level\n:mod:`torch` namespace that accept :class:`Tensor` operands?","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":485,"to":493}}}}],["1087",{"pageContent":"If your custom python type defines a method named ``__torch_function__``, PyTorch\nwill invoke your ``__torch_function__`` implementation when an instance of your\ncustom class is passed to a function in the :mod:`torch` namespace. This makes\nit possible to define custom implementations for any of the functions in the\n:mod:`torch` namespace which your ``__torch_function__`` implementation can call,\nallowing your users to make use of your custom type with existing PyTorch\nworkflows that they have already written for :class:`Tensor`. This works with\n\"duck\" types that are unrelated to :class:`Tensor` as well as user-defined\nsubclasses of :class:`Tensor`.\n\nExtending :mod:`torch` with a :class:`Tensor`-like type\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":495,"to":506}}}}],["1088",{"pageContent":"Extending :mod:`torch` with a :class:`Tensor`-like type\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. note:: This functionality is inspired by the NumPy ``__array_function__``\n          protocol. See `the NumPy documentation\n          <https://numpy.org/doc/stable/user/basics.dispatch.html#basics-dispatch>`_\n          and `NEP-0018\n          <https://numpy.org/neps/nep-0018-array-function-protocol.html>`_ for\n          more details.\n\nTo make this concrete, let's begin with a simple example that illustrates the\nAPI dispatch mechanism. We'll create a custom type that represents a 2D scalar\ntensor, parametrized by the order ``N`` and value along the diagonal entries,\n``value``::\n\n     class ScalarTensor(object):\n        def __init__(self, N, value):\n            self._N = N\n            self._value = value\n\n        def __repr__(self):\n            return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":506,"to":527}}}}],["1089",{"pageContent":"def __repr__(self):\n            return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n        def tensor(self):\n            return self._value * torch.eye(self._N)\n\nThis first iteration of the design isn't very useful. The main functionality of\n``ScalarTensor`` is to provide a more compact string representation of a scalar\ntensor than in the base tensor class::\n\n  >>> d = ScalarTensor(5, 2)\n  >>> d\n  ScalarTensor(N=5, value=2)\n  >>> d.tensor()\n  tensor([[2., 0., 0., 0., 0.],\n          [0., 2., 0., 0., 0.],\n          [0., 0., 2., 0., 0.],\n          [0., 0., 0., 2., 0.],\n          [0., 0., 0., 0., 2.]])\n\nIf we try to use this object with the :mod:`torch` API, we will run\ninto issues::\n\n  >>> import torch\n  >>> torch.mean(d)\n  TypeError: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":527,"to":552}}}}],["1090",{"pageContent":">>> import torch\n  >>> torch.mean(d)\n  TypeError: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor\n\nAdding a ``__torch_function__`` implementation to ``ScalarTensor`` makes it\npossible for the above operation to succeed. Let's re-do our implementation,\nthis time adding a ``__torch_function__`` implementation::\n\n  HANDLED_FUNCTIONS = {}\n  class ScalarTensor(object):\n      def __init__(self, N, value):\n          self._N = N\n          self._value = value\n\n      def __repr__(self):\n          return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n      def tensor(self):\n          return self._value * torch.eye(self._N)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":552,"to":570}}}}],["1091",{"pageContent":"def __repr__(self):\n          return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n      def tensor(self):\n          return self._value * torch.eye(self._N)\n\n      @classmethod\n      def __torch_function__(cls, func, types, args=(), kwargs=None):\n          if kwargs is None:\n              kwargs = {}\n          if func not in HANDLED_FUNCTIONS or not all(\n              issubclass(t, (torch.Tensor, ScalarTensor))\n              for t in types\n          ):\n              return NotImplemented\n          return HANDLED_FUNCTIONS[func](*args, **kwargs)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":570,"to":585}}}}],["1092",{"pageContent":"The ``__torch_function__`` method takes four arguments: ``func``, a reference\nto the torch API function that is being overridden, ``types``, the list of\ntypes of Tensor-likes that implement ``__torch_function__``, ``args``, the\ntuple of arguments passed to the function, and ``kwargs``, the dict of keyword\narguments passed to the function. It uses a global dispatch table named\n``HANDLED_FUNCTIONS`` to store custom implementations. The keys of this\ndictionary are functions in the ``torch`` namespace and the values are\nimplementations for ``ScalarTensor``.\n\n.. note:: Using a global dispatch table is not a mandated part of the\n          ``__torch_function__`` API, it is just a useful design pattern for\n          structuring your override implementations.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":587,"to":598}}}}],["1093",{"pageContent":".. note:: Using a global dispatch table is not a mandated part of the\n          ``__torch_function__`` API, it is just a useful design pattern for\n          structuring your override implementations.\n\nThis class definition isn't quite enough to make ``torch.mean`` do the right\nthing when we pass it a ``ScalarTensor`` -- we also need to define an\nimplementation for ``torch.mean`` for ``ScalarTensor`` operands and add the\nimplementation to the ``HANDLED_FUNCTIONS`` dispatch table dictionary. One way\nof doing this is to define a decorator::\n\n  import functools\n  def implements(torch_function):\n      \"\"\"Register a torch function override for ScalarTensor\"\"\"\n      def decorator(func):\n          functools.update_wrapper(func, torch_function)\n          HANDLED_FUNCTIONS[torch_function] = func\n          return func\n      return decorator\n\nwhich can be applied to the implementation of our override::\n\n  @implements(torch.mean)\n  def mean(input):\n      return float(input._value) / input._N","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":598,"to":621}}}}],["1094",{"pageContent":"which can be applied to the implementation of our override::\n\n  @implements(torch.mean)\n  def mean(input):\n      return float(input._value) / input._N\n\nWith this change we can now use ``torch.mean`` with ``ScalarTensor``::\n\n  >>> d = ScalarTensor(5, 2)\n  >>> torch.mean(d)\n  0.4\n\nOf course ``torch.mean`` is an example of the simplest kind of function to\noverride since it only takes one operand. We can use the same machinery to\noverride a function that takes more than one operand, any one of which might be\na tensor or tensor-like that defines ``__torch_function__``, for example for\n:func:`torch.add`::\n\n  def ensure_tensor(data):\n      if isinstance(data, ScalarTensor):\n          return data.tensor()\n      return torch.as_tensor(data)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":621,"to":642}}}}],["1095",{"pageContent":"def ensure_tensor(data):\n      if isinstance(data, ScalarTensor):\n          return data.tensor()\n      return torch.as_tensor(data)\n\n  @implements(torch.add)\n  def add(input, other):\n     try:\n         if input._N == other._N:\n             return ScalarTensor(input._N, input._value + other._value)\n         else:\n             raise ValueError(\"Shape mismatch!\")\n     except AttributeError:\n         return torch.add(ensure_tensor(input), ensure_tensor(other))\n\nThis version has a fast path for when both operands are ``ScalarTensor``\ninstances and also a slower path which degrades to converting the data to\ntensors when either operand is not a ``ScalarTensor``. That makes the override\nfunction correctly when either operand is a ``ScalarTensor`` or a regular\n:class:`Tensor`::\n\n  >>> s = ScalarTensor(2, 2)\n  >>> torch.add(s, s)\n  ScalarTensor(N=2, value=4)\n  >>> t = torch.tensor([[1, 1,], [1, 1]])\n  >>> torch.add(s, t)\n  tensor([[3., 1.],\n          [1., 3.]])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":642,"to":669}}}}],["1096",{"pageContent":">>> s = ScalarTensor(2, 2)\n  >>> torch.add(s, s)\n  ScalarTensor(N=2, value=4)\n  >>> t = torch.tensor([[1, 1,], [1, 1]])\n  >>> torch.add(s, t)\n  tensor([[3., 1.],\n          [1., 3.]])\n\nNote that our implementation of ``add`` does not take ``alpha`` or ``out`` as\nkeyword arguments like :func:`torch.add` does::\n\n  >>> torch.add(s, s, alpha=2)\n  TypeError: add() got an unexpected keyword argument 'alpha'\n\nFor speed and flexibility the ``__torch_function__`` dispatch mechanism does not\ncheck that the signature of an override function matches the signature of the\nfunction being overrided in the :mod:`torch` API. For some applications ignoring\noptional arguments would be fine but to ensure full compatibility with\n:class:`Tensor`, user implementations of torch API functions should take care to\nexactly emulate the API of the function that is being overrided.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":669,"to":688}}}}],["1097",{"pageContent":"Functions in the :mod:`torch` API that do not have explicit overrides will\nreturn ``NotImplemented`` from ``__torch_function__``. If all operands with\n``__torch_function__`` defined on them return ``NotImplemented``, PyTorch will\nraise a ``TypeError``. This means that most of the time operations that do not\nhave explicit overrides for a type will raise a ``TypeError`` when an instance\nof such a type is passed::\n\n  >>> torch.mul(s, 3)\n  TypeError: no implementation found for 'torch.mul' on types that\n  implement __torch_function__: [ScalarTensor]\n\nIn practice this means that if you would like to implement your overrides using\na ``__torch_function__`` implementation along these lines, you will need to\nexplicitly implement the full :mod:`torch` API or the entire subset of the API\nthat you care about for your use case. This may be a tall order as the full\n:mod:`torch` API is quite extensive.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":690,"to":705}}}}],["1098",{"pageContent":"Another option is to not return ``NotImplemented`` for operations that are not\nhandled but to instead pass a :class:`Tensor` to the original :mod:`torch`\nfunction when no override is available. For example, if we change our\nimplementation of ``__torch_function__`` for ``ScalarTensor`` to the one below::\n\n  @classmethod\n  def __torch_function__(cls, func, types, args=(), kwargs=None):\n      if kwargs is None:\n          kwargs = {}\n      if func not in HANDLED_FUNCTIONS or not all(\n              issubclass(t, (torch.Tensor, ScalarTensor))\n              for t in types\n          ):\n          args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]\n          return func(*args, **kwargs)\n      return HANDLED_FUNCTIONS[func](*args, **kwargs)\n\nThen :func:`torch.mul` will work correctly, although the return type will always\nbe a :class:`Tensor` rather than a :class:`ScalarTensor`, even if both operands\nare :class:`ScalarTensor` instances::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":707,"to":726}}}}],["1099",{"pageContent":"Then :func:`torch.mul` will work correctly, although the return type will always\nbe a :class:`Tensor` rather than a :class:`ScalarTensor`, even if both operands\nare :class:`ScalarTensor` instances::\n\n  >>> s = ScalarTensor(2, 2)\n  >>> torch.mul(s, s)\n  tensor([[4., 0.],\n          [0., 4.]])\n\nAlso see the ``MetadataTensor`` example below for another variation on this\npattern but instead always returns a ``MetadataTensor`` to propagate metadata\nthrough operations in the :mod:`torch` API.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":726,"to":737}}}}],["1100",{"pageContent":"Also see the ``MetadataTensor`` example below for another variation on this\npattern but instead always returns a ``MetadataTensor`` to propagate metadata\nthrough operations in the :mod:`torch` API.\n\nThe ``__torch_function__`` protocol is designed for full coverage of the API,\npartial coverage may lead to undesirable results, in particular, certain\nfunctions raising a ``TypeError``. This is especially true for subclasses,\nwhere all three of `torch.add`, `torch.Tensor.__add__` and `torch.Tensor.add`\nmust be covered, even if they return exactly the same result. Failing to do\nthis may also lead to infinite recursion. If one requires the implementation\nof a function from ``torch.Tensor`` subclasses, they must use\n``super().__torch_function__`` inside their implementation.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":737,"to":748}}}}],["1101",{"pageContent":"Subclassing ``torch.Tensor``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAs of version 1.7.0, methods on ``torch.Tensor`` and functions in public\n``torch.*`` namespaces applied on ``torch.Tensor`` subclasses\nwill return subclass instances instead of ``torch.Tensor`` instances::\n\n  >>> class SubTensor(torch.Tensor):\n  ...     pass\n  >>> type(torch.add(SubTensor([0]), SubTensor([1]))).__name__\n  'SubTensor'\n  >>> type(torch.add(SubTensor([0]), torch.tensor([1]))).__name__\n  'SubTensor'\n\nIf multiple subclasses exist, the lowest one in the hierarchy will be chosen by\ndefault. If there is no unique way to determine such a case, then a\n``TypeError`` is raised::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":751,"to":766}}}}],["1102",{"pageContent":"If multiple subclasses exist, the lowest one in the hierarchy will be chosen by\ndefault. If there is no unique way to determine such a case, then a\n``TypeError`` is raised::\n\n  >>> type(torch.add(SubTensor2([0]), SubTensor([1]))).__name__\n  'SubTensor2'\n  >>> type(torch.add(SubTensor2([0]), torch.tensor([1]))).__name__\n  'SubTensor2'\n  >>> torch.add(SubTensor([0]), OtherSubTensor([1]))\n  Traceback (most recent call last):\n    File \"<stdin>\", line 1, in <module>\n  TypeError: no implementation found for 'torch.add' on types that implement __torch_function__: [SubTensor, OtherSubTensor]\n\nIf one wishes to have a global override for all tensor methods, one can use\n``__torch_function__``. Here is an example that logs all function/method\ncalls::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":766,"to":781}}}}],["1103",{"pageContent":"If one wishes to have a global override for all tensor methods, one can use\n``__torch_function__``. Here is an example that logs all function/method\ncalls::\n\n  class LoggingTensor(torch.Tensor):\n      @classmethod\n      def __torch_function__(cls, func, types, args=(), kwargs=None):\n          # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion\n          if func is not torch.Tensor.__repr__:\n              logging.info(f\"func: {func.__name__}, args: {args!r}, kwargs: {kwargs!r}\")\n          if kwargs is None:\n              kwargs = {}\n          return super().__torch_function__(func, types, args, kwargs)\n\nHowever, if one instead wishes to override a method on the Tensor subclass,\nthere one can do so either by directly overriding the method (by defining\nit for a subclass), or by using ``__torch_function__`` and matching with\n``func``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":781,"to":798}}}}],["1104",{"pageContent":"One should be careful within ``__torch_function__`` for subclasses to always\ncall ``super().__torch_function__(func, ...)`` instead of ``func`` directly,\nas was the case before version 1.7.0. Failing to do this may cause ``func``\nto recurse back into ``__torch_function__`` and therefore cause infinite\nrecursion.\n\nExtending :mod:`torch` with a :class:`Tensor` wrapper type\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAnother useful case is a type that wraps a :class:`Tensor`, either as an\nattribute or via subclassing. Below we implement a special case of this sort of\ntype, a ``MetadataTensor`` that attaches a dictionary of metadata to a\n:class:`Tensor` that is propagated through :mod:`torch` operations. Since this\nis a generic sort of wrapping for the full :mod:`torch` API, we do not need to\nindividually implement each override so we can make the ``__torch_function__``\nimplementation more permissive about what operations are allowed::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":800,"to":815}}}}],["1105",{"pageContent":"class MetadataTensor(object):\n      def __init__(self, data, metadata=None, **kwargs):\n          self._t = torch.as_tensor(data, **kwargs)\n          self._metadata = metadata\n\n      def __repr__(self):\n          return \"Metadata:\\n{}\\n\\ndata:\\n{}\".format(self._metadata, self._t)\n\n      @classmethod\n      def __torch_function__(cls, func, types, args=(), kwargs=None):\n          if kwargs is None:\n              kwargs = {}\n          metadatas = tuple(a._metadata for a in args if hasattr(a, '_metadata'))\n          args = [a._t if hasattr(a, '_t') else a for a in args]\n          assert len(metadatas) > 0\n          ret = func(*args, **kwargs)\n          return MetadataTensor(ret, metadata=metadatas[0])\n\nThis simple implementation won't necessarily work with every function in the\n:mod:`torch` API but it is good enough to capture most common operations::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":817,"to":836}}}}],["1106",{"pageContent":"This simple implementation won't necessarily work with every function in the\n:mod:`torch` API but it is good enough to capture most common operations::\n\n  >>> metadata = {'owner': 'Ministry of Silly Walks'}\n  >>> m = MetadataTensor([[1, 2], [3, 4]], metadata=metadata)\n  >>> t = torch.tensor([[1, 2], [1, 2]])\n  >>> torch.add(t, m)\n  Metadata:\n  {'owner': 'Ministry of Silly Walks'}\n\n  data:\n  tensor([[2, 4],\n          [4, 6]])\n  >>> torch.mul(t, m)\n  Metadata:\n  {'owner': 'Ministry of Silly Walks'}\n\n  data:\n  tensor([[1, 4],\n          [3, 8]])\n\nOperations on multiple types that define ``__torch_function__``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIt is possible to use the torch API with multiple distinct types that each have\na ``__torch_function__`` implementation, but special care must be taken. In such\na case the rules are:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":836,"to":862}}}}],["1107",{"pageContent":"It is possible to use the torch API with multiple distinct types that each have\na ``__torch_function__`` implementation, but special care must be taken. In such\na case the rules are:\n\n* The dispatch operation gathers all distinct implementations of\n  ``__torch_function__`` for each operand and calls them in order: subclasses\n  before superclasses, and otherwise left to right in the operator expression.\n* If any value other than ``NotImplemented`` is returned, that value is\n  returned as the result. Implementations can register that they do not\n  implement an operation by returning ``NotImplemented``.\n* If all of the ``__torch_function__`` implementations return\n  ``NotImplemented``, PyTorch raises a ``TypeError``.\n\nTesting Coverage of Overrides for the PyTorch API\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":862,"to":876}}}}],["1108",{"pageContent":"Testing Coverage of Overrides for the PyTorch API\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nOne troublesome aspect of implementing ``__torch_function__`` is that if some\noperations do and others do not have overrides, users will at best see an\ninconsistent experience, or at worst will see errors raised at runtime when they\nuse a function that does not have an override. To ease this process, PyTorch\nprovides a developer-facing API for ensuring full support for\n``__torch_function__`` overrides. This API is private and may be subject to\nchanges without warning in the future.\n\nFirst, to get a listing of all overridable functions, use\n``torch.overrides._get_overridable_functions``. This returns a dictionary whose\nkeys are namespaces in the ``PyTorch`` Python API and whose values are a list of\nfunctions in that namespace that can be overriden. For example, let's print the\nnames of the first 5 functions in ``torch.nn.functional`` that can be\noverriden::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":876,"to":892}}}}],["1109",{"pageContent":">>> from torch.overrides import get_overridable_functions\n  >>> func_dict = get_overridable_functions()\n  >>> nn_funcs = func_dict[torch.nn.functional]\n  >>> print([f.__name__ for f in nn_funcs[:5])\n  ['adaptive_avg_pool1d', 'adaptive_avg_pool2d', 'adaptive_avg_pool3d',\n   'adaptive_max_pool1d', 'adaptive_max_pool1d_with_indices']\n\nThis listing of functions makes it possible to iterate over all overridable\nfunctions, however in practice this is not enough to write tests for all of\nthese functions without laboriously and manually copying the signature of each\nfunction for each test. To ease this process, the\n``torch.overrides._get_testing_overrides`` function returns a dictionary mapping\noverridable functions in the ``PyTorch`` API to dummy lambda functions that have\nthe same signature as the original function but unconditionally return -1. These\nfunctions are most useful to use with ``inspect`` to analyze the function\nsignature of the original ``PyTorch`` function::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":894,"to":909}}}}],["1110",{"pageContent":">>> import inspect\n  >>> from torch.overrides import get_testing_overrides\n  >>> override_dict = get_testing_overrides()\n  >>> dummy_add = override_dict[torch.add]\n  >>> inspect.signature(dummy_add)\n  <Signature (input, other, out=None)>\n\nFinally, ``torch.overrides.get_ignored_functions`` returns a tuple of functions\nthat explicitly cannot be overrided by ``__torch_function__``. This list can be\nuseful to confirm that a function that isn't present in the dictionary returned\nby ``get_overridable_functions`` cannot be overriden.\n\n\nWriting custom C++ extensions\n-----------------------------\n\nSee this\n`PyTorch tutorial <https://pytorch.org/tutorials/advanced/cpp_extension.html>`_\nfor a detailed explanation and examples.\n\nDocumentations are available at :doc:`../cpp_extension`.\n\n\nWriting custom C extensions\n---------------------------\n\nExample available at\n`this GitHub repository <https://github.com/pytorch/extension-ffi>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/extending.rst","loc":{"lines":{"from":911,"to":938}}}}],["1111",{"pageContent":"Frequently Asked Questions\n==========================\n\nMy model reports \"cuda runtime error(2): out of memory\"\n-------------------------------------------------------\n\nAs the error message suggests, you have run out of memory on your\nGPU.  Since we often deal with large amounts of data in PyTorch,\nsmall mistakes can rapidly cause your program to use up all of your\nGPU; fortunately, the fixes in these cases are often simple.\nHere are a few common things to check:\n\n**Don't accumulate history across your training loop.**\nBy default, computations involving variables that require gradients\nwill keep history.  This means that you should avoid using such\nvariables in computations which will live beyond your training loops,\ne.g., when tracking statistics. Instead, you should detach the variable\nor access its underlying data.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/faq.rst","loc":{"lines":{"from":1,"to":18}}}}],["1112",{"pageContent":"Sometimes, it can be non-obvious when differentiable variables can\noccur.  Consider the following training loop (abridged from `source\n<https://discuss.pytorch.org/t/high-memory-usage-while-training/162>`_):\n\n.. code-block:: python\n\n    total_loss = 0\n    for i in range(10000):\n        optimizer.zero_grad()\n        output = model(input)\n        loss = criterion(output)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss\n\nHere, ``total_loss`` is accumulating history across your training loop, since\n``loss`` is a differentiable variable with autograd history. You can fix this by\nwriting `total_loss += float(loss)` instead.\n\nOther instances of this problem:\n`1 <https://discuss.pytorch.org/t/resolved-gpu-out-of-memory-error-with-batch-size-1/3719>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/faq.rst","loc":{"lines":{"from":20,"to":40}}}}],["1113",{"pageContent":"Other instances of this problem:\n`1 <https://discuss.pytorch.org/t/resolved-gpu-out-of-memory-error-with-batch-size-1/3719>`_.\n\n**Don't hold onto tensors and variables you don't need.**\nIf you assign a Tensor or Variable to a local, Python will not\ndeallocate until the local goes out of scope.  You can free\nthis reference by using ``del x``.  Similarly, if you assign\na Tensor or Variable to a member variable of an object, it will\nnot deallocate until the object goes out of scope.  You will\nget the best memory usage if you don't hold onto temporaries\nyou don't need.\n\nThe scopes of locals can be larger than you expect.  For example:\n\n.. code-block:: python\n\n    for i in range(5):\n        intermediate = f(input[i])\n        result += g(intermediate)\n    output = h(result)\n    return output\n\nHere, ``intermediate`` remains live even while ``h`` is executing,\nbecause its scope extrudes past the end of the loop.  To free it\nearlier, you should ``del intermediate`` when you are done with it.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/faq.rst","loc":{"lines":{"from":40,"to":64}}}}],["1114",{"pageContent":"Here, ``intermediate`` remains live even while ``h`` is executing,\nbecause its scope extrudes past the end of the loop.  To free it\nearlier, you should ``del intermediate`` when you are done with it.\n\n**Avoid running RNNs on sequences that are too large.**\nThe amount of memory required to backpropagate through an RNN scales\nlinearly with the length of the RNN input; thus, you will run out of memory\nif you try to feed an RNN a sequence that is too long.\n\nThe technical term for this phenomenon is `backpropagation through time\n<https://en.wikipedia.org/wiki/Backpropagation_through_time>`_,\nand there are plenty of references for how to implement truncated\nBPTT, including in the `word language model <https://github.com/pytorch/examples/tree/master/word_language_model>`_ example; truncation is handled by the\n``repackage`` function as described in\n`this forum post <https://discuss.pytorch.org/t/help-clarifying-repackage-hidden-in-word-language-model/226>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/faq.rst","loc":{"lines":{"from":64,"to":78}}}}],["1115",{"pageContent":"**Don't use linear layers that are too large.**\nA linear layer ``nn.Linear(m, n)`` uses :math:`O(nm)` memory: that is to say,\nthe memory requirements of the weights\nscales quadratically with the number of features.  It is very easy\nto `blow through your memory <https://github.com/pytorch/pytorch/issues/958>`_\nthis way (and remember that you will need at least twice the size of the\nweights, since you also need to store the gradients.)\n\n**Consider checkpointing.**\nYou can trade-off memory for compute by using `checkpoint <https://pytorch.org/docs/stable/checkpoint.html>`_.\n\nMy GPU memory isn't freed properly\n----------------------------------\nPyTorch uses a caching memory allocator to speed up memory allocations. As a\nresult, the values shown in ``nvidia-smi`` usually don't reflect the true\nmemory usage. See :ref:`cuda-memory-management` for more details about GPU\nmemory management.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/faq.rst","loc":{"lines":{"from":80,"to":96}}}}],["1116",{"pageContent":"If your GPU memory isn't freed even after Python quits, it is very likely that\nsome Python subprocesses are still alive. You may find them via\n``ps -elf | grep python`` and manually kill them with ``kill -9 [pid]``.\n\nMy out of memory exception handler can't allocate memory\n--------------------------------------------------------\nYou may have some code that tries to recover from out of memory errors.\n\n.. code-block:: python\n\n    try:\n        run_model(batch_size)\n    except RuntimeError: # Out of memory\n        for _ in range(batch_size):\n            run_model(1)\n\nBut find that when you do run out of memory, your recovery code can't allocate\neither. That's because the python exception object holds a reference to the\nstack frame where the error was raised. Which prevents the original tensor\nobjects from being freed. The solution is to move you OOM recovery code outside\nof the ``except`` clause.\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/faq.rst","loc":{"lines":{"from":98,"to":120}}}}],["1117",{"pageContent":".. code-block:: python\n\n    oom = False\n    try:\n        run_model(batch_size)\n    except RuntimeError: # Out of memory\n        oom = True\n\n    if oom:\n        for _ in range(batch_size):\n            run_model(1)\n\n\n.. _dataloader-workers-random-seed:\n\nMy data loader workers return identical random numbers\n-------------------------------------------------------\nYou are likely using other libraries to generate random numbers in the dataset\nand worker subprocesses are started via ``fork``. See\n:class:`torch.utils.data.DataLoader`'s documentation for how to\nproperly set up random seeds in workers with its :attr:`worker_init_fn` option.\n\n.. _pack-rnn-unpack-with-data-parallelism:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/faq.rst","loc":{"lines":{"from":120,"to":142}}}}],["1118",{"pageContent":".. _pack-rnn-unpack-with-data-parallelism:\n\nMy recurrent network doesn't work with data parallelism\n-------------------------------------------------------\nThere is a subtlety in using the\n``pack sequence -> recurrent network -> unpack sequence`` pattern in a\n:class:`~torch.nn.Module` with :class:`~torch.nn.DataParallel` or\n:func:`~torch.nn.parallel.data_parallel`. Input to each the :meth:`forward` on\neach device will only be part of the entire input. Because the unpack operation\n:func:`torch.nn.utils.rnn.pad_packed_sequence` by default only pads up to the\nlongest input it sees, i.e., the longest on that particular device, size\nmismatches will happen when results are gathered together. Therefore, you can\ninstead take advantage of the :attr:`total_length` argument of\n:func:`~torch.nn.utils.rnn.pad_packed_sequence` to make sure that the\n:meth:`forward` calls return sequences of same length. For example, you can\nwrite::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/faq.rst","loc":{"lines":{"from":142,"to":157}}}}],["1119",{"pageContent":"from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n    class MyModule(nn.Module):\n        # ... __init__, other methods, etc.\n\n        # padded_input is of shape [B x T x *] (batch_first mode) and contains\n        # the sequences sorted by lengths\n        #   B is the batch size\n        #   T is max sequence length\n        def forward(self, padded_input, input_lengths):\n            total_length = padded_input.size(1)  # get the max sequence length\n            packed_input = pack_padded_sequence(padded_input, input_lengths,\n                                                batch_first=True)\n            packed_output, _ = self.my_lstm(packed_input)\n            output, _ = pad_packed_sequence(packed_output, batch_first=True,\n                                            total_length=total_length)\n            return output\n\n\n    m = MyModule().cuda()\n    dp_m = nn.DataParallel(m)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/faq.rst","loc":{"lines":{"from":159,"to":179}}}}],["1120",{"pageContent":"m = MyModule().cuda()\n    dp_m = nn.DataParallel(m)\n\n\nAdditionally, extra care needs to be taken when batch dimension is dim ``1``\n(i.e., ``batch_first=False``) with data parallelism. In this case, the first\nargument of pack_padded_sequence ``padding_input`` will be of shape\n``[T x B x *]`` and should be scattered along dim ``1``, but the second argument\n``input_lengths`` will be of shape ``[B]`` and should be scattered along dim\n``0``. Extra code to manipulate the tensor shapes will be needed.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/faq.rst","loc":{"lines":{"from":179,"to":188}}}}],["1121",{"pageContent":".. _gradcheck-mechanics:\n\nGradcheck mechanics\n===================\n\nThis note presents an overview of how the :meth:`~torch.autograd.gradcheck` and :meth:`~torch.autograd.gradgradcheck` functions work.\n\nIt will cover both forward and backward mode AD for both real and complex-valued functions as well as higher-order derivatives.\nThis note also covers both the default behavior of gradcheck as well as the case where :code:`fast_mode=True` argument is passed (referred to as fast gradcheck below).\n\n.. contents:: :local:\n    :depth: 2\n\nNotations and background information\n------------------------------------\n\nThroughout this note, we will use the following convention:\n\n1. :math:`x`, :math:`y`, :math:`a`, :math:`b`, :math:`v`, :math:`u`, :math:`ur` and :math:`ui` are real-valued vectors and :math:`z` is a complex-valued vector that can be rewritten in terms of two real-valued vectors as :math:`z = a + i b`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":1,"to":19}}}}],["1122",{"pageContent":"2. :math:`N` and :math:`M` are two integers that we will use for the dimension of the input and output space respectively.\n\n3. :math:`f: \\mathcal{R}^N \\to \\mathcal{R}^M` is our basic real-to-real function such that :math:`y = f(x)`.\n\n4. :math:`g: \\mathcal{C}^N \\to \\mathcal{R}^M` is our basic complex-to-real function such that :math:`y = g(z)`.\n\n\nFor the simple real-to-real case, we write as :math:`J_f` the Jacobian matrix associated with :math:`f` of size :math:`M \\times N`.\nThis matrix contains all the partial derivatives such that the entry at position :math:`(i, j)` contains :math:`\\frac{\\partial y_i}{\\partial x_j}`.\nBackward mode AD is then computing, for a given vector :math:`v` of size :math:`M`, the quantity :math:`v^T J_f`.\nForward mode AD on the other hand is computing, for a given vector :math:`u` of size :math:`N`, the quantity :math:`J_f u`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":21,"to":31}}}}],["1123",{"pageContent":"For functions that contain complex values, the story is a lot more complex. We only provide the gist here and the full description can be found at :ref:`complex_autograd-doc`.\n\nThe constraints to satisfy complex differentiability (Cauchy-Riemann equations) are too restrictive for all real-valued loss functions, so we instead opted to use Wirtinger calculus.\nIn a basic setting of Wirtinger calculus, the chain rule requires access to both the Wirtinger derivative (called :math:`W` below) and the Conjugate Wirtinger derivative (called :math:`CW` below).\nBoth :math:`W` and :math:`CW` need to be propagated because in general, despite their name, one is not the complex conjugate of the other.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":33,"to":37}}}}],["1124",{"pageContent":"To avoid having to propagate both values, for backward mode AD, we always work under the assumption that the function whose derivative is being calculated is either a real-valued function or is part of a bigger real-valued function. This assumption means that all the intermediary gradients we compute during the backward pass are also associated with real-valued functions.\nIn practice, this assumption is not restrictive when doing optimization as such problem require real-valued objectives (as there is no natural ordering of the complex numbers).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":39,"to":40}}}}],["1125",{"pageContent":"Under this assumption, using :math:`W` and :math:`CW` definitions, we can show that :math:`W = CW^*` (we use :math:`*` to denote complex conjugation here) and so only one of the two values actually need to be \"backwarded through the graph\" as the other one can easily be recovered.\nTo simplify internal computations, PyTorch uses :math:`2 * CW` as the value it backwards and returns when the user asks for gradients.\nSimilarly to the real case, when the output is actually in :math:`\\mathcal{R}^M`, backward mode AD does not compute :math:`2 * CW` but only :math:`v^T (2 * CW)` for a given vector :math:`v \\in \\mathcal{R}^M`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":42,"to":44}}}}],["1126",{"pageContent":"For forward mode AD, we use a similar logic, in this case, assuming that the function is part of a larger function whose input is in :math:`\\mathcal{R}`. Under this assumption, we can make a similar claim that every intermediary result corresponds to a function whose input is in :math:`\\mathcal{R}` and in this case, using :math:`W` and :math:`CW` definitions, we can show that :math:`W = CW` for the intermediary functions.\nTo make sure the forward and backward mode compute the same quantities in the elementary case of a one dimensional function, the forward mode also computes :math:`2 * CW`.\nSimilarly to the real case, when the input is actually in :math:`\\mathcal{R}^N`, forward mode AD does not compute :math:`2 * CW` but only :math:`(2 * CW) u` for a given vector :math:`u \\in \\mathcal{R}^N`.\n\n\nDefault backward mode gradcheck behavior\n----------------------------------------\n\nReal-to-real functions\n^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":46,"to":55}}}}],["1127",{"pageContent":"Default backward mode gradcheck behavior\n----------------------------------------\n\nReal-to-real functions\n^^^^^^^^^^^^^^^^^^^^^^\n\nTo test a function :math:`f: \\mathcal{R}^N \\to \\mathcal{R}^M, x \\to y`, we reconstruct the full Jacobian matrix :math:`J_f` of size :math:`M \\times N` in two ways: analytically and numerically.\nThe analytical version uses our backward mode AD while the numerical version uses finite difference.\nThe two reconstructed Jacobian matrices are then compared elementwise for equality.\n\nDefault real input numerical evaluation\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nIf we consider the elementary case of a one-dimensional function (:math:`N = M = 1`), then we can use the basic finite difference formula from `the wikipedia article <https://en.wikipedia.org/wiki/Finite_difference>`_. We use the \"central difference\" for better numerical properties:\n\n.. math::\n    \\frac{\\partial y}{\\partial x} \\approx \\frac{f(x + eps) - f(x - eps)}{2 * eps}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":55,"to":71}}}}],["1128",{"pageContent":".. math::\n    \\frac{\\partial y}{\\partial x} \\approx \\frac{f(x + eps) - f(x - eps)}{2 * eps}\n\nThis formula easily generalizes for multiple outputs (:math:`M \\gt 1`) by having :math:`\\frac{\\partial y}{\\partial x}` be a column vector of size :math:`M \\times 1` like :math:`f(x + eps)`.\nIn that case, the above formula can be re-used as-is and approximates the full Jacobian matrix with only two evaluations of the user function (namely :math:`f(x + eps)` and :math:`f(x - eps)`).\n\nIt is more computationally expensive to handle the case with multiple inputs (:math:`N \\gt 1`). In this scenario, we loop over all the inputs one after the other and apply the :math:`eps` perturbation for each element of :math:`x` one after the other. This allows us to reconstruct the :math:`J_f` matrix column by column.\n\nDefault real input analytical evaluation\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":71,"to":80}}}}],["1129",{"pageContent":"Default real input analytical evaluation\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nFor the analytical evaluation, we use the fact, as described above, that backward mode AD computes :math:`v^T J_f`.\nFor functions with a single output, we simply use :math:`v = 1` to recover the full Jacobian matrix with a single backward pass.\n\nFor functions with more than one output, we resort to a for-loop which iterates over the outputs where each :math:`v` is a one-hot vector corresponding to each output one after the other. This allows to reconstruct the :math:`J_f` matrix row by row.\n\nComplex-to-real functions\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTo test a function :math:`g: \\mathcal{C}^N \\to \\mathcal{R}^M, z \\to y` with :math:`z = a + i b`, we reconstruct the (complex-valued) matrix that contains :math:`2 * CW`.\n\nDefault complex input numerical evaluation\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":80,"to":94}}}}],["1130",{"pageContent":"Default complex input numerical evaluation\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nConsider the elementary case where :math:`N = M = 1` first. We know from (chapter 3 of) `this research paper <https://arxiv.org/pdf/1701.00392.pdf>`_ that:\n\n.. math::\n    CW := \\frac{\\partial y}{\\partial z^*} = \\frac{1}{2} * (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b})\n\nNote that :math:`\\frac{\\partial y}{\\partial a}` and :math:`\\frac{\\partial y}{\\partial b}`, in the above equation, are :math:`\\mathcal{R} \\to \\mathcal{R}` derivatives.\nTo evaluate these numerically, we use the method described above for the real-to-real case.\nThis allows us to compute the :math:`CW` matrix and then multiply it by :math:`2`.\n\nNote that the code, as of time of writing, computes this value in a slightly convoluted way:\n\n.. code:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":94,"to":108}}}}],["1131",{"pageContent":"Note that the code, as of time of writing, computes this value in a slightly convoluted way:\n\n.. code:: python\n\n    # Code from https://github.com/pytorch/pytorch/blob/58eb23378f2a376565a66ac32c93a316c45b6131/torch/autograd/gradcheck.py#L99-L105\n    # Notation changes in this code block:\n    # s here is y above\n    # x, y here are a, b above\n\n    ds_dx = compute_gradient(eps)\n    ds_dy = compute_gradient(eps * 1j)\n    # conjugate wirtinger derivative\n    conj_w_d = 0.5 * (ds_dx + ds_dy * 1j)\n    # wirtinger derivative\n    w_d = 0.5 * (ds_dx - ds_dy * 1j)\n    d[d_idx] = grad_out.conjugate() * conj_w_d + grad_out * w_d.conj()\n\n    # Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.\n\n\nDefault complex input analytical evaluation\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":108,"to":129}}}}],["1132",{"pageContent":"Default complex input analytical evaluation\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nSince backward mode AD computes exactly twice the :math:`CW` derivative already, we simply use the same trick as for the real-to-real case here and reconstruct the matrix row by row when there are multiple real outputs.\n\nFunctions with complex outputs\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn this case, the user-provided function does not follow the assumption from the autograd that the function we compute backward AD for is real-valued.\nThis means that using autograd directly on this function is not well defined.\nTo solve this, we will replace the test of the function :math:`h: \\mathcal{P}^N \\to \\mathcal{C}^M` (where :math:`\\mathcal{P}` can be either :math:`\\mathcal{R}` or :math:`\\mathcal{C}`), with two functions: :math:`hr` and :math:`hi` such that:\n\n.. math::\n    \\begin{aligned}\n        hr(q) &:= real(f(q)) \\\\\n        hi(q) &:= imag(f(q))\n    \\end{aligned}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":129,"to":145}}}}],["1133",{"pageContent":".. math::\n    \\begin{aligned}\n        hr(q) &:= real(f(q)) \\\\\n        hi(q) &:= imag(f(q))\n    \\end{aligned}\n\nwhere :math:`q \\in \\mathcal{P}`.\nWe then do a basic gradcheck for both :math:`hr` and :math:`hi` using either the real-to-real or complex-to-real case described above, depending on :math:`\\mathcal{P}`.\n\nNote that, the code, as of time of writing, does not create these functions explicitly but perform the chain rule with the :math:`real` or :math:`imag` functions manually by passing the :math:`\\text{grad\\_out}` arguments to the different functions.\nWhen :math:`\\text{grad\\_out} = 1`, then we are considering :math:`hr`.\nWhen :math:`\\text{grad\\_out} = 1j`, then we are considering :math:`hi`.\n\n\nFast backward mode gradcheck\n----------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":145,"to":160}}}}],["1134",{"pageContent":"Fast backward mode gradcheck\n----------------------------\n\nWhile the above formulation of gradcheck is great, both, to ensure correctness and debuggability, it is very slow because it reconstructs the full Jacobian matrices.\nThis section presents a way to perform gradcheck in a faster way without affecting its correctness.\nThe debuggability can be recovered by adding special logic when we detect an error. In that case, we can run the default version that reconstructs the full matrix to give full details to the user.\n\nThe high level strategy here is to find a scalar quantity that can be computed efficiently by both the numerical and analytical methods and that represents the full matrix computed by the slow gradcheck well enough to ensure that it will catch any discrepancy in the Jacobians.\n\nFast gradcheck for real-to-real functions\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":160,"to":170}}}}],["1135",{"pageContent":"Fast gradcheck for real-to-real functions\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe scalar quantity that we want to compute here is :math:`v^T J_f u` for a given random vector :math:`v \\in \\mathcal{R}^M` and a random unit norm vector :math:`u \\in \\mathcal{R}^N`.\n\nFor the numerical evaluation, we can efficiently compute\n\n.. math::\n    J_f u \\approx \\frac{f(x + u * eps) - f(x - u * eps)}{2 * eps}.\n\nWe then perform the dot product between this vector and :math:`v` to get the scalar value of interest.\n\nFor the analytical version, we can use backward mode AD to compute :math:`v^T J_f` directly. We then perform the dot product with :math:`u` to get the expected value.\n\nFast gradcheck for complex-to-real functions\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSimilar to the real-to-real case, we want to perform a reduction of the full matrix. But the :math:`2 * CW` matrix is complex-valued and so in this case, we will compare to complex scalars.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":170,"to":187}}}}],["1136",{"pageContent":"Similar to the real-to-real case, we want to perform a reduction of the full matrix. But the :math:`2 * CW` matrix is complex-valued and so in this case, we will compare to complex scalars.\n\nDue to some constraints on what we can compute efficiently in the numerical case and to keep the number of numerical evaluations to a minimum, we compute the following (albeit surprising) scalar value:\n\n.. math::\n    s := 2 * v^T (real(CW) ur + i * imag(CW) ui)\n\nwhere :math:`v \\in \\mathcal{R}^M`, :math:`ur \\in \\mathcal{R}^N` and :math:`ui \\in \\mathcal{R}^N`.\n\nFast complex input numerical evaluation\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nWe first consider how to compute :math:`s` with a numerical method. To do so, keeping in mind that we're considering :math:`g: \\mathcal{C}^N \\to \\mathcal{R}^M, z \\to y` with :math:`z = a + i b`, and that :math:`CW = \\frac{1}{2} * (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b})`,  we rewrite it as follows:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":187,"to":199}}}}],["1137",{"pageContent":".. math::\n    \\begin{aligned}\n        s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\\\\n          &= 2 * v^T (\\frac{1}{2} * \\frac{\\partial y}{\\partial a} ur + i * \\frac{1}{2} * \\frac{\\partial y}{\\partial b} ui) \\\\\n          &= v^T (\\frac{\\partial y}{\\partial a} ur + i * \\frac{\\partial y}{\\partial b} ui) \\\\\n          &= v^T ((\\frac{\\partial y}{\\partial a} ur) + i * (\\frac{\\partial y}{\\partial b} ui))\n    \\end{aligned}\n\nIn this formula, we can see that :math:`\\frac{\\partial y}{\\partial a} ur` and :math:`\\frac{\\partial y}{\\partial b} ui` can be evaluated the same way as the fast version for the real-to-real case.\nOnce these real-valued quantities have been computed, we can reconstruct the complex vector on the right side and do a dot product with the real-valued :math:`v` vector.\n\nFast complex input analytical evaluation\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nFor the analytical case, things are simpler and we rewrite the formula as:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":201,"to":215}}}}],["1138",{"pageContent":"Fast complex input analytical evaluation\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nFor the analytical case, things are simpler and we rewrite the formula as:\n\n.. math::\n    \\begin{aligned}\n        s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\\\\n          &= v^T real(2 * CW) ur + i * v^T imag(2 * CW) ui) \\\\\n          &= real(v^T (2 * CW)) ur + i * imag(v^T (2 * CW)) ui\n    \\end{aligned}\n\nWe can thus use the fact that the backward mode AD provides us with an efficient way to compute :math:`v^T (2 * CW)` and then perform a dot product of the real part with :math:`ur` and the imaginary part with :math:`ui` before reconstructing the final complex scalar :math:`s`.\n\nWhy not use a complex :math:`u`\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":215,"to":230}}}}],["1139",{"pageContent":"Why not use a complex :math:`u`\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nAt this point, you might be wondering why we did not select a complex :math:`u` and just performed the reduction :math:`2 * v^T CW u'`.\nTo dive into this, in this paragraph, we will use the complex version of :math:`u` noted :math:`u' = ur' + i ui'`.\nUsing such complex :math:`u'`, the problem is that when doing the numerical evaluation, we would need to compute:\n\n.. math::\n    \\begin{aligned}\n        2*CW u' &= (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b})(ur' + i ui') \\\\\n                &= \\frac{\\partial y}{\\partial a} ur' + i \\frac{\\partial y}{\\partial a} ui' + i \\frac{\\partial y}{\\partial b} ur' - \\frac{\\partial y}{\\partial b} ui'\n    \\end{aligned}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":230,"to":241}}}}],["1140",{"pageContent":"Which would require four evaluations of real-to-real finite difference (twice as much compared to the approached proposed above).\nSince this approach does not have more degrees of freedom (same number of real valued variables) and we try to get the fastest possible evaluation here, we use the other formulation above.\n\n\nFast gradcheck for functions with complex outputs\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nJust like in the slow case, we consider two real-valued functions and use the appropriate rule from above for each function.\n\nGradgradcheck implementation\n-----------------------------\n\nPyTorch also provide a utility to verify second order gradients. The goal here is to make sure that the backward implementation is also properly differentiable and computes the right thing.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":243,"to":255}}}}],["1141",{"pageContent":"PyTorch also provide a utility to verify second order gradients. The goal here is to make sure that the backward implementation is also properly differentiable and computes the right thing.\n\nThis feature is implemented by considering the function :math:`F: x, v \\to v^T J_f` and use the gradcheck defined above on this function.\nNote that :math:`v` in this case is just a random vector with the same type as :math:`f(x)`.\n\nThe fast version of gradgradcheck is implemented by using the fast version of gradcheck on that same function :math:`F`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/gradcheck.rst","loc":{"lines":{"from":255,"to":260}}}}],["1142",{"pageContent":".. _hip-semantics:\n\nHIP (ROCm) semantics\n====================\n\nROCm\\ |trade| is AMD’s open source software platform for GPU-accelerated high\nperformance computing and machine learning. HIP is ROCm's C++ dialect designed\nto ease conversion of CUDA applications to portable C++ code. HIP is used when\nconverting existing CUDA applications like PyTorch to portable C++ and for new\nprojects that require portability between AMD and NVIDIA.\n\n.. _hip_as_cuda:\n\nHIP Interfaces Reuse the CUDA Interfaces\n----------------------------------------\n\nPyTorch for HIP intentionally reuses the existing :mod:`torch.cuda` interfaces.\nThis helps to accelerate the porting of existing PyTorch code and models because\nvery few code changes are necessary, if any.\n\nThe example from :ref:`cuda-semantics` will work exactly the same for HIP::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/hip.rst","loc":{"lines":{"from":1,"to":21}}}}],["1143",{"pageContent":"The example from :ref:`cuda-semantics` will work exactly the same for HIP::\n\n    cuda = torch.device('cuda')     # Default HIP device\n    cuda0 = torch.device('cuda:0')  # 'rocm' or 'hip' are not valid, use 'cuda'\n    cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\n    x = torch.tensor([1., 2.], device=cuda0)\n    # x.device is device(type='cuda', index=0)\n    y = torch.tensor([1., 2.]).cuda()\n    # y.device is device(type='cuda', index=0)\n\n    with torch.cuda.device(1):\n        # allocates a tensor on GPU 1\n        a = torch.tensor([1., 2.], device=cuda)\n\n        # transfers a tensor from CPU to GPU 1\n        b = torch.tensor([1., 2.]).cuda()\n        # a.device and b.device are device(type='cuda', index=1)\n\n        # You can also use ``Tensor.to`` to transfer a tensor:\n        b2 = torch.tensor([1., 2.]).to(device=cuda)\n        # b.device and b2.device are device(type='cuda', index=1)\n\n        c = a + b\n        # c.device is device(type='cuda', index=1)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/hip.rst","loc":{"lines":{"from":21,"to":45}}}}],["1144",{"pageContent":"c = a + b\n        # c.device is device(type='cuda', index=1)\n\n        z = x + y\n        # z.device is device(type='cuda', index=0)\n\n        # even within a context, you can specify the device\n        # (or give a GPU index to the .cuda call)\n        d = torch.randn(2, device=cuda2)\n        e = torch.randn(2).to(cuda2)\n        f = torch.randn(2).cuda(cuda2)\n        # d.device, e.device, and f.device are all device(type='cuda', index=2)\n\n.. _checking_for_hip:\n\nChecking for HIP\n----------------\n\nWhether you are using PyTorch for CUDA or HIP, the result of calling\n:meth:`~torch.cuda.is_available` will be the same. If you are using a PyTorch\nthat has been built with GPU support, it will return `True`. If you must check\nwhich version of PyTorch you are using, refer to this example below::\n\n    if torch.cuda.is_available() and torch.version.hip:\n        # do something specific for HIP\n    elif torch.cuda.is_available() and torch.version.cuda:\n        # do something specific for CUDA","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/hip.rst","loc":{"lines":{"from":45,"to":71}}}}],["1145",{"pageContent":"if torch.cuda.is_available() and torch.version.hip:\n        # do something specific for HIP\n    elif torch.cuda.is_available() and torch.version.cuda:\n        # do something specific for CUDA\n\n.. |trade|  unicode:: U+02122 .. TRADEMARK SIGN\n   :ltrim:\n\n.. _tf32_on_rocm:\n\nTensorFloat-32(TF32) on ROCm\n----------------------------\n\nTF32 is not supported on ROCm.\n\n.. _rocm-memory-management:\n\nMemory management\n-----------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/hip.rst","loc":{"lines":{"from":71,"to":89}}}}],["1146",{"pageContent":".. _tf32_on_rocm:\n\nTensorFloat-32(TF32) on ROCm\n----------------------------\n\nTF32 is not supported on ROCm.\n\n.. _rocm-memory-management:\n\nMemory management\n-----------------\n\nPyTorch uses a caching memory allocator to speed up memory allocations. This\nallows fast memory deallocation without device synchronizations. However, the\nunused memory managed by the allocator will still show as if used in\n``rocm-smi``. You can use :meth:`~torch.cuda.memory_allocated` and\n:meth:`~torch.cuda.max_memory_allocated` to monitor memory occupied by\ntensors, and use :meth:`~torch.cuda.memory_reserved` and\n:meth:`~torch.cuda.max_memory_reserved` to monitor the total amount of memory\nmanaged by the caching allocator. Calling :meth:`~torch.cuda.empty_cache`\nreleases all **unused** cached memory from PyTorch so that those can be used\nby other GPU applications. However, the occupied GPU memory by tensors will not\nbe freed so it can not increase the amount of GPU memory available for PyTorch.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/hip.rst","loc":{"lines":{"from":89,"to":111}}}}],["1147",{"pageContent":"For more advanced users, we offer more comprehensive memory benchmarking via\n:meth:`~torch.cuda.memory_stats`. We also offer the capability to capture a\ncomplete snapshot of the memory allocator state via\n:meth:`~torch.cuda.memory_snapshot`, which can help you understand the\nunderlying allocation patterns produced by your code.\n\nTo debug memory errors, set\n``PYTORCH_NO_CUDA_MEMORY_CACHING=1`` in your environment to disable caching.\n\n.. _hipfft-plan-cache:\n\nhipFFT/rocFFT plan cache\n------------------------\n\nSetting the size of the cache for hipFFT/rocFFT plans is not supported.\n\n.. _torch-distributed-backends:\n\ntorch.distributed backends\n--------------------------\n\nCurrently, only the \"nccl\" and \"gloo\" backends for torch.distributed are supported on ROCm.\n\n.. _cuda-api-to_hip-api-mappings:\n\nCUDA API to HIP API mappings in C++\n-----------------------------------\n\nPlease refer: https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP_API_Guide.html","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/hip.rst","loc":{"lines":{"from":113,"to":141}}}}],["1148",{"pageContent":".. _cuda-api-to_hip-api-mappings:\n\nCUDA API to HIP API mappings in C++\n-----------------------------------\n\nPlease refer: https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP_API_Guide.html\n\nNOTE: The CUDA_VERSION macro, cudaRuntimeGetVersion and cudaDriverGetVersion APIs do not\nsemantically map to the same values as HIP_VERSION macro, hipRuntimeGetVersion and\nhipDriverGetVersion APIs. Please do not use them interchangeably when doing version checks.\n\nFor example: Instead of using\n\n``#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000`` to implicitly exclude ROCm/HIP,\n\nuse the following to not take the code path for ROCm/HIP:\n\n``#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(USE_ROCM)``\n\nAlternatively, if it is desired to take the code path for ROCm/HIP:\n\n``#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || defined(USE_ROCM)``\n\nOr if it is desired to take the code path for ROCm/HIP only for specific HIP versions:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/hip.rst","loc":{"lines":{"from":141,"to":164}}}}],["1149",{"pageContent":"``#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || defined(USE_ROCM)``\n\nOr if it is desired to take the code path for ROCm/HIP only for specific HIP versions:\n\n``#if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || (defined(USE_ROCM) && ROCM_VERSION >= 40300)``\n\n\nRefer to CUDA Semantics doc\n---------------------------\n\nFor any sections not listed here, please refer to the CUDA semantics doc: :ref:`cuda-semantics`\n\n\nEnabling kernel asserts\n-----------------------\n\nKernel asserts are supported on ROCm, but they are disabled due to performance overhead. It can be enabled\nby recompiling the PyTorch from source.\n\nPlease add below line as an argument to cmake command parameters::\n\n    -DROCM_FORCE_ENABLE_GPU_ASSERTS:BOOL=ON","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/hip.rst","loc":{"lines":{"from":164,"to":185}}}}],["1150",{"pageContent":"Features for large-scale deployments\n====================================\n\n.. contents:: :local:\n\nThis note talks about several extension points and tricks that might be useful\nwhen running PyTorch within a larger system or operating multiple systems using\nPyTorch in a larger organization.\n\nIt doesn't cover topics of deploying models to production. Check\n:mod:`torch.jit` or one of the corresponding tutorials.\n\nThe note assumes that you either build PyTorch from source in your\norganization or have an ability to statically link additional code to be loaded\nwhen PyTorch is used. Therefore, many of the hooks are exposed as C++ APIs that\ncan be triggered once in a centralized place, e.g. in static initialization\ncode.\n\nFleet-wide operator profiling\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/large_scale_deployments.rst","loc":{"lines":{"from":1,"to":20}}}}],["1151",{"pageContent":"Fleet-wide operator profiling\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPyTorch comes with :mod:`torch.autograd.profiler` capable of measuring time\ntaken by individual operators on demand. One can use the same mechanism to do\n\"always ON\" measurements for any process running PyTorch. It might be useful for\ngathering information about PyTorch workloads running in a given process or\nacross the entire set of machines.\n\nNew callbacks for any operator invocation can be added with\n``torch::addGlobalCallback``. Hooks will be called with\n``torch::RecordFunction`` struct that describes invocation\ncontext (e.g. `name`). If enabled, ``RecordFunction::inputs()`` contains arguments\nof the function represented as ``torch::IValue`` variant type. Note, that inputs\nlogging is relatively expensive and thus has to be enabled explicitly.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/large_scale_deployments.rst","loc":{"lines":{"from":20,"to":34}}}}],["1152",{"pageContent":"The operator callbacks also have access to ``c10::ThreadLocalDebugInfo::get()``\ninterface that returns a pointer to the struct holding the debug information.\nThis debug information can be set earlier by using ``at::DebugInfoGuard`` object.\nDebug information is propagated through the forward (including async ``fork``\ntasks) and backward passes and can be useful for passing some extra information\nabout execution environment (e.g. model id) from the higher layers of the\napplication down to the operator callbacks.\n\nInvoking callbacks adds some overhead, so usually it's useful to just randomly\nsample operator invocations. This can be enabled on per-callback basis with an\noptional sampling rate passed into ``torch::addGlobalCallback``.\n\nNote, that ``addGlobalCallback`` is not thread-safe and can be called only when no\nPyTorch operator is running. Usually, it's a good idea to call them once during\ninitialization.\n\nHere's an example:\n\n.. code-block:: cpp","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/large_scale_deployments.rst","loc":{"lines":{"from":36,"to":54}}}}],["1153",{"pageContent":"Here's an example:\n\n.. code-block:: cpp\n\n    // Called somewhere in the program beginning\n    void init() {\n        // Sample one in a hundred operator runs randomly\n        addGlobalCallback(\n          RecordFunctionCallback(\n            &onFunctionEnter,\n            &onFunctionExit)\n          .needsInputs(true)\n          .samplingProb(0.01)\n        );\n        // Note, to enable observers in the model calling thread,\n        // call enableRecordFunction() in the thread before running a model\n    }\n\n    void onFunctionEnter(const RecordFunction& fn) {\n        std::cerr << \"Before function \" << fn.name()\n                  << \" with \" << fn.inputs().size() << \" inputs\" << std::endl;\n    }\n\n    void onFunctionExit(const RecordFunction& fn) {\n        std::cerr << \"After function \" << fn.name();\n    }\n\nAPI usage logging\n^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/large_scale_deployments.rst","loc":{"lines":{"from":54,"to":82}}}}],["1154",{"pageContent":"void onFunctionExit(const RecordFunction& fn) {\n        std::cerr << \"After function \" << fn.name();\n    }\n\nAPI usage logging\n^^^^^^^^^^^^^^^^^\n\nWhen running in a broader ecosystem, for example in managed job scheduler, it's\noften useful to track which binaries invoke particular PyTorch APIs. There\nexists simple instrumentation injected at several important API points that\ntriggers a given callback. Because usually PyTorch is invoked in one-off python\nscripts, the callback fires only once for a given process for each of the APIs.\n\n``c10::SetAPIUsageHandler`` can be used to register API usage instrumentation\nhandler. Passed argument is going to be an \"api key\" identifying used point, for\nexample ``python.import`` for PyTorch extension import or\n``torch.script.compile`` if TorchScript compilation was triggered.\n\n.. code-block:: cpp\n\n    SetAPIUsageLogger([](const std::string& event_name) {\n        std::cerr << \"API was used: \" << event_name << std::endl;\n    });","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/large_scale_deployments.rst","loc":{"lines":{"from":82,"to":104}}}}],["1155",{"pageContent":".. code-block:: cpp\n\n    SetAPIUsageLogger([](const std::string& event_name) {\n        std::cerr << \"API was used: \" << event_name << std::endl;\n    });\n\nNote for developers: new API trigger points can be added in code with\n``C10_LOG_API_USAGE_ONCE(\"my_api\")`` in C++ or\n``torch._C._log_api_usage_once(\"my.api\")`` in Python.\n\nAttaching metadata to saved TorchScript models\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTorchScript modules can be saved as an archive file that bundles serialized\nparameters and module code as TorchScript (see :meth:`torch.jit.save`). It's\noften convenient to bundle additional information together with the model, for\nexample, description of model producer or auxiliary artifacts.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/large_scale_deployments.rst","loc":{"lines":{"from":104,"to":120}}}}],["1156",{"pageContent":"It can be achieved by passing the ``_extra_files`` argument to\n:meth:`torch.jit.save` and ``torch::jit::load`` to store and retrieve\narbitrary binary blobs during saving process. Since TorchScript files are\nregular ZIP archives, extra information gets stored as regular files inside\narchive's ``extra/`` directory.\n\nThere's also a global hook allowing to attach extra files to any TorchScript\narchive produced in the current process. It might be useful to tag models with\nproducer metadata, akin to JPEG metadata produced by digital cameras. Example\nusage might look like:\n\n.. code-block:: cpp\n\n    SetExportModuleExtraFilesHook([](const Module&) {\n        ExtraFilesMap files;\n        files[\"producer_info.json\"] = \"{\\\"user\\\": \\\"\" + getenv(\"USER\") + \"\\\"}\";\n        return files;\n    });\n\n\nBuild environment considerations\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/large_scale_deployments.rst","loc":{"lines":{"from":122,"to":143}}}}],["1157",{"pageContent":"Build environment considerations\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTorchScript's compilation needs to have access to the original python files as\nit uses python's ``inspect.getsource`` call. In certain production environments\nit might require explicitly deploying ``.py`` files along with precompiled\n``.pyc``.\n\nCommon extension points\n^^^^^^^^^^^^^^^^^^^^^^^\n\nPyTorch APIs are generally loosely coupled and it's easy to replace a component\nwith specialized version. Common extension points include:\n\n* Custom operators implemented in C++ - see `tutorial for more details <https://pytorch.org/tutorials/advanced/cpp_extension.html>`_.\n* Custom data reading can be often integrated directly by invoking corresponding python library. Existing functionality of :mod:`torch.utils.data` can be utilized by extending :class:`~torch.utils.data.Dataset` or :class:`~torch.utils.data.IterableDataset`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/large_scale_deployments.rst","loc":{"lines":{"from":143,"to":158}}}}],["1158",{"pageContent":".. _modules:\n\nModules\n=======\n\nPyTorch uses modules to represent neural networks. Modules are:\n\n* **Building blocks of stateful computation.**\n  PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\n  easy construction of elaborate, multi-layer neural networks.\n* **Tightly integrated with PyTorch's**\n  `autograd <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`_\n  **system.** Modules make it simple to specify learnable parameters for PyTorch's Optimizers to update.\n* **Easy to work with and transform.** Modules are straightforward to save and restore, transfer between\n  CPU / GPU / TPU devices, prune, quantize, and more.\n\nThis note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well.\n\n.. contents:: :local:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":1,"to":21}}}}],["1159",{"pageContent":".. contents:: :local:\n\nA Simple Custom Module\n----------------------\n\nTo get started, let's look at a simpler, custom version of PyTorch's :class:`~torch.nn.Linear` module.\nThis module applies an affine transformation to its input.\n\n.. code-block:: python\n\n   import torch\n   from torch import nn\n\n   class MyLinear(nn.Module):\n     def __init__(self, in_features, out_features):\n       super().__init__()\n       self.weight = nn.Parameter(torch.randn(in_features, out_features))\n       self.bias = nn.Parameter(torch.randn(out_features))\n\n     def forward(self, input):\n       return (input @ self.weight) + self.bias\n\nThis simple module has the following fundamental characteristics of modules:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":21,"to":43}}}}],["1160",{"pageContent":"* **It inherits from the base Module class.**\n  All modules should subclass :class:`~torch.nn.Module` for composability with other modules.\n* **It defines some \"state\" that is used in computation.**\n  Here, the state consists of randomly-initialized ``weight`` and ``bias`` tensors that define the affine\n  transformation. Because each of these is defined as a :class:`~torch.nn.parameter.Parameter`, they are\n  *registered* for the module and will automatically be tracked and returned from calls\n  to :func:`~torch.nn.Module.parameters`. Parameters can be\n  considered the \"learnable\" aspects of the module's computation (more on this later). Note that modules\n  are not required to have state, and can also be stateless.\n* **It defines a forward() function that performs the computation.** For this affine transformation module, the input\n  is matrix-multiplied with the ``weight`` parameter (using the ``@`` short-hand notation) and added to the ``bias``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":45,"to":55}}}}],["1161",{"pageContent":"is matrix-multiplied with the ``weight`` parameter (using the ``@`` short-hand notation) and added to the ``bias``\n  parameter to produce the output. More generally, the ``forward()`` implementation for a module can perform arbitrary\n  computation involving any number of inputs and outputs.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":55,"to":57}}}}],["1162",{"pageContent":"This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:\n\n.. code-block:: python\n\n   m = MyLinear(4, 3)\n   sample_input = torch.randn(4)\n   m(sample_input)\n   : tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":59,"to":67}}}}],["1163",{"pageContent":"m = MyLinear(4, 3)\n   sample_input = torch.randn(4)\n   m(sample_input)\n   : tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)\n\nNote that the module itself is callable, and that calling it invokes its ``forward()`` function.\nThis name is in reference to the concepts of \"forward pass\" and \"backward pass\", which apply to each module.\nThe \"forward pass\" is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \"backward pass\" computes gradients of\nmodule outputs with respect to its inputs, which can be used for \"training\" parameters through gradient\ndescent methods. PyTorch's autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement a ``backward()`` function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail in\n:ref:`Neural Network Training with Modules`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":67,"to":80}}}}],["1164",{"pageContent":"The full set of parameters registered by the module can be iterated through via a call to\n:func:`~torch.nn.Module.parameters` or :func:`~torch.nn.Module.named_parameters`,\nwhere the latter includes each parameter's name:\n\n.. code-block:: python\n\n   for parameter in m.named_parameters():\n     print(parameter)\n   : ('weight', Parameter containing:\n   tensor([[ 1.0597,  1.1796,  0.8247],\n           [-0.5080, -1.2635, -1.1045],\n           [ 0.0593,  0.2469, -1.4299],\n           [-0.4926, -0.5457,  0.4793]], requires_grad=True))\n   ('bias', Parameter containing:\n   tensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))\n\nIn general, the parameters registered by a module are aspects of the module's computation that should be\n\"learned\". A later section of this note shows how to update these parameters using one of PyTorch's Optimizers.\nBefore we get to that, however, let's first examine how modules can be composed with one another.\n\nModules as Building Blocks\n--------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":82,"to":103}}}}],["1165",{"pageContent":"Modules as Building Blocks\n--------------------------\n\nModules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using the :class:`~torch.nn.Sequential` module. It allows us to chain together\nmultiple modules:\n\n.. code-block:: python\n\n   net = nn.Sequential(\n     MyLinear(4, 3),\n     nn.ReLU(),\n     MyLinear(3, 1)\n   )\n\n   sample_input = torch.randn(4)\n   net(sample_input)\n   : tensor([-0.6749], grad_fn=<AddBackward0>)\n\nNote that :class:`~torch.nn.Sequential` automatically feeds the output of the first ``MyLinear`` module as input\ninto the :class:`~torch.nn.ReLU`, and the output of that as input into the second ``MyLinear`` module. As\nshown, it is limited to in-order chaining of modules with a single input and output.\n\nIn general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module's computation.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":103,"to":127}}}}],["1166",{"pageContent":"In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module's computation.\n\nFor example, here's a simple neural network implemented as a custom module:\n\n.. code-block:: python\n\n   import torch.nn.functional as F\n\n   class Net(nn.Module):\n     def __init__(self):\n       super().__init__()\n       self.l0 = MyLinear(4, 3)\n       self.l1 = MyLinear(3, 1)\n     def forward(self, x):\n       x = self.l0(x)\n       x = F.relu(x)\n       x = self.l1(x)\n       return x\n\nThis module is composed of two \"children\" or \"submodules\" (\\ ``l0`` and ``l1``\\ ) that define the layers of\nthe neural network and are utilized for computation within the module's ``forward()`` method. Immediate\nchildren of a module can be iterated through via a call to :func:`~torch.nn.Module.children` or\n:func:`~torch.nn.Module.named_children`:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":127,"to":152}}}}],["1167",{"pageContent":".. code-block:: python\n\n   net = Net()\n   for child in net.named_children():\n     print(child)\n   : ('l0', MyLinear())\n   ('l1', MyLinear())\n\nTo go deeper than just the immediate children, :func:`~torch.nn.Module.modules` and\n:func:`~torch.nn.Module.named_modules` *recursively* iterate through a module and its child modules:\n\n.. code-block:: python\n\n   class BigNet(nn.Module):\n     def __init__(self):\n       super().__init__()\n       self.l1 = MyLinear(5, 4)\n       self.net = Net()\n     def forward(self, x):\n       return self.net(self.l1(x))\n\n   big_net = BigNet()\n   for module in big_net.named_modules():\n     print(module)\n   : ('', BigNet(\n     (l1): MyLinear()\n     (net): Net(\n       (l0): MyLinear()\n       (l1): MyLinear()\n     )\n   ))\n   ('l1', MyLinear())\n   ('net', Net(\n     (l0): MyLinear()\n     (l1): MyLinear()\n   ))\n   ('net.l0', MyLinear())\n   ('net.l1', MyLinear())","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":152,"to":189}}}}],["1168",{"pageContent":"Sometimes, it's necessary for a module to dynamically define submodules.\nThe :class:`~torch.nn.ModuleList` and :class:`~torch.nn.ModuleDict` modules are useful here; they\nregister submodules from a list or dict:\n\n.. code-block:: python\n\n   class DynamicNet(nn.Module):\n     def __init__(self, num_layers):\n       super().__init__()\n       self.linears = nn.ModuleList(\n         [MyLinear(4, 4) for _ in range(num_layers)])\n       self.activations = nn.ModuleDict({\n         'relu': nn.ReLU(),\n         'lrelu': nn.LeakyReLU()\n       })\n       self.final = MyLinear(4, 1)\n     def forward(self, x, act):\n       for linear in self.linears:\n         x = linear(x)\n       x = self.activations[act](x)\n       x = self.final(x)\n       return x\n\n   dynamic_net = DynamicNet(3)\n   sample_input = torch.randn(4)\n   output = dynamic_net(sample_input, 'relu')","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":191,"to":216}}}}],["1169",{"pageContent":"dynamic_net = DynamicNet(3)\n   sample_input = torch.randn(4)\n   output = dynamic_net(sample_input, 'relu')\n\nFor any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls to :func:`~torch.nn.Module.parameters` and :func:`~torch.nn.Module.named_parameters` will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":216,"to":224}}}}],["1170",{"pageContent":"for parameter in dynamic_net.named_parameters():\n     print(parameter)\n   : ('linears.0.weight', Parameter containing:\n   tensor([[-1.2051,  0.7601,  1.1065,  0.1963],\n           [ 3.0592,  0.4354,  1.6598,  0.9828],\n           [-0.4446,  0.4628,  0.8774,  1.6848],\n           [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))\n   ('linears.0.bias', Parameter containing:\n   tensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))\n   ('linears.1.weight', Parameter containing:\n   tensor([[ 2.1113, -0.0623, -1.0806,  0.3508],\n           [-0.0550,  1.5317,  1.1064, -0.5562],\n           [-0.4028, -0.6942,  1.5793, -1.0140],\n           [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))\n   ('linears.1.bias', Parameter containing:\n   tensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))\n   ('linears.2.weight', Parameter containing:\n   tensor([[-2.6340, -0.3887, -0.9979,  0.0767],\n           [-0.3526,  0.8756, -1.5847, -0.6016],","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":226,"to":244}}}}],["1171",{"pageContent":"('linears.2.weight', Parameter containing:\n   tensor([[-2.6340, -0.3887, -0.9979,  0.0767],\n           [-0.3526,  0.8756, -1.5847, -0.6016],\n           [-0.3269, -0.1608,  0.2897, -2.0829],\n           [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))\n   ('linears.2.bias', Parameter containing:\n   tensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))\n   ('final.weight', Parameter containing:\n   tensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))\n   ('final.bias', Parameter containing:\n   tensor([0.3381], requires_grad=True))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":244,"to":254}}}}],["1172",{"pageContent":"It's also easy to move all parameters to a different device or change their precision using\n:func:`~torch.nn.Module.to`:\n\n.. code-block:: python\n\n   # Move all parameters to a CUDA device\n   dynamic_net.to(device='cuda')\n\n   # Change precision of all parameters\n   dynamic_net.to(dtype=torch.float64)\n\n   dynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\n   : tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n\nMore generally, an arbitrary function can be applied to a module and its submodules recursively by\nusing the :func:`~torch.nn.Module.apply` function. For example, to apply custom initialization to parameters\nof a module and its submodules:\n\n.. code-block:: python\n\n   # Define a function to initialize Linear weights.\n   # Note that no_grad() is used here to avoid tracking this computation in the autograd graph.\n   @torch.no_grad()\n   def init_weights(m):\n     if isinstance(m, nn.Linear):\n       nn.init.xavier_normal_(m.weight)\n       m.bias.fill_(0.0)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":256,"to":282}}}}],["1173",{"pageContent":"# Apply the function recursively on the module and its submodules.\n   dynamic_net.apply(init_weights)\n\nThese examples show how elaborate neural networks can be formed through module composition and conveniently\nmanipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch\nprovides a large library of performant modules within the :mod:`torch.nn` namespace that perform common neural\nnetwork operations like pooling, convolutions, loss functions, etc.\n\nIn the next section, we give a full example of training a neural network.\n\nFor more information, check out:\n\n* Library of PyTorch-provided modules: `torch.nn <https://pytorch.org/docs/stable/nn.html>`_\n* Defining neural net modules: https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html\n\n.. _Neural Network Training with Modules:\n\nNeural Network Training with Modules\n------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":284,"to":302}}}}],["1174",{"pageContent":".. _Neural Network Training with Modules:\n\nNeural Network Training with Modules\n------------------------------------\n\nOnce a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s\nOptimizers from :mod:`torch.optim`:\n\n.. code-block:: python\n\n   # Create the network (from previous section) and optimizer\n   net = Net()\n   optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\n   # Run a sample training loop that \"teaches\" the network\n   # to output the constant zero function\n   for _ in range(10000):\n     input = torch.randn(4)\n     output = net(input)\n     loss = torch.abs(output)\n     net.zero_grad()\n     loss.backward()\n     optimizer.step()\n\n   # After training, switch the module to eval mode to do inference, compute performance metrics, etc.\n   # (see discussion below for a description of training and evaluation modes)\n   ...\n   net.eval()\n   ...","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":302,"to":330}}}}],["1175",{"pageContent":"In this simplified example, the network learns to simply output zero, as any non-zero output is \"penalized\" according\nto its absolute value by employing :func:`torch.abs` as a loss function. While this is not a very interesting task, the\nkey parts of training are present:\n\n* A network is created.\n* An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network’s\n  parameters are associated with it.\n* A training loop...\n    * acquires an input,\n    * runs the network,\n    * computes a loss,\n    * zeros the network’s parameters’ gradients,\n    * calls loss.backward() to update the parameters’ gradients,\n    * calls optimizer.step() to apply the gradients to the parameters.\n\nAfter the above snippet has been run, note that the network's parameters have changed. In particular, examining the\nvalue of ``l1``\\ 's ``weight`` parameter shows that its values are now much closer to 0 (as may be expected):\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":332,"to":350}}}}],["1176",{"pageContent":".. code-block:: python\n\n   print(net.l1.weight)\n   : Parameter containing:\n   tensor([[-0.0013],\n           [ 0.0030],\n           [-0.0008]], requires_grad=True)\n\nNote that the above process is done entirely while the network module is in \"training mode\". Modules default to\ntraining mode and can be switched between training and evaluation modes using :func:`~torch.nn.Module.train` and\n:func:`~torch.nn.Module.eval`. They can behave differently depending on which mode they are in. For example, the\n:class:`~torch.nn.BatchNorm` module maintains a running mean and variance during training that are not updated\nwhen the module is in evaluation mode. In general, modules should be in training mode during training\nand only switched to evaluation mode for inference or evaluation. Below is an example of a custom module\nthat behaves differently between the two modes:\n\n.. code-block:: python\n\n   class ModalModule(nn.Module):\n     def __init__(self):\n       super().__init__()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":350,"to":370}}}}],["1177",{"pageContent":".. code-block:: python\n\n   class ModalModule(nn.Module):\n     def __init__(self):\n       super().__init__()\n\n     def forward(self, x):\n       if self.training:\n         # Add a constant only in training mode.\n         return x + 1.\n       else:\n         return x\n\n\n   m = ModalModule()\n   x = torch.randn(4)\n\n   print('training mode output: {}'.format(m(x)))\n   : tensor([1.6614, 1.2669, 1.0617, 1.6213, 0.5481])\n\n   m.eval()\n   print('evaluation mode output: {}'.format(m(x)))\n   : tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])\n\nTraining neural networks can often be tricky. For more information, check out:\n\n* Using Optimizers: https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html.\n* Neural network training: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n* Introduction to autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n\nModule State\n------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":370,"to":401}}}}],["1178",{"pageContent":"Module State\n------------\n\nIn the previous section, we demonstrated training a module's \"parameters\", or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving its ``state_dict`` (i.e. \"state dictionary\"):\n\n.. code-block:: python\n\n   # Save the module\n   torch.save(net.state_dict(), 'net.pt')\n\n   ...\n\n   # Load the module later on\n   new_net = Net()\n   new_net.load_state_dict(torch.load('net.pt'))\n   : <All keys matched successfully>\n\nA module's ``state_dict`` contains state that affects its computation. This includes, but is not limited to, the\nmodule's parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \"buffers\", both \"persistent\"\nand \"non-persistent\". Following is an overview of the various types of state a module can have:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":401,"to":422}}}}],["1179",{"pageContent":"* **Parameters**\\ : learnable aspects of computation; contained within the ``state_dict``\n* **Buffers**\\ : non-learnable aspects of computation\n\n  * **Persistent** buffers: contained within the ``state_dict`` (i.e. serialized when saving & loading)\n  * **Non-persistent** buffers: not contained within the ``state_dict`` (i.e. left out of serialization)\n\nAs a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module's ``state_dict`` so that it will be\nrestored when loading a serialized form of the module, but we don't want it to be learnable.\nThis snippet shows how to use :func:`~torch.nn.Module.register_buffer` to accomplish this:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":424,"to":435}}}}],["1180",{"pageContent":".. code-block:: python\n\n   class RunningMean(nn.Module):\n     def __init__(self, num_features, momentum=0.9):\n       super().__init__()\n       self.momentum = momentum\n       self.register_buffer('mean', torch.zeros(num_features))\n     def forward(self, x):\n       self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n       return self.mean\n\nNow, the current value of the running mean is considered part of the module's ``state_dict``\nand will be properly restored when loading the module from disk:\n\n.. code-block:: python\n\n   m = RunningMean(4)\n   for _ in range(10):\n     input = torch.randn(4)\n     m(input)\n\n   print(m.state_dict())\n   : OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))\n\n   # Serialized form will contain the 'mean' tensor\n   torch.save(m.state_dict(), 'mean.pt')\n\n   m_loaded = RunningMean(4)\n   m_loaded.load_state_dict(torch.load('mean.pt'))\n   assert(torch.all(m.mean == m_loaded.mean))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":435,"to":464}}}}],["1181",{"pageContent":"m_loaded = RunningMean(4)\n   m_loaded.load_state_dict(torch.load('mean.pt'))\n   assert(torch.all(m.mean == m_loaded.mean))\n\nAs mentioned previously, buffers can be left out of the module's ``state_dict`` by marking them as non-persistent:\n\n.. code-block:: python\n\n   self.register_buffer('unserialized_thing', torch.randn(5), persistent=False)\n\nBoth persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with\n:func:`~torch.nn.Module.to`:\n\n.. code-block:: python\n\n   # Moves all module parameters and buffers to the specified device / dtype\n   m.to(device='cuda', dtype=torch.float64)\n\nBuffers of a module can be iterated over using :func:`~torch.nn.Module.buffers` or\n:func:`~torch.nn.Module.named_buffers`.\n\n.. code-block:: python\n\n   for buffer in m.named_buffers():\n     print(buffer)\n\nThe following class demonstrates the various ways of registering parameters and buffers within a module:\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":464,"to":492}}}}],["1182",{"pageContent":"for buffer in m.named_buffers():\n     print(buffer)\n\nThe following class demonstrates the various ways of registering parameters and buffers within a module:\n\n.. code-block:: python\n\n   class StatefulModule(nn.Module):\n     def __init__(self):\n       super().__init__()\n       # Setting a nn.Parameter as an attribute of the module automatically registers the tensor\n       # as a parameter of the module.\n       self.param1 = nn.Parameter(torch.randn(2))\n\n       # Alternative string-based way to register a parameter.\n       self.register_parameter('param2', nn.Parameter(torch.randn(3)))\n\n       # Reserves the \"param3\" attribute as a parameter, preventing it from being set to anything\n       # except a parameter. \"None\" entries like this will not be present in the module's state_dict.\n       self.register_parameter('param3', None)\n\n       # Registers a list of parameters.\n       self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":492,"to":514}}}}],["1183",{"pageContent":"# Registers a list of parameters.\n       self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])\n\n       # Registers a dictionary of parameters.\n       self.param_dict = nn.ParameterDict({\n         'foo': nn.Parameter(torch.randn(3)),\n         'bar': nn.Parameter(torch.randn(4))\n       })\n\n       # Registers a persistent buffer (one that appears in the module's state_dict).\n       self.register_buffer('buffer1', torch.randn(4), persistent=True)\n\n       # Registers a non-persistent buffer (one that does not appear in the module's state_dict).\n       self.register_buffer('buffer2', torch.randn(5), persistent=False)\n\n       # Reserves the \"buffer3\" attribute as a buffer, preventing it from being set to anything\n       # except a buffer. \"None\" entries like this will not be present in the module's state_dict.\n       self.register_buffer('buffer3', None)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":514,"to":531}}}}],["1184",{"pageContent":"# Adding a submodule registers its parameters as parameters of the module.\n       self.linear = nn.Linear(2, 3)\n\n   m = StatefulModule()\n\n   # Save and load state_dict.\n   torch.save(m.state_dict(), 'state.pt')\n   m_loaded = StatefulModule()\n   m_loaded.load_state_dict(torch.load('state.pt'))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":533,"to":541}}}}],["1185",{"pageContent":"# Note that non-persistent buffer \"buffer2\" and reserved attributes \"param3\" and \"buffer3\" do\n   # not appear in the state_dict.\n   print(m_loaded.state_dict())\n   : OrderedDict([('param1', tensor([-0.0322,  0.9066])),\n                  ('param2', tensor([-0.4472,  0.1409,  0.4852])),\n                  ('buffer1', tensor([ 0.6949, -0.1944,  1.2911, -2.1044])),\n                  ('param_list.0', tensor([ 0.4202, -0.1953])),\n                  ('param_list.1', tensor([ 1.5299, -0.8747])),\n                  ('param_list.2', tensor([-1.6289,  1.4898])),\n                  ('param_dict.bar', tensor([-0.6434,  1.5187,  0.0346, -0.4077])),\n                  ('param_dict.foo', tensor([-0.0845, -1.4324,  0.7022])),\n                  ('linear.weight', tensor([[-0.3915, -0.6176],\n                                            [ 0.6062, -0.5992],\n                                            [ 0.4452, -0.2843]])),\n                  ('linear.bias', tensor([-0.3710, -0.0795, -0.3947]))])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":543,"to":557}}}}],["1186",{"pageContent":"For more information, check out:\n\n* Saving and loading: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n* Serialization semantics: https://pytorch.org/docs/master/notes/serialization.html\n* What is a state dict? https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\n\nModule Initialization\n---------------------\n\nBy default, parameters and floating-point buffers for modules provided by :mod:`torch.nn` are initialized during\nmodule instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to\nperform well historically for the module type. For certain use cases, it may be desired to initialize with a different\ndtype, device (e.g. GPU), or initialization technique.\n\nExamples:\n\n.. code-block:: python\n\n   # Initialize module directly onto GPU.\n   m = nn.Linear(5, 3, device='cuda')\n\n   # Initialize module with 16-bit floating point parameters.\n   m = nn.Linear(5, 3, dtype=torch.half)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":559,"to":581}}}}],["1187",{"pageContent":"# Initialize module directly onto GPU.\n   m = nn.Linear(5, 3, device='cuda')\n\n   # Initialize module with 16-bit floating point parameters.\n   m = nn.Linear(5, 3, dtype=torch.half)\n\n   # Skip default parameter initialization and perform custom (e.g. orthogonal) initialization.\n   m = torch.nn.utils.skip_init(nn.Linear, 5, 3)\n   nn.init.orthogonal_(m.weight)\n\nNote that the device and dtype options demonstrated above also apply to any floating-point buffers registered\nfor the module:\n\n.. code-block:: python\n\n   m = nn.BatchNorm2d(3, dtype=torch.half)\n   print(m.running_mean)\n   : tensor([0., 0., 0.], dtype=torch.float16)\n\nWhile module writers can use any device or dtype to initialize parameters in their custom modules, good practice is\nto use ``dtype=torch.float`` and ``device='cpu'`` by default as well. Optionally, you can provide full flexibility\nin these areas for your custom module by conforming to the convention demonstrated above that all\n:mod:`torch.nn` modules follow:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":581,"to":603}}}}],["1188",{"pageContent":"* Provide a ``device`` constructor kwarg that applies to any parameters / buffers registered by the module.\n* Provide a ``dtype`` constructor kwarg that applies to any parameters / floating-point buffers registered by\n  the module.\n* Only use initialization functions (i.e. functions from :mod:`torch.nn.init`) on parameters and buffers within the\n  module's constructor. Note that this is only required to use :func:`~torch.nn.utils.skip_init`; see\n  `this page <https://pytorch.org/tutorials/prototype/skip_param_init.html#updating-modules-to-support-skipping-initialization>`_ for an explanation.\n\nFor more information, check out:\n\n* Skipping module parameter initialization: https://pytorch.org/tutorials/prototype/skip_param_init.html\n\nModule Hooks\n------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":605,"to":617}}}}],["1189",{"pageContent":"For more information, check out:\n\n* Skipping module parameter initialization: https://pytorch.org/tutorials/prototype/skip_param_init.html\n\nModule Hooks\n------------\n\nIn :ref:`Neural Network Training with Modules`, we demonstrated the training process for a module, which iteratively\nperforms forward and backward passes, updating module parameters each iteration. For more control\nover this process, PyTorch provides \"hooks\" that can perform arbitrary computation during a forward or backward\npass, even modifying how the pass is done if desired. Some useful examples for this functionality include\ndebugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules\nyou haven't written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules.\n\nPyTorch provides two types of hooks for modules:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":617,"to":631}}}}],["1190",{"pageContent":"* **Forward hooks** are called during the forward pass. They can be installed for a given module with\n  :func:`~torch.nn.Module.register_forward_pre_hook` and :func:`~torch.nn.Module.register_forward_hook`.\n  These hooks will be called respectively just before the forward function is called and just after it is called.\n  Alternatively, these hooks can be installed globally for all modules with the analogous\n  :func:`~torch.nn.modules.module.register_module_forward_pre_hook` and\n  :func:`~torch.nn.modules.module.register_module_forward_hook` functions.\n* **Backward hooks** are called during the backward pass. They can be installed with\n  :func:`~torch.nn.Module.register_full_backward_pre_hook` and :func:`~torch.nn.Module.register_full_backward_hook`.\n  These hooks will be called when the backward for this Module has been computed.\n  :func:`~torch.nn.Module.register_full_backward_pre_hook` will allow the user to access the gradients for outputs","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":633,"to":642}}}}],["1191",{"pageContent":"These hooks will be called when the backward for this Module has been computed.\n  :func:`~torch.nn.Module.register_full_backward_pre_hook` will allow the user to access the gradients for outputs\n  while :func:`~torch.nn.Module.register_full_backward_hook` will allow the user to access the gradients\n  both the inputs and outputs. Alternatively, they can be installed globally for all modules with\n  :func:`~torch.nn.modules.module.register_module_full_backward_hook` and\n  :func:`~torch.nn.modules.module.register_module_full_backward_pre_hook`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":642,"to":647}}}}],["1192",{"pageContent":"All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module's ``forward()`` function.\n\nBelow is an example demonstrating usage of forward and backward hooks:\n\n.. code-block:: python\n\n   torch.manual_seed(1)\n\n   def forward_pre_hook(m, inputs):\n     # Allows for examination and modification of the input before the forward pass.\n     # Note that inputs are always wrapped in a tuple.\n     input = inputs[0]\n     return input + 1.\n\n   def forward_hook(m, inputs, output):\n     # Allows for examination of inputs / outputs and modification of the outputs\n     # after the forward pass. Note that inputs are always wrapped in a tuple while outputs\n     # are passed as-is.\n\n     # Residual computation a la ResNet.\n     return output + inputs[0]","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":649,"to":671}}}}],["1193",{"pageContent":"# Residual computation a la ResNet.\n     return output + inputs[0]\n\n   def backward_hook(m, grad_inputs, grad_outputs):\n     # Allows for examination of grad_inputs / grad_outputs and modification of\n     # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and\n     # grad_outputs are always wrapped in tuples.\n     new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]\n     return new_grad_inputs\n\n   # Create sample module & input.\n   m = nn.Linear(3, 3)\n   x = torch.randn(2, 3, requires_grad=True)\n\n   # ==== Demonstrate forward hooks. ====\n   # Run input through module before and after adding hooks.\n   print('output with no forward hooks: {}'.format(m(x)))\n   : output with no forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n                                           [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":671,"to":689}}}}],["1194",{"pageContent":"# Note that the modified input results in a different output.\n   forward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)\n   print('output with forward pre hook: {}'.format(m(x)))\n   : output with forward pre hook: tensor([[-0.5752, -0.7421,  0.4942],\n                                           [-0.0736,  0.5461,  0.0838]], grad_fn=<AddmmBackward>)\n\n   # Note the modified output.\n   forward_hook_handle = m.register_forward_hook(forward_hook)\n   print('output with both forward hooks: {}'.format(m(x)))\n   : output with both forward hooks: tensor([[-1.0980,  0.6396,  0.4666],\n                                             [ 0.3634,  0.6538,  1.0256]], grad_fn=<AddBackward0>)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":691,"to":701}}}}],["1195",{"pageContent":"# Remove hooks; note that the output here matches the output before adding hooks.\n   forward_pre_hook_handle.remove()\n   forward_hook_handle.remove()\n   print('output after removing forward hooks: {}'.format(m(x)))\n   : output after removing forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n                                                  [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)\n\n   # ==== Demonstrate backward hooks. ====\n   m(x).sum().backward()\n   print('x.grad with no backwards hook: {}'.format(x.grad))\n   : x.grad with no backwards hook: tensor([[ 0.4497, -0.5046,  0.3146],\n                                            [ 0.4497, -0.5046,  0.3146]])\n\n   # Clear gradients before running backward pass again.\n   m.zero_grad()\n   x.grad.zero_()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":703,"to":718}}}}],["1196",{"pageContent":"# Clear gradients before running backward pass again.\n   m.zero_grad()\n   x.grad.zero_()\n\n   m.register_full_backward_hook(backward_hook)\n   m(x).sum().backward()\n   print('x.grad with backwards hook: {}'.format(x.grad))\n   : x.grad with backwards hook: tensor([[42., 42., 42.],\n                                         [42., 42., 42.]])\n\nAdvanced Features\n-----------------\n\nPyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare available for custom-written modules, with the small caveat that certain features may require modules to conform\nto particular constraints in order to be supported. In-depth discussion of these features and the corresponding\nrequirements can be found in the links below.\n\nDistributed Training\n********************","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":718,"to":737}}}}],["1197",{"pageContent":"Distributed Training\n********************\n\nVarious methods for distributed training exist within PyTorch, both for scaling up training using multiple GPUs\nas well as training across multiple machines. Check out the\n`distributed training overview page <https://pytorch.org/tutorials/beginner/dist_overview.html>`_ for\ndetailed information on how to utilize these.\n\nProfiling Performance\n*********************\n\nThe `PyTorch Profiler <https://pytorch.org/tutorials/beginner/profiler.html>`_ can be useful for identifying\nperformance bottlenecks within your models. It measures and outputs performance characteristics for\nboth memory usage and time spent.\n\nImproving Performance with Quantization\n***************************************\n\nApplying quantization techniques to modules can improve performance and memory usage by utilizing lower\nbitwidths than floating-point precision. Check out the various PyTorch-provided mechanisms for quantization\n`here <https://pytorch.org/docs/stable/quantization.html>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":737,"to":757}}}}],["1198",{"pageContent":"Improving Memory Usage with Pruning\n***********************************\n\nLarge deep learning models are often over-parametrized, resulting in high memory usage. To combat this, PyTorch\nprovides mechanisms for model pruning, which can help reduce memory usage while maintaining task accuracy. The\n`Pruning tutorial <https://pytorch.org/tutorials/intermediate/pruning_tutorial.html>`_ describes how to utilize\nthe pruning techniques PyTorch provides or define custom pruning techniques as necessary.\n\nParametrizations\n****************\n\nFor certain applications, it can be beneficial to constrain the parameter space during model training. For example,\nenforcing orthogonality of the learned parameters can improve convergence for RNNs. PyTorch provides a mechanism for\napplying `parametrizations <https://pytorch.org/tutorials/intermediate/parametrizations.html>`_ such as this, and\nfurther allows for custom constraints to be defined.\n\nTransforming Modules with FX\n****************************","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":759,"to":776}}}}],["1199",{"pageContent":"Transforming Modules with FX\n****************************\n\nThe `FX <https://pytorch.org/docs/stable/fx.html>`_ component of PyTorch provides a flexible way to transform\nmodules by operating directly on module computation graphs. This can be used to programmatically generate or\nmanipulate modules for a broad array of use cases. To explore FX, check out these examples of using FX for\n`convolution + batch norm fusion <https://pytorch.org/tutorials/intermediate/fx_conv_bn_fuser.html>`_ and\n`CPU performance analysis <https://pytorch.org/tutorials/intermediate/fx_profiling_tutorial.html>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/modules.rst","loc":{"lines":{"from":776,"to":783}}}}],["1200",{"pageContent":".. _MPS-Backend:\n\nMPS backend\n===========\n\n:mod:`mps` device enables high-performance\ntraining on GPU for MacOS devices with Metal programming framework.  It\nintroduces a new device to map Machine Learning computational graphs and\nprimitives on highly efficient Metal Performance Shaders Graph framework and\ntuned kernels provided by Metal Performance Shaders framework respectively.\n\nThe new MPS backend extends the PyTorch ecosystem and provides existing scripts\ncapabilities to setup and run operations on GPU.\n\nTo get started, simply move your Tensor and Module to the ``mps`` device:\n\n.. code:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/mps.rst","loc":{"lines":{"from":1,"to":17}}}}],["1201",{"pageContent":"To get started, simply move your Tensor and Module to the ``mps`` device:\n\n.. code:: python\n\n    # Check that MPS is available\n    if not torch.backends.mps.is_available():\n        if not torch.backends.mps.is_built():\n            print(\"MPS not available because the current PyTorch install was not \"\n                  \"built with MPS enabled.\")\n        else:\n            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n                  \"and/or you do not have an MPS-enabled device on this machine.\")\n\n    else:\n        mps_device = torch.device(\"mps\")\n\n        # Create a Tensor directly on the mps device\n        x = torch.ones(5, device=mps_device)\n        # Or\n        x = torch.ones(5, device=\"mps\")\n\n        # Any operation happens on the GPU\n        y = x * 2\n\n        # Move your model to mps just like any other device\n        model = YourFavoriteNet()\n        model.to(mps_device)\n\n        # Now every call runs on the GPU\n        pred = model(x)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/mps.rst","loc":{"lines":{"from":17,"to":46}}}}],["1202",{"pageContent":".. _multiprocessing-best-practices:\n\nMultiprocessing best practices\n==============================\n\n:mod:`torch.multiprocessing` is a drop in replacement for Python's\n:mod:`python:multiprocessing` module. It supports the exact same operations,\nbut extends it, so that all tensors sent through a\n:class:`python:multiprocessing.Queue`, will have their data moved into shared\nmemory and will only send a handle to another process.\n\n.. note::\n\n    When a :class:`~torch.Tensor` is sent to another process, the\n    :class:`~torch.Tensor` data is shared. If :attr:`torch.Tensor.grad` is\n    not ``None``, it is also shared. After a :class:`~torch.Tensor` without\n    a :attr:`torch.Tensor.grad` field is sent to the other process, it\n    creates a standard process-specific ``.grad`` :class:`~torch.Tensor` that\n    is not automatically shared across all processes, unlike how the\n    :class:`~torch.Tensor`'s data has been shared.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/multiprocessing.rst","loc":{"lines":{"from":1,"to":20}}}}],["1203",{"pageContent":"This allows to implement various training methods, like Hogwild, A3C, or any\nothers that require asynchronous operation.\n\n.. _multiprocessing-cuda-note:\n\nCUDA in multiprocessing\n-----------------------\n\nThe CUDA runtime does not support the ``fork`` start method; either the ``spawn`` or ``forkserver`` start method are\nrequired to use CUDA in subprocesses.\n\n.. note::\n  The start method can be set via either creating a context with\n  ``multiprocessing.get_context(...)`` or directly using\n  ``multiprocessing.set_start_method(...)``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/multiprocessing.rst","loc":{"lines":{"from":22,"to":36}}}}],["1204",{"pageContent":".. note::\n  The start method can be set via either creating a context with\n  ``multiprocessing.get_context(...)`` or directly using\n  ``multiprocessing.set_start_method(...)``.\n\nUnlike CPU tensors, the sending process is required to keep the original tensor\nas long as the receiving process retains a copy of the tensor. It is implemented\nunder the hood but requires users to follow the best practices for the program\nto run correctly. For example, the sending process must stay alive as long as\nthe consumer process has references to the tensor, and the refcounting can not\nsave you if the consumer process exits abnormally via a fatal signal. See\n:ref:`this section <multiprocessing-cuda-sharing-details>`.\n\nSee also: :ref:`cuda-nn-ddp-instead`\n\n\nBest practices and tips\n-----------------------\n\nAvoiding and fighting deadlocks\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/multiprocessing.rst","loc":{"lines":{"from":36,"to":56}}}}],["1205",{"pageContent":"See also: :ref:`cuda-nn-ddp-instead`\n\n\nBest practices and tips\n-----------------------\n\nAvoiding and fighting deadlocks\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThere are a lot of things that can go wrong when a new process is spawned, with\nthe most common cause of deadlocks being background threads. If there's any\nthread that holds a lock or imports a module, and ``fork`` is called, it's very\nlikely that the subprocess will be in a corrupted state and will deadlock or\nfail in a different way. Note that even if you don't, Python built in\nlibraries do - no need to look further than :mod:`python:multiprocessing`.\n:class:`python:multiprocessing.Queue` is actually a very complex class, that\nspawns multiple threads used to serialize, send and receive objects, and they\ncan cause aforementioned problems too. If you find yourself in such situation\ntry using a :class:`~python:multiprocessing.queues.SimpleQueue`, that doesn't\nuse any additional threads.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/multiprocessing.rst","loc":{"lines":{"from":56,"to":75}}}}],["1206",{"pageContent":"We're trying our best to make it easy for you and ensure these deadlocks don't\nhappen but some things are out of our control. If you have any issues you can't\ncope with for a while, try reaching out on forums, and we'll see if it's an\nissue we can fix.\n\nReuse buffers passed through a Queue\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRemember that each time you put a :class:`~torch.Tensor` into a\n:class:`python:multiprocessing.Queue`, it has to be moved into shared memory.\nIf it's already shared, it is a no-op, otherwise it will incur an additional\nmemory copy that can slow down the whole process. Even if you have a pool of\nprocesses sending data to a single one, make it send the buffers back - this\nis nearly free and will let you avoid a copy when sending next batch.\n\nAsynchronous multiprocess training (e.g. Hogwild)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/multiprocessing.rst","loc":{"lines":{"from":77,"to":93}}}}],["1207",{"pageContent":"Asynchronous multiprocess training (e.g. Hogwild)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nUsing :mod:`torch.multiprocessing`, it is possible to train a model\nasynchronously, with parameters either shared all the time, or being\nperiodically synchronized. In the first case, we recommend sending over the whole\nmodel object, while in the latter, we advise to only send the\n:meth:`~torch.nn.Module.state_dict`.\n\nWe recommend using :class:`python:multiprocessing.Queue` for passing all kinds\nof PyTorch objects between processes. It is possible to e.g. inherit the tensors\nand storages already in shared memory, when using the ``fork`` start method,\nhowever it is very bug prone and should be used with care, and only by advanced\nusers. Queues, even though they're sometimes a less elegant solution, will work\nproperly in all cases.\n\n.. warning::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/multiprocessing.rst","loc":{"lines":{"from":93,"to":109}}}}],["1208",{"pageContent":".. warning::\n\n    You should be careful about having global statements, that are not guarded\n    with an ``if __name__ == '__main__'``. If a different start method than\n    ``fork`` is used, they will be executed in all subprocesses.\n\nHogwild\n~~~~~~~\n\nA concrete Hogwild implementation can be found in the `examples repository`__,\nbut to showcase the overall structure of the code, there's also a minimal\nexample below as well::\n\n    import torch.multiprocessing as mp\n    from model import MyModel\n\n    def train(model):\n        # Construct data_loader, optimizer, etc.\n        for data, labels in data_loader:\n            optimizer.zero_grad()\n            loss_fn(model(data), labels).backward()\n            optimizer.step()  # This will update the shared parameters","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/multiprocessing.rst","loc":{"lines":{"from":109,"to":130}}}}],["1209",{"pageContent":"if __name__ == '__main__':\n        num_processes = 4\n        model = MyModel()\n        # NOTE: this is required for the ``fork`` method to work\n        model.share_memory()\n        processes = []\n        for rank in range(num_processes):\n            p = mp.Process(target=train, args=(model,))\n            p.start()\n            processes.append(p)\n        for p in processes:\n            p.join()\n\n.. __: https://github.com/pytorch/examples/tree/master/mnist_hogwild","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/multiprocessing.rst","loc":{"lines":{"from":132,"to":145}}}}],["1210",{"pageContent":".. _numerical_accuracy:\n\nNumerical accuracy\n==================","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":1,"to":4}}}}],["1211",{"pageContent":"In modern computers, floating point numbers are represented using IEEE 754 standard.\nFor more details on floating point arithmetics and IEEE 754 standard, please see\n`Floating point arithmetic <https://en.wikipedia.org/wiki/Floating-point_arithmetic>`_\nIn particular, note that floating point provides limited accuracy (about 7 decimal digits\nfor single precision floating point numbers, about 16 decimal digits for double precision\nfloating point numbers) and that floating point addition and multiplication are not\nassociative, so the order of the operations affects the results.\nBecause of this, PyTorch is not guaranteed\nto produce bitwise identical results for floating point computations that are\nmathematically identical. Similarly, bitwise identical results are not guaranteed across\nPyTorch releases, individual commits, or different platforms. In particular, CPU and GPU\nresults can be different even for bitwise-identical inputs and even after controlling for\nthe sources of randomness.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":6,"to":18}}}}],["1212",{"pageContent":"Batched computations or slice computations\n------------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":20,"to":21}}}}],["1213",{"pageContent":"Many operations in PyTorch support batched computation, where the same operation is performed\nfor the elements of the batches of inputs. An example of this is :meth:`torch.mm` and\n:meth:`torch.bmm`. It is possible to implement batched computation as a loop over batch elements,\nand apply the necessary math operations to the individual batch elements, for efficiency reasons\nwe are not doing that, and typically perform computation for the whole batch. The mathematical\nlibraries that we are calling, and PyTorch internal implementations of operations can produces\nslightly different results in this case, compared to non-batched computations. In particular,\nlet ``A`` and ``B`` be 3D tensors with the dimensions suitable for batched matrix multiplication.\nThen ``(A@B)[0]`` (the first element of the batched result) is not guaranteed to be bitwise\nidentical to ``A[0]@B[0]`` (the matrix product of the first elements of the input batches)\neven though mathematically it's an identical computation.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":23,"to":33}}}}],["1214",{"pageContent":"Similarly, an operation applied to a tensor slice is not guaranteed to produce results that are\nidentical to the slice of the result of the same operation applied to the full tensor. E.g. let\n``A`` be a 2-dimensional tensor. ``A.sum(-1)[0]`` is not guaranteed to be bitwise equal to\n``A[:,0].sum()``.\n\n\nExtremal values\n---------------\n\nWhen inputs contain large values such that intermediate results may overflow the range of the\nused datatype, the end result may overflow too, even though it is representable in the original\ndatatype. E.g.:\n\n.. code:: python\n\n    import torch\n    a=torch.tensor([1e20, 1e20]) # fp32 type by default\n    a.norm() # produces tensor(inf)\n    a.double().norm() # produces tensor(1.4142e+20, dtype=torch.float64), representable in fp32\n\n.. _Linear Algebra Stability:\n\nLinear algebra (``torch.linalg``)\n---------------------------------\n\nNon-finite values\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":35,"to":61}}}}],["1215",{"pageContent":".. _Linear Algebra Stability:\n\nLinear algebra (``torch.linalg``)\n---------------------------------\n\nNon-finite values\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe external libraries (backends) that ``torch.linalg`` uses provide no guarantees on their behaviour\nwhen the inputs have non-finite values like ``inf`` or ``NaN``. As such, neither does PyTorch.\nThe operations may return a tensor with non-finite values, or raise an exception, or even segfault.\n\nConsider using :func:`torch.isfinite` before calling these functions to detect this situation.\n\nExtremal values in linalg\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nFunctions within ``torch.linalg`` have more `Extremal Values`_ than other PyTorch functions.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":61,"to":78}}}}],["1216",{"pageContent":"Extremal values in linalg\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nFunctions within ``torch.linalg`` have more `Extremal Values`_ than other PyTorch functions.\n\n:ref:`linalg solvers` and :ref:`linalg inverses` assume that the input matrix ``A`` is invertible. If it is close to\nbeing non-invertible (for example, if it has a very small singular value), then these algorithms may silently return\nincorrect results. These matrices are said to be `ill-conditioned <https://nhigham.com/2020/03/19/what-is-a-condition-number/>`_.\nIf provided with ill-conditioned inputs, the result of these functions they may vary when using the same inputs on different\ndevices or when using different backends via the keyword ``driver``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":78,"to":87}}}}],["1217",{"pageContent":"Spectral operations like ``svd``, ``eig``, and ``eigh`` may also return incorrect results (and their gradients may be infinite)\nwhen their inputs have singular values that are close to each other. This is because the algorithms used to compute these decompositions\nstruggle to converge for these inputs.\n\nRunning the computation in ``float64`` (as NumPy does by default) often helps, but it does not solve these issues in all cases.\nAnalyzing the spectrum of the inputs via :func:`torch.linalg.svdvals` or their condition number via :func:`torch.linalg.cond`\nmay help to detect these issues.\n\n\nTensorFloat-32(TF32) on Nvidia Ampere devices\n---------------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":89,"to":99}}}}],["1218",{"pageContent":"On Ampere Nvidia GPUs, PyTorch can use TensorFloat32 (TF32) to speed up mathematically intensive operations, in particular matrix multiplications and convolutions.\nWhen an operation is performed using TF32 tensor cores, only the first 10 bits of the input mantissa are read.\nThis may reduce accuracy and produce surprising results (e.g., multiplying a matrix by the identity matrix may produce results that are different from the input).\nBy default, TF32 tensor cores are disabled for matrix multiplications and enabled for convolutions, although most neural network workloads have the same convergence behavior when using TF32 as they have with fp32.\nWe recommend enabling TF32 tensor cores for matrix multiplications with ``torch.backends.cuda.matmul.allow_tf32 = True`` if your network does not need full float32 precision.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":101,"to":105}}}}],["1219",{"pageContent":"We recommend enabling TF32 tensor cores for matrix multiplications with ``torch.backends.cuda.matmul.allow_tf32 = True`` if your network does not need full float32 precision.\nIf your network needs full float32 precision for both matrix multiplications and convolutions, then TF32 tensor cores can also be disabled for convolutions with ``torch.backends.cudnn.allow_tf32 = False``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":105,"to":106}}}}],["1220",{"pageContent":"For more information see :ref:`TensorFloat32<tf32_on_ampere>`.\n\nReduced Precision Reduction for FP16  and BF16 GEMMs\n----------------------------------------------------\nHalf-precision GEMM operations are typically done with intermediate accumulations (reduction) in single-precision for numerical accuracy and improved resilience to overflow. For performance, certain GPU architectures, especially more recent ones, allow a few truncations of the intermediate accumulation results to the reduced precision (e.g., half-precision). This change is often benign from the perspective of model convergence, though it may lead to unexpected results (e.g., ``inf`` values when the final result should be be representable in half-precision).\nIf reduced-precision reductions are problematic, they can be turned off with\n``torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":108,"to":114}}}}],["1221",{"pageContent":"A similar flag exists for BF16 GEMM operations and is turned off by default. If BF16\nreduced-precision reductions are problematic, they can be turned off with\n``torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False``\n\nFor more information see :ref:`allow_fp16_reduced_precision_reduction<fp16reducedprecision>` and :ref:`allow_bf16_reduced_precision_reduction<bf16reducedprecision>`\n\n.. _fp16_on_mi200:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":116,"to":122}}}}],["1222",{"pageContent":"For more information see :ref:`allow_fp16_reduced_precision_reduction<fp16reducedprecision>` and :ref:`allow_bf16_reduced_precision_reduction<bf16reducedprecision>`\n\n.. _fp16_on_mi200:\n\nReduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices\n------------------------------------------------------------------------------------\nOn AMD Instinct MI200 GPUs, the FP16 and BF16 V_DOT2 and MFMA matrix instructions flush input and output denormal values to zero. FP32 and FP64 MFMA matrix instructions do not flush input and output denormal values to zero. The affected instructions are only used by rocBLAS (GEMM) and MIOpen (convolution) kernels; all other PyTorch operations will not encounter this behavior. All other supported AMD GPUs will not encounter this behavior.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":122,"to":128}}}}],["1223",{"pageContent":"rocBLAS and MIOpen provide alternate implementations for affected FP16 operations. Alternate implementations for BF16 operations are not provided; BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values. For the FP16 alternate implementations, FP16 input values are cast to an intermediate BF16 value and then cast back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged.\n\nWhen training using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. Denormal values more frequently occur in the backward pass of training during gradient calculation. PyTorch by default will use the rocBLAS and MIOpen alternate implementations during the backward pass. The default behavior can be overridden using environment variables, ROCBLAS_INTERNAL_FP16_ALT_IMPL and MIOPEN_DEBUG_CONVOLUTION_ATTRIB_FP16_ALT_IMPL. The behavior of these environment variables is as follows:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":130,"to":132}}}}],["1224",{"pageContent":"+---------------+-----------+-----------+\n|               | forward   | backward  |\n+===============+===========+===========+\n| Env unset     | original  | alternate |\n+---------------+-----------+-----------+\n| Env set to 1  | alternate | alternate |\n+---------------+-----------+-----------+\n| Env set to 0  | original  | original  |\n+---------------+-----------+-----------+\n\nThe following is the list of operations where rocBLAS may be used:\n\n* torch.addbmm\n* torch.addmm\n* torch.baddbmm\n* torch.bmm\n* torch.mm\n* torch.nn.GRUCell\n* torch.nn.LSTMCell\n* torch.nn.Linear\n* torch.sparse.addmm\n* the following torch._C._ConvBackend implementations:\n\n  * slowNd\n  * slowNd_transposed\n  * slowNd_dilated\n  * slowNd_dilated_transposed\n\nThe following is the list of operations where MIOpen may be used:\n\n* torch.nn.Conv[Transpose]Nd\n* the following torch._C._ConvBackend implementations:\n\n  * ConvBackend::Miopen\n  * ConvBackend::MiopenDepthwise\n  * ConvBackend::MiopenTranspose","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/numerical_accuracy.rst","loc":{"lines":{"from":134,"to":169}}}}],["1225",{"pageContent":".. _reproducibility:\n\nReproducibility\n===============\n\nCompletely reproducible results are not guaranteed across PyTorch releases,\nindividual commits, or different platforms. Furthermore, results may not be\nreproducible between CPU and GPU executions, even when using identical seeds.\n\nHowever, there are some steps you can take to limit the number of sources of\nnondeterministic behavior for a specific platform, device, and PyTorch release.\nFirst, you can control sources of randomness that can cause multiple executions\nof your application to behave differently. Second, you can configure PyTorch\nto avoid using nondeterministic algorithms for some operations, so that multiple\ncalls to those operations, given the same inputs, will produce the same result.\n\n.. warning::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/randomness.rst","loc":{"lines":{"from":1,"to":17}}}}],["1226",{"pageContent":".. warning::\n\n    Deterministic operations are often slower than nondeterministic operations, so\n    single-run performance may decrease for your model. However, determinism may\n    save time in development by facilitating experimentation, debugging, and\n    regression testing.\n\nControlling sources of randomness\n.................................\n\nPyTorch random number generator\n-------------------------------\nYou can use :meth:`torch.manual_seed()` to seed the RNG for all devices (both\nCPU and CUDA)::\n\n    import torch\n    torch.manual_seed(0)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/randomness.rst","loc":{"lines":{"from":17,"to":33}}}}],["1227",{"pageContent":"PyTorch random number generator\n-------------------------------\nYou can use :meth:`torch.manual_seed()` to seed the RNG for all devices (both\nCPU and CUDA)::\n\n    import torch\n    torch.manual_seed(0)\n\nSome PyTorch operations may use random numbers internally.\n:meth:`torch.svd_lowrank()` does this, for instance. Consequently, calling it\nmultiple times back-to-back with the same input arguments may give different\nresults. However, as long as :meth:`torch.manual_seed()` is set to a constant\nat the beginning of an application and all other sources of nondeterminism have\nbeen eliminated, the same series of random numbers will be generated each time\nthe application is run in the same environment.\n\nIt is also possible to obtain identical results from an operation that uses\nrandom numbers by setting :meth:`torch.manual_seed()` to the same value between\nsubsequent calls.\n\nPython\n------\n\nFor custom operators, you might need to set python seed as well::\n\n    import random\n    random.seed(0)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/randomness.rst","loc":{"lines":{"from":33,"to":59}}}}],["1228",{"pageContent":"Python\n------\n\nFor custom operators, you might need to set python seed as well::\n\n    import random\n    random.seed(0)\n\nRandom number generators in other libraries\n-------------------------------------------\nIf you or any of the libraries you are using rely on NumPy, you can seed the global\nNumPy RNG with::\n\n    import numpy as np\n    np.random.seed(0)\n\nHowever, some applications and libraries may use NumPy Random Generator objects,\nnot the global RNG\n(`<https://numpy.org/doc/stable/reference/random/generator.html>`_), and those will\nneed to be seeded consistently as well.\n\nIf you are using any other libraries that use random number generators, refer to\nthe documentation for those libraries to see how to set consistent seeds for them.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/randomness.rst","loc":{"lines":{"from":59,"to":81}}}}],["1229",{"pageContent":"If you are using any other libraries that use random number generators, refer to\nthe documentation for those libraries to see how to set consistent seeds for them.\n\nCUDA convolution benchmarking\n-----------------------------\nThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism\nacross multiple executions of an application. When a cuDNN convolution is called with a\nnew set of size parameters, an optional feature can run multiple convolution algorithms,\nbenchmarking them to find the fastest one. Then, the fastest algorithm will be used\nconsistently during the rest of the process for the corresponding set of size parameters.\nDue to benchmarking noise and different hardware, the benchmark may select different\nalgorithms on subsequent runs, even on the same machine.\n\nDisabling the benchmarking feature with :code:`torch.backends.cudnn.benchmark = False`\ncauses cuDNN to deterministically select an algorithm, possibly at the cost of reduced\nperformance.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/randomness.rst","loc":{"lines":{"from":81,"to":96}}}}],["1230",{"pageContent":"Disabling the benchmarking feature with :code:`torch.backends.cudnn.benchmark = False`\ncauses cuDNN to deterministically select an algorithm, possibly at the cost of reduced\nperformance.\n\nHowever, if you do not need reproducibility across multiple executions of your application,\nthen performance might improve if the benchmarking feature is enabled with\n:code:`torch.backends.cudnn.benchmark = True`.\n\nNote that this setting is different from the :code:`torch.backends.cudnn.deterministic`\nsetting discussed below.\n\nAvoiding nondeterministic algorithms\n....................................\n:meth:`torch.use_deterministic_algorithms` lets you configure PyTorch to use\ndeterministic algorithms instead of nondeterministic ones where available, and\nto throw an error if an operation is known to be nondeterministic (and without\na deterministic alternative).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/randomness.rst","loc":{"lines":{"from":96,"to":112}}}}],["1231",{"pageContent":"Please check the documentation for :meth:`torch.use_deterministic_algorithms()`\nfor a full list of affected operations. If an operation does not act correctly\naccording to the documentation, or if you need a deterministic implementation\nof an operation that does not have one, please submit an issue:\n`<https://github.com/pytorch/pytorch/issues?q=label:%22module:%20determinism%22>`_\n\nFor example, running the nondeterministic CUDA implementation of :meth:`torch.Tensor.index_add_`\nwill throw an error::\n\n    >>> import torch\n    >>> torch.use_deterministic_algorithms(True)\n    >>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))\n    Traceback (most recent call last):\n    File \"<stdin>\", line 1, in <module>\n    RuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set\n    'torch.use_deterministic_algorithms(True)'. ...","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/randomness.rst","loc":{"lines":{"from":114,"to":129}}}}],["1232",{"pageContent":"When :meth:`torch.bmm` is called with sparse-dense CUDA tensors it typically uses a\nnondeterministic algorithm, but when the deterministic flag is turned on, its alternate\ndeterministic implementation will be used::\n\n    >>> import torch\n    >>> torch.use_deterministic_algorithms(True)\n    >>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())\n    tensor([[[ 1.1900, -2.3409],\n             [ 0.4796,  0.8003]],\n            [[ 0.1509,  1.8027],\n             [ 0.0333, -1.1444]]], device='cuda:0')\n\nFurthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you\nshould set the environment variable `CUBLAS_WORKSPACE_CONFIG` according to CUDA documentation:\n`<https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility>`_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/randomness.rst","loc":{"lines":{"from":131,"to":145}}}}],["1233",{"pageContent":"CUDA convolution determinism\n----------------------------\nWhile disabling CUDA convolution benchmarking (discussed above) ensures that\nCUDA selects the same algorithm each time an application is run, that algorithm\nitself may be nondeterministic, unless either\n:code:`torch.use_deterministic_algorithms(True)` or\n:code:`torch.backends.cudnn.deterministic = True` is set. The latter setting\ncontrols only this behavior, unlike :meth:`torch.use_deterministic_algorithms`\nwhich will make other PyTorch operations behave deterministically, too.\n\nCUDA RNN and LSTM\n-----------------\nIn some versions of CUDA, RNNs and LSTM networks may have non-deterministic behavior.\nSee :meth:`torch.nn.RNN` and :meth:`torch.nn.LSTM` for details and workarounds.\n\nDataLoader\n..........\n\nDataLoader will reseed workers following :ref:`data-loading-randomness` algorithm.\nUse :meth:`worker_init_fn` and `generator` to preserve reproducibility::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/randomness.rst","loc":{"lines":{"from":147,"to":166}}}}],["1234",{"pageContent":"DataLoader\n..........\n\nDataLoader will reseed workers following :ref:`data-loading-randomness` algorithm.\nUse :meth:`worker_init_fn` and `generator` to preserve reproducibility::\n\n    def seed_worker(worker_id):\n        worker_seed = torch.initial_seed() % 2**32\n        numpy.random.seed(worker_seed)\n        random.seed(worker_seed)\n\n    g = torch.Generator()\n    g.manual_seed(0)\n\n    DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        worker_init_fn=seed_worker,\n        generator=g,\n    )","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/randomness.rst","loc":{"lines":{"from":166,"to":186}}}}],["1235",{"pageContent":"Serialization semantics\n=======================\n\nThis note describes how you can save and load PyTorch tensors and module states\nin Python, and how to serialize Python modules so they can be loaded in C++.\n\n.. contents:: Table of Contents\n\n.. _saving-loading-tensors:\n\nSaving and loading tensors\n--------------------------\n\n:func:`torch.save` and :func:`torch.load` let you easily save and load tensors:\n\n::\n\n    >>> t = torch.tensor([1., 2.])\n    >>> torch.save(t, 'tensor.pt')\n    >>> torch.load('tensor.pt')\n    tensor([1., 2.])\n\nBy convention, PyTorch files are typically written with a ‘.pt’ or ‘.pth’ extension.\n\n:func:`torch.save` and :func:`torch.load` use Python’s pickle by default,\nso you can also save multiple tensors as part of Python objects like tuples,\nlists, and dicts:\n\n::\n\n    >>> d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])}\n    >>> torch.save(d, 'tensor_dict.pt')\n    >>> torch.load('tensor_dict.pt')\n    {'a': tensor([1., 2.]), 'b': tensor([3., 4.])}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":1,"to":34}}}}],["1236",{"pageContent":"::\n\n    >>> d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])}\n    >>> torch.save(d, 'tensor_dict.pt')\n    >>> torch.load('tensor_dict.pt')\n    {'a': tensor([1., 2.]), 'b': tensor([3., 4.])}\n\nCustom data structures that include PyTorch tensors can also be saved if the\ndata structure is pickle-able.\n\n.. _preserve-storage-sharing:\n\nSaving and loading tensors preserves views\n---------------------------------------------\n\nSaving tensors preserves their view relationships:\n\n::\n\n    >>> numbers = torch.arange(1, 10)\n    >>> evens = numbers[1::2]\n    >>> torch.save([numbers, evens], 'tensors.pt')\n    >>> loaded_numbers, loaded_evens = torch.load('tensors.pt')\n    >>> loaded_evens *= 2\n    >>> loaded_numbers\n    tensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9])\n\nBehind the scenes, these tensors share the same \"storage.\" See\n`Tensor Views <https://pytorch.org/docs/master/tensor_view.html>`_ for more\non views and storage.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":34,"to":63}}}}],["1237",{"pageContent":"Behind the scenes, these tensors share the same \"storage.\" See\n`Tensor Views <https://pytorch.org/docs/master/tensor_view.html>`_ for more\non views and storage.\n\nWhen PyTorch saves tensors it saves their storage objects and tensor\nmetadata separately. This is an implementation detail that may change in the\nfuture, but it typically saves space and lets PyTorch easily\nreconstruct the view relationships between the loaded tensors. In the above\nsnippet, for example, only a single storage is written to 'tensors.pt'.\n\nIn some cases, however, saving the current storage objects may be unnecessary\nand create prohibitively large files. In the following snippet a storage much\nlarger than the saved tensor is written to a file:\n\n::\n\n    >>> large = torch.arange(1, 1000)\n    >>> small = large[0:5]\n    >>> torch.save(small, 'small.pt')\n    >>> loaded_small = torch.load('small.pt')\n    >>> loaded_small.storage().size()\n    999","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":63,"to":84}}}}],["1238",{"pageContent":"::\n\n    >>> large = torch.arange(1, 1000)\n    >>> small = large[0:5]\n    >>> torch.save(small, 'small.pt')\n    >>> loaded_small = torch.load('small.pt')\n    >>> loaded_small.storage().size()\n    999\n\nInstead of saving only the five values in the `small` tensor to 'small.pt,'\nthe 999 values in the storage it shares with `large` were saved and loaded.\n\nWhen saving tensors with fewer elements than their storage objects, the size of\nthe saved file can be reduced by first cloning the tensors. Cloning a tensor\nproduces a new tensor with a new storage object containing only the values\nin the tensor:\n\n::\n\n    >>> large = torch.arange(1, 1000)\n    >>> small = large[0:5]\n    >>> torch.save(small.clone(), 'small.pt')  # saves a clone of small\n    >>> loaded_small = torch.load('small.pt')\n    >>> loaded_small.storage().size()\n    5","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":84,"to":108}}}}],["1239",{"pageContent":"Since the cloned tensors are independent of each other, however, they have\nnone of the view relationships the original tensors did. If both file size and\nview relationships are important when saving tensors smaller than their\nstorage objects, then care must be taken to construct new tensors that minimize\nthe size of their storage objects but still have the desired view relationships\nbefore saving.\n\n.. _saving-loading-python-modules:\n\nSaving and loading torch.nn.Modules\n-----------------------------------\n\nSee also: `Tutorial: Saving and loading modules <https://pytorch.org/tutorials/beginner/saving_loading_models.html>`_\n\nIn PyTorch, a module’s state is frequently serialized using a ‘state dict.’\nA module’s state dict contains all of its parameters and persistent buffers:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":110,"to":127}}}}],["1240",{"pageContent":"In PyTorch, a module’s state is frequently serialized using a ‘state dict.’\nA module’s state dict contains all of its parameters and persistent buffers:\n\n::\n\n    >>> bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n    >>> list(bn.named_parameters())\n    [('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)),\n     ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))]\n\n    >>> list(bn.named_buffers())\n    [('running_mean', tensor([0., 0., 0.])),\n     ('running_var', tensor([1., 1., 1.])),\n     ('num_batches_tracked', tensor(0))]\n\n    >>> bn.state_dict()\n    OrderedDict([('weight', tensor([1., 1., 1.])),\n                 ('bias', tensor([0., 0., 0.])),\n                 ('running_mean', tensor([0., 0., 0.])),\n                 ('running_var', tensor([1., 1., 1.])),\n                 ('num_batches_tracked', tensor(0))])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":127,"to":147}}}}],["1241",{"pageContent":"Instead of saving a module directly, for compatibility reasons it is recommended\nto instead save only its state dict. Python modules even have a function,\n:meth:`~torch.nn.Module.load_state_dict`, to restore their states from a state dict:\n\n::\n\n    >>> torch.save(bn.state_dict(), 'bn.pt')\n    >>> bn_state_dict = torch.load('bn.pt')\n    >>> new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n    >>> new_bn.load_state_dict(bn_state_dict)\n    <All keys matched successfully>\n\nNote that the state dict is first loaded from its file with :func:`torch.load`\nand the state then restored with :meth:`~torch.nn.Module.load_state_dict`.\n\nEven custom modules and modules containing other modules have state dicts and\ncan use this pattern:\n\n::\n\n    # A module with two linear layers\n    >>> class MyModule(torch.nn.Module):\n          def __init__(self):\n            super().__init__()\n            self.l0 = torch.nn.Linear(4, 2)\n            self.l1 = torch.nn.Linear(2, 1)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":149,"to":174}}}}],["1242",{"pageContent":"def forward(self, input):\n            out0 = self.l0(input)\n            out0_relu = torch.nn.functional.relu(out0)\n            return self.l1(out0_relu)\n\n    >>> m = MyModule()\n    >>> m.state_dict()\n    OrderedDict([('l0.weight', tensor([[ 0.1400, 0.4563, -0.0271, -0.4406],\n                                       [-0.3289, 0.2827, 0.4588, 0.2031]])),\n                 ('l0.bias', tensor([ 0.0300, -0.1316])),\n                 ('l1.weight', tensor([[0.6533, 0.3413]])),\n                 ('l1.bias', tensor([-0.1112]))])\n\n    >>> torch.save(m.state_dict(), 'mymodule.pt')\n    >>> m_state_dict = torch.load('mymodule.pt')\n    >>> new_m = MyModule()\n    >>> new_m.load_state_dict(m_state_dict)\n    <All keys matched successfully>\n\n.. _serializing-python-modules:\n\nSerializing torch.nn.Modules and loading them in C++\n----------------------------------------------------\n\nSee also: `Tutorial: Loading a TorchScript Model in C++ <https://pytorch.org/tutorials/advanced/cpp_export.html>`_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":176,"to":200}}}}],["1243",{"pageContent":"See also: `Tutorial: Loading a TorchScript Model in C++ <https://pytorch.org/tutorials/advanced/cpp_export.html>`_\n\nScriptModules can be serialized as a TorchScript program and loaded\nusing :func:`torch.jit.load`.\nThis serialization encodes all the modules’ methods, submodules, parameters,\nand attributes, and it allows the serialized program to be loaded in C++\n(i.e. without Python).\n\nThe distinction between :func:`torch.jit.save` and :func:`torch.save` may not\nbe immediately clear. :func:`torch.save` saves Python objects with pickle.\nThis is especially useful for prototyping, researching, and training.\n:func:`torch.jit.save`, on the other hand, serializes ScriptModules to a format\nthat can be loaded in Python or C++. This is useful when saving and loading C++\nmodules or for running modules trained in Python with C++, a common practice\nwhen deploying PyTorch models.\n\nTo script, serialize and load a module in Python:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":200,"to":218}}}}],["1244",{"pageContent":"To script, serialize and load a module in Python:\n\n::\n\n    >>> scripted_module = torch.jit.script(MyModule())\n    >>> torch.jit.save(scripted_module, 'mymodule.pt')\n    >>> torch.jit.load('mymodule.pt')\n    RecursiveScriptModule( original_name=MyModule\n                          (l0): RecursiveScriptModule(original_name=Linear)\n                          (l1): RecursiveScriptModule(original_name=Linear) )\n\n\nTraced modules can also be saved with :func:`torch.jit.save`, with the caveat\nthat only the traced code path is serialized. The following example demonstrates\nthis:\n\n::\n\n    # A module with control flow\n    >>> class ControlFlowModule(torch.nn.Module):\n          def __init__(self):\n            super().__init__()\n            self.l0 = torch.nn.Linear(4, 2)\n            self.l1 = torch.nn.Linear(2, 1)\n\n          def forward(self, input):\n            if input.dim() > 1:\n                return torch.tensor(0)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":218,"to":245}}}}],["1245",{"pageContent":"def forward(self, input):\n            if input.dim() > 1:\n                return torch.tensor(0)\n\n            out0 = self.l0(input)\n            out0_relu = torch.nn.functional.relu(out0)\n            return self.l1(out0_relu)\n\n    >>> traced_module = torch.jit.trace(ControlFlowModule(), torch.randn(4))\n    >>> torch.jit.save(traced_module, 'controlflowmodule_traced.pt')\n    >>> loaded = torch.jit.load('controlflowmodule_traced.pt')\n    >>> loaded(torch.randn(2, 4)))\n    tensor([[-0.1571], [-0.3793]], grad_fn=<AddBackward0>)\n\n    >>> scripted_module = torch.jit.script(ControlFlowModule(), torch.randn(4))\n    >>> torch.jit.save(scripted_module, 'controlflowmodule_scripted.pt')\n    >>> loaded = torch.jit.load('controlflowmodule_scripted.pt')\n    >> loaded(torch.randn(2, 4))\n    tensor(0)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":245,"to":263}}}}],["1246",{"pageContent":"The above module has an if statement that is not triggered by the traced inputs,\nand so is not part of the traced module and not serialized with it.\nThe scripted module, however, contains the if statement and is serialized with it.\nSee the `TorchScript documentation <https://pytorch.org/docs/stable/jit.html>`_\nfor more on scripting and tracing.\n\nFinally, to load the module in C++:\n\n::\n\n    >>> torch::jit::script::Module module;\n    >>> module = torch::jit::load('controlflowmodule_scripted.pt');\n\nSee the `PyTorch C++ API documentation <https://pytorch.org/cppdocs/>`_\nfor details about how to use PyTorch modules in C++.\n\n.. _saving-loading-across-versions:\n\nSaving and loading ScriptModules across PyTorch versions\n-----------------------------------------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":265,"to":284}}}}],["1247",{"pageContent":".. _saving-loading-across-versions:\n\nSaving and loading ScriptModules across PyTorch versions\n-----------------------------------------------------------\n\nThe PyTorch Team recommends saving and loading modules with the same version of\nPyTorch. Older versions of PyTorch may not support newer modules, and newer\nversions may have removed or modified older behavior. These changes are\nexplicitly described in\nPyTorch’s `release notes <https://github.com/pytorch/pytorch/releases>`_,\nand modules relying on functionality that has changed may need to be updated\nto continue working properly. In limited cases, detailed below, PyTorch will\npreserve the historic behavior of serialized ScriptModules so they do not require\nan update.\n\ntorch.div performing integer division\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn PyTorch 1.5 and earlier :func:`torch.div` would perform floor division when\ngiven two integer inputs:\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":284,"to":305}}}}],["1248",{"pageContent":"torch.div performing integer division\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn PyTorch 1.5 and earlier :func:`torch.div` would perform floor division when\ngiven two integer inputs:\n\n::\n\n    # PyTorch 1.5 (and earlier)\n    >>> a = torch.tensor(5)\n    >>> b = torch.tensor(3)\n    >>> a / b\n    tensor(1)\n\nIn PyTorch 1.7, however, :func:`torch.div` will always perform a true division\nof its inputs, just like division in Python 3:\n\n::\n\n    # PyTorch 1.7\n    >>> a = torch.tensor(5)\n    >>> b = torch.tensor(3)\n    >>> a / b\n    tensor(1.6667)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":305,"to":328}}}}],["1249",{"pageContent":"::\n\n    # PyTorch 1.7\n    >>> a = torch.tensor(5)\n    >>> b = torch.tensor(3)\n    >>> a / b\n    tensor(1.6667)\n\nThe behavior of :func:`torch.div` is preserved in serialized ScriptModules.\nThat is, ScriptModules serialized with versions of PyTorch before 1.6 will continue\nto see :func:`torch.div` perform floor division when given two integer inputs\neven when loaded with newer versions of PyTorch. ScriptModules using :func:`torch.div`\nand serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of\nPyTorch, however, since those earlier versions do not understand the new behavior.\n\ntorch.full always inferring a float dtype\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn PyTorch 1.5 and earlier :func:`torch.full` always returned a float tensor,\nregardless of the fill value it’s given:\n\n::\n\n    # PyTorch 1.5 and earlier\n    >>> torch.full((3,), 1)  # Note the integer fill value...\n    tensor([1., 1., 1.])     # ...but float tensor!","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":328,"to":353}}}}],["1250",{"pageContent":"::\n\n    # PyTorch 1.5 and earlier\n    >>> torch.full((3,), 1)  # Note the integer fill value...\n    tensor([1., 1., 1.])     # ...but float tensor!\n\nIn PyTorch 1.7, however, :func:`torch.full` will infer the returned tensor’s\ndtype from the fill value:\n\n::\n\n    # PyTorch 1.7\n    >>> torch.full((3,), 1)\n    tensor([1, 1, 1])\n\n    >>> torch.full((3,), True)\n    tensor([True, True, True])\n\n    >>> torch.full((3,), 1.)\n    tensor([1., 1., 1.])\n\n    >>> torch.full((3,), 1 + 1j)\n    tensor([1.+1.j, 1.+1.j, 1.+1.j])\n\nThe behavior of :func:`torch.full` is preserved in serialized ScriptModules. That is,\nScriptModules serialized with versions of PyTorch before 1.6 will continue to see\ntorch.full return float tensors by default, even when given bool or\ninteger fill values. ScriptModules using :func:`torch.full` and serialized on PyTorch 1.6\nand later cannot be loaded in earlier versions of PyTorch, however, since those\nearlier versions do not understand the new behavior.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/serialization.rst","loc":{"lines":{"from":353,"to":382}}}}],["1251",{"pageContent":"Windows FAQ\n==========================\n\nBuilding from source\n--------------------\n\nInclude optional components\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThere are two supported components for Windows PyTorch:\nMKL and MAGMA. Here are the steps to build with them.\n\n.. code-block:: bat\n\n    REM Make sure you have 7z and curl installed.\n\n    REM Download MKL files\n    curl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O\n    7z x -aoa mkl_2020.2.254.7z -omkl\n\n    REM Download MAGMA files\n    REM version available:\n    REM 2.5.4 (CUDA 10.1 10.2 11.0 11.1) x (Debug Release)\n    REM 2.5.3 (CUDA 10.1 10.2 11.0) x (Debug Release)\n    REM 2.5.2 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\n    REM 2.5.1 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\n    set CUDA_PREFIX=cuda102\n    set CONFIG=release\n    curl -k https://s3.amazonaws.com/ossci-windows/magma_2.5.4_%CUDA_PREFIX%_%CONFIG%.7z -o magma.7z\n    7z x -aoa magma.7z -omagma","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/windows.rst","loc":{"lines":{"from":1,"to":30}}}}],["1252",{"pageContent":"REM Setting essential environment variables\n    set \"CMAKE_INCLUDE_PATH=%cd%\\mkl\\include\"\n    set \"LIB=%cd%\\mkl\\lib;%LIB%\"\n    set \"MAGMA_HOME=%cd%\\magma\"\n\nSpeeding CUDA build for Windows\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nVisual Studio doesn't support parallel custom task currently.\nAs an alternative, we can use ``Ninja`` to parallelize CUDA\nbuild tasks. It can be used by typing only a few lines of code.\n\n.. code-block:: bat\n\n    REM Let's install ninja first.\n    pip install ninja\n\n    REM Set it as the cmake generator\n    set CMAKE_GENERATOR=Ninja\n\n\nOne key install script\n^^^^^^^^^^^^^^^^^^^^^^\n\nYou can take a look at `this set of scripts\n<https://github.com/peterjc123/pytorch-scripts>`_.\nIt will lead the way for you.\n\nExtension\n---------\n\nCFFI Extension\n^^^^^^^^^^^^^^\n\nThe support for CFFI Extension is very experimental. You must specify\nadditional ``libraries`` in ``Extension`` object to make it build on\nWindows.\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/windows.rst","loc":{"lines":{"from":32,"to":70}}}}],["1253",{"pageContent":"CFFI Extension\n^^^^^^^^^^^^^^\n\nThe support for CFFI Extension is very experimental. You must specify\nadditional ``libraries`` in ``Extension`` object to make it build on\nWindows.\n\n.. code-block:: python\n\n   ffi = create_extension(\n       '_ext.my_lib',\n       headers=headers,\n       sources=sources,\n       define_macros=defines,\n       relative_to=__file__,\n       with_cuda=with_cuda,\n       extra_compile_args=[\"-std=c99\"],\n       libraries=['ATen', '_C'] # Append cuda libraries when necessary, like cudart\n   )\n\nCpp Extension\n^^^^^^^^^^^^^\n\nThis type of extension has better support compared with\nthe previous one. However, it still needs some manual\nconfiguration. First, you should open the\n**x86_x64 Cross Tools Command Prompt for VS 2017**.\nAnd then, you can start your compiling process.\n\nInstallation\n------------\n\nPackage not found in win-32 channel.\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: bat\n\n    Solving environment: failed","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/windows.rst","loc":{"lines":{"from":70,"to":107}}}}],["1254",{"pageContent":"Installation\n------------\n\nPackage not found in win-32 channel.\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: bat\n\n    Solving environment: failed\n\n    PackagesNotFoundError: The following packages are not available from current channels:\n\n    - pytorch\n\n    Current channels:\n    - https://conda.anaconda.org/pytorch/win-32\n    - https://conda.anaconda.org/pytorch/noarch\n    - https://repo.continuum.io/pkgs/main/win-32\n    - https://repo.continuum.io/pkgs/main/noarch\n    - https://repo.continuum.io/pkgs/free/win-32\n    - https://repo.continuum.io/pkgs/free/noarch\n    - https://repo.continuum.io/pkgs/r/win-32\n    - https://repo.continuum.io/pkgs/r/noarch\n    - https://repo.continuum.io/pkgs/pro/win-32\n    - https://repo.continuum.io/pkgs/pro/noarch\n    - https://repo.continuum.io/pkgs/msys2/win-32\n    - https://repo.continuum.io/pkgs/msys2/noarch\n\nPyTorch doesn't work on 32-bit system. Please use Windows and\nPython 64-bit version.\n\n\nImport error\n^^^^^^^^^^^^\n\n.. code-block:: python","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/windows.rst","loc":{"lines":{"from":107,"to":142}}}}],["1255",{"pageContent":"PyTorch doesn't work on 32-bit system. Please use Windows and\nPython 64-bit version.\n\n\nImport error\n^^^^^^^^^^^^\n\n.. code-block:: python\n\n    from torch._C import *\n\n    ImportError: DLL load failed: The specified module could not be found.\n\n\nThe problem is caused by the missing of the essential files. Actually,\nwe include almost all the essential files that PyTorch need for the conda\npackage except VC2017 redistributable and some mkl libraries.\nYou can resolve this by typing the following command.\n\n.. code-block:: bat\n\n    conda install -c peterjc123 vc vs2017_runtime\n    conda install mkl_fft intel_openmp numpy mkl","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/windows.rst","loc":{"lines":{"from":142,"to":164}}}}],["1256",{"pageContent":".. code-block:: bat\n\n    conda install -c peterjc123 vc vs2017_runtime\n    conda install mkl_fft intel_openmp numpy mkl\n\nAs for the wheels package, since we didn't pack some libraries and VS2017\nredistributable files in, please make sure you install them manually.\nThe `VS 2017 redistributable installer\n<https://aka.ms/vs/15/release/VC_redist.x64.exe>`_ can be downloaded.\nAnd you should also pay attention to your installation of Numpy. Make sure it\nuses MKL instead of OpenBLAS. You may type in the following command.\n\n.. code-block:: bat\n\n    pip install numpy mkl intel-openmp mkl_fft\n\nAnother possible cause may be you are using GPU version without NVIDIA\ngraphics cards. Please replace your GPU package with the CPU one.\n\n.. code-block:: python\n\n    from torch._C import *\n\n    ImportError: DLL load failed: The operating system cannot run %1.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/windows.rst","loc":{"lines":{"from":164,"to":187}}}}],["1257",{"pageContent":".. code-block:: python\n\n    from torch._C import *\n\n    ImportError: DLL load failed: The operating system cannot run %1.\n\n\nThis is actually an upstream issue of Anaconda. When you initialize your\nenvironment with conda-forge channel, this issue will emerge. You may fix\nthe intel-openmp libraries through this command.\n\n.. code-block:: bat\n\n    conda install -c defaults intel-openmp -f\n\n\nUsage (multiprocessing)\n-------------------------------------------------------\n\nMultiprocessing error without if-clause protection\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: python\n\n    RuntimeError:\n           An attempt has been made to start a new process before the\n           current process has finished its bootstrapping phase.\n\n       This probably means that you are not using fork to start your\n       child processes and you have forgotten to use the proper idiom\n       in the main module:\n\n           if __name__ == '__main__':\n               freeze_support()\n               ...","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/windows.rst","loc":{"lines":{"from":187,"to":221}}}}],["1258",{"pageContent":"if __name__ == '__main__':\n               freeze_support()\n               ...\n\n       The \"freeze_support()\" line can be omitted if the program\n       is not going to be frozen to produce an executable.\n\nThe implementation of ``multiprocessing`` is different on Windows, which\nuses ``spawn`` instead of ``fork``. So we have to wrap the code with an\nif-clause to protect the code from executing multiple times. Refactor\nyour code into the following structure.\n\n.. code-block:: python\n\n    import torch\n\n    def main()\n        for i, data in enumerate(dataloader):\n            # do something here\n\n    if __name__ == '__main__':\n        main()\n\n\nMultiprocessing error \"Broken pipe\"\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: python\n\n    ForkingPickler(file, protocol).dump(obj)\n\n    BrokenPipeError: [Errno 32] Broken pipe","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/windows.rst","loc":{"lines":{"from":221,"to":252}}}}],["1259",{"pageContent":"Multiprocessing error \"Broken pipe\"\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: python\n\n    ForkingPickler(file, protocol).dump(obj)\n\n    BrokenPipeError: [Errno 32] Broken pipe\n\nThis issue happens when the child process ends before the parent process\nfinishes sending data. There may be something wrong with your code. You\ncan debug your code by reducing the ``num_worker`` of\n:class:`~torch.utils.data.DataLoader` to zero and see if the issue persists.\n\nMultiprocessing error \"driver shut down\"\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n::\n\n    Couldn’t open shared file mapping: <torch_14808_1591070686>, error code: <1455> at torch\\lib\\TH\\THAllocator.c:154\n\n    [windows] driver shut down","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/windows.rst","loc":{"lines":{"from":252,"to":273}}}}],["1260",{"pageContent":"::\n\n    Couldn’t open shared file mapping: <torch_14808_1591070686>, error code: <1455> at torch\\lib\\TH\\THAllocator.c:154\n\n    [windows] driver shut down\n\nPlease update your graphics driver. If this persists, this may be that your\ngraphics card is too old or the calculation is too heavy for your card. Please\nupdate the TDR settings according to this `post\n<https://www.pugetsystems.com/labs/hpc/Working-around-TDR-in-Windows-for-a-better-GPU-computing-experience-777/>`_.\n\nCUDA IPC operations\n^^^^^^^^^^^^^^^^^^^\n\n.. code-block:: python\n\n   THCudaCheck FAIL file=torch\\csrc\\generic\\StorageSharing.cpp line=252 error=63 : OS call failed or operation not supported on this OS\n\nThey are not supported on Windows. Something like doing multiprocessing on CUDA\ntensors cannot succeed, there are two alternatives for this.\n\n1. Don't use ``multiprocessing``. Set the ``num_worker`` of\n:class:`~torch.utils.data.DataLoader` to zero.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/windows.rst","loc":{"lines":{"from":273,"to":295}}}}],["1261",{"pageContent":"1. Don't use ``multiprocessing``. Set the ``num_worker`` of\n:class:`~torch.utils.data.DataLoader` to zero.\n\n2. Share CPU tensors instead. Make sure your custom\n:class:`~torch.utils.data.DataSet` returns CPU tensors.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/notes/windows.rst","loc":{"lines":{"from":295,"to":299}}}}],["1262",{"pageContent":"torch.onnx\n==========\n\n.. contents:: :local:\n\n.. automodule:: torch.onnx\n\n`Open Neural Network eXchange (ONNX) <https://onnx.ai/>`_ is an open standard\nformat for representing machine learning models. The torch.onnx module can export\nPyTorch models to ONNX. The model can then be consumed by any of the many\n`runtimes that support ONNX <https://onnx.ai/supported-tools.html#deployModel>`_.\n\nExample: AlexNet from PyTorch to ONNX\n-------------------------------------\n\nHere is a simple script which exports a pretrained AlexNet to an ONNX file named ``alexnet.onnx``.\nThe call to ``torch.onnx.export`` runs the model once to trace its execution and then exports the\ntraced model to the specified file::\n\n    import torch\n    import torchvision\n\n    dummy_input = torch.randn(10, 3, 224, 224, device=\"cuda\")\n    model = torchvision.models.alexnet(pretrained=True).cuda()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":1,"to":24}}}}],["1263",{"pageContent":"import torch\n    import torchvision\n\n    dummy_input = torch.randn(10, 3, 224, 224, device=\"cuda\")\n    model = torchvision.models.alexnet(pretrained=True).cuda()\n\n    # Providing input and output names sets the display names for values\n    # within the model's graph. Setting these does not change the semantics\n    # of the graph; it is only for readability.\n    #\n    # The inputs to the network consist of the flat list of inputs (i.e.\n    # the values you would pass to the forward() method) followed by the\n    # flat list of parameters. You can partially specify names, i.e. provide\n    # a list here shorter than the number of inputs to the model, and we will\n    # only set that subset of names, starting from the beginning.\n    input_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\n    output_names = [ \"output1\" ]\n\n    torch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":24,"to":42}}}}],["1264",{"pageContent":"torch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)\n\nThe resulting ``alexnet.onnx`` file contains a binary `protocol buffer <https://developers.google.com/protocol-buffers/>`_\nwhich contains both the network structure and parameters of the model you exported\n(in this case, AlexNet).  The argument ``verbose=True`` causes the\nexporter to print out a human-readable representation of the model::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":42,"to":47}}}}],["1265",{"pageContent":"# These are the inputs and parameters to the network, which have taken on\n    # the names we specified earlier.\n    graph(%actual_input_1 : Float(10, 3, 224, 224)\n          %learned_0 : Float(64, 3, 11, 11)\n          %learned_1 : Float(64)\n          %learned_2 : Float(192, 64, 5, 5)\n          %learned_3 : Float(192)\n          # ---- omitted for brevity ----\n          %learned_14 : Float(1000, 4096)\n          %learned_15 : Float(1000)) {\n      # Every statement consists of some output tensors (and their types),\n      # the operator to be run (with its attributes, e.g., kernels, strides,\n      # etc.), its input tensors (%actual_input_1, %learned_0, %learned_1)\n      %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]\n      %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":49,"to":63}}}}],["1266",{"pageContent":"%18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n      %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n      # ---- omitted for brevity ----\n      %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n      # Dynamic means that the shape is not known. This may be because of a\n      # limitation of our implementation (which we would like to fix in a\n      # future release) or shapes which are truly dynamic.\n      %30 : Dynamic = onnx::Shape(%29), scope: AlexNet\n      %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet\n      %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\n      %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\n      # ---- omitted for brevity ----","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":63,"to":74}}}}],["1267",{"pageContent":"%32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\n      %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\n      # ---- omitted for brevity ----\n      %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]\n      return (%output1);\n    }","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":74,"to":79}}}}],["1268",{"pageContent":"You can also verify the output using the `ONNX <https://github.com/onnx/onnx/>`_ library,\nwhich you can install using ``pip``::\n\n    pip install onnx\n\nThen, you can run::\n\n    import onnx\n\n    # Load the ONNX model\n    model = onnx.load(\"alexnet.onnx\")\n\n    # Check that the model is well formed\n    onnx.checker.check_model(model)\n\n    # Print a human readable representation of the graph\n    print(onnx.helper.printable_graph(model.graph))\n\nYou can also run the exported model with one of the many\n`runtimes that support ONNX <https://onnx.ai/supported-tools.html#deployModel>`_.\nFor example after installing `ONNX Runtime <https://www.onnxruntime.ai>`_, you can\nload and run the model::\n\n    import onnxruntime as ort\n\n    ort_session = ort.InferenceSession(\"alexnet.onnx\")\n\n    outputs = ort_session.run(\n        None,\n        {\"actual_input_1\": np.random.randn(10, 3, 224, 224).astype(np.float32)},\n    )\n    print(outputs[0])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":81,"to":112}}}}],["1269",{"pageContent":"outputs = ort_session.run(\n        None,\n        {\"actual_input_1\": np.random.randn(10, 3, 224, 224).astype(np.float32)},\n    )\n    print(outputs[0])\n\nHere is a more involved `tutorial on exporting a model and running it with ONNX Runtime <https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html>`_.\n\n.. _tracing-vs-scripting:\n\nTracing vs Scripting\n--------------------\n\nInternally, :func:`torch.onnx.export()` requires a :class:`torch.jit.ScriptModule` rather than\na :class:`torch.nn.Module`. If the passed-in model is not already a ``ScriptModule``,\n``export()`` will use *tracing* to convert it to one:\n\n.. TODO(justinchuby): Add a word on recommending tracing over scripting for most use cases.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":112,"to":129}}}}],["1270",{"pageContent":".. TODO(justinchuby): Add a word on recommending tracing over scripting for most use cases.\n\n* **Tracing**: If ``torch.onnx.export()`` is called with a Module that is not already a\n  ``ScriptModule``, it first does the equivalent of :func:`torch.jit.trace`, which executes the model\n  once with the given ``args`` and records all operations that happen during that execution. This\n  means that if your model is dynamic, e.g., changes behavior depending on input data, the exported\n  model will *not* capture this dynamic behavior.\n  We recommend examining the exported model and making sure the operators look\n  reasonable. Tracing will unroll loops and if statements, exporting a static graph that is exactly\n  the same as the traced run. If you want to export your model with dynamic control flow, you will\n  need to use *scripting*.\n\n* **Scripting**: Compiling a model via scripting preserves dynamic control flow and is valid for inputs\n  of different sizes. To use scripting:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":129,"to":142}}}}],["1271",{"pageContent":"* **Scripting**: Compiling a model via scripting preserves dynamic control flow and is valid for inputs\n  of different sizes. To use scripting:\n\n  * Use :func:`torch.jit.script` to produce a ``ScriptModule``.\n  * Call ``torch.onnx.export()`` with the ``ScriptModule`` as the model. The ``args`` are still required,\n    but they will be used internally only to produce example outputs, so that the types and shapes of the\n    outputs can be captured. No tracing will be performed.\n\nSee `Introduction to TorchScript <https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html>`_\nand `TorchScript <jit.html>`_ for more details, including how to compose tracing and scripting to suit the\nparticular requirements of different models.\n\n\nAvoiding Pitfalls\n-----------------\n\nAvoid NumPy and built-in Python types\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":142,"to":159}}}}],["1272",{"pageContent":"Avoiding Pitfalls\n-----------------\n\nAvoid NumPy and built-in Python types\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPyTorch models can be written using NumPy or Python types and functions, but\nduring :ref:`tracing<tracing-vs-scripting>`, any variables of NumPy or Python\ntypes (rather than torch.Tensor) are converted to constants, which will produce\nthe wrong result if those values should change depending on the inputs.\n\nFor example, rather than using numpy functions on numpy.ndarrays: ::\n\n    # Bad! Will be replaced with constants during tracing.\n    x, y = np.random.rand(1, 2), np.random.rand(1, 2)\n    np.concatenate((x, y), axis=1)\n\nUse torch operators on torch.Tensors: ::\n\n    # Good! Tensor operations will be captured during tracing.\n    x, y = torch.randn(1, 2), torch.randn(1, 2)\n    torch.cat((x, y), dim=1)\n\n\nAnd rather than use :func:`torch.Tensor.item` (which converts a Tensor to a Python\nbuilt-in number): ::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":159,"to":184}}}}],["1273",{"pageContent":"And rather than use :func:`torch.Tensor.item` (which converts a Tensor to a Python\nbuilt-in number): ::\n\n    # Bad! y.item() will be replaced with a constant during tracing.\n    def forward(self, x, y):\n        return x.reshape(y.item(), -1)\n\nUse torch's support for implicit casting of single-element tensors: ::\n\n    # Good! y will be preserved as a variable during tracing.\n    def forward(self, x, y):\n        return x.reshape(y, -1)\n\nAvoid Tensor.data\n^^^^^^^^^^^^^^^^^\n\nUsing the Tensor.data field can produce an incorrect trace and therefore an incorrect ONNX graph.\nUse :func:`torch.Tensor.detach` instead. (Work is ongoing to\n`remove Tensor.data entirely <https://github.com/pytorch/pytorch/issues/30987>`_).\n\nAvoid in-place operations when using tensor.shape in tracing mode\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":184,"to":205}}}}],["1274",{"pageContent":"Avoid in-place operations when using tensor.shape in tracing mode\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn tracing mode, shapes obtained from ``tensor.shape`` are traced as tensors,\nand share the same memory. This might cause a mismatch the final output values.\nAs a workaround, avoid the use of inplace operations in these scenarios.\nFor example, in the model::\n\n    class Model(torch.nn.Module):\n      def forward(self, states):\n          batch_size, seq_length = states.shape[:2]\n          real_seq_length = seq_length\n          real_seq_length += 2\n          return real_seq_length + seq_length\n\n``real_seq_length`` and ``seq_length`` share the same memory in tracing mode.\nThis could be avoided by rewriting the inplace operation::\n\n    real_seq_length = real_seq_length + 2\n\nLimitations\n-----------\n\nTypes\n^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":205,"to":229}}}}],["1275",{"pageContent":"real_seq_length = real_seq_length + 2\n\nLimitations\n-----------\n\nTypes\n^^^^^\n\n* Only :class:`torch.Tensors`, numeric types that can be trivially converted to torch.Tensors (e.g. float, int),\n  and tuples and lists of those types are supported as model inputs or outputs. Dict and str inputs and\n  outputs are accepted in :ref:`tracing<tracing-vs-scripting>` mode, but:\n\n  * Any computation that depends on the value of a dict or a str input **will be replaced with the\n    constant value** seen during the one traced execution.\n  * Any output that is a dict will be silently replaced with a **flattened sequence of its values\n    (keys will be removed)**. E.g. ``{\"foo\": 1, \"bar\": 2}`` becomes ``(1, 2)``.\n  * Any output that is a str will be silently removed.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":229,"to":245}}}}],["1276",{"pageContent":"* Certain operations involving tuples and lists are not supported in\n  :ref:`scripting<tracing-vs-scripting>` mode due to limited support in ONNX for nested sequences.\n  In particular appending a tuple to a list is not supported. In tracing mode, the nested sequences\n  will be flattened automatically during the tracing.\n\nDifferences in Operator Implementations\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nDue to differences in implementations of operators, running the exported model on different runtimes\nmay produce different results from each other or from PyTorch. Normally these differences are\nnumerically small, so this should only be a concern if your application is sensitive to these\nsmall differences.\n\n.. _tensor-indexing:\n\nUnsupported Tensor Indexing Patterns\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":247,"to":263}}}}],["1277",{"pageContent":".. _tensor-indexing:\n\nUnsupported Tensor Indexing Patterns\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTensor indexing patterns that cannot be exported are listed below.\nIf you are experiencing issues exporting a model that does not include any of\nthe unsupported patterns below, please double check that you are exporting with\nthe latest ``opset_version``.\n\nReads / Gets\n~~~~~~~~~~~~\n\nWhen indexing into a tensor for reading, the following patterns are not supported: ::\n\n  # Tensor indices that includes negative values.\n  data[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]\n  # Workarounds: use positive index values.\n\nWrites / Sets\n~~~~~~~~~~~~~\n\nWhen indexing into a Tensor for writing, the following patterns are not supported: ::\n\n  # Multiple tensor indices if any has rank >= 2\n  data[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data\n  # Workarounds: use single tensor index with rank >= 2,\n  #              or multiple consecutive tensor indices with rank == 1.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":263,"to":290}}}}],["1278",{"pageContent":"# Multiple tensor indices that are not consecutive\n  data[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data\n  # Workarounds: transpose `data` such that tensor indices are consecutive.\n\n  # Tensor indices that includes negative values.\n  data[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data\n  # Workarounds: use positive index values.\n\n  # Implicit broadcasting required for new_data.\n  data[torch.tensor([[0, 2], [1, 1]]), 1:3] = new_data\n  # Workarounds: expand new_data explicitly.\n  # Example:\n  #   data shape: [3, 4, 5]\n  #   new_data shape: [5]\n  #   expected new_data shape after broadcasting: [2, 2, 2, 5]\n\nAdding support for operators\n----------------------------\n\nWhen exporting a model that includes unsupported operators, you'll see an error message like:\n\n.. code-block:: text\n\n    RuntimeError: ONNX export failed: Couldn't export operator foo\n\nWhen that happens, there are a few things you can do:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":292,"to":317}}}}],["1279",{"pageContent":".. code-block:: text\n\n    RuntimeError: ONNX export failed: Couldn't export operator foo\n\nWhen that happens, there are a few things you can do:\n\n#. Change the model to not use that operator.\n#. Create a symbolic function to convert the operator and register it as a custom symbolic function.\n#. Contribute to PyTorch to add the same symbolic function to :mod:`torch.onnx` itself.\n\nIf you decided to implement a symbolic function (we hope you will contribute it back to PyTorch!), here is how you can get started:\n\nONNX exporter internals\n^^^^^^^^^^^^^^^^^^^^^^^\n\nA \"symbolic function\" is a function that decomposes a PyTorch operator into a\ncomposition of a series of ONNX operators.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":317,"to":333}}}}],["1280",{"pageContent":"ONNX exporter internals\n^^^^^^^^^^^^^^^^^^^^^^^\n\nA \"symbolic function\" is a function that decomposes a PyTorch operator into a\ncomposition of a series of ONNX operators.\n\nDuring export, each node (which contains a PyTorch operator) in the TorchScript\ngraph is visited by the exporter in topological order.\nUpon visiting a node, the exporter looks for a registered symbolic functions for\nthat operator. Symbolic functions are implemented in Python. A symbolic function for\nan op named ``foo`` would look something like::\n\n\n    def foo(\n      g,\n      input_0: torch._C.Value,\n      input_1: torch._C.Value) -> Union[None, torch._C.Value, List[torch._C.Value]]:\n      \"\"\"\n      Adds the ONNX operations representing this PyTorch function by updating the\n      graph g with `g.op()` calls.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":333,"to":352}}}}],["1281",{"pageContent":"Args:\n        g (Graph): graph to write the ONNX representation into.\n        input_0 (Value): value representing the variables which contain\n            the first input for this operator.\n        input_1 (Value): value representing the variables which contain\n            the second input for this operator.\n\n      Returns:\n        A Value or List of Values specifying the ONNX nodes that compute something\n        equivalent to the original PyTorch operator with the given inputs.\n\n        None if it cannot be converted to ONNX.\n      \"\"\"\n      ...\n\nThe ``torch._C`` types are Python wrappers around the types defined in C++ in\n`ir.h <https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/ir/ir.h>`_.\n\nThe process for adding a symbolic function depends on the type of operator.\n\n.. _adding-support-aten:\n\nATen operators\n^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":354,"to":377}}}}],["1282",{"pageContent":"The process for adding a symbolic function depends on the type of operator.\n\n.. _adding-support-aten:\n\nATen operators\n^^^^^^^^^^^^^^\n\n`ATen <https://pytorch.org/cppdocs/#aten>`_ is PyTorch's built-in tensor library.\nIf the operator is an ATen operator (shows up in the TorchScript graph with the prefix\n``aten::``), make sure it is not supported already.\n\nList of supported operators\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nVisit the auto generated :doc:`list of supported TorchScript operators <../onnx_supported_aten_ops>`\nfor details on which operator are supported in each ``opset_version``.\n\nAdding support for an aten or quantized operator\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf the operator is not in the list above:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":377,"to":397}}}}],["1283",{"pageContent":"* Define the symbolic function in ``torch/onnx/symbolic_opset<version>.py``, for example\n  `torch/onnx/symbolic_opset9.py <https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py>`_.\n  Make sure the function has the same name as the ATen function, which may be declared in\n  ``torch/_C/_VariableFunctions.pyi`` or ``torch/nn/functional.pyi`` (these files are generated at\n  build time, so will not appear in your checkout until you build PyTorch).\n* By default, the first arg is the ONNX graph.\n  Other arg names must EXACTLY match the names in the ``.pyi`` file,\n  because dispatch is done with keyword arguments.\n* In the symbolic function, if the operator is in the\n  `ONNX standard operator set <https://github.com/onnx/onnx/blob/master/docs/Operators.md>`_,\n  we only need to create a node to represent the ONNX operator in the graph.\n  If not, we can compose several standard operators that have the\n  equivalent semantics to the ATen operator.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":399,"to":411}}}}],["1284",{"pageContent":"Here is an example of handling missing symbolic function for the ``ELU`` operator.\n\nIf we run the following code::\n\n    print(\n        torch.jit.trace(\n            torch.nn.ELU(), # module\n            torch.ones(1)   # example input\n        ).graph\n    )\n\nWe see something like::\n\n  graph(%self : __torch__.torch.nn.modules.activation.___torch_mangle_0.ELU,\n        %input : Float(1, strides=[1], requires_grad=0, device=cpu)):\n    %4 : float = prim::Constant[value=1.]()\n    %5 : int = prim::Constant[value=1]()\n    %6 : int = prim::Constant[value=1]()\n    %7 : Float(1, strides=[1], requires_grad=0, device=cpu) = aten::elu(%input, %4, %5, %6)\n    return (%7)\n\nSince we see ``aten::elu`` in the graph, we know this is an ATen operator.\n\nWe check the `ONNX operator list <https://github.com/onnx/onnx/blob/master/docs/Operators.md>`_,\nand confirm that ``Elu`` is standardized in ONNX.\n\nWe find a signature for ``elu`` in ``torch/nn/functional.pyi``::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":413,"to":439}}}}],["1285",{"pageContent":"We find a signature for ``elu`` in ``torch/nn/functional.pyi``::\n\n    def elu(input: Tensor, alpha: float = ..., inplace: bool = ...) -> Tensor: ...\n\nWe add the following lines to ``symbolic_opset9.py``::\n\n    def elu(g, input: torch.Value, alpha: torch.Value, inplace: bool = False):\n        return g.op(\"Elu\", input, alpha_f=alpha)\n\nNow PyTorch is able to export models containing the ``aten::elu`` operator!\n\nSee the ``torch/onnx/symbolic_opset*.py`` files for more examples.\n\n\ntorch.autograd.Functions\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf the operator is a sub-class of :class:`torch.autograd.Function`, there are three ways\nto export it.\n\nStatic Symbolic Method\n~~~~~~~~~~~~~~~~~~~~~~\n\nYou can add a static method named ``symbolic`` to your function class. It should return\nONNX operators that represent the function's behavior in ONNX. For example::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":439,"to":463}}}}],["1286",{"pageContent":"You can add a static method named ``symbolic`` to your function class. It should return\nONNX operators that represent the function's behavior in ONNX. For example::\n\n    class MyRelu(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, input: torch.Tensor) -> torch.Tensor:\n            ctx.save_for_backward(input)\n            return input.clamp(min=0)\n\n        @staticmethod\n        def symbolic(g: torch.Graph, input: torch.Value) -> torch.Value:\n            return g.op(\"Clip\", input, g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.float)))\n\n.. FIXME(justinchuby): PythonOps are too complicated and the example below\n..  uses private methods we do not expose. We are looking to\n..  improve the experience. Since SymbolicContext is deprecated, we think\n..  defining a symbolic staticmethod is a better way to go for now.\n\n.. PythonOp Symbolic\n.. ~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":463,"to":482}}}}],["1287",{"pageContent":".. PythonOp Symbolic\n.. ~~~~~~~~~~~~~~~~~\n\n.. Alternatively, you can register a custom symbolic function.\n.. This gives the symbolic function access to more info through the\n.. ``torch.onnx.SymbolicContext`` object, which gets passed in as the first\n.. argument (before the ``Graph`` object).\n\n.. All autograd ``Function``\\ s appear in the TorchScript graph as ``prim::PythonOp`` nodes.\n.. In order to differentiate between different ``Function`` subclasses, the\n.. symbolic function should use the ``name`` kwarg which gets set to the name of the class.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":482,"to":492}}}}],["1288",{"pageContent":".. Custom symbolic functions should add type and shape information by calling ``setType(...)``\n.. on Value objects before returning them (implemented in C++ by\n.. . ``torch::jit::Value::setType``). This is not required, but it can help the exporter's\n.. shape and type inference for down-stream nodes. For a non-trivial example of ``setType``, see\n.. ``test_aten_embedding_2`` in\n.. `test_operators.py <https://github.com/pytorch/pytorch/blob/master/test/onnx/test_operators.py>`_.\n\n.. The example below shows how you can access ``requires_grad`` via the ``Node`` object:\n\n..     class MyClip(torch.autograd.Function):\n..         @staticmethod\n..         def forward(ctx, input, min):\n..             ctx.save_for_backward(input)\n..             return input.clamp(min=min)\n\n..     class MyRelu(torch.autograd.Function):\n..         @staticmethod\n..         def forward(ctx, input):\n..             ctx.save_for_backward(input)\n..             return input.clamp(min=0)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":494,"to":513}}}}],["1289",{"pageContent":"..     class MyRelu(torch.autograd.Function):\n..         @staticmethod\n..         def forward(ctx, input):\n..             ctx.save_for_backward(input)\n..             return input.clamp(min=0)\n\n..     def symbolic_python_op(g: \"GraphContext\", *args, **kwargs):\n..         n = ctx.cur_node\n..         print(\"original node: \", n)\n..         for i, out in enumerate(n.outputs()):\n..             print(\"original output {}: {}, requires grad: {}\".format(i, out, out.requiresGrad()))\n..         import torch.onnx.symbolic_helper as sym_helper\n..         for i, arg in enumerate(args):\n..             requires_grad = arg.requiresGrad() if sym_helper._is_value(arg) else False\n..             print(\"arg {}: {}, requires grad: {}\".format(i, arg, requires_grad))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":513,"to":527}}}}],["1290",{"pageContent":"..         name = kwargs[\"name\"]\n..         ret = None\n..         if name == \"MyClip\":\n..             ret = g.op(\"Clip\", args[0], args[1])\n..         elif name == \"MyRelu\":\n..             ret = g.op(\"Relu\", args[0])\n..         else:\n..             # Logs a warning and returns None\n..             return _unimplemented(\"prim::PythonOp\", \"unknown node kind: \" + name)\n..         # Copy type and shape from original node.\n..         ret.setType(n.type())\n..         return ret\n\n..     from torch.onnx import register_custom_op_symbolic\n.. .     register_custom_op_symbolic(\"prim::PythonOp\", symbolic_python_op, 1)\n\nInline Autograd Function\n~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":529,"to":546}}}}],["1291",{"pageContent":"..     from torch.onnx import register_custom_op_symbolic\n.. .     register_custom_op_symbolic(\"prim::PythonOp\", symbolic_python_op, 1)\n\nInline Autograd Function\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn cases where a static symbolic method is not provided for its subsequent :class:`torch.autograd.Function` or\nwhere a function to register ``prim::PythonOp`` as custom symbolic functions is not provided,\n:func:`torch.onnx.export` tries to inline the graph that corresponds to that :class:`torch.autograd.Function` such that\nthis function is broken down into individual operators that were used within the function.\nThe export should be successful as long as these individual operators are supported. For example::\n\n    class MyLogExp(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, input: torch.Tensor) -> torch.Tensor:\n            ctx.save_for_backward(input)\n            h = input.exp()\n            return h.log().log()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":546,"to":563}}}}],["1292",{"pageContent":"There is no static symbolic method present for this model, yet it is exported as follows::\n\n    graph(%input : Float(1, strides=[1], requires_grad=0, device=cpu)):\n        %1 : float = onnx::Exp[](%input)\n        %2 : float = onnx::Log[](%1)\n        %3 : float = onnx::Log[](%2)\n        return (%3)\n\nIf you need to avoid inlining of :class:`torch.autograd.Function`, you should export models with\n``operator_export_type`` set to ``ONNX_FALLTHROUGH`` or ``ONNX_ATEN_FALLBACK``.\n\nCustom operators\n^^^^^^^^^^^^^^^^\n\nYou can export your model with custom operators that includes a combination of many standard ONNX ops,\nor are driven by self-defined C++ backend.\n\nONNX-script functions\n~~~~~~~~~~~~~~~~~~~~~\n\nIf an operator is not a standard ONNX op, but can be composed of multiple existing ONNX ops, you can utilize\n`ONNX-script <https://github.com/microsoft/onnx-script>`_ to create an external ONNX function to support the operator.\nYou can export it by following this example::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":565,"to":587}}}}],["1293",{"pageContent":"import onnxscript\n    # There are three opset version needed to be aligned\n    # This is (1) the opset version in ONNX function\n    from onnxscript.onnx_opset import opset15 as op\n    opset_version = 15\n\n    x = torch.randn(1, 2, 3, 4, requires_grad=True)\n    model = torch.nn.SELU()\n\n    custom_opset = onnxscript.values.Opset(domain=\"onnx-script\", version=1)\n\n    @onnxscript.script(custom_opset)\n    def Selu(X):\n        alpha = 1.67326  # auto wrapped as Constants\n        gamma = 1.0507\n        alphaX = op.CastLike(alpha, X)\n        gammaX = op.CastLike(gamma, X)\n        neg = gammaX * (alphaX * op.Exp(X) - alphaX)\n        pos = gammaX * X\n        zero = op.CastLike(0, X)\n        return op.Where(X <= zero, neg, pos)\n\n    # setType API provides shape/type to ONNX shape/type inference\n    def custom_selu(g: jit_utils.GraphContext, X):\n        return g.onnxscript_op(Selu, X).setType(X.type())","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":589,"to":613}}}}],["1294",{"pageContent":"# setType API provides shape/type to ONNX shape/type inference\n    def custom_selu(g: jit_utils.GraphContext, X):\n        return g.onnxscript_op(Selu, X).setType(X.type())\n\n    # Register custom symbolic function\n    # There are three opset version needed to be aligned\n    # This is (2) the opset version in registry\n    torch.onnx.register_custom_op_symbolic(\n        symbolic_name=\"aten::selu\",\n        symbolic_fn=custom_selu,\n        opset_version=opset_version,\n    )\n\n    # There are three opset version needed to be aligned\n    # This is (2) the opset version in exporter\n    torch.onnx.export(\n        model,\n        x,\n        \"model.onnx\",\n        opset_version=opset_version,\n        # only needed if you want to specify an opset version > 1.\n        custom_opsets={\"onnx-script\": 2}\n    )","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":613,"to":635}}}}],["1295",{"pageContent":"The example above exports it as a custom operator in the \"onnx-script\" opset.\nWhen exporting a custom operator, you can specify the custom domain version using the\n``custom_opsets`` dictionary at export. If not specified, the custom opset version defaults to 1.\n\nNOTE: Be careful to align the opset version mentioned in the above example, and make sure they are consumed in exporter step.\nThe example usage of how to write a onnx-script function is a beta version in terms of the active development on onnx-script.\nPlease follow the latest `ONNX-script <https://github.com/microsoft/onnx-script>`_\n\nC++ Operators\n~~~~~~~~~~~~~\n\nIf a model uses a custom operator implemented in C++ as described in\n`Extending TorchScript with Custom C++ Operators <https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>`_,\nyou can export it by following this example::\n\n    from torch.onnx import symbolic_helper","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":637,"to":652}}}}],["1296",{"pageContent":"from torch.onnx import symbolic_helper\n\n\n    # Define custom symbolic function\n    @symbolic_helper.parse_args(\"v\", \"v\", \"f\", \"i\")\n    def symbolic_foo_forward(g, input1, input2, attr1, attr2):\n        return g.op(\"custom_domain::Foo\", input1, input2, attr1_f=attr1, attr2_i=attr2)\n\n\n    # Register custom symbolic function\n    torch.onnx.register_custom_op_symbolic(\"custom_ops::foo_forward\", symbolic_foo_forward, 9)\n\n\n    class FooModel(torch.nn.Module):\n        def __init__(self, attr1, attr2):\n            super().__init__()\n            self.attr1 = attr1\n            self.attr2 = attr2\n\n        def forward(self, input1, input2):\n            # Calling custom op\n            return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":652,"to":673}}}}],["1297",{"pageContent":"def forward(self, input1, input2):\n            # Calling custom op\n            return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)\n\n\n    model = FooModel(attr1, attr2)\n    torch.onnx.export(\n        model,\n        (example_input1, example_input1),\n        \"model.onnx\",\n        # only needed if you want to specify an opset version > 1.\n        custom_opsets={\"custom_domain\": 2}\n    )\n\nThe example above exports it as a custom operator in the \"custom_domain\" opset.\nWhen exporting a custom operator, you can specify the custom domain version using the\n``custom_opsets`` dictionary at export. If not specified, the custom opset version defaults to 1.\n\nThe runtime that consumes the model needs to support the custom op. See\n`Caffe2 custom ops <https://caffe2.ai/docs/custom-operators.html>`_,\n`ONNX Runtime custom ops <https://onnxruntime.ai/docs/reference/operators/add-custom-op.html>`_,\nor your runtime of choice's documentation.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":673,"to":694}}}}],["1298",{"pageContent":"Discovering all unconvertible ATen ops at once\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWhen export fails due to an unconvertible ATen op, there may in fact be more\nthan one such op but the error message only mentions the first. To discover\nall of the unconvertible ops in one go you can::\n\n    # prepare model, args, opset_version\n    ...\n\n    torch_script_graph, unconvertible_ops = torch.onnx.utils.unconvertible_ops(\n        model, args, opset_version=opset_version\n    )\n\n    print(set(unconvertible_ops))\n\nThe set is approximated because some ops may be removed during the conversion\nprocess and don't need to be converted. Some other ops may have partial support\nthat will fail conversion with particular inputs, but this should give you a\ngeneral idea of what ops are not supported. Please feel free to open GitHub Issues\nfor op support requests.\n\nFrequently Asked Questions\n--------------------------\nQ: I have exported my LSTM model, but its input size seems to be fixed?","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":697,"to":721}}}}],["1299",{"pageContent":"Frequently Asked Questions\n--------------------------\nQ: I have exported my LSTM model, but its input size seems to be fixed?\n\n  The tracer records the shapes of the example inputs. If the model should accept\n  inputs of dynamic shapes, set ``dynamic_axes`` when calling :func:`torch.onnx.export`.\n\nQ: How to export models containing loops?\n\n  See `Tracing vs Scripting`_.\n\nQ: How to export models with primitive type inputs (e.g. int, float)?\n\n  Support for primitive numeric type inputs was added in PyTorch 1.9.\n  However, the exporter does not support models with str inputs.\n\nQ: Does ONNX support implicit scalar datatype casting?","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":721,"to":737}}}}],["1300",{"pageContent":"Support for primitive numeric type inputs was added in PyTorch 1.9.\n  However, the exporter does not support models with str inputs.\n\nQ: Does ONNX support implicit scalar datatype casting?\n\n  The ONNX standard does not, but the exporter will try to handle that part.\n  Scalars are exported as constant tensors.\n  The exporter will figure out the right data type for scalars. In rare cases when it is unable\n  to do so, you will need to manually specify the datatype with e.g. `dtype=torch.float32`.\n  If you see any errors, please [create a GitHub issue](https://github.com/pytorch/pytorch/issues).\n\nQ: Are lists of Tensors exportable to ONNX?\n\n  Yes, for ``opset_version`` >= 11, since ONNX introduced the Sequence type in opset 11.\n\n\nContributing / developing\n-------------------------\n`Developer docs <https://github.com/pytorch/pytorch/wiki/PyTorch-ONNX-exporter>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":737,"to":755}}}}],["1301",{"pageContent":"Contributing / developing\n-------------------------\n`Developer docs <https://github.com/pytorch/pytorch/wiki/PyTorch-ONNX-exporter>`_.\n\nFunctions\n---------\n.. autofunction:: export\n.. autofunction:: export_to_pretty_string\n.. autofunction:: register_custom_op_symbolic\n.. autofunction:: unregister_custom_op_symbolic\n.. autofunction:: select_model_mode_for_export\n.. autofunction:: is_in_onnx_export\n.. autofunction:: enable_log\n.. autofunction:: disable_log\n.. autofunction:: torch.onnx.verification.find_mismatch\n\nClasses\n-------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    JitScalarType\n    torch.onnx.verification.GraphInfo\n    torch.onnx.verification.VerificationOptions\n\nPreview: torch.onnx TorchDynamo Exporter\n----------------------------------------\n\n.. warning::\n  The ONNX exporter for TorchDynamo is under active development and is\n  subject to rapid change.\n\n.. autofunction:: torch.onnx.dynamo_export","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":755,"to":790}}}}],["1302",{"pageContent":".. warning::\n  The ONNX exporter for TorchDynamo is under active development and is\n  subject to rapid change.\n\n.. autofunction:: torch.onnx.dynamo_export\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    torch.onnx.ExportOptions\n    torch.onnx.ExportOutput\n    torch.onnx.ExportOutputSerializer","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx.rst","loc":{"lines":{"from":790,"to":803}}}}],["1303",{"pageContent":"torch.onnx diagnostics\n======================\n\n.. contents:: :local:\n.. automodule:: torch.onnx._internal.diagnostics\n.. currentmodule:: torch.onnx._internal.diagnostics\n\nOverview\n--------\n\nNOTE: This feature is underdevelopment and is subject to change.\n\nThe goal is to improve the diagnostics to help users debug and improve their model export to ONNX.\n\n- The diagnostics are emitted in machine parsable `Static Analysis Results Interchange Format (SARIF) <https://docs.oasis-open.org/sarif/sarif/v2.1.0/sarif-v2.1.0.html>`__.\n- A new clearer, structured way to add new and keep track of diagnostic rules.\n- Serve as foundation for more future improvements consuming the diagnostics.\n\n\nDiagnostic Rules\n----------------\n\n.. toctree::\n    :glob:\n\n    generated/onnx_diagnostics_rules/*\n\nAPI Reference\n-------------\n\n.. autoclass:: torch.onnx._internal.diagnostics.ExportDiagnostic\n    :members:\n\n.. autoclass:: torch.onnx._internal.diagnostics.infra.DiagnosticEngine\n    :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx_diagnostics.rst","loc":{"lines":{"from":1,"to":35}}}}],["1304",{"pageContent":":orphan:\n\nONNX supported TorchScript operators\n====================================\n\n.. This file is automatically generated during the documentation build\n.. by cross referencing ONNX operator symbolics with TorchScript operators via\n.. ``docs/source/scripts/build_onnx_supported_aten_op_csv_table.py``.\n.. Do not modify directly and instead `rebuild the docs <https://github.com/pytorch/pytorch#building-the-documentation>`_.\n\nThis page lists the TorchScript operators that are supported/unsupported by ONNX export.\n\nSupported operators\n-------------------\n\n.. csv-table:: ONNX support for TorchScript operators\n   :file: ../build/onnx/auto_gen_supported_op_list.csv\n   :widths: 70, 30\n   :header-rows: 1\n\n\nUnsupported operators\n---------------------\n\nOperators that are not yet supported\n\n.. csv-table:: Unsupported operators\n   :file: ../build/onnx/auto_gen_unsupported_op_list.csv\n   :widths: 70, 30\n   :header-rows: 1","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/onnx_supported_aten_ops.rst","loc":{"lines":{"from":1,"to":30}}}}],["1305",{"pageContent":"torch.optim\n===================================\n\n.. automodule:: torch.optim\n\nHow to use an optimizer\n-----------------------\n\nTo use :mod:`torch.optim` you have to construct an optimizer object that will hold\nthe current state and will update the parameters based on the computed gradients.\n\nConstructing it\n^^^^^^^^^^^^^^^\n\nTo construct an :class:`Optimizer` you have to give it an iterable containing the\nparameters (all should be :class:`~torch.autograd.Variable` s) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc.\n\nExample::\n\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    optimizer = optim.Adam([var1, var2], lr=0.0001)\n\nPer-parameter options\n^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":1,"to":25}}}}],["1306",{"pageContent":"Example::\n\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    optimizer = optim.Adam([var1, var2], lr=0.0001)\n\nPer-parameter options\n^^^^^^^^^^^^^^^^^^^^^\n\n:class:`Optimizer` s also support specifying per-parameter options. To do this, instead\nof passing an iterable of :class:`~torch.autograd.Variable` s, pass in an iterable of\n:class:`dict` s. Each of them will define a separate parameter group, and should contain\na ``params`` key, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group.\n\n.. note::\n\n    You can still pass options as keyword arguments. They will be used as\n    defaults, in the groups that didn't override them. This is useful when you\n    only want to vary a single option, while keeping all others consistent\n    between parameter groups.\n\n\nFor example, this is very useful when one wants to specify per-layer learning rates::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":25,"to":48}}}}],["1307",{"pageContent":"For example, this is very useful when one wants to specify per-layer learning rates::\n\n    optim.SGD([\n                    {'params': model.base.parameters()},\n                    {'params': model.classifier.parameters(), 'lr': 1e-3}\n                ], lr=1e-2, momentum=0.9)\n\nThis means that ``model.base``'s parameters will use the default learning rate of ``1e-2``,\n``model.classifier``'s parameters will use a learning rate of ``1e-3``, and a momentum of\n``0.9`` will be used for all parameters.\n\nTaking an optimization step\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAll optimizers implement a :func:`~Optimizer.step` method, that updates the\nparameters. It can be used in two ways:\n\n``optimizer.step()``\n~~~~~~~~~~~~~~~~~~~~\n\nThis is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.\n:func:`~torch.autograd.Variable.backward`.\n\nExample::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":48,"to":72}}}}],["1308",{"pageContent":"This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.\n:func:`~torch.autograd.Variable.backward`.\n\nExample::\n\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n\n``optimizer.step(closure)``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSome optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it.\n\nExample::\n\n    for input, target in dataset:\n        def closure():\n            optimizer.zero_grad()\n            output = model(input)\n            loss = loss_fn(output, target)\n            loss.backward()\n            return loss\n        optimizer.step(closure)\n\n.. _optimizer-algorithms:\n\nBase class\n----------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":72,"to":107}}}}],["1309",{"pageContent":".. _optimizer-algorithms:\n\nBase class\n----------\n\n.. autoclass:: Optimizer\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Optimizer.add_param_group\n    Optimizer.load_state_dict\n    Optimizer.state_dict\n    Optimizer.step\n    Optimizer.zero_grad\n\nAlgorithms\n----------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Adadelta\n    Adagrad\n    Adam\n    AdamW\n    SparseAdam\n    Adamax\n    ASGD\n    LBFGS\n    NAdam\n    RAdam\n    RMSprop\n    Rprop\n    SGD\n\nMany of our algorithms have various implementations optimized for performance,\nreadability and/or generality, so we attempt to default to the generally fastest\nimplementation for the current device if no particular implementation has been\nspecified by the user.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":107,"to":148}}}}],["1310",{"pageContent":"We have 3 major categories of implementations: for-loop, foreach (multi-tensor), and\nfused. The most straightforward implementations are for-loops over the parameters with\nbig chunks of computation. For-looping is usually slower than our foreach\nimplementations, which combine parameters into a multi-tensor and run the big chunks\nof computation all at once, thereby saving many sequential kernel calls. A few of our\noptimizers have even faster fused implementations, which fuse the big chunks of\ncomputation into one kernel. We can think of foreach implementations as fusing\nhorizontally and fused implementations as fusing vertically on top of that.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":150,"to":157}}}}],["1311",{"pageContent":"In general, the performance ordering of the 3 implementations is fused > foreach > for-loop.\nSo when applicable, we default to foreach over for-loop. Applicable means the foreach\nimplementation is available, the user has not specified any implementation-specific kwargs\n(e.g., fused, foreach, differentiable), and all tensors are native and on CUDA. Note that\nwhile fused should be even faster than foreach, the implementations are newer and we would\nlike to give them more bake-in time before flipping the switch everywhere. You are welcome\nto try them out though!\n\nBelow is a table showing the available and default implementations of each algorithm:\n\n.. csv-table::\n    :header: \"Algorithm\", \"Default\", \"Has foreach?\", \"Has fused?\"\n    :widths: 25, 25, 25, 25\n    :delim: ;","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":159,"to":172}}}}],["1312",{"pageContent":".. csv-table::\n    :header: \"Algorithm\", \"Default\", \"Has foreach?\", \"Has fused?\"\n    :widths: 25, 25, 25, 25\n    :delim: ;\n\n    :class:`Adadelta`;foreach;yes;no\n    :class:`Adagrad`;foreach;yes;no\n    :class:`Adam`;foreach;yes;yes\n    :class:`AdamW`;foreach;yes;yes\n    :class:`SparseAdam`;for-loop;no;no\n    :class:`Adamax`;foreach;yes;no\n    :class:`ASGD`;foreach;yes;no\n    :class:`LBFGS`;for-loop;no;no\n    :class:`NAdam`;foreach;yes;no\n    :class:`RAdam`;foreach;yes;no\n    :class:`RMSprop`;foreach;yes;no\n    :class:`Rprop`;foreach;yes;no\n    :class:`SGD`;foreach;yes;no\n\nHow to adjust learning rate\n---------------------------\n\n:mod:`torch.optim.lr_scheduler` provides several methods to adjust the learning\nrate based on the number of epochs. :class:`torch.optim.lr_scheduler.ReduceLROnPlateau`\nallows dynamic learning rate reducing based on some validation measurements.\n\nLearning rate scheduling should be applied after optimizer's update; e.g., you\nshould write your code this way:\n\nExample::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":172,"to":201}}}}],["1313",{"pageContent":"Learning rate scheduling should be applied after optimizer's update; e.g., you\nshould write your code this way:\n\nExample::\n\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    scheduler = ExponentialLR(optimizer, gamma=0.9)\n\n    for epoch in range(20):\n        for input, target in dataset:\n            optimizer.zero_grad()\n            output = model(input)\n            loss = loss_fn(output, target)\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n\nMost learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it.\n\nExample::\n\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n    scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":201,"to":226}}}}],["1314",{"pageContent":"optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n    scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n    scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n\n    for epoch in range(20):\n        for input, target in dataset:\n            optimizer.zero_grad()\n            output = model(input)\n            loss = loss_fn(output, target)\n            loss.backward()\n            optimizer.step()\n        scheduler1.step()\n        scheduler2.step()\n\nIn many places in the documentation, we will use the following template to refer to schedulers\nalgorithms.\n\n    >>> scheduler = ...\n    >>> for epoch in range(100):\n    >>>     train(...)\n    >>>     validate(...)\n    >>>     scheduler.step()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":226,"to":247}}}}],["1315",{"pageContent":">>> scheduler = ...\n    >>> for epoch in range(100):\n    >>>     train(...)\n    >>>     validate(...)\n    >>>     scheduler.step()\n\n.. warning::\n  Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\n  the optimizer's update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\n  the learning rate scheduler (calling ``scheduler.step()``) before the optimizer's update\n  (calling ``optimizer.step()``), this will skip the first value of the learning rate schedule.\n  If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\n  if you are calling ``scheduler.step()`` at the wrong time.\n\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":247,"to":264}}}}],["1316",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    lr_scheduler.LambdaLR\n    lr_scheduler.MultiplicativeLR\n    lr_scheduler.StepLR\n    lr_scheduler.MultiStepLR\n    lr_scheduler.ConstantLR\n    lr_scheduler.LinearLR\n    lr_scheduler.ExponentialLR\n    lr_scheduler.PolynomialLR\n    lr_scheduler.CosineAnnealingLR\n    lr_scheduler.ChainedScheduler\n    lr_scheduler.SequentialLR\n    lr_scheduler.ReduceLROnPlateau\n    lr_scheduler.CyclicLR\n    lr_scheduler.OneCycleLR\n    lr_scheduler.CosineAnnealingWarmRestarts\n\nStochastic Weight Averaging\n---------------------------\n\n:mod:`torch.optim.swa_utils` implements Stochastic Weight Averaging (SWA). In particular,\n:class:`torch.optim.swa_utils.AveragedModel` class implements SWA models,\n:class:`torch.optim.swa_utils.SWALR` implements the SWA learning rate scheduler and\n:func:`torch.optim.swa_utils.update_bn` is a utility function used to update SWA batch\nnormalization statistics at the end of training.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":264,"to":291}}}}],["1317",{"pageContent":"SWA has been proposed in `Averaging Weights Leads to Wider Optima and Better Generalization`_.\n\n.. _`Averaging Weights Leads to Wider Optima and Better Generalization`: https://arxiv.org/abs/1803.05407\n\nConstructing averaged models\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n`AveragedModel` class serves to compute the weights of the SWA model. You can create an\naveraged model by running:\n\n>>> swa_model = AveragedModel(model)\n\nHere the model ``model`` can be an arbitrary :class:`torch.nn.Module` object. ``swa_model``\nwill keep track of the running averages of the parameters of the ``model``. To update these\naverages, you can use the :func:`update_parameters` function:\n\n>>> swa_model.update_parameters(model)\n\n\nSWA learning rate schedules\n^^^^^^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":293,"to":313}}}}],["1318",{"pageContent":">>> swa_model.update_parameters(model)\n\n\nSWA learning rate schedules\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nTypically, in SWA the learning rate is set to a high constant value. :class:`SWALR` is a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group:\n\n>>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\n>>>         anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)\n\nYou can also use cosine annealing to a fixed value instead of linear annealing by setting\n``anneal_strategy=\"cos\"``.\n\n\nTaking care of batch normalization\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n:func:`update_bn` is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloader ``loader`` at the end of training:\n\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":313,"to":337}}}}],["1319",{"pageContent":">>> torch.optim.swa_utils.update_bn(loader, swa_model)\n\n:func:`update_bn` applies the ``swa_model`` to every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model.\n\n.. warning::\n    :func:`update_bn` assumes that each batch in the dataloader ``loader`` is either a tensors or a list of\n    tensors where the first element is the tensor that the network ``swa_model`` should be applied to.\n    If your dataloader has a different structure, you can update the batch normalization statistics of the\n    ``swa_model`` by doing a forward pass with the ``swa_model`` on each element of the dataset.\n\n\nCustom averaging strategies\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBy default, :class:`torch.optim.swa_utils.AveragedModel` computes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with the\n``avg_fn`` parameter. In the following example ``ema_model`` computes an exponential moving average.\n\nExample:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":337,"to":356}}}}],["1320",{"pageContent":"Example:\n\n>>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n>>>         0.1 * averaged_model_parameter + 0.9 * model_parameter\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)\n\n\nPutting it all together\n^^^^^^^^^^^^^^^^^^^^^^^\n\nIn the example below, ``swa_model`` is the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":356,"to":368}}}}],["1321",{"pageContent":">>> loader, optimizer, model, loss_fn = ...\n>>> swa_model = torch.optim.swa_utils.AveragedModel(model)\n>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n>>> swa_start = 160\n>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>       if epoch > swa_start:\n>>>           swa_model.update_parameters(model)\n>>>           swa_scheduler.step()\n>>>       else:\n>>>           scheduler.step()\n>>>\n>>> # Update bn statistics for the swa_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n>>> # Use swa_model to make predictions on test data\n>>> preds = swa_model(test_input)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/optim.rst","loc":{"lines":{"from":370,"to":390}}}}],["1322",{"pageContent":".. automodule:: torch.package\n.. py:module:: torch.package.analyze\n\n.. currentmodule:: torch.package\n\ntorch.package\n=============\n``torch.package`` adds support for creating packages containing both artifacts and arbitrary\nPyTorch code. These packages can be saved, shared, used to load and execute models\nat a later date or on a different machine, and can even be deployed to production using\n``torch::deploy``.\n\nThis document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more about ``torch.package`` and how to use it.\n\n\n.. warning::\n\n    This module depends on the ``pickle`` module which is not secure. Only unpackage data you trust.\n\n    It is possible to construct malicious pickle data which will **execute arbitrary code during unpickling**.\n    Never unpackage data that could have come from an untrusted source, or that could have been tampered with.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":1,"to":22}}}}],["1323",{"pageContent":"For more information, review the `documentation <https://docs.python.org/3/library/pickle.html>`_ for the ``pickle`` module.\n\n\n.. contents:: :local:\n    :depth: 2\n\n\nTutorials\n---------\nPackaging your first model\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nA tutorial that guides you through packaging and unpackaging a simple model is available\n`on Colab <https://colab.research.google.com/drive/1lFZkLyViGfXxB-m3jqlyTQuYToo3XLo->`_.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.\n\nHow do I...\n-----------\nSee what is inside a package?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTreat the package like a ZIP archive\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThe container format for a ``torch.package`` is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:\n\n* ``unzip my_package.pt`` will unzip the ``torch.package`` archive to disk, where you can freely inspect its contents.\n\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":24,"to":52}}}}],["1324",{"pageContent":"* ``unzip my_package.pt`` will unzip the ``torch.package`` archive to disk, where you can freely inspect its contents.\n\n\n::\n\n    $ unzip my_package.pt && tree my_package\n    my_package\n    ├── .data\n    │   ├── 94304870911616.storage\n    │   ├── 94304900784016.storage\n    │   ├── extern_modules\n    │   └── version\n    ├── models\n    │   └── model_1.pkl\n    └── torchvision\n        └── models\n            ├── resnet.py\n            └── utils.py\n    ~ cd my_package && cat torchvision/models/resnet.py\n    ...\n\n\n* The Python ``zipfile`` module provides a standard way to read and write ZIP archive contents.\n\n\n::\n\n    from zipfile import ZipFile\n    with ZipFile(\"my_package.pt\") as myzip:\n        file_bytes = myzip.read(\"torchvision/models/resnet.py\")\n        # edit file_bytes in some way\n        myzip.writestr(\"torchvision/models/resnet.py\", new_file_bytes)\n\n\n* vim has the ability to natively read ZIP archives. You can even edit files and :``write`` them back into the archive!\n\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":52,"to":89}}}}],["1325",{"pageContent":"* vim has the ability to natively read ZIP archives. You can even edit files and :``write`` them back into the archive!\n\n\n::\n\n    # add this to your .vimrc to treat `*.pt` files as zip files\n    au BufReadCmd *.pt call zip#Browse(expand(\"<amatch>\"))\n\n    ~ vi my_package.pt\n\n\nUse the ``file_structure()`` API\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n:class:`PackageImporter` provides a ``file_structure()`` method, which will return a printable\nand queryable :class:`Directory` object. The :class:`Directory` object is a simple directory structure that you can use to explore the\ncurrent contents of a ``torch.package``.\n\nThe :class:`Directory` object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-style ``include`` and ``exclude`` filtering arguments.\n\n\n::\n\n    with PackageExporter('my_package.pt') as pe:\n        pe.save_pickle('models', 'model_1.pkl', mod)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":89,"to":113}}}}],["1326",{"pageContent":"::\n\n    with PackageExporter('my_package.pt') as pe:\n        pe.save_pickle('models', 'model_1.pkl', mod)\n\n    importer = PackageImporter('my_package.pt')\n    # can limit printed items with include/exclude args\n    print(importer.file_structure(include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storage\"))\n    print(importer.file_structure()) # will print out all files\n\n\nOutput:\n\n\n::\n\n    # filtered with glob pattern:\n    #    include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storage\"\n    ─── my_package.pt\n        ├── models\n        │   └── model_1.pkl\n        └── torchvision\n            └── models\n                └── utils.py\n\n    # all files\n    ─── my_package.pt\n        ├── .data\n        │   ├── 94304870911616.storage\n        │   ├── 94304900784016.storage\n        │   ├── extern_modules\n        │   └── version\n        ├── models\n        │   └── model_1.pkl\n        └── torchvision\n            └── models\n                ├── resnet.py\n                └── utils.py","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":113,"to":150}}}}],["1327",{"pageContent":"You can also query :class:`Directory` objects with the ``has_file()`` method.\n\n\n::\n\n    importer_file_structure = importer.file_structure()\n    found: bool = importer_file_structure.has_file(\"package_a/subpackage.py\")\n\nSee why a given module was included as a dependency?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nSay there is a given module ``foo``, and you want to know why your :class:`PackageExporter` is pulling in ``foo`` as a dependency.\n\n:meth:`PackageExporter.get_rdeps` will return all modules that directly depend on ``foo``.\n\nIf you would like to see how a given module ``src`` depends on ``foo``, the :meth:`PackageExporter.all_paths` method will\nreturn a DOT-formatted graph showing all the dependency paths between ``src`` and ``foo``.\n\nIf you would just like to see the whole dependency graph of your :class:`PackageExporter`, you can use :meth:`PackageExporter.dependency_graph_string`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":153,"to":171}}}}],["1328",{"pageContent":"If you would just like to see the whole dependency graph of your :class:`PackageExporter`, you can use :meth:`PackageExporter.dependency_graph_string`.\n\n\nInclude arbitrary resources with my package and access them later?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n:class:`PackageExporter` exposes three methods, ``save_pickle``, ``save_text`` and ``save_binary`` that allow you to save\nPython objects, text, and binary data to a package.\n\n\n::\n\n    with torch.PackageExporter(\"package.pt\") as exporter:\n        # Pickles the object and saves to `my_resources/tensor.pkl` in the archive.\n        exporter.save_pickle(\"my_resources\", \"tensor.pkl\", torch.randn(4))\n        exporter.save_text(\"config_stuff\", \"words.txt\", \"a sample string\")\n        exporter.save_binary(\"raw_data\", \"binary\", my_bytes)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":171,"to":186}}}}],["1329",{"pageContent":":class:`PackageImporter` exposes complementary methods named ``load_pickle``, ``load_text`` and ``load_binary`` that allow you to load\nPython objects, text and binary data from a package.\n\n\n::\n\n    importer = torch.PackageImporter(\"package.pt\")\n    my_tensor = importer.load_pickle(\"my_resources\", \"tensor.pkl\")\n    text = importer.load_text(\"config_stuff\", \"words.txt\")\n    binary = importer.load_binary(\"raw_data\", \"binary\")\n\n\nCustomize how a class is packaged?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n``torch.package`` allows for the customization of how classes are packaged. This behavior is accessed through defining the method\n``__reduce_package__`` on a class and by defining a corresponding de-packaging function. This is similar to defining ``__reduce__`` for\nPython’s normal pickling process.\n\nSteps:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":189,"to":207}}}}],["1330",{"pageContent":"Steps:\n\n1. Define the method ``__reduce_package__(self, exporter: PackageExporter)`` on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the ``PackageExporter`` when it encounters an instance of the target class.\n2. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature’s first parameter should be a ``PackageImporter`` instance, and the rest of the parameters are user defined.\n\n\n::\n\n    # foo.py [Example of customizing how class Foo is packaged]\n    from torch.package import PackageExporter, PackageImporter\n    import time","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":207,"to":217}}}}],["1331",{"pageContent":"::\n\n    # foo.py [Example of customizing how class Foo is packaged]\n    from torch.package import PackageExporter, PackageImporter\n    import time\n\n\n    class Foo:\n        def __init__(self, my_string: str):\n            super().__init__()\n            self.my_string = my_string\n            self.time_imported = 0\n            self.time_exported = 0\n\n        def __reduce_package__(self, exporter: PackageExporter):\n            \"\"\"\n            Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when\n            saving an instance of this object. This method should do the work to save this\n            object inside of the ``torch.package`` archive.\n\n            Returns function w/ arguments to load the object from a\n            ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.\n            \"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":217,"to":239}}}}],["1332",{"pageContent":"Returns function w/ arguments to load the object from a\n            ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.\n            \"\"\"\n\n            # use this pattern to ensure no naming conflicts with normal dependencies,\n            # anything saved under this module name shouldn't conflict with other\n            # items in the package\n            generated_module_name = f\"foo-generated._{exporter.get_unique_id()}\"\n            exporter.save_text(\n                generated_module_name,\n                \"foo.txt\",\n                self.my_string + \", with exporter modification!\",\n            )\n            time_exported = time.clock_gettime(1)\n\n            # returns de-packaging function w/ arguments to invoke with\n            return (unpackage_foo, (generated_module_name, time_exported,))","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":239,"to":255}}}}],["1333",{"pageContent":"# returns de-packaging function w/ arguments to invoke with\n            return (unpackage_foo, (generated_module_name, time_exported,))\n\n\n    def unpackage_foo(\n        importer: PackageImporter, generated_module_name: str, time_exported: float\n    ) -> Foo:\n        \"\"\"\n        Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function\n        when depickling a Foo object.\n        Performs work of loading and returning a Foo instance from a ``torch.package`` archive.\n        \"\"\"\n        time_imported = time.clock_gettime(1)\n        foo = Foo(importer.load_text(generated_module_name, \"foo.txt\"))\n        foo.time_imported = time_imported\n        foo.time_exported = time_exported\n        return foo\n\n\n::\n\n    # example of saving instances of class Foo\n\n    import torch\n    from torch.package import PackageImporter, PackageExporter\n    import foo","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":255,"to":280}}}}],["1334",{"pageContent":"::\n\n    # example of saving instances of class Foo\n\n    import torch\n    from torch.package import PackageImporter, PackageExporter\n    import foo\n\n    foo_1 = foo.Foo(\"foo_1 initial string\")\n    foo_2 = foo.Foo(\"foo_2 initial string\")\n    with PackageExporter('foo_package.pt') as pe:\n        # save as normal, no extra work necessary\n        pe.save_pickle('foo_collection', 'foo1.pkl', foo_1)\n        pe.save_pickle('foo_collection', 'foo2.pkl', foo_2)\n\n    pi = PackageImporter('foo_package.pt')\n    print(pi.file_structure())\n    imported_foo = pi.load_pickle('foo_collection', 'foo1.pkl')\n    print(f\"foo_1 string: '{imported_foo.my_string}'\")\n    print(f\"foo_1 export time: {imported_foo.time_exported}\")\n    print(f\"foo_1 import time: {imported_foo.time_imported}\")\n\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":280,"to":303}}}}],["1335",{"pageContent":"::\n\n    # output of running above script\n    ─── foo_package\n        ├── foo-generated\n        │   ├── _0\n        │   │   └── foo.txt\n        │   └── _1\n        │       └── foo.txt\n        ├── foo_collection\n        │   ├── foo1.pkl\n        │   └── foo2.pkl\n        └── foo.py\n\n    foo_1 string: 'foo_1 initial string, with reduction modification!'\n    foo_1 export time: 9857706.650140837\n    foo_1 import time: 9857706.652698385\n\n\nTest in my source code whether or not it is executing inside a package?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nA :class:`PackageImporter` will add the attribute ``__torch_package__`` to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not.\n\n\n::\n\n    # In foo/bar.py:\n\n    if \"__torch_package__\" in dir():  # true if the code is being loaded from a package\n        def is_in_package():\n            return True","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":303,"to":334}}}}],["1336",{"pageContent":"::\n\n    # In foo/bar.py:\n\n    if \"__torch_package__\" in dir():  # true if the code is being loaded from a package\n        def is_in_package():\n            return True\n\n        UserException = Exception\n    else:\n        def is_in_package():\n            return False\n\n        UserException = UnpackageableException\n\n\nNow, the code will behave differently depending on whether it’s imported normally through your Python environment or imported from a\n``torch.package``.\n\n\n::\n\n    from foo.bar import is_in_package\n\n    print(is_in_package())  # False\n\n    loaded_module = PackageImporter(my_package).import_module(\"foo.bar\")\n    loaded_module.is_in_package()  # True","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":334,"to":361}}}}],["1337",{"pageContent":"::\n\n    from foo.bar import is_in_package\n\n    print(is_in_package())  # False\n\n    loaded_module = PackageImporter(my_package).import_module(\"foo.bar\")\n    loaded_module.is_in_package()  # True\n\n\n**Warning**: in general, it’s bad practice to have code that behaves differently depending on whether it’s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.\n\n\nPatch code into a package?\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n:class:`PackageExporter` offers a ``save_source_string()`` method that allows one to save arbitrary Python source code to a module of your choosing.\n\n\n::\n\n    with PackageExporter(f) as exporter:\n        # Save the my_module.foo available in your current Python environment.\n        exporter.save_module(\"my_module.foo\")","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":361,"to":385}}}}],["1338",{"pageContent":"::\n\n    with PackageExporter(f) as exporter:\n        # Save the my_module.foo available in your current Python environment.\n        exporter.save_module(\"my_module.foo\")\n\n        # This saves the provided string to my_module/foo.py in the package archive.\n        # It will override the my_module.foo that was previously saved.\n        exporter.save_source_string(\"my_module.foo\", textwrap.dedent(\n            \"\"\"\\\n            def my_function():\n                print('hello world')\n            \"\"\"\n        ))\n\n        # If you want to treat my_module.bar as a package\n        # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)\n        # pass is_package=True,\n        exporter.save_source_string(\"my_module.bar\",\n                                    \"def foo(): print('hello')\\n\",\n                                    is_package=True)\n\n    importer = PackageImporter(f)\n    importer.import_module(\"my_module.foo\").my_function()  # prints 'hello world'","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":385,"to":408}}}}],["1339",{"pageContent":"importer = PackageImporter(f)\n    importer.import_module(\"my_module.foo\").my_function()  # prints 'hello world'\n\n\nAccess package contents from packaged code?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n:class:`PackageImporter` implements the\n`importlib.resources <https://docs.python.org/3/library/importlib.html#module-importlib.resources>`_\nAPI for accessing resources from inside a package.\n\n\n::\n\n    with PackageExporter(f) as exporter:\n        # saves text to my_resource/a.txt in the archive\n        exporter.save_text(\"my_resource\", \"a.txt\", \"hello world!\")\n        # saves the tensor to my_pickle/obj.pkl\n        exporter.save_pickle(\"my_pickle\", \"obj.pkl\", torch.ones(2, 2))\n\n        # see below for module contents\n        exporter.save_module(\"foo\")\n        exporter.save_module(\"bar\")\n\n\nThe ``importlib.resources`` API allows access to resources from within packaged code.\n\n\n::\n\n    # foo.py:\n    import importlib.resources\n    import my_resource","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":408,"to":439}}}}],["1340",{"pageContent":"The ``importlib.resources`` API allows access to resources from within packaged code.\n\n\n::\n\n    # foo.py:\n    import importlib.resources\n    import my_resource\n\n    # returns \"hello world!\"\n    def get_my_resource():\n        return importlib.resources.read_text(my_resource, \"a.txt\")\n\n\nUsing ``importlib.resources`` is the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent :class:`PackageImporter` instance itself from within\npackaged code.\n\n\n::\n\n    # bar.py:\n    import torch_package_importer # this is the PackageImporter that imported this module.\n\n    # Prints \"hello world!\", equivalent to importlib.resources.read_text\n    def get_my_resource():\n        return torch_package_importer.load_text(\"my_resource\", \"a.txt\")","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":439,"to":465}}}}],["1341",{"pageContent":"# Prints \"hello world!\", equivalent to importlib.resources.read_text\n    def get_my_resource():\n        return torch_package_importer.load_text(\"my_resource\", \"a.txt\")\n\n    # You also do things that the importlib.resources API does not support, like loading\n    # a pickled object from the package.\n    def get_my_pickle():\n        return torch_package_importer.load_pickle(\"my_pickle\", \"obj.pkl\")\n\n\nDistinguish between packaged code and non-packaged code?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTo tell if an object’s code is from a ``torch.package``, use the ``torch.package.is_from_package()`` function.\nNote: if an object is from a package but its definition is from a module marked ``extern`` or from ``stdlib``,\nthis check will return ``False``.\n\n\n::\n\n    importer = PackageImporter(f)\n    mod = importer.import_module('foo')\n    obj = importer.load_pickle('model', 'model.pkl')\n    txt = importer.load_text('text', 'my_test.txt')","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":465,"to":487}}}}],["1342",{"pageContent":"::\n\n    importer = PackageImporter(f)\n    mod = importer.import_module('foo')\n    obj = importer.load_pickle('model', 'model.pkl')\n    txt = importer.load_text('text', 'my_test.txt')\n\n    assert is_from_package(mod)\n    assert is_from_package(obj)\n    assert not is_from_package(txt) # str is from stdlib, so this will return False\n\n\nRe-export an imported object?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTo re-export an object that was previously imported by a :class:`PackageImporter`, you must make the new :class:`PackageExporter`\naware of the original :class:`PackageImporter` so that it can find source code for your object’s dependencies.\n\n\n::\n\n    importer = PackageImporter(f)\n    obj = importer.load_pickle(\"model\", \"model.pkl\")\n\n    # re-export obj in a new package\n    with PackageExporter(f2, importer=(importer, sys_importer)) as exporter:\n        exporter.save_pickle(\"model\", \"model.pkl\", obj)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":487,"to":512}}}}],["1343",{"pageContent":"# re-export obj in a new package\n    with PackageExporter(f2, importer=(importer, sys_importer)) as exporter:\n        exporter.save_pickle(\"model\", \"model.pkl\", obj)\n\n\nPackage a TorchScript module?\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTo package a TorchScript model, use the same ``save_pickle`` and ``load_pickle`` APIs as you would with any other object.\nSaving TorchScript objects that are attributes or submodules is supported as well with no extra work.\n\n\n::\n\n    # save TorchScript just like any other object\n    with PackageExporter(file_name) as e:\n        e.save_pickle(\"res\", \"script_model.pkl\", scripted_model)\n        e.save_pickle(\"res\", \"mixed_model.pkl\", python_model_with_scripted_submodule)\n    # load as normal\n    importer = PackageImporter(file_name)\n    loaded_script = importer.load_pickle(\"res\", \"script_model.pkl\")\n    loaded_mixed = importer.load_pickle(\"res\", \"mixed_model.pkl\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":512,"to":532}}}}],["1344",{"pageContent":"Explanation\n-----------\n``torch.package`` Format Overview\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nA ``torch.package`` file is a ZIP archive which conventionally uses the ``.pt`` extension. Inside the ZIP archive, there are two kinds of files:\n\n* Framework files, which are placed in the ``.data/``.\n* User files, which is everything else.\n\nAs an example, this is what a fully packaged ResNet model from ``torchvision`` looks like:\n\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":535,"to":547}}}}],["1345",{"pageContent":"* Framework files, which are placed in the ``.data/``.\n* User files, which is everything else.\n\nAs an example, this is what a fully packaged ResNet model from ``torchvision`` looks like:\n\n\n::\n\n    resnet\n    ├── .data  # All framework-specific data is stored here.\n    │   │      # It's named to avoid conflicts with user-serialized code.\n    │   ├── 94286146172688.storage  # tensor data\n    │   ├── 94286146172784.storage\n    │   ├── extern_modules  # text file with names of extern modules (e.g. 'torch')\n    │   ├── version         # version metadata\n    │   ├── ...\n    ├── model  # the pickled model\n    │   └── model.pkl\n    └── torchvision  # all code dependencies are captured as source files\n        └── models\n            ├── resnet.py\n            └── utils.py","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":547,"to":568}}}}],["1346",{"pageContent":"Framework files\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nThe ``.data/`` directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThe ``torch.package`` format makes no guarantees about the contents of ``.data/``, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load older ``torch.packages``).\n\nCurrently, the ``.data/`` directory contains the following items:\n\n* ``version``: a version number for the serialized format, so that the ``torch.package`` import infrastructures knows how to load this package.\n* ``extern_modules``: a list of modules that are considered ``extern:class:`PackageImporter`. ``extern`` modules will be imported using the loading environment’s system importer.\n* ``*.storage``: serialized tensor data.\n\n\n::\n\n    .data\n    ├── 94286146172688.storage\n    ├── 94286146172784.storage\n    ├── extern_modules\n    ├── version\n    ├── ...","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":571,"to":591}}}}],["1347",{"pageContent":"::\n\n    .data\n    ├── 94286146172688.storage\n    ├── 94286146172784.storage\n    ├── extern_modules\n    ├── version\n    ├── ...\n\n\nUser files\n\"\"\"\"\"\"\"\"\"\"\nAll other files in the archive were put there by a user. The layout is identical to a Python\n`regular package <https://docs.python.org/3/reference/import.html#regular-packages>`_. For a deeper dive in how Python packaging works,\nplease consult `this essay <https://www.python.org/doc/essays/packages/>`_ (it’s slightly out of date, so double-check implementation details\nwith the `Python reference documentation <https://docs.python.org/3/library/importlib.html>`_).\n\n\n::\n\n    <package root>\n    ├── model  # the pickled model\n    │   └── model.pkl\n    ├── another_package\n    │   ├── __init__.py\n    │   ├── foo.txt         # a resource file , see importlib.resources\n    │   └── ...\n    └── torchvision\n        └── models\n            ├── resnet.py   # torchvision.models.resnet\n            └── utils.py    # torchvision.models.utils","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":591,"to":621}}}}],["1348",{"pageContent":"How ``torch.package`` finds your code's dependencies\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAnalyzing an object's dependencies\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nWhen you issue a ``save_pickle(obj, ...)`` call, :class:`PackageExporter` will pickle the object normally. Then, it uses the\n``pickletools`` standard library module to parse the pickle bytecode.\n\nIn a pickle, an object is saved along with a ``GLOBAL`` opcode that describes where to find the implementation of the object’s type, like:\n\n\n::\n\n    GLOBAL 'torchvision.models.resnet Resnet`\n\n\nThe dependency resolver will gather up all ``GLOBAL`` ops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consult `the Python docs <https://docs.python.org/3/library/pickle.html>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":624,"to":640}}}}],["1349",{"pageContent":"Analyzing a module's dependencies\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nWhen a Python module is identified as a dependency, ``torch.package`` walks the module’s python AST representation and looks for import statements with\nfull support for the standard forms: ``from x import y``, ``import z``, ``from w import v as u``, etc. When one of these import statements are\nencountered, ``torch.package`` registers the imported modules as dependencies that are then themselves parsed in the same AST walking way.\n\n**Note**: AST parsing has limited support for the ``__import__(...)`` syntax and does not support ``importlib.import_module`` calls. In general, you should\nnot expect dynamic imports to be detected by ``torch.package``.\n\n\nDependency Management\n^^^^^^^^^^^^^^^^^^^^^\n``torch.package`` automatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an *action* to take.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":642,"to":655}}}}],["1350",{"pageContent":"The allowed actions are:\n\n* ``intern``: put this module into the package.\n* ``extern``: declare this module as an external dependency of the package.\n* ``mock``: stub out this module.\n* ``deny``: depending on this module will raise an error during package export.\n\nFinally, there is one more important action that is not technically part of ``torch.package``:\n\n* Refactoring: remove or change the dependencies in your code.\n\nNote that actions are only defined on entire Python modules. There is no way to package “just” a function or class from a module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that’s what ``torch.package`` uses.\n\nActions are applied to modules using patterns. Patterns can either be module names (``\"foo.bar\"``) or globs (like ``\"foo.**\"``). You associate a pattern\nwith an action using methods on :class:`PackageExporter`, e.g.\n\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":657,"to":676}}}}],["1351",{"pageContent":"::\n\n    my_exporter.intern(\"torchvision.**\")\n    my_exporter.extern(\"numpy\")\n\n\nIf a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken.\n\n\n``intern``\n\"\"\"\"\"\"\"\"\"\"\nIf a module is ``intern``-ed, it will be placed into the package.\n\nThis action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet from ``torchvision``,\nyou will need to ``intern`` the module torchvision.models.resnet.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":676,"to":691}}}}],["1352",{"pageContent":"On package import, when your packaged code tries to import an ``intern``-ed module, PackageImporter will look inside your package for that module.\nIf it can’t find that module, an error will be raised. This ensures that each :class:`PackageImporter` is isolated from the loading environment—even\nif you have ``my_interned_module`` available in both your package and the loading environment, :class:`PackageImporter` will only use the version in your\npackage.\n\n**Note**: Only Python source modules can be ``intern``-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to ``intern`` them. These kinds of modules need to be ``mock``-ed or ``extern``-ed.\n\n\n``extern``\n\"\"\"\"\"\"\"\"\"\"\nIf a module is ``extern``-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on ``package_exporter.extern_modules``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":693,"to":705}}}}],["1353",{"pageContent":"On package import, when the packaged code tries to import an ``extern``-ed module, :class:`PackageImporter` will use the default Python importer to find\nthat module, as if you did ``importlib.import_module(\"my_externed_module\")``. If it can’t find that module, an error will be raised.\n\nIn this way, you can depend on third-party libraries like ``numpy`` and ``scipy`` from within your package without having to package them too.\n\n**Warning**: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of ``extern``.\n\n\n``mock``\n\"\"\"\"\"\"\"\"\nIf a module is ``mock``-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so that ``from my_mocked_module import foo`` will not error), but any use of that object will raise a ``NotImplementedError``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":707,"to":719}}}}],["1354",{"pageContent":"``mock`` should be used for code that you “know” will not be needed in the loaded package, but you still want available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.\n\n**Warning**: In general, ``mock`` should be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.\n\n\nRefactoring\n\"\"\"\"\"\"\"\"\"\"\"\nThe best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!):\n\n**Include only what you use**. Do not leave unused imports in your code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":721,"to":734}}}}],["1355",{"pageContent":"**Include only what you use**. Do not leave unused imports in your code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them.\n\n**Qualify your imports**. For example, instead of writing import foo and later using ``foo.bar.baz``, prefer to write ``from foo.bar import baz``. This more\nprecisely specifies your real dependency (``foo.bar``) and lets the dependency resolver know you don’t need all of ``foo``.\n\n**Split up large files with unrelated functionality into smaller ones**. If your ``utils`` module contains a hodge-podge of unrelated functionality, any module\nthat depends on ``utils`` will need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":734,"to":742}}}}],["1356",{"pageContent":"Patterns\n\"\"\"\"\"\"\"\"\nPatterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buck\n`glob() <https://docs.bazel.build/versions/master/be/functions.html#glob>`_.\n\nA module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g. ``foo.bar.baz``.\n\nA pattern contains one or more segments. Segments can be:\n\n* A literal string (e.g. ``foo``), which matches exactly.\n* A string containing a wildcard (e.g. ``torch``, or ``foo*baz*``). The wildcard matches any string, including the empty string.\n* A double wildcard (``**``). This matches against zero or more complete segments.\n\nExamples:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":745,"to":759}}}}],["1357",{"pageContent":"Examples:\n\n* ``torch.**``: matches ``torch`` and all its submodules, e.g. ``torch.nn`` and ``torch.nn.functional``.\n* ``torch.*``: matches ``torch.nn`` or ``torch.functional``, but not ``torch.nn.functional`` or ``torch``\n* ``torch*.**``: matches ``torch``, ``torchvision``, and all of their submodules\n\nWhen specifying actions, you can pass multiple patterns, e.g.\n\n\n::\n\n    exporter.intern([\"torchvision.models.**\", \"torchvision.utils.**\"])\n\n\nA module will match against this action if it matches any of the patterns.\n\nYou can also specify patterns to exclude, e.g.\n\n\n::\n\n    exporter.mock(\"**\", exclude=[\"torchvision.**\"])\n\n\nA module will not match against this action if it matches any of the exclude patterns. In this example, we are mocking all modules except\n``torchvision`` and its submodules.\n\nWhen a module could potentially match against multiple actions, the first action defined will be taken.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":759,"to":786}}}}],["1358",{"pageContent":"When a module could potentially match against multiple actions, the first action defined will be taken.\n\n\n``torch.package`` sharp edges\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAvoid global state in your modules\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nPython makes it really easy to bind objects and run code at module-level scope. This is generally fine—after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state.\n\nMutable global state is quite useful—it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used with ``torch.package``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":786,"to":798}}}}],["1359",{"pageContent":"Every :class:`PackageImporter` creates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.\n\nTypes are not shared between packages and the loading environment\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\nAny class that you import from a :class:`PackageImporter` will be a version of the class specific to that importer. For example:\n\n\n::\n\n    from foo import MyClass\n\n    my_class_instance = MyClass()\n\n    with PackageExporter(f) as exporter:\n        exporter.save_module(\"foo\")\n\n    importer = PackageImporter(f)\n    imported_MyClass = importer.import_module(\"foo\").MyClass\n\n    assert isinstance(my_class_instance, MyClass)  # works\n    assert isinstance(my_class_instance, imported_MyClass)  # ERROR!","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":800,"to":822}}}}],["1360",{"pageContent":"assert isinstance(my_class_instance, MyClass)  # works\n    assert isinstance(my_class_instance, imported_MyClass)  # ERROR!\n\n\nIn this example, ``MyClass`` and ``imported_MyClass`` are *not the same type*. In this specific example, ``MyClass`` and ``imported_MyClass`` have exactly the\nsame implementation, so you might think it’s okay to consider them the same class. But consider the situation where ``imported_MyClass`` is coming from an\nolder package with an entirely different implementation of ``MyClass`` — in that case, it’s unsafe to consider them the same class.\n\nUnder the hood, each importer has a prefix that allows it to uniquely identify classes:\n\n\n::\n\n    print(MyClass.__name__)  # prints \"foo.MyClass\"\n    print(imported_MyClass.__name__)  # prints <torch_package_0>.foo.MyClass\n\n\nThat means you should not expect ``isinstance`` checks to work when one of the arguments is from a package and the other is not. If you need this\nfunctionality, consider the following options:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":822,"to":840}}}}],["1361",{"pageContent":"That means you should not expect ``isinstance`` checks to work when one of the arguments is from a package and the other is not. If you need this\nfunctionality, consider the following options:\n\n* Doing duck typing (just using the class instead of explicitly checking that it is of a given type).\n* Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tag ``self.handler = \"handle_me_this_way\"`` and have client code check for the value of ``handler`` instead of checking the type directly.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":840,"to":844}}}}],["1362",{"pageContent":"How ``torch.package`` keeps packages isolated from each other\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nEach :class:`PackageImporter` instance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules marked ``extern``. If you use multiple :class:`PackageImporter` instances to load a single package, you will get\nmultiple independent environments that do not interact.\n\nThis is achieved by extending Python’s import infrastructure with a custom importer. :class:`PackageImporter` provides the same core API as the\n``importlib`` importer; namely, it implements the ``import_module`` and ``__import__`` methods.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":847,"to":854}}}}],["1363",{"pageContent":"When you invoke :meth:`PackageImporter.import_module`, :class:`PackageImporter` will construct and return a new module, much as the system importer does.\nHowever, :class:`PackageImporter` patches the returned module to use ``self`` (i.e. that :class:`PackageImporter` instance) to fulfill future import\nrequests by looking in the package rather than searching the user’s Python environment.\n\nMangling\n\"\"\"\"\"\"\"\"\nTo avoid confusion (“is this ``foo.bar`` object the one from my package, or the one from my Python environment?”), :class:`PackageImporter` mangles the\n``__name__`` and ``__file__`` of all imported modules, by adding a *mangle prefix* to them.\n\nFor ``__name__``, a name like ``torchvision.models.resnet18`` becomes ``<torch_package_0>.torchvision.models.resnet18``.\n\nFor ``__file__``, a name like ``torchvision/models/resnet18.py`` becomes ``<torch_package_0>.torchvision/modules/resnet18.py``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":856,"to":867}}}}],["1364",{"pageContent":"For ``__file__``, a name like ``torchvision/models/resnet18.py`` becomes ``<torch_package_0>.torchvision/modules/resnet18.py``.\n\nName mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consult\n``mangling.md`` in ``torch/package/``.\n\n\nAPI Reference\n-------------\n.. autoclass:: torch.package.PackagingError\n\n.. autoclass:: torch.package.EmptyMatchError\n\n.. autoclass:: torch.package.PackageExporter\n  :members:\n\n  .. automethod:: __init__\n\n.. autoclass:: torch.package.PackageImporter\n  :members:\n\n  .. automethod:: __init__\n\n.. autoclass:: torch.package.Directory\n  :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/package.rst","loc":{"lines":{"from":867,"to":891}}}}],["1365",{"pageContent":".. _pipeline-parallelism:\n\nPipeline Parallelism\n====================\n\nPipeline parallelism was original introduced in the\n`Gpipe <https://arxiv.org/abs/1811.06965>`__  paper and is an efficient\ntechnique to train large models on multiple GPUs.\n\n.. warning ::\n     Pipeline Parallelism is experimental and subject to change.\n\nModel Parallelism using multiple GPUs\n-------------------------------------\n\nTypically for large models which don't fit on a single GPU, model parallelism\nis employed where certain parts of the model are placed on different GPUs.\nAlthough, if this is done naively for sequential models, the training process\nsuffers from GPU under utilization since only one GPU is active at one time as\nshown in the figure below:\n\n.. figure:: _static/img/pipeline_parallelism/no_pipe.png","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/pipeline.rst","loc":{"lines":{"from":1,"to":22}}}}],["1366",{"pageContent":".. figure:: _static/img/pipeline_parallelism/no_pipe.png\n\n   The figure represents a model with 4 layers placed on 4 different GPUs\n   (vertical axis). The horizontal axis represents training this model through\n   time demonstrating that only 1 GPU is utilized at a time\n   (`image source <https://arxiv.org/abs/1811.06965>`__).\n\nPipelined Execution\n-------------------\n\nTo alleviate this problem, pipeline parallelism splits the input minibatch into\nmultiple microbatches and pipelines the execution of these microbatches across\nmultiple GPUs. This is outlined in the figure below:\n\n.. figure:: _static/img/pipeline_parallelism/pipe.png","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/pipeline.rst","loc":{"lines":{"from":22,"to":36}}}}],["1367",{"pageContent":".. figure:: _static/img/pipeline_parallelism/pipe.png\n\n   The figure represents a model with 4 layers placed on 4 different GPUs\n   (vertical axis). The horizontal axis represents training this model through\n   time demonstrating that the GPUs are utilized much more efficiently.\n   However, there still exists a bubble (as demonstrated in the figure) where\n   certain GPUs are not utilized.\n   (`image source <https://arxiv.org/abs/1811.06965>`__).\n\nPipe APIs in PyTorch\n--------------------\n.. autoclass:: torch.distributed.pipeline.sync.Pipe\n   :members: forward\n\nSkip connections\n^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/pipeline.rst","loc":{"lines":{"from":36,"to":51}}}}],["1368",{"pageContent":"Pipe APIs in PyTorch\n--------------------\n.. autoclass:: torch.distributed.pipeline.sync.Pipe\n   :members: forward\n\nSkip connections\n^^^^^^^^^^^^^^^^\n\nCertain models like `ResNeXt <https://pytorch.org/hub/pytorch_vision_resnext/>`__\nare not completely sequential and have skip connections between layers.\nNaively implementing as part of pipeline parallelism would imply that\nwe need to copy outputs for certain layers through multiple GPUs till\nwe eventually reach the GPU where the layer for the skip connection resides.\nTo avoid this copy overhead, we provide APIs below to stash and pop Tensors\nin different layers of the model.\n\n.. autofunction:: torch.distributed.pipeline.sync.skip.skippable.skippable\n.. autoclass:: torch.distributed.pipeline.sync.skip.skippable.stash\n.. autoclass:: torch.distributed.pipeline.sync.skip.skippable.pop\n.. autofunction:: torch.distributed.pipeline.sync.skip.skippable.verify_skippables\n\nTutorials\n---------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/pipeline.rst","loc":{"lines":{"from":51,"to":73}}}}],["1369",{"pageContent":"Tutorials\n---------\n\nThe following tutorials give a good overview of how to use the\n:class:`~torch.distributed.pipeline.sync.Pipe` API to train your models with the\nrest of the components that PyTorch provides:\n\n- `Training Transformer models using Pipeline Parallelism <https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html>`__\n- `Training Transformer models using Distributed Data Parallel and Pipeline Parallelism <https://pytorch.org/tutorials/advanced/ddp_pipeline.html>`__\n\nAcknowledgements\n----------------\n\nThe implementation for pipeline parallelism is based on `fairscale's pipe implementation <https://github.com/facebookresearch/fairscale/tree/main/fairscale/nn/pipe>`__ and\n`torchgpipe <https://github.com/kakaobrain/torchgpipe>`__. We would like to\nthank both teams for their contributions and guidance towards bringing pipeline\nparallelism into PyTorch.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/pipeline.rst","loc":{"lines":{"from":73,"to":89}}}}],["1370",{"pageContent":".. currentmodule:: torch.profiler\n\ntorch.profiler\n==============\n\nOverview\n--------\n.. automodule:: torch.profiler\n\n\nAPI Reference\n-------------\n\n.. autoclass:: torch.profiler._KinetoProfile\n  :members:\n\n.. autoclass:: torch.profiler.profile\n  :members:\n\n.. autoclass:: torch.profiler.ProfilerAction\n  :members:\n\n.. autoclass:: torch.profiler.ProfilerActivity\n  :members:\n\n.. autofunction:: torch.profiler.schedule\n\n.. autofunction:: torch.profiler.tensorboard_trace_handler\n\nIntel Instrumentation and Tracing Technology APIs\n-------------------------------------------------\n\n.. autofunction:: torch.profiler.itt.is_available\n\n.. autofunction:: torch.profiler.itt.mark\n\n.. autofunction:: torch.profiler.itt.range_push\n\n.. autofunction:: torch.profiler.itt.range_pop","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/profiler.rst","loc":{"lines":{"from":1,"to":39}}}}],["1371",{"pageContent":"Quantization Accuracy Debugging\n-------------------------------\n\nThis document provides high level strategies for improving quantization\naccuracy. If a quantized model has error compared to the original model,\nwe can categorize the error into:\n\n1. **data insensitive error** - caused by intrinsic model quantization error,\n   large portion of input data has large error\n2. **data sensitive error** - caused by outlier input data, small\n   portion of input data has large error\n3. **implementation error** - quantized kernel is not matching reference implementation\n\nData insensitive error\n~~~~~~~~~~~~~~~~~~~~~~\n\nGeneral tips\n^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-accuracy-debugging.rst","loc":{"lines":{"from":1,"to":18}}}}],["1372",{"pageContent":"Data insensitive error\n~~~~~~~~~~~~~~~~~~~~~~\n\nGeneral tips\n^^^^^^^^^^^^\n\n1. For PTQ, ensure that the data you are calibrating with is representative\n   of your dataset. For example, for a classification problem a general\n   guideline is to have multiple samples in every category, and the overall\n   number of samples should be at least 100. There is no penalty for\n   calibrating with more data other than calibration time.\n2. If your model has Conv-BN or Linear-BN patterns, consider fusing them.\n   If you are using FX graph mode quantization, this is done automatically\n   by the workflow. If you are using Eager mode quantization, you can do\n   this manually with the ``torch.ao.quantization.fuse_modules`` API.\n3. Increase the precision of dtype of the problematic ops. Usually, fp32\n   will have the highest accuracy, followed by fp16, followed by dynamically\n   quantized int8, followed by statically quantized int8.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-accuracy-debugging.rst","loc":{"lines":{"from":18,"to":35}}}}],["1373",{"pageContent":"1. Note: this is trading off performance for accuracy.\n   2. Note: availability of kernels per dtype per op can vary by backend.\n   3. Note: dtype conversions add an additional performance cost. For example,\n      ``fp32_op -> quant -> int8_op -> dequant -> fp32_op -> quant -> int8_op -> dequant``\n      will have a performance penalty compared to\n      ``fp32_op -> fp32_op -> quant -> int8_op -> int8_op -> dequant``\n      because of a higher number of required dtype conversions.\n\n4. If you are using PTQ, consider using QAT to recover some of the accuracy loss\n   from quantization.\n\nInt8 quantization tips\n^^^^^^^^^^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-accuracy-debugging.rst","loc":{"lines":{"from":37,"to":49}}}}],["1374",{"pageContent":"4. If you are using PTQ, consider using QAT to recover some of the accuracy loss\n   from quantization.\n\nInt8 quantization tips\n^^^^^^^^^^^^^^^^^^^^^^\n\n1. If you are using per-tensor weight quantization, consider using per-channel\n   weight quantization.\n2. If you are doing inference on `fbgemm`, ensure that you set the `reduce_range`\n   argument to `False` if your CPU is Cooperlake or newer, and to `True` otherwise.\n3. Audit the input activation distribution variation across different samples.\n   If this variation is high, the layer may be suitable for dynamic quantization\n   but not static quantization.\n\nData sensitive error\n~~~~~~~~~~~~~~~~~~~~\n\nIf you are using static quantization and a small portion of your input data is\nresulting in high quantization error, you can try:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-accuracy-debugging.rst","loc":{"lines":{"from":49,"to":67}}}}],["1375",{"pageContent":"Data sensitive error\n~~~~~~~~~~~~~~~~~~~~\n\nIf you are using static quantization and a small portion of your input data is\nresulting in high quantization error, you can try:\n\n1. Adjust your calibration dataset to make it more representative of your\n   inference dataset.\n2. Manually inspect (using Numeric Suite) which layers have high quantization\n   error. For these layers, consider leaving them in floating point or adjusting\n   the observer settings to choose a better scale and zero_point.\n\n\nImplementation error\n~~~~~~~~~~~~~~~~~~~~\n\nIf you are using PyTorch quantization with your own backend\nyou may see differences between the reference implementation of an\noperation (such as ``dequant -> op_fp32 -> quant``) and the quantized implementation\n(such as `op_int8`) of the op on the target hardware. This could mean one of two things:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-accuracy-debugging.rst","loc":{"lines":{"from":67,"to":86}}}}],["1376",{"pageContent":"1. the differences (usually small) are expected due to specific behavior of\n   the target kernel on the target hardware compared to fp32/cpu. An example of this\n   is accumulating in an integer dtype. Unless the kernel guarantees bitwise\n   equivalency with the reference implementation, this is expected.\n2. the kernel on the target hardware has an accuracy issue. In this case, reach\n   out to the kernel developer.\n\nNumerical Debugging Tooling (prototype)\n---------------------------------------\n\n.. toctree::\n    :hidden:\n\n    torch.ao.ns._numeric_suite\n    torch.ao.ns._numeric_suite_fx\n\n.. warning ::\n     Numerical debugging tooling is early prototype and subject to change.\n\n* :ref:`torch_ao_ns_numeric_suite`\n  Eager mode numeric suite\n* :ref:`torch_ao_ns_numeric_suite_fx`\n  FX numeric suite","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-accuracy-debugging.rst","loc":{"lines":{"from":88,"to":110}}}}],["1377",{"pageContent":"Quantization Backend Configuration\n----------------------------------\n\nFX Graph Mode Quantization allows the user to configure various\nquantization behaviors of an op in order to match the expectation\nof their backend.\n\nIn the future, this document will contain a detailed spec of\nthese configurations.\n\n\nDefault values for native configurations\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBelow is the output of the configuration for quantization of ops\nin x86 and qnnpack (PyTorch's default quantized backends).\n\nResults:\n\n.. literalinclude:: scripts/quantization_backend_configs/default_backend_config.txt","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-backend-configuration.rst","loc":{"lines":{"from":1,"to":20}}}}],["1378",{"pageContent":"Quantization API Reference\n-------------------------------\n\ntorch.ao.quantization\n~~~~~~~~~~~~~~~~~~~~~\n\nThis module contains Eager mode quantization APIs.\n\n.. currentmodule:: torch.ao.quantization\n\nTop level APIs\n^^^^^^^^^^^^^^\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    quantize\n    quantize_dynamic\n    quantize_qat\n    prepare\n    prepare_qat\n    convert\n\nPreparing model for quantization\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    fuse_modules\n    QuantStub\n    DeQuantStub\n    QuantWrapper\n    add_quant_dequant\n\nUtility functions\n^^^^^^^^^^^^^^^^^\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    swap_module\n    propagate_qconfig_\n    default_eval_fn\n\ntorch.ao.quantization.quantize_fx\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module contains FX graph mode quantization APIs (prototype).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":1,"to":55}}}}],["1379",{"pageContent":"swap_module\n    propagate_qconfig_\n    default_eval_fn\n\ntorch.ao.quantization.quantize_fx\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module contains FX graph mode quantization APIs (prototype).\n\n.. currentmodule:: torch.ao.quantization.quantize_fx\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    prepare_fx\n    prepare_qat_fx\n    convert_fx\n    fuse_fx\n\ntorch.ao.quantization.qconfig_mapping\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module contains QConfigMapping for configuring FX graph mode quantization.\n\n.. currentmodule:: torch.ao.quantization.qconfig_mapping\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    QConfigMapping\n    get_default_qconfig_mapping\n    get_default_qat_qconfig_mapping\n\ntorch.ao.quantization.backend_config\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":55,"to":93}}}}],["1380",{"pageContent":"QConfigMapping\n    get_default_qconfig_mapping\n    get_default_qat_qconfig_mapping\n\ntorch.ao.quantization.backend_config\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module contains BackendConfig, a config object that defines how quantization is supported\nin a backend. Currently only used by FX Graph Mode Quantization, but we may extend Eager Mode\nQuantization to work with this as well.\n\n.. currentmodule:: torch.ao.quantization.backend_config\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    BackendConfig\n    BackendPatternConfig\n    DTypeConfig\n    DTypeWithConstraints\n    ObservationType\n\ntorch.ao.quantization.fx.custom_config\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module contains a few CustomConfig classes that's used in both eager mode and FX graph mode quantization\n\n\n.. currentmodule:: torch.ao.quantization.fx.custom_config\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":93,"to":128}}}}],["1381",{"pageContent":".. currentmodule:: torch.ao.quantization.fx.custom_config\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    FuseCustomConfig\n    PrepareCustomConfig\n    ConvertCustomConfig\n    StandaloneModuleConfigEntry\n\ntorch (quantization related functions)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis describes the quantization related functions of the `torch` namespace.\n\n.. currentmodule:: torch\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    quantize_per_tensor\n    quantize_per_channel\n    dequantize\n\ntorch.Tensor (quantization related methods)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nQuantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor.\n\n.. currentmodule:: torch.Tensor\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":128,"to":167}}}}],["1382",{"pageContent":".. currentmodule:: torch.Tensor\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    view\n    as_strided\n    expand\n    flatten\n    select\n    ne\n    eq\n    ge\n    le\n    gt\n    lt\n    copy_\n    clone\n    dequantize\n    equal\n    int_repr\n    max\n    mean\n    min\n    q_scale\n    q_zero_point\n    q_per_channel_scales\n    q_per_channel_zero_points\n    q_per_channel_axis\n    resize_\n    sort\n    topk\n\n\ntorch.ao.quantization.observer\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module contains observers which are used to collect statistics about\nthe values observed during calibration (PTQ) or training (QAT).\n\n.. currentmodule:: torch.ao.quantization.observer\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":167,"to":214}}}}],["1383",{"pageContent":".. currentmodule:: torch.ao.quantization.observer\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    ObserverBase\n    MinMaxObserver\n    MovingAverageMinMaxObserver\n    PerChannelMinMaxObserver\n    MovingAveragePerChannelMinMaxObserver\n    HistogramObserver\n    PlaceholderObserver\n    RecordingObserver\n    NoopObserver\n    get_observer_state_dict\n    load_observer_state_dict\n    default_observer\n    default_placeholder_observer\n    default_debug_observer\n    default_weight_observer\n    default_histogram_observer\n    default_per_channel_weight_observer\n    default_dynamic_quant_observer\n    default_float_qparams_observer\n\ntorch.ao.quantization.fake_quantize\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements modules which are used to perform fake quantization\nduring QAT.\n\n.. currentmodule:: torch.ao.quantization.fake_quantize\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":214,"to":252}}}}],["1384",{"pageContent":".. currentmodule:: torch.ao.quantization.fake_quantize\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    FakeQuantizeBase\n    FakeQuantize\n    FixedQParamsFakeQuantize\n    FusedMovingAvgObsFakeQuantize\n    default_fake_quant\n    default_weight_fake_quant\n    default_per_channel_weight_fake_quant\n    default_histogram_fake_quant\n    default_fused_act_fake_quant\n    default_fused_wt_fake_quant\n    default_fused_per_channel_wt_fake_quant\n    disable_fake_quant\n    enable_fake_quant\n    disable_observer\n    enable_observer\n\ntorch.ao.quantization.qconfig\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module defines `QConfig` objects which are used\nto configure quantization settings for individual ops.\n\n.. currentmodule:: torch.ao.quantization.qconfig\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":252,"to":286}}}}],["1385",{"pageContent":".. currentmodule:: torch.ao.quantization.qconfig\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    QConfig\n    default_qconfig\n    default_debug_qconfig\n    default_per_channel_qconfig\n    default_dynamic_qconfig\n    float16_dynamic_qconfig\n    float16_static_qconfig\n    per_channel_dynamic_qconfig\n    float_qparams_weight_only_qconfig\n    default_qat_qconfig\n    default_weight_only_qconfig\n    default_activation_only_qconfig\n    default_qat_qconfig_v2\n\ntorch.ao.nn.intrinsic\n~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: torch.ao.nn.intrinsic\n.. automodule:: torch.ao.nn.intrinsic.modules\n\nThis module implements the combined (fused) modules conv + relu which can\nthen be quantized.\n\n.. currentmodule:: torch.ao.nn.intrinsic\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":286,"to":320}}}}],["1386",{"pageContent":".. currentmodule:: torch.ao.nn.intrinsic\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    ConvReLU1d\n    ConvReLU2d\n    ConvReLU3d\n    LinearReLU\n    ConvBn1d\n    ConvBn2d\n    ConvBn3d\n    ConvBnReLU1d\n    ConvBnReLU2d\n    ConvBnReLU3d\n    BNReLU2d\n    BNReLU3d\n\ntorch.ao.nn.intrinsic.qat\n~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: torch.ao.nn.intrinsic.qat\n.. automodule:: torch.ao.nn.intrinsic.qat.modules\n\n\nThis module implements the versions of those fused operations needed for\nquantization aware training.\n\n.. currentmodule:: torch.ao.nn.intrinsic.qat\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    LinearReLU\n    ConvBn1d\n    ConvBnReLU1d\n    ConvBn2d\n    ConvBnReLU2d\n    ConvReLU2d\n    ConvBn3d\n    ConvBnReLU3d\n    ConvReLU3d\n    update_bn_stats\n    freeze_bn_stats","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":320,"to":366}}}}],["1387",{"pageContent":"LinearReLU\n    ConvBn1d\n    ConvBnReLU1d\n    ConvBn2d\n    ConvBnReLU2d\n    ConvReLU2d\n    ConvBn3d\n    ConvBnReLU3d\n    ConvReLU3d\n    update_bn_stats\n    freeze_bn_stats\n\ntorch.ao.nn.intrinsic.quantized\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: torch.ao.nn.intrinsic.quantized\n.. automodule:: torch.ao.nn.intrinsic.quantized.modules\n\n\nThis module implements the quantized implementations of fused operations\nlike conv + relu. No BatchNorm variants as it's usually folded into convolution\nfor inference.\n\n.. currentmodule:: torch.ao.nn.intrinsic.quantized\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    BNReLU2d\n    BNReLU3d\n    ConvReLU1d\n    ConvReLU2d\n    ConvReLU3d\n    LinearReLU\n\ntorch.ao.nn.intrinsic.quantized.dynamic\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: torch.ao.nn.intrinsic.quantized.dynamic\n.. automodule:: torch.ao.nn.intrinsic.quantized.dynamic.modules","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":366,"to":405}}}}],["1388",{"pageContent":"torch.ao.nn.intrinsic.quantized.dynamic\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: torch.ao.nn.intrinsic.quantized.dynamic\n.. automodule:: torch.ao.nn.intrinsic.quantized.dynamic.modules\n\nThis module implements the quantized dynamic implementations of fused operations\nlike linear + relu.\n\n.. currentmodule:: torch.ao.nn.intrinsic.quantized.dynamic\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    LinearReLU\n\ntorch.ao.nn.qat\n~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: torch.ao.nn.qat\n.. automodule:: torch.ao.nn.qat.modules\n\nThis module implements versions of the key nn modules **Conv2d()** and\n**Linear()** which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization.\n\n.. currentmodule:: torch.ao.nn.qat\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Conv2d\n    Conv3d\n    Linear","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":405,"to":440}}}}],["1389",{"pageContent":".. currentmodule:: torch.ao.nn.qat\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Conv2d\n    Conv3d\n    Linear\n\ntorch.ao.nn.qat.dynamic\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: torch.ao.nn.qat.dynamic\n.. automodule:: torch.ao.nn.qat.dynamic.modules\n\nThis module implements versions of the key nn modules such as **Linear()**\nwhich run in FP32 but with rounding applied to simulate the effect of INT8\nquantization and will be dynamically quantized during inference.\n\n.. currentmodule:: torch.ao.nn.qat.dynamic\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Linear\n\ntorch.ao.nn.quantized\n~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: torch.ao.nn.quantized\n   :noindex:\n.. automodule:: torch.ao.nn.quantized.modules\n\nThis module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and `torch.nn.ReLU`.\n\n.. currentmodule:: torch.ao.nn.quantized","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":440,"to":478}}}}],["1390",{"pageContent":"This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and `torch.nn.ReLU`.\n\n.. currentmodule:: torch.ao.nn.quantized\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    ReLU6\n    Hardswish\n    ELU\n    LeakyReLU\n    Sigmoid\n    BatchNorm2d\n    BatchNorm3d\n    Conv1d\n    Conv2d\n    Conv3d\n    ConvTranspose1d\n    ConvTranspose2d\n    ConvTranspose3d\n    Embedding\n    EmbeddingBag\n    FloatFunctional\n    FXFloatFunctional\n    QFunctional\n    Linear\n    LayerNorm\n    GroupNorm\n    InstanceNorm1d\n    InstanceNorm2d\n    InstanceNorm3d\n\ntorch.ao.nn.quantized.functional\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: torch.ao.nn.quantized.functional\n\nThis module implements the quantized versions of the functional layers such as\n~`torch.nn.functional.conv2d` and `torch.nn.functional.relu`. Note:\n:meth:`~torch.nn.functional.relu` supports quantized inputs.\n\n.. currentmodule:: torch.ao.nn.quantized.functional","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":478,"to":521}}}}],["1391",{"pageContent":".. currentmodule:: torch.ao.nn.quantized.functional\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    avg_pool2d\n    avg_pool3d\n    adaptive_avg_pool2d\n    adaptive_avg_pool3d\n    conv1d\n    conv2d\n    conv3d\n    interpolate\n    linear\n    max_pool1d\n    max_pool2d\n    celu\n    leaky_relu\n    hardtanh\n    hardswish\n    threshold\n    elu\n    hardsigmoid\n    clamp\n    upsample\n    upsample_bilinear\n    upsample_nearest\n\ntorch.ao.nn.quantizable\n~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements the quantizable versions of some of the nn layers.\nThese modules can be used in conjunction with the custom module mechanism,\nby providing the ``custom_module_config`` argument to both prepare and convert.\n\n.. currentmodule:: torch.ao.nn.quantizable\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    LSTM\n    MultiheadAttention","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":521,"to":566}}}}],["1392",{"pageContent":".. currentmodule:: torch.ao.nn.quantizable\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    LSTM\n    MultiheadAttention\n\n\ntorch.ao.nn.quantized.dynamic\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. automodule:: torch.ao.nn.quantized.dynamic\n.. automodule:: torch.ao.nn.quantized.dynamic.modules\n\nDynamically quantized :class:`~torch.nn.Linear`, :class:`~torch.nn.LSTM`,\n:class:`~torch.nn.LSTMCell`, :class:`~torch.nn.GRUCell`, and\n:class:`~torch.nn.RNNCell`.\n\n.. currentmodule:: torch.ao.nn.quantized.dynamic\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Linear\n    LSTM\n    GRU\n    RNNCell\n    LSTMCell\n    GRUCell\n\nQuantized dtypes and quantization schemes\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":566,"to":601}}}}],["1393",{"pageContent":"Linear\n    LSTM\n    GRU\n    RNNCell\n    LSTMCell\n    GRUCell\n\nQuantized dtypes and quantization schemes\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNote that operator implementations currently only\nsupport per channel quantization for weights of the **conv** and **linear**\noperators. Furthermore, the input data is\nmapped linearly to the quantized data and vice versa\nas follows:\n\n    .. math::\n\n        \\begin{aligned}\n            \\text{Quantization:}&\\\\\n            &Q_\\text{out} = \\text{clamp}(x_\\text{input}/s+z, Q_\\text{min}, Q_\\text{max})\\\\\n            \\text{Dequantization:}&\\\\\n            &x_\\text{out} = (Q_\\text{input}-z)*s\n        \\end{aligned}\n\nwhere :math:`\\text{clamp}(.)` is the same as :func:`~torch.clamp` while the\nscale :math:`s` and zero point :math:`z` are then computed\nas described in :class:`~torch.ao.quantization.observer.MinMaxObserver`, specifically:\n\n    .. math::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":601,"to":630}}}}],["1394",{"pageContent":".. math::\n\n        \\begin{aligned}\n            \\text{if Symmetric:}&\\\\\n            &s = 2 \\max(|x_\\text{min}|, x_\\text{max}) /\n                \\left( Q_\\text{max} - Q_\\text{min} \\right) \\\\\n            &z = \\begin{cases}\n                0 & \\text{if dtype is qint8} \\\\\n                128 & \\text{otherwise}\n            \\end{cases}\\\\\n            \\text{Otherwise:}&\\\\\n                &s = \\left( x_\\text{max} - x_\\text{min}  \\right ) /\n                    \\left( Q_\\text{max} - Q_\\text{min} \\right ) \\\\\n                &z = Q_\\text{min} - \\text{round}(x_\\text{min} / s)\n        \\end{aligned}\n\nwhere :math:`[x_\\text{min}, x_\\text{max}]` denotes the range of the input data while\n:math:`Q_\\text{min}` and :math:`Q_\\text{max}` are respectively the minimum and maximum values of the quantized dtype.\n\nNote that the choice of :math:`s` and :math:`z` implies that zero is represented with no quantization error whenever zero is within\nthe range of the input data or symmetric quantization is being used.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":630,"to":650}}}}],["1395",{"pageContent":"Note that the choice of :math:`s` and :math:`z` implies that zero is represented with no quantization error whenever zero is within\nthe range of the input data or symmetric quantization is being used.\n\nAdditional data types and quantization schemes can be implemented through\nthe `custom operator mechanism <https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>`_.\n\n* :attr:`torch.qscheme` — Type to describe the quantization scheme of a tensor.\n  Supported types:\n\n  * :attr:`torch.per_tensor_affine` — per tensor, asymmetric\n  * :attr:`torch.per_channel_affine` — per channel, asymmetric\n  * :attr:`torch.per_tensor_symmetric` — per tensor, symmetric\n  * :attr:`torch.per_channel_symmetric` — per channel, symmetric\n\n* ``torch.dtype`` — Type to describe the data. Supported types:\n\n  * :attr:`torch.quint8` — 8-bit unsigned integer\n  * :attr:`torch.qint8` — 8-bit signed integer\n  * :attr:`torch.qint32` — 32-bit signed integer","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":650,"to":668}}}}],["1396",{"pageContent":"* :attr:`torch.quint8` — 8-bit unsigned integer\n  * :attr:`torch.qint8` — 8-bit signed integer\n  * :attr:`torch.qint32` — 32-bit signed integer\n\n\n.. These modules are missing docs. Adding them here only for tracking\n.. automodule:: torch.ao.nn.quantizable.modules\n   :noindex:\n.. automodule:: torch.ao.nn.quantized.reference\n   :noindex:\n.. automodule:: torch.ao.nn.quantized.reference.modules\n   :noindex:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":668,"to":679}}}}],["1397",{"pageContent":".. automodule:: torch.nn.quantizable\n.. automodule:: torch.nn.qat.dynamic.modules\n.. automodule:: torch.nn.qat.modules\n.. automodule:: torch.nn.qat\n.. automodule:: torch.nn.intrinsic.qat.modules\n.. automodule:: torch.nn.quantized.dynamic\n.. automodule:: torch.nn.intrinsic\n.. automodule:: torch.nn.intrinsic.quantized.modules\n.. automodule:: torch.quantization.fx\n.. automodule:: torch.nn.intrinsic.quantized.dynamic\n.. automodule:: torch.nn.qat.dynamic\n.. automodule:: torch.nn.intrinsic.qat\n.. automodule:: torch.nn.quantized.modules\n.. automodule:: torch.nn.intrinsic.quantized\n.. automodule:: torch.nn.quantizable.modules\n.. automodule:: torch.nn.quantized\n.. automodule:: torch.nn.intrinsic.quantized.dynamic.modules\n.. automodule:: torch.nn.quantized.dynamic.modules\n.. automodule:: torch.quantization\n.. automodule:: torch.nn.intrinsic.modules","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization-support.rst","loc":{"lines":{"from":681,"to":700}}}}],["1398",{"pageContent":".. _quantization-doc:\n\nQuantization\n============\n\n.. automodule:: torch.ao.quantization\n.. automodule:: torch.ao.quantization.fx\n\n.. warning ::\n     Quantization is in beta and subject to change.\n\nIntroduction to Quantization\n----------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1,"to":13}}}}],["1399",{"pageContent":".. automodule:: torch.ao.quantization\n.. automodule:: torch.ao.quantization.fx\n\n.. warning ::\n     Quantization is in beta and subject to change.\n\nIntroduction to Quantization\n----------------------------\n\nQuantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with reduced precision rather than\nfull precision (floating point) values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements. Hardware support for INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":13,"to":32}}}}],["1400",{"pageContent":"PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision.\n\nAt lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss.\n\nQuantization API Summary\n-----------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":34,"to":49}}}}],["1401",{"pageContent":"Quantization API Summary\n-----------------------------\n\nPyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.\n\nEager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":49,"to":54}}}}],["1402",{"pageContent":"FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it's a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with ``torch.fx``). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we'll provide general guidelines, but to actually make it work, users might need to be familiar with ``torch.fx``, especially on how to make a model symbolically traceable.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":56,"to":56}}}}],["1403",{"pageContent":"New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of `using FX Graph Mode Quantization <https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html>`_ or fall back to eager mode quantization.\n\nThe following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":58,"to":60}}}}],["1404",{"pageContent":"+-----------------+-------------------+-------------------+\n|                 |Eager Mode         |FX Graph           |\n|                 |Quantization       |Mode               |\n|                 |                   |Quantization       |\n+-----------------+-------------------+-------------------+\n|Release          |beta               |prototype          |\n|Status           |                   |                   |\n+-----------------+-------------------+-------------------+\n|Operator         |Manual             |Automatic          |\n|Fusion           |                   |                   |\n+-----------------+-------------------+-------------------+\n|Quant/DeQuant    |Manual             |Automatic          |\n|Placement        |                   |                   |\n+-----------------+-------------------+-------------------+\n|Quantizing       |Supported          |Supported          |\n|Modules          |                   |                   |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":62,"to":77}}}}],["1405",{"pageContent":"+-----------------+-------------------+-------------------+\n|Quantizing       |Supported          |Supported          |\n|Modules          |                   |                   |\n+-----------------+-------------------+-------------------+\n|Quantizing       |Manual             |Automatic          |\n|Functionals/Torch|                   |                   |\n|Ops              |                   |                   |\n+-----------------+-------------------+-------------------+\n|Support for      |Limited Support    |Fully              |\n|Customization    |                   |Supported          |\n+-----------------+-------------------+-------------------+\n|Quantization Mode|Post Training      |Post Training      |\n|Support          |Quantization:      |Quantization:      |\n|                 |Static, Dynamic,   |Static, Dynamic,   |\n|                 |Weight Only        |Weight Only        |\n|                 |                   |                   |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":77,"to":92}}}}],["1406",{"pageContent":"|                 |Static, Dynamic,   |Static, Dynamic,   |\n|                 |Weight Only        |Weight Only        |\n|                 |                   |                   |\n|                 |Quantization Aware |Quantization Aware |\n|                 |Training:          |Training:          |\n|                 |Static             |Static             |\n+-----------------+-------------------+-------------------+\n|Input/Output     |``torch.nn.Module``|``torch.nn.Module``|\n|Model Type       |                   |(May need some     |\n|                 |                   |refactors to make  |\n|                 |                   |the model          |\n|                 |                   |compatible with FX |\n|                 |                   |Graph Mode         |\n|                 |                   |Quantization)      |\n+-----------------+-------------------+-------------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":92,"to":106}}}}],["1407",{"pageContent":"There are three types of quantization supported:\n\n1. dynamic quantization (weights quantized with activations read/stored in\n   floating point and quantized for compute)\n2. static quantization (weights quantized, activations quantized, calibration\n   required post training)\n3. static quantization aware training (weights quantized, activations quantized,\n   quantization numerics modeled during training)\n\nPlease see our `Introduction to Quantization on PyTorch\n<https://pytorch.org/blog/introduction-to-quantization-on-pytorch/>`_ blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.\n\nOperator coverage varies between dynamic and static quantization and is captured in the table below.\nNote that for FX quantization, the corresponding functionals are also supported.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":109,"to":124}}}}],["1408",{"pageContent":"+---------------------------+-------------------+--------------------+\n|                           |Static             | Dynamic            |\n|                           |Quantization       | Quantization       |\n+---------------------------+-------------------+--------------------+\n| | nn.Linear               | | Y               | | Y                |\n| | nn.Conv1d/2d/3d         | | Y               | | N                |\n+---------------------------+-------------------+--------------------+\n| | nn.LSTM                 | | Y (through      | | Y                |\n| |                         | | custom modules) | |                  |\n| | nn.GRU                  | | N               | | Y                |\n+---------------------------+-------------------+--------------------+\n| | nn.RNNCell              | | N               | | Y                |\n| | nn.GRUCell              | | N               | | Y                |\n| | nn.LSTMCell             | | N               | | Y                |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":126,"to":139}}}}],["1409",{"pageContent":"| | nn.GRUCell              | | N               | | Y                |\n| | nn.LSTMCell             | | N               | | Y                |\n+---------------------------+-------------------+--------------------+\n|nn.EmbeddingBag            | Y (activations    |                    |\n|                           | are in fp32)      | Y                  |\n+---------------------------+-------------------+--------------------+\n|nn.Embedding               | Y                 | N                  |\n+---------------------------+-------------------+--------------------+\n| nn.MultiheadAttention     | Y (through        | Not supported      |\n|                           | custom modules)   |                    |\n+---------------------------+-------------------+--------------------+\n| Activations               | Broadly supported | Un-changed,        |\n|                           |                   | computations       |\n|                           |                   | stay in fp32       |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":139,"to":152}}}}],["1410",{"pageContent":"|                           |                   | computations       |\n|                           |                   | stay in fp32       |\n+---------------------------+-------------------+--------------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":152,"to":154}}}}],["1411",{"pageContent":"Eager Mode Quantization\n^^^^^^^^^^^^^^^^^^^^^^^\nFor a general introduction to the quantization flow, including different types of quantization, please take a look at `General Quantization Flow`_.\n\nPost Training Dynamic Quantization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for LSTM and Transformer type models with\nsmall batch size.\n\nDiagram::\n\n  # original model\n  # all tensors and computations are in floating point\n  previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                   /\n  linear_weight_fp32","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":157,"to":177}}}}],["1412",{"pageContent":"Diagram::\n\n  # original model\n  # all tensors and computations are in floating point\n  previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                   /\n  linear_weight_fp32\n\n  # dynamically quantized model\n  # linear and LSTM weights are in int8\n  previous_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32\n                       /\n     linear_weight_int8\n\nPTDQ API Example::\n\n  import torch\n\n  # define a floating point model\n  class M(torch.nn.Module):\n      def __init__(self):\n          super().__init__()\n          self.fc = torch.nn.Linear(4, 4)\n\n      def forward(self, x):\n          x = self.fc(x)\n          return x\n\n  # create a model instance\n  model_fp32 = M()\n  # create a quantized model instance\n  model_int8 = torch.ao.quantization.quantize_dynamic(\n      model_fp32,  # the original model\n      {torch.nn.Linear},  # a set of layers to dynamically quantize\n      dtype=torch.qint8)  # the target dtype for quantized weights","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":177,"to":211}}}}],["1413",{"pageContent":"# run the model\n  input_fp32 = torch.randn(4, 4, 4, 4)\n  res = model_int8(input_fp32)\n\nTo learn more about dynamic quantization please see our `dynamic quantization tutorial\n<https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html>`_.\n\nPost Training Static Quantization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPost Training Static Quantization (PTQ static) quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Static Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.\n\nWe may need to modify the model before applying post training static quantization. Please see `Model Preparation for Eager Mode Static Quantization`_.\n\nDiagram::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":213,"to":232}}}}],["1414",{"pageContent":"We may need to modify the model before applying post training static quantization. Please see `Model Preparation for Eager Mode Static Quantization`_.\n\nDiagram::\n\n    # original model\n    # all tensors and computations are in floating point\n    previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                        /\n        linear_weight_fp32\n\n    # statically quantized model\n    # weights and activations are in int8\n    previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                        /\n      linear_weight_int8\n\nPTSQ API Example::\n\n  import torch","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":232,"to":250}}}}],["1415",{"pageContent":"PTSQ API Example::\n\n  import torch\n\n  # define a floating point model where some layers could be statically quantized\n  class M(torch.nn.Module):\n      def __init__(self):\n          super().__init__()\n          # QuantStub converts tensors from floating point to quantized\n          self.quant = torch.ao.quantization.QuantStub()\n          self.conv = torch.nn.Conv2d(1, 1, 1)\n          self.relu = torch.nn.ReLU()\n          # DeQuantStub converts tensors from quantized to floating point\n          self.dequant = torch.ao.quantization.DeQuantStub()\n\n      def forward(self, x):\n          # manually specify where tensors will be converted from floating\n          # point to quantized in the quantized model\n          x = self.quant(x)\n          x = self.conv(x)\n          x = self.relu(x)\n          # manually specify where tensors will be converted from quantized\n          # to floating point in the quantized model\n          x = self.dequant(x)\n          return x","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":250,"to":274}}}}],["1416",{"pageContent":"# create a model instance\n  model_fp32 = M()\n\n  # model must be set to eval mode for static quantization logic to work\n  model_fp32.eval()\n\n  # attach a global qconfig, which contains information about what kind\n  # of observers to attach. Use 'x86' for server inference and 'qnnpack'\n  # for mobile inference. Other quantization configurations such as selecting\n  # symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n  # can be specified here.\n  # Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n  # for server inference.\n  # model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\n  model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":276,"to":290}}}}],["1417",{"pageContent":"# Fuse the activations to preceding layers, where applicable.\n  # This needs to be done manually depending on the model architecture.\n  # Common fusions include `conv + relu` and `conv + batchnorm + relu`\n  model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n\n  # Prepare the model for static quantization. This inserts observers in\n  # the model that will observe activation tensors during calibration.\n  model_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)\n\n  # calibrate the prepared model to determine quantization parameters for activations\n  # in a real world setting, the calibration would be done with a representative dataset\n  input_fp32 = torch.randn(4, 1, 4, 4)\n  model_fp32_prepared(input_fp32)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":292,"to":304}}}}],["1418",{"pageContent":"# Convert the observed model to a quantized model. This does several things:\n  # quantizes the weights, computes and stores the scale and bias value to be\n  # used with each activation tensor, and replaces key operators with quantized\n  # implementations.\n  model_int8 = torch.ao.quantization.convert(model_fp32_prepared)\n\n  # run the model, relevant calculations will happen in int8\n  res = model_int8(input_fp32)\n\nTo learn more about static quantization, please see the `static quantization tutorial\n<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_.\n\nQuantization Aware Training for Static Quantization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":306,"to":319}}}}],["1419",{"pageContent":"Quantization Aware Training for Static Quantization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nQuantization Aware Training (QAT) models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods. We can do QAT for static, dynamic or weight only quantization.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.\n\nWe may need to modify the model before applying post training static quantization. Please see `Model Preparation for Eager Mode Static Quantization`_.\n\nDiagram::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":319,"to":333}}}}],["1420",{"pageContent":"We may need to modify the model before applying post training static quantization. Please see `Model Preparation for Eager Mode Static Quantization`_.\n\nDiagram::\n\n  # original model\n  # all tensors and computations are in floating point\n  previous_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                        /\n      linear_weight_fp32\n\n  # model with fake_quants for modeling quantization numerics during training\n  previous_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32\n                             /\n     linear_weight_fp32 -- fq\n\n  # quantized model\n  # weights and activations are in int8\n  previous_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                       /\n     linear_weight_int8\n\nQAT API Example::\n\n  import torch","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":333,"to":356}}}}],["1421",{"pageContent":"QAT API Example::\n\n  import torch\n\n  # define a floating point model where some layers could benefit from QAT\n  class M(torch.nn.Module):\n      def __init__(self):\n          super().__init__()\n          # QuantStub converts tensors from floating point to quantized\n          self.quant = torch.ao.quantization.QuantStub()\n          self.conv = torch.nn.Conv2d(1, 1, 1)\n          self.bn = torch.nn.BatchNorm2d(1)\n          self.relu = torch.nn.ReLU()\n          # DeQuantStub converts tensors from quantized to floating point\n          self.dequant = torch.ao.quantization.DeQuantStub()\n\n      def forward(self, x):\n          x = self.quant(x)\n          x = self.conv(x)\n          x = self.bn(x)\n          x = self.relu(x)\n          x = self.dequant(x)\n          return x\n\n  # create a model instance\n  model_fp32 = M()\n\n  # model must be set to eval for fusion to work\n  model_fp32.eval()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":356,"to":384}}}}],["1422",{"pageContent":"# create a model instance\n  model_fp32 = M()\n\n  # model must be set to eval for fusion to work\n  model_fp32.eval()\n\n  # attach a global qconfig, which contains information about what kind\n  # of observers to attach. Use 'x86' for server inference and 'qnnpack'\n  # for mobile inference. Other quantization configurations such as selecting\n  # symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n  # can be specified here.\n  # Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n  # for server inference.\n  # model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\n  model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n\n  # fuse the activations to preceding layers, where applicable\n  # this needs to be done manually depending on the model architecture\n  model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32,\n      [['conv', 'bn', 'relu']])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":384,"to":403}}}}],["1423",{"pageContent":"# Prepare the model for QAT. This inserts observers and fake_quants in\n  # the model needs to be set to train for QAT logic to work\n  # the model that will observe weight and activation tensors during calibration.\n  model_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused.train())\n\n  # run the training loop (not shown)\n  training_loop(model_fp32_prepared)\n\n  # Convert the observed model to a quantized model. This does several things:\n  # quantizes the weights, computes and stores the scale and bias value to be\n  # used with each activation tensor, fuses modules where appropriate,\n  # and replaces key operators with quantized implementations.\n  model_fp32_prepared.eval()\n  model_int8 = torch.ao.quantization.convert(model_fp32_prepared)\n\n  # run the model, relevant calculations will happen in int8\n  res = model_int8(input_fp32)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":405,"to":421}}}}],["1424",{"pageContent":"# run the model, relevant calculations will happen in int8\n  res = model_int8(input_fp32)\n\nTo learn more about quantization aware training, please see the `QAT\ntutorial\n<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>`_.\n\nModel Preparation for Eager Mode Static Quantization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIt is necessary to currently make some modifications to the model definition\nprior to Eager mode quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":421,"to":433}}}}],["1425",{"pageContent":"1. Convert any operations that require output requantization (and thus have\n   additional parameters) from functionals to module form (for example,\n   using ``torch.nn.ReLU`` instead of ``torch.nn.functional.relu``).\n2. Specify which parts of the model need to be quantized either by assigning\n   ``.qconfig`` attributes on submodules or by specifying ``qconfig_mapping``.\n   For example, setting ``model.conv1.qconfig = None`` means that the\n   ``model.conv`` layer will not be quantized, and setting\n   ``model.linear1.qconfig = custom_qconfig`` means that the quantization\n   settings for ``model.linear1`` will be using ``custom_qconfig`` instead\n   of the global qconfig.\n\nFor static quantization techniques which quantize activations, the user needs\nto do the following in addition:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":435,"to":447}}}}],["1426",{"pageContent":"For static quantization techniques which quantize activations, the user needs\nto do the following in addition:\n\n1. Specify where activations are quantized and de-quantized. This is done using\n   :class:`~torch.ao.quantization.QuantStub` and\n   :class:`~torch.ao.quantization.DeQuantStub` modules.\n2. Use :class:`~torch.ao.nn.quantized.FloatFunctional` to wrap tensor operations\n   that require special handling for quantization into modules. Examples\n   are operations like ``add`` and ``cat`` which require special handling to\n   determine output quantization parameters.\n3. Fuse modules: combine operations/modules into a single module to obtain\n   higher accuracy and performance. This is done using the\n   :func:`~torch.ao.quantization.fuse_modules` API, which takes in lists of modules\n   to be fused. We currently support the following fusions:\n   [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":447,"to":461}}}}],["1427",{"pageContent":"(Prototype) FX Graph Mode Quantization\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThere are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through `qconfig_mapping` (an argument of the `prepare_fx` function).\n\nFXPTQ API Example::\n\n  import torch\n  from torch.ao.quantization import (\n    get_default_qconfig_mapping,\n    get_default_qat_qconfig_mapping,\n    QConfigMapping,\n  )\n  import torch.ao.quantization.quantize_fx as quantize_fx\n  import copy\n\n  model_fp = UserModel()\n\n  #\n  # post training dynamic/weight_only quantization\n  #","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":463,"to":483}}}}],["1428",{"pageContent":"model_fp = UserModel()\n\n  #\n  # post training dynamic/weight_only quantization\n  #\n\n  # we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model\n  model_to_quantize = copy.deepcopy(model_fp)\n  model_to_quantize.eval()\n  qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)\n  # a tuple of one or more example inputs are needed to trace the model\n  example_inputs = (input_fp32)\n  # prepare\n  model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n  # no calibration needed when we only have dynamic/weight_only quantization\n  # quantize\n  model_quantized = quantize_fx.convert_fx(model_prepared)\n\n  #\n  # post training static quantization\n  #","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":483,"to":503}}}}],["1429",{"pageContent":"#\n  # post training static quantization\n  #\n\n  model_to_quantize = copy.deepcopy(model_fp)\n  qconfig_mapping = get_default_qconfig_mapping(\"qnnpack\")\n  model_to_quantize.eval()\n  # prepare\n  model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n  # calibrate (not shown)\n  # quantize\n  model_quantized = quantize_fx.convert_fx(model_prepared)\n\n  #\n  # quantization aware training for static quantization\n  #\n\n  model_to_quantize = copy.deepcopy(model_fp)\n  qconfig_mapping = get_default_qat_qconfig_mapping(\"qnnpack\")\n  model_to_quantize.train()\n  # prepare\n  model_prepared = quantize_fx.prepare_qat_fx(model_to_quantize, qconfig_mapping, example_inputs)\n  # training loop (not shown)\n  # quantize\n  model_quantized = quantize_fx.convert_fx(model_prepared)\n\n  #\n  # fusion\n  #\n  model_to_quantize = copy.deepcopy(model_fp)\n  model_fused = quantize_fx.fuse_fx(model_to_quantize)\n\nPlease see the following tutorials for more information about FX Graph Mode Quantization:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":503,"to":535}}}}],["1430",{"pageContent":"Please see the following tutorials for more information about FX Graph Mode Quantization:\n\n- `User Guide on Using FX Graph Mode Quantization <https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html>`_\n- `FX Graph Mode Post Training Static Quantization <https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html>`_\n- `FX Graph Mode Post Training Dynamic Quantization <https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html>`_\n\nQuantization Stack\n------------------------\nQuantization is the process to convert a floating point model to a quantized model. So at high level the quantization stack can be split into two parts: 1). The building blocks or abstractions for a quantized model 2). The building blocks or abstractions for the quantization flow that converts a floating point model to a quantized model","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":535,"to":543}}}}],["1431",{"pageContent":"Quantized Model\n^^^^^^^^^^^^^^^^^^^^^^^\nQuantized Tensor\n~~~~~~~~~~~~~~~~~\nIn order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero\\_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":545,"to":554}}}}],["1432",{"pageContent":"PyTorch supports both per tensor and per channel symmetric and asymmetric quantization. Per tensor means that all the values within the tensor are quantized the same way with the same quantization parameters. Per channel means that for each dimension, typically the channel dimension of a tensor, the values in the tensor are quantized with different quantization parameters. This allows for less error in converting tensors to quantized values since outlier values would only impact the channel it was in, instead of the entire Tensor.\n\nThe mapping is performed by converting the floating point tensors using\n\n.. image:: math-quantizer-equation.png\n   :width: 40%\n\nNote that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error.\n\nHere are a few key attributes for quantized Tensor:\n\n* QScheme (torch.qscheme): a enum that specifies the way we quantize the Tensor","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":556,"to":569}}}}],["1433",{"pageContent":"Here are a few key attributes for quantized Tensor:\n\n* QScheme (torch.qscheme): a enum that specifies the way we quantize the Tensor\n\n  * torch.per_tensor_affine\n  * torch.per_tensor_symmetric\n  * torch.per_channel_affine\n  * torch.per_channel_symmetric\n\n* dtype (torch.dtype): data type of the quantized Tensor\n\n  * torch.quint8\n  * torch.qint8\n  * torch.qint32\n  * torch.float16\n\n* quantization parameters (varies based on QScheme): parameters for the chosen way of quantization\n\n  * torch.per_tensor_affine would have quantization parameters of\n\n    * scale (float)\n    * zero_point (int)\n  * torch.per_channel_affine would have quantization parameters of\n\n    * per_channel_scales (list of float)\n    * per_channel_zero_points (list of int)\n    * axis (int)\n\nQuantize and Dequantize\n~~~~~~~~~~~~~~~~~~~~~~~\nThe input and output of a model are floating point Tensors, but activations in the quantized model are quantized, so we need operators to convert between floating point and quantized Tensors.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":569,"to":599}}}}],["1434",{"pageContent":"* Quantize (float -> quantized)\n\n  * torch.quantize_per_tensor(x, scale, zero_point, dtype)\n  * torch.quantize_per_channel(x, scales, zero_points, axis, dtype)\n  * torch.quantize_per_tensor_dynamic(x, dtype, reduce_range)\n  * to(torch.float16)\n\n* Dequantize (quantized -> float)\n\n  * quantized_tensor.dequantize() - calling dequantize on a torch.float16 Tensor will convert the Tensor back to torch.float\n  * torch.dequantize(x)\n\nQuantized Operators/Modules\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n* Quantized Operator are the operators that takes quantized Tensor as inputs, and outputs a quantized Tensor.\n* Quantized Modules are PyTorch Modules that performs quantized operations. They are typically defined for weighted operations like linear and conv.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":601,"to":616}}}}],["1435",{"pageContent":"Quantized Engine\n~~~~~~~~~~~~~~~~~~~~\nWhen a quantized model is executed, the qengine (torch.backends.quantized.engine) specifies which backend is to be used for execution. It is important to ensure that the qengine is compatible with the quantized model in terms of value range of quantized activation and weights.\n\nQuantization Flow\n^^^^^^^^^^^^^^^^^^^^^^^\n\nObserver and FakeQuantize\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n* Observer are PyTorch Modules used to:\n\n  * collect tensor statistics like min value and max value of the Tensor passing through the observer\n  * and calculate quantization parameters based on the collected tensor statistics\n* FakeQuantize are PyTorch Modules used to:\n\n  * simulate quantization (performing quantize/dequantize) for a Tensor in the network\n  * it can calculate quantization parameters based on the collected statistics from observer, or it can learn the quantization parameters as well","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":618,"to":634}}}}],["1436",{"pageContent":"QConfig\n~~~~~~~~~~~\n* QConfig is a namedtuple of Observer or FakeQuantize Module class that can are configurable with qscheme, dtype etc. it is used to configure how an operator should be observed\n\n  * Quantization configuration for an operator/module\n\n    * different types of Observer/FakeQuantize\n    * dtype\n    * qscheme\n    * quant_min/quant_max: can be used to simulate lower precision Tensors\n  * Currently supports configuration for activation and weight\n  * We insert input/weight/output observer based on the qconfig that is configured for a given operator or module\n\nGeneral Quantization Flow\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nIn general, the flow is the following\n\n* prepare\n\n  * insert Observer/FakeQuantize modules based on user specified qconfig\n\n* calibrate/train (depending on post training quantization or quantization aware training)\n\n  * allow Observers to collect statistics or FakeQuantize modules to learn the quantization parameters\n\n* convert","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":636,"to":661}}}}],["1437",{"pageContent":"* allow Observers to collect statistics or FakeQuantize modules to learn the quantization parameters\n\n* convert\n\n  * convert a calibrated/trained model to a quantized model\n\nThere are different modes of quantization, they can be classified in two ways:\n\nIn terms of where we apply the quantization flow, we have:\n\n1. Post Training Quantization (apply quantization after training, quantization parameters are calculated based on sample calibration data)\n2. Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data)\n\nAnd in terms of how we quantize the operators, we can have:\n\n- Weight Only Quantization (only weight is statically quantized)\n- Dynamic Quantization (weight is statically quantized, activation is dynamically quantized)\n- Static Quantization (both weight and activations are statically quantized)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":661,"to":678}}}}],["1438",{"pageContent":"We can mix different ways of quantizing operators in the same quantization flow. For example, we can have post training quantization that has both statically and dynamically quantized operators.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":680,"to":680}}}}],["1439",{"pageContent":"Quantization Support Matrix\n--------------------------------------\nQuantization Mode Support\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+-----------------------------+------------------------------------------------------+----------------+----------------+------------+-----------------+\n|                             |Quantization                                          |Dataset         | Works Best For | Accuracy   |      Notes      |\n|                             |Mode                                                  |Requirement     |                |            |                 |\n+-----------------------------+---------------------------------+--------------------+----------------+----------------+------------+-----------------+\n|Post Training Quantization   |Dynamic/Weight Only Quantization |activation          |None            |LSTM, MLP,      |good        |Easy to use,     |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":682,"to":690}}}}],["1440",{"pageContent":"|Post Training Quantization   |Dynamic/Weight Only Quantization |activation          |None            |LSTM, MLP,      |good        |Easy to use,     |\n|                             |                                 |dynamically         |                |Embedding,      |            |close to static  |\n|                             |                                 |quantized (fp16,    |                |Transformer     |            |quantization when|\n|                             |                                 |int8) or not        |                |                |            |performance is   |\n|                             |                                 |quantized, weight   |                |                |            |compute or memory|\n|                             |                                 |statically quantized|                |                |            |bound due to     |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":690,"to":695}}}}],["1441",{"pageContent":"|                             |                                 |statically quantized|                |                |            |bound due to     |\n|                             |                                 |(fp16, int8, in4)   |                |                |            |weights          |\n|                             +---------------------------------+--------------------+----------------+----------------+------------+-----------------+\n|                             |Static Quantization              |activation and      |calibration     |CNN             |good        |Provides best    |\n|                             |                                 |weights statically  |dataset         |                |            |perf, may have   |\n|                             |                                 |quantized (int8)    |                |                |            |big impact on    |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":695,"to":700}}}}],["1442",{"pageContent":"|                             |                                 |quantized (int8)    |                |                |            |big impact on    |\n|                             |                                 |                    |                |                |            |accuracy, good   |\n|                             |                                 |                    |                |                |            |for hardwares    |\n|                             |                                 |                    |                |                |            |that only support|\n|                             |                                 |                    |                |                |            |int8 computation |\n+-----------------------------+---------------------------------+--------------------+----------------+----------------+------------+-----------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":700,"to":705}}}}],["1443",{"pageContent":"+-----------------------------+---------------------------------+--------------------+----------------+----------------+------------+-----------------+\n|                             |Dynamic Quantization             |activation and      |fine-tuning     |MLP, Embedding  |best        |Limited support  |\n|                             |                                 |weight are fake     |dataset         |                |            |for now          |\n|                             |                                 |quantized           |                |                |            |                 |\n|                             +---------------------------------+--------------------+----------------+----------------+------------+-----------------+\n|                             |Static Quantization              |activation and      |fine-tuning     |CNN, MLP,       |best        |Typically used   |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":705,"to":710}}}}],["1444",{"pageContent":"|                             |Static Quantization              |activation and      |fine-tuning     |CNN, MLP,       |best        |Typically used   |\n|                             |                                 |weight are fake     |dataset         |Embedding       |            |when static      |\n|                             |                                 |quantized           |                |                |            |quantization     |\n|                             |                                 |                    |                |                |            |leads to bad     |\n|                             |                                 |                    |                |                |            |accuracy, and    |\n|                             |                                 |                    |                |                |            |used to close the|","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":710,"to":715}}}}],["1445",{"pageContent":"|                             |                                 |                    |                |                |            |used to close the|\n|                             |                                 |                    |                |                |            |accuracy gap     |\n|Quantization Aware Training  |                                 |                    |                |                |            |                 |\n+-----------------------------+---------------------------------+--------------------+----------------+----------------+------------+-----------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":715,"to":718}}}}],["1446",{"pageContent":"Please see our `Introduction to Quantization on Pytorch\n<https://pytorch.org/blog/introduction-to-quantization-on-pytorch/>`_ blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes.\n\nQuantization Flow Support\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nPyTorch provides two modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.\n\nEager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":720,"to":729}}}}],["1447",{"pageContent":"FX Graph Mode Quantization is an automated quantization framework in PyTorch, and currently it's a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with ``torch.fx``). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we'll provide general guidelines, but to actually make it work, users might need to be familiar with ``torch.fx``, especially on how to make a model symbolically traceable.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":731,"to":731}}}}],["1448",{"pageContent":"New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of `using FX Graph Mode Quantization <https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html>`_ or fall back to eager mode quantization.\n\nThe following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":731,"to":733}}}}],["1449",{"pageContent":"+-----------------+-------------------+-------------------+\n|                 |Eager Mode         |FX Graph           |\n|                 |Quantization       |Mode               |\n|                 |                   |Quantization       |\n+-----------------+-------------------+-------------------+\n|Release          |beta               |prototype          |\n|Status           |                   |                   |\n+-----------------+-------------------+-------------------+\n|Operator         |Manual             |Automatic          |\n|Fusion           |                   |                   |\n+-----------------+-------------------+-------------------+\n|Quant/DeQuant    |Manual             |Automatic          |\n|Placement        |                   |                   |\n+-----------------+-------------------+-------------------+\n|Quantizing       |Supported          |Supported          |\n|Modules          |                   |                   |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":735,"to":750}}}}],["1450",{"pageContent":"+-----------------+-------------------+-------------------+\n|Quantizing       |Supported          |Supported          |\n|Modules          |                   |                   |\n+-----------------+-------------------+-------------------+\n|Quantizing       |Manual             |Automatic          |\n|Functionals/Torch|                   |                   |\n|Ops              |                   |                   |\n+-----------------+-------------------+-------------------+\n|Support for      |Limited Support    |Fully              |\n|Customization    |                   |Supported          |\n+-----------------+-------------------+-------------------+\n|Quantization Mode|Post Training      |Post Training      |\n|Support          |Quantization:      |Quantization:      |\n|                 |Static, Dynamic,   |Static, Dynamic,   |\n|                 |Weight Only        |Weight Only        |\n|                 |                   |                   |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":750,"to":765}}}}],["1451",{"pageContent":"|                 |Static, Dynamic,   |Static, Dynamic,   |\n|                 |Weight Only        |Weight Only        |\n|                 |                   |                   |\n|                 |Quantization Aware |Quantization Aware |\n|                 |Training:          |Training:          |\n|                 |Static             |Static             |\n+-----------------+-------------------+-------------------+\n|Input/Output     |``torch.nn.Module``|``torch.nn.Module``|\n|Model Type       |                   |(May need some     |\n|                 |                   |refactors to make  |\n|                 |                   |the model          |\n|                 |                   |compatible with FX |\n|                 |                   |Graph Mode         |\n|                 |                   |Quantization)      |\n+-----------------+-------------------+-------------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":765,"to":779}}}}],["1452",{"pageContent":"Backend/Hardware Support\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n+-----------------+---------------+------------+------------+------------+\n|Hardware         |Kernel Library |Eager Mode  |FX Graph    |Quantization|\n|                 |               |Quantization|Mode        |Mode Support|\n|                 |               |            |Quantization|            |\n+-----------------+---------------+------------+------------+------------+\n|server CPU       |fbgemm/onednn  |Supported                |All         |\n|                 |               |                         |Supported   |\n+-----------------+---------------+                         |            +\n|mobile CPU       |qnnpack/xnnpack|                         |            |\n|                 |               |                         |            |\n+-----------------+---------------+------------+------------+------------+\n|server GPU       |TensorRT (early|Not support |Supported   |Static      |","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1419,"to":1432}}}}],["1453",{"pageContent":"+-----------------+---------------+------------+------------+------------+\n|server GPU       |TensorRT (early|Not support |Supported   |Static      |\n|                 |prototype)     |this it     |            |Quantization|\n|                 |               |requires a  |            |            |\n|                 |               |graph       |            |            |\n+-----------------+---------------+------------+------------+------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1432,"to":1437}}}}],["1454",{"pageContent":"Today, PyTorch supports the following backends for running quantized operators efficiently:\n\n* x86 CPUs with AVX2 support or higher (without AVX2 some operations have inefficient implementations), via `x86` optimized by `fbgemm <https://github.com/pytorch/FBGEMM>`_ and `onednn <https://github.com/oneapi-src/oneDNN>`_ (see the details at `RFC <https://github.com/pytorch/pytorch/issues/83888>`_)\n* ARM CPUs (typically found in mobile/embedded devices), via `qnnpack <https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/quantized/cpu/qnnpack>`_\n* (early prototype) support for NVidia GPU via `TensorRT <https://developer.nvidia.com/tensorrt>`_ through `fx2trt` (to be open sourced)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1439,"to":1443}}}}],["1455",{"pageContent":"Note for native CPU backends\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nWe expose both `x86` and `qnnpack` with the same native pytorch quantized operators, so we need additional flag to distinguish between them. The corresponding implementation of  `x86` and `qnnpack` is chosen automatically based on the PyTorch build mode, though users have the option to override this by setting `torch.backends.quantization.engine` to `x86` or `qnnpack`.\n\nWhen preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. The qconfig controls the type of observers used\nduring the quantization passes. The qengine controls whether `x86` or `qnnpack`\nspecific packing function is used when packing weights for\nlinear and convolution functions and modules. For example:\n\nDefault settings for x86::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1446,"to":1457}}}}],["1456",{"pageContent":"Default settings for x86::\n\n    # set the qconfig for PTQ\n    # Note: the old 'fbgemm' is still available but 'x86' is the recommended default on x86 CPUs\n    qconfig = torch.ao.quantization.get_default_qconfig('x86')\n    # or, set the qconfig for QAT\n    qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n    # set the qengine to control weight packing\n    torch.backends.quantized.engine = 'x86'\n\nDefault settings for qnnpack::\n\n    # set the qconfig for PTQ\n    qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')\n    # or, set the qconfig for QAT\n    qconfig = torch.ao.quantization.get_default_qat_qconfig('qnnpack')\n    # set the qengine to control weight packing\n    torch.backends.quantized.engine = 'qnnpack'\n\nOperator Support\n^^^^^^^^^^^^^^^^^^^^\n\nOperator coverage varies between dynamic and static quantization and is captured in the table below.\nNote that for FX Graph Mode Quantization, the corresponding functionals are also supported.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1457,"to":1480}}}}],["1457",{"pageContent":"+---------------------------+-------------------+--------------------+\n|                           |Static             | Dynamic            |\n|                           |Quantization       | Quantization       |\n+---------------------------+-------------------+--------------------+\n| | nn.Linear               | | Y               | | Y                |\n| | nn.Conv1d/2d/3d         | | Y               | | N                |\n+---------------------------+-------------------+--------------------+\n| | nn.LSTM                 | | N               | | Y                |\n| | nn.GRU                  | | N               | | Y                |\n+---------------------------+-------------------+--------------------+\n| | nn.RNNCell              | | N               | | Y                |\n| | nn.GRUCell              | | N               | | Y                |\n| | nn.LSTMCell             | | N               | | Y                |\n+---------------------------+-------------------+--------------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1482,"to":1495}}}}],["1458",{"pageContent":"| | nn.LSTMCell             | | N               | | Y                |\n+---------------------------+-------------------+--------------------+\n|nn.EmbeddingBag            | Y (activations    |                    |\n|                           | are in fp32)      | Y                  |\n+---------------------------+-------------------+--------------------+\n|nn.Embedding               | Y                 | N                  |\n+---------------------------+-------------------+--------------------+\n|nn.MultiheadAttention      |Not Supported      | Not supported      |\n+---------------------------+-------------------+--------------------+\n|Activations                |Broadly supported  | Un-changed,        |\n|                           |                   | computations       |\n|                           |                   | stay in fp32       |\n+---------------------------+-------------------+--------------------+","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1495,"to":1507}}}}],["1459",{"pageContent":"Note: this will be updated with some information generated from native backend_config_dict soon.\n\nQuantization API Reference\n---------------------------\n\nThe :doc:`Quantization API Reference <quantization-support>` contains documentation\nof quantization APIs, such as quantization passes, quantized tensor operations,\nand supported quantized modules and functions.\n\n.. toctree::\n    :hidden:\n\n    quantization-support\n\nQuantization Backend Configuration\n----------------------------------\n\nThe :doc:`Quantization Backend Configuration <quantization-backend-configuration>` contains documentation\non how to configure the quantization workflows for various backends.\n\n.. toctree::\n    :hidden:\n\n    quantization-backend-configuration\n\nQuantization Accuracy Debugging\n-------------------------------\n\nThe :doc:`Quantization Accuracy Debugging <quantization-accuracy-debugging>` contains documentation\non how to debug quantization accuracy.\n\n.. toctree::\n    :hidden:\n\n    quantization-accuracy-debugging","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1509,"to":1543}}}}],["1460",{"pageContent":"The :doc:`Quantization Accuracy Debugging <quantization-accuracy-debugging>` contains documentation\non how to debug quantization accuracy.\n\n.. toctree::\n    :hidden:\n\n    quantization-accuracy-debugging\n\nQuantization Customizations\n---------------------------\n\nWhile default implementations of observers to select the scale factor and bias\nbased on observed tensor data are provided, developers can provide their own\nquantization functions. Quantization can be applied selectively to different\nparts of the model or configured differently for different parts of the model.\n\nWe also provide support for per channel quantization for **conv1d()**, **conv2d()**,\n**conv3d()** and **linear()**.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1543,"to":1560}}}}],["1461",{"pageContent":"We also provide support for per channel quantization for **conv1d()**, **conv2d()**,\n**conv3d()** and **linear()**.\n\nQuantization workflows work by adding (e.g. adding observers as\n``.observer`` submodule) or replacing (e.g. converting ``nn.Conv2d`` to\n``nn.quantized.Conv2d``) submodules in the model's module hierarchy. It\nmeans that the model stays a regular ``nn.Module``-based instance throughout the\nprocess and thus can work with the rest of PyTorch APIs.\n\nQuantization Custom Module API\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nBoth Eager mode and FX graph mode quantization APIs provide a hook for the user\nto specify module quantized in a custom way, with user defined logic for\nobservation and quantization. The user needs to specify:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1560,"to":1574}}}}],["1462",{"pageContent":"1. The Python type of the source fp32 module (existing in the model)\n2. The Python type of the observed module (provided by user). This module needs\n   to define a `from_float` function which defines how the observed module is\n   created from the original fp32 module.\n3. The Python type of the quantized module (provided by user). This module needs\n   to define a `from_observed` function which defines how the quantized module is\n   created from the observed module.\n4. A configuration describing (1), (2), (3) above, passed to the quantization APIs.\n\n\nThe framework will then do the following:\n\n1. during the `prepare` module swaps, it will convert every module of type\n   specified in (1) to the type specified in (2), using the `from_float` function of\n   the class in (2).\n2. during the `convert` module swaps, it will convert every module of type\n   specified in (2) to the type specified in (3), using the `from_observed` function\n   of the class in (3).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1576,"to":1593}}}}],["1463",{"pageContent":"Currently, there is a requirement that `ObservedCustomModule` will have a single\nTensor output, and an observer will be added by the framework (not by the user)\non that output. The observer will be stored under the `activation_post_process` key\nas an attribute of the custom module instance. Relaxing these restrictions may\nbe done at a future time.\n\nCustom API Example::\n\n  import torch\n  import torch.ao.nn.quantized as nnq\n  from torch.ao.quantization import QConfigMapping\n  import torch.ao.quantization.quantize_fx\n\n  # original fp32 module to replace\n  class CustomModule(torch.nn.Module):\n      def __init__(self):\n          super().__init__()\n          self.linear = torch.nn.Linear(3, 3)\n\n      def forward(self, x):\n          return self.linear(x)\n\n  # custom observed module, provided by user\n  class ObservedCustomModule(torch.nn.Module):\n      def __init__(self, linear):\n          super().__init__()\n          self.linear = linear\n\n      def forward(self, x):\n          return self.linear(x)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1595,"to":1624}}}}],["1464",{"pageContent":"def forward(self, x):\n          return self.linear(x)\n\n      @classmethod\n      def from_float(cls, float_module):\n          assert hasattr(float_module, 'qconfig')\n          observed = cls(float_module.linear)\n          observed.qconfig = float_module.qconfig\n          return observed\n\n  # custom quantized module, provided by user\n  class StaticQuantCustomModule(torch.nn.Module):\n      def __init__(self, linear):\n          super().__init__()\n          self.linear = linear\n\n      def forward(self, x):\n          return self.linear(x)\n\n      @classmethod\n      def from_observed(cls, observed_module):\n          assert hasattr(observed_module, 'qconfig')\n          assert hasattr(observed_module, 'activation_post_process')\n          observed_module.linear.activation_post_process = \\\n              observed_module.activation_post_process\n          quantized = cls(nnq.Linear.from_float(observed_module.linear))\n          return quantized\n\n  #\n  # example API call (Eager mode quantization)\n  #","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1624,"to":1654}}}}],["1465",{"pageContent":"m = torch.nn.Sequential(CustomModule()).eval()\n  prepare_custom_config_dict = {\n      \"float_to_observed_custom_module_class\": {\n          CustomModule: ObservedCustomModule\n      }\n  }\n  convert_custom_config_dict = {\n      \"observed_to_quantized_custom_module_class\": {\n          ObservedCustomModule: StaticQuantCustomModule\n      }\n  }\n  m.qconfig = torch.ao.quantization.default_qconfig\n  mp = torch.ao.quantization.prepare(\n      m, prepare_custom_config_dict=prepare_custom_config_dict)\n  # calibration (not shown)\n  mq = torch.ao.quantization.convert(\n      mp, convert_custom_config_dict=convert_custom_config_dict)\n  #\n  # example API call (FX graph mode quantization)\n  #\n  m = torch.nn.Sequential(CustomModule()).eval()\n  qconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_qconfig)\n  prepare_custom_config_dict = {\n      \"float_to_observed_custom_module_class\": {\n          \"static\": {\n              CustomModule: ObservedCustomModule,\n          }\n      }\n  }","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1656,"to":1684}}}}],["1466",{"pageContent":"prepare_custom_config_dict = {\n      \"float_to_observed_custom_module_class\": {\n          \"static\": {\n              CustomModule: ObservedCustomModule,\n          }\n      }\n  }\n  convert_custom_config_dict = {\n      \"observed_to_quantized_custom_module_class\": {\n          \"static\": {\n              ObservedCustomModule: StaticQuantCustomModule,\n          }\n      }\n  }\n  mp = torch.ao.quantization.quantize_fx.prepare_fx(\n      m, qconfig_mapping, torch.randn(3,3), prepare_custom_config=prepare_custom_config_dict)\n  # calibration (not shown)\n  mq = torch.ao.quantization.quantize_fx.convert_fx(\n      mp, convert_custom_config=convert_custom_config_dict)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1684,"to":1702}}}}],["1467",{"pageContent":"Best Practices\n--------------\n\n1. If you are using the ``x86`` backend, we need to use 7 bits instead of 8 bits. Make sure you reduce the range for the ``quant\\_min``, ``quant\\_max``, e.g.\nif ``dtype`` is ``torch.quint8``, make sure to set a custom ``quant_min`` to be ``0`` and ``quant_max`` to be ``127`` (``255`` / ``2``)\nif ``dtype`` is ``torch.qint8``, make sure to set a custom ``quant_min`` to be ``-64`` (``-128`` / ``2``) and ``quant_max`` to be ``63`` (``127`` / ``2``), we already set this correctly if\nyou call the `torch.ao.quantization.get_default_qconfig(backend)` or `torch.ao.quantization.get_default_qat_qconfig(backend)` function to get the default ``qconfig`` for\n``x86`` or ``qnnpack`` backend\n\nFrequently Asked Questions\n--------------------------\n\n1. How can I do quantized inference on GPU?:\n\n   We don't have official GPU support yet, but this is an area of active development, you can find more information\n   `here <https://github.com/pytorch/pytorch/issues/87395>`_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1704,"to":1719}}}}],["1468",{"pageContent":"We don't have official GPU support yet, but this is an area of active development, you can find more information\n   `here <https://github.com/pytorch/pytorch/issues/87395>`_\n\n2. Where can I get ONNX support for my quantized model?:\n\n   You can open an issue in `GitHub - onnx/onnx <https://github.com/onnx/onnx>`_  when you encounter problems with ONNX,\n   or reach out to people in this list: `PyTorch Governance | Maintainers | ONNX exporter <https://pytorch.org/docs/stable/community/persons_of_interest.html#onnx-exporter>`_\n\n3. How can I use quantization with LSTM's?:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1719,"to":1727}}}}],["1469",{"pageContent":"3. How can I use quantization with LSTM's?:\n\n   LSTM is supported through our custom module api in both eager mode and fx graph mode quantization. Examples can be found at\n   Eager Mode: `pytorch/test_quantized_op.py TestQuantizedOps.test_custom_module_lstm <https://github.com/pytorch/pytorch/blob/9b88dcf248e717ca6c3f8c5e11f600825547a561/test/quantization/core/test_quantized_op.py#L2782>`_\n   FX Graph Mode: `pytorch/test_quantize_fx.py TestQuantizeFx.test_static_lstm <https://github.com/pytorch/pytorch/blob/9b88dcf248e717ca6c3f8c5e11f600825547a561/test/quantization/fx/test_quantize_fx.py#L4116>`_\n\nCommon Errors\n---------------------------------------\n\nPassing a non-quantized Tensor into a quantized kernel\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf you see an error similar to::\n\n  RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1727,"to":1741}}}}],["1470",{"pageContent":"If you see an error similar to::\n\n  RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...\n\nThis means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use ``torch.ao.quantization.QuantStub`` to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example::\n\n  class M(torch.nn.Module):\n      def __init__(self):\n          super().__init__()\n          self.quant = torch.ao.quantization.QuantStub()\n          self.conv = torch.nn.Conv2d(1, 1, 1)\n\n      def forward(self, x):\n          # during the convert step, this will be replaced with a\n          # `quantize_per_tensor` call\n          x = self.quant(x)\n          x = self.conv(x)\n          return x\n\nPassing a quantized Tensor into a non-quantized kernel\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf you see an error similar to::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1741,"to":1766}}}}],["1471",{"pageContent":"Passing a quantized Tensor into a non-quantized kernel\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf you see an error similar to::\n\n  RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend.\n\nThis means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use ``torch.ao.quantization.DeQuantStub`` to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example::\n\n  class M(torch.nn.Module):\n      def __init__(self):\n          super().__init__()\n          self.quant = torch.ao.quantization.QuantStub()\n          self.conv1 = torch.nn.Conv2d(1, 1, 1)\n          # this module will not be quantized (see `qconfig = None` logic below)\n          self.conv2 = torch.nn.Conv2d(1, 1, 1)\n          self.dequant = torch.ao.quantization.DeQuantStub()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1766,"to":1785}}}}],["1472",{"pageContent":"def forward(self, x):\n          # during the convert step, this will be replaced with a\n          # `quantize_per_tensor` call\n          x = self.quant(x)\n          x = self.conv1(x)\n          # during the convert step, this will be replaced with a\n          # `dequantize` call\n          x = self.dequant(x)\n          x = self.conv2(x)\n          return x\n\n  m = M()\n  m.qconfig = some_qconfig\n  # turn off quantization for conv2\n  m.conv2.qconfig = None\n\nSaving and Loading Quantized models\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWhen calling ``torch.load`` on a quantized model, if you see an error like::\n\n  AttributeError: 'LinearPackedParams' object has no attribute '_modules'\n\nThis is because directly saving and loading a quantized model using ``torch.save`` and ``torch.load``\nis not supported. To save/load quantized models, the following ways can be used:\n\n1. Saving/Loading the quantized model state_dict\n\nAn example::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1787,"to":1815}}}}],["1473",{"pageContent":"1. Saving/Loading the quantized model state_dict\n\nAn example::\n\n  class M(torch.nn.Module):\n      def __init__(self):\n          super().__init__()\n          self.linear = nn.Linear(5, 5)\n          self.relu = nn.ReLU()\n\n      def forward(self, x):\n          x = self.linear(x)\n          x = self.relu(x)\n          return x\n\n  m = M().eval()\n  prepare_orig = prepare_fx(m, {'' : default_qconfig})\n  prepare_orig(torch.rand(5, 5))\n  quantized_orig = convert_fx(prepare_orig)\n\n  # Save/load using state_dict\n  b = io.BytesIO()\n  torch.save(quantized_orig.state_dict(), b)\n\n  m2 = M().eval()\n  prepared = prepare_fx(m2, {'' : default_qconfig})\n  quantized = convert_fx(prepared)\n  b.seek(0)\n  quantized.load_state_dict(torch.load(b))\n\n2. Saving/Loading scripted quantized models using ``torch.jit.save`` and ``torch.jit.load``\n\nAn example::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1815,"to":1847}}}}],["1474",{"pageContent":"2. Saving/Loading scripted quantized models using ``torch.jit.save`` and ``torch.jit.load``\n\nAn example::\n\n  # Note: using the same model M from previous example\n  m = M().eval()\n  prepare_orig = prepare_fx(m, {'' : default_qconfig})\n  prepare_orig(torch.rand(5, 5))\n  quantized_orig = convert_fx(prepare_orig)\n\n  # save/load using scripted model\n  scripted = torch.jit.script(quantized_orig)\n  b = io.BytesIO()\n  torch.jit.save(scripted, b)\n  b.seek(0)\n  scripted_quantized = torch.jit.load(b)\n\nSymbolic Trace Error when using FX Graph Mode Quantization\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nSymbolic traceability is a requirement for `(Prototype) FX Graph Mode Quantization`_, so if you pass a PyTorch Model that is not symbolically traceable to `torch.ao.quantization.prepare_fx` or `torch.ao.quantization.prepare_qat_fx`, we might see an error like the following::\n\n  torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1847,"to":1868}}}}],["1475",{"pageContent":"torch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\nPlease take a look at `Limitations of Symbolic Tracing <https://docs-preview.pytorch.org/76223/fx.html#limitations-of-symbolic-tracing>`_ and use - `User Guide on Using FX Graph Mode Quantization <https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html>`_ to workaround the problem.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1868,"to":1870}}}}],["1476",{"pageContent":".. torch.ao is missing documentation. Since part of it is mentioned here, adding them here for now.\n.. They are here for tracking purposes until they are more permanently fixed.\n.. py:module:: torch.ao\n.. py:module:: torch.ao.nn\n.. py:module:: torch.ao.nn.quantizable\n.. py:module:: torch.ao.nn.quantizable.modules\n.. py:module:: torch.ao.nn.quantized\n.. py:module:: torch.ao.nn.quantized.reference\n.. py:module:: torch.ao.nn.quantized.reference.modules\n.. py:module:: torch.ao.nn.sparse\n.. py:module:: torch.ao.nn.sparse.quantized\n.. py:module:: torch.ao.nn.sparse.quantized.dynamic\n.. py:module:: torch.ao.ns\n.. py:module:: torch.ao.ns.fx\n.. py:module:: torch.ao.quantization.backend_config\n.. py:module:: torch.ao.pruning\n.. py:module:: torch.ao.pruning.scheduler\n.. py:module:: torch.ao.pruning.sparsifier","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/quantization.rst","loc":{"lines":{"from":1873,"to":1890}}}}],["1477",{"pageContent":"torch.random\n===================================\n\n.. currentmodule:: torch.random\n\n.. automodule:: torch.random\n   :members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/random.rst","loc":{"lines":{"from":1,"to":7}}}}],["1478",{"pageContent":":orphan:\n\n.. _distributed-autograd-design:\n\nDistributed Autograd Design\n===========================\n\nThis note will present the detailed design for distributed autograd and walk\nthrough the internals of the same. Make sure you're familiar with\n:ref:`autograd-mechanics` and the :ref:`distributed-rpc-framework` before\nproceeding.\n\nBackground\n^^^^^^^^^^\n\nLet's say you have two nodes and a very simple model partitioned across two\nnodes. This can be implemented using :mod:`torch.distributed.rpc` as follows:\n\n.. code::\n\n  import torch\n  import torch.distributed.rpc as rpc\n\n  def my_add(t1, t2):\n    return torch.add(t1, t2)\n\n  # On worker 0:\n  t1 = torch.rand((3, 3), requires_grad=True)\n  t2 = torch.rand((3, 3), requires_grad=True)\n\n  # Perform some computation remotely.\n  t3 = rpc.rpc_sync(\"worker1\", my_add, args=(t1, t2))\n\n  # Perform some computation locally based on remote result.\n  t4 = torch.rand((3, 3), requires_grad=True)\n  t5 = torch.mul(t3, t4)\n\n  # Compute some loss.\n  loss = t5.sum()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":1,"to":39}}}}],["1479",{"pageContent":"# Perform some computation locally based on remote result.\n  t4 = torch.rand((3, 3), requires_grad=True)\n  t5 = torch.mul(t3, t4)\n\n  # Compute some loss.\n  loss = t5.sum()\n\nThe main motivation behind distributed autograd is to enable running a backward\npass on such distributed models with the ``loss`` that we've computed and\nrecord appropriate gradients for all tensors that require gradients.\n\n.. attaching_send_recv_functions:\n\nAutograd recording during the forward pass\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPyTorch builds the autograd graph during the forward pass and this graph is\nused to execute the backward pass. For more details see\n:ref:`how-autograd-encodes-history`.\n\nFor distributed autograd, we need to keep track of all RPCs during the forward\npass to ensure the backward pass is executed appropriately. For this purpose,\nwe attach ``send`` and ``recv`` functions to the autograd graph when we perform\nan RPC.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":39,"to":62}}}}],["1480",{"pageContent":"- The ``send`` function is attached to the source of the RPC and its output\n  edges point to the autograd function for the input tensors of the RPC.\n  The input for this function during the backward pass is received from the\n  destination as the output of the appropriate ``recv`` function.\n- The ``recv`` function is attached to the destination of the RPC and its\n  inputs are retrieved from operators executed on the destination using the\n  input tensors. The output gradients of this function are sent to the source\n  node to the appropriate ``send`` function during the backward pass.\n- Each ``send-recv`` pair is assigned a globally unique ``autograd_message_id``\n  to uniquely identify the pair. This is useful to look up the corresponding\n  function on a remote node during the backward pass.\n- For :ref:`rref`, whenever we call :meth:`torch.distributed.rpc.RRef.to_here`\n  we attach an appropriate ``send-recv`` pair for the tensors involved.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":64,"to":76}}}}],["1481",{"pageContent":"As an example, this is what the autograd graph for our example above would look\nlike (t5.sum() excluded for simplicity):\n\n.. image:: ../_static/img/distributed_autograd/send_recv_functions.png\n\n.. autograd_context:\n\nDistributed Autograd Context\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nEach forward and backward pass that uses distributed autograd is assigned a\nunique :class:`torch.distributed.autograd.context` and this context has a\nglobally unique ``autograd_context_id``. This context is created on each node\nas needed.\n\nThis context serves the following purpose:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":78,"to":93}}}}],["1482",{"pageContent":"1. Multiple nodes running distributed backward passes might accumulate\n   gradients on the same tensor and as a result the ``.grad`` field of the\n   tensor would have gradients from a variety of distributed backward passes\n   before we have the opportunity to run the optimizer. This is similar to\n   calling :meth:`torch.autograd.backward` multiple times locally. In order to\n   provide a way of separating out the gradients for each backward pass, the\n   gradients are accumulated in the :class:`torch.distributed.autograd.context`\n   for each backward pass.\n2. During the forward pass we store the ``send`` and ``recv`` functions for\n   each autograd pass in this context. This ensures we hold references to the\n   appropriate nodes in the autograd graph to keep it alive. In addition to\n   this, it is easy to look up the appropriate ``send`` and ``recv`` functions\n   during the backward pass.\n3. In general we also use this context to store some metadata for each\n   distributed autograd pass.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":95,"to":109}}}}],["1483",{"pageContent":"|\n\nFrom the user's perspective the autograd context is setup as follows:\n\n.. code::\n\n  import torch.distributed.autograd as dist_autograd\n  with dist_autograd.context() as context_id:\n    loss = model.forward()\n    dist_autograd.backward(context_id, loss)\n\nIt is important to note that your model's forward pass must be invoked within\nthe distributed autograd context manager, as a valid context is needed in\norder to ensure that all ``send`` and ``recv`` functions are stored properly\nto run the backward pass across all participating nodes.\n\nDistributed Backward Pass\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn this section we outline the challenge of computing dependencies accurately\nduring a distributed backward pass and describe a couple of algorithms (with\ntradeoffs) on how we can execute a distributed backward pass.\n\nComputing dependencies\n----------------------\n\nConsider the following piece of code being run on a single machine\n\n.. code::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":111,"to":139}}}}],["1484",{"pageContent":"Computing dependencies\n----------------------\n\nConsider the following piece of code being run on a single machine\n\n.. code::\n\n  import torch\n  a = torch.rand((3, 3), requires_grad=True)\n  b = torch.rand((3, 3), requires_grad=True)\n  c = torch.rand((3, 3), requires_grad=True)\n  d = a + b\n  e = b * c\n  d.sum.().backward()\n\nThis is what the autograd graph for the code above would look like:\n\n.. image:: ../_static/img/distributed_autograd/local_dependencies.png\n  :scale: 80%","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":139,"to":157}}}}],["1485",{"pageContent":"This is what the autograd graph for the code above would look like:\n\n.. image:: ../_static/img/distributed_autograd/local_dependencies.png\n  :scale: 80%\n\nThe first step the autograd engine performs as part of the backward pass is\ncomputing the number of dependencies for each node in the autograd graph. This\nhelps the autograd engine know when a node in the graph is ready for execution.\nThe numbers in brackets for ``add(1)`` and ``mul(0)`` denote the number of\ndependencies. As you can see, this means during the backward pass the ``add``\nnode needs 1 input and the ``mul`` node doesn't need any inputs (in other\nwords doesn't need to be executed). The local autograd engine computes these\ndependencies by traversing the graph from the root nodes (``d`` in this case).\n\nThe fact that certain nodes in the autograd graph might not be executed in the\nbackward pass poses a challenge for distributed autograd. Consider this piece\nof code which uses RPC.\n\n.. code::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":157,"to":175}}}}],["1486",{"pageContent":"The fact that certain nodes in the autograd graph might not be executed in the\nbackward pass poses a challenge for distributed autograd. Consider this piece\nof code which uses RPC.\n\n.. code::\n\n  import torch\n  import torch.distributed.rpc as rpc\n\n  a = torch.rand((3, 3), requires_grad=True)\n  b = torch.rand((3, 3), requires_grad=True)\n  c = torch.rand((3, 3), requires_grad=True)\n\n  d = rpc.rpc_sync(\"worker1\", torch.add, args=(a, b))\n  e = rpc.rpc_sync(\"worker1\", torch.mul, args=(b, c))\n  loss = d.sum()\n\nThe associated autograd graph for the code above would be:\n\n.. image:: ../_static/img/distributed_autograd/distributed_dependencies.png\n\nComputing dependencies of this distributed autograd graph is much more\nchallenging and requires some overhead (either in terms of computation or\nnetwork communication).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":175,"to":198}}}}],["1487",{"pageContent":"Computing dependencies of this distributed autograd graph is much more\nchallenging and requires some overhead (either in terms of computation or\nnetwork communication).\n\nFor performance sensitive applications we can avoid a\nlot of overhead by assuming every ``send`` and ``recv`` function are valid as\npart of the backward pass (most applications don't perform RPCs that aren't\nused). This simplifies the distributed autograd algorithm and is much more\nefficient, but at the cost that the application needs to be aware of the\nlimitations. This algorithm is called the `FAST mode algorithm`_ and is\ndescribed in detail below.\n\nIn the general case it might not be necessary that every ``send`` and ``recv``\nfunction is valid as part of the backward pass. To address this, we have\nproposed a `SMART mode algorithm`_ which is described in a later section.\nPlease note that currently, only the `FAST` mode algorithm is implemented.\n\n.. _fast-mode-algorithm:\n\nFAST mode algorithm\n-------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":198,"to":218}}}}],["1488",{"pageContent":".. _fast-mode-algorithm:\n\nFAST mode algorithm\n-------------------\n\nThe key assumption of this algorithm is that each ``send`` function has a\ndependency of 1 when we run a backward pass. In other words, we assume we'll\nreceive a gradient over RPC from another node.\n\nThe algorithm is as follows:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":218,"to":227}}}}],["1489",{"pageContent":"1. We start from the worker which has the roots for the backward pass\n   (all roots must be local).\n2. Lookup all the ``send`` functions for the current\n   `Distributed Autograd Context`_.\n3. Compute dependencies locally starting from the provided roots and all the\n   ``send`` functions we retrieved.\n4. After computing dependencies, kick off the local autograd engine with the\n   provided roots.\n5. When the autograd engine executes the ``recv`` function, the ``recv``\n   function sends the input gradients via RPC to the appropriate worker.\n   Each ``recv`` function knows the destination worker id since it is recorded\n   as part of the forward pass. The ``recv`` function also sends over the\n   ``autograd_context_id`` and ``autograd_message_id`` to the remote host.\n6. When this request is received on the remote host, we use the\n   ``autograd_context_id`` and ``autograd_message_id`` to look up the\n   appropriate ``send`` function.\n7. If this is the first time a worker has received a request for the given","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":229,"to":245}}}}],["1490",{"pageContent":"``autograd_context_id`` and ``autograd_message_id`` to look up the\n   appropriate ``send`` function.\n7. If this is the first time a worker has received a request for the given\n   ``autograd_context_id``, it will compute dependencies locally as described\n   in points 1-3 above.\n8. The ``send`` function retrieved in 6. is then enqueued for execution on the\n   local autograd engine for that worker.\n9. Finally, instead of accumulating the gradients on the ``.grad`` field of the\n   Tensor, we accumulate the gradients separately per\n   `Distributed Autograd Context`_. The gradients are stored in a\n   ``Dict[Tensor, Tensor]``, which is basically a map from Tensor to its\n   associated gradient and this map can be retrieved using the\n   :meth:`~torch.distributed.autograd.get_gradients` API.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":245,"to":257}}}}],["1491",{"pageContent":"|\n\nAs an example the complete code with distributed autograd would be as follows:\n\n.. code::\n\n  import torch\n  import torch.distributed.autograd as dist_autograd\n  import torch.distributed.rpc as rpc\n\n  def my_add(t1, t2):\n    return torch.add(t1, t2)\n\n  # On worker 0:\n\n  # Setup the autograd context. Computations that take\n  # part in the distributed backward pass must be within\n  # the distributed autograd context manager.\n  with dist_autograd.context() as context_id:\n    t1 = torch.rand((3, 3), requires_grad=True)\n    t2 = torch.rand((3, 3), requires_grad=True)\n\n    # Perform some computation remotely.\n    t3 = rpc.rpc_sync(\"worker1\", my_add, args=(t1, t2))\n\n    # Perform some computation locally based on remote result.\n    t4 = torch.rand((3, 3), requires_grad=True)\n    t5 = torch.mul(t3, t4)\n\n    # Compute some loss.\n    loss = t5.sum()\n\n    # Run the backward pass.\n    dist_autograd.backward(context_id, [loss])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":259,"to":292}}}}],["1492",{"pageContent":"# Compute some loss.\n    loss = t5.sum()\n\n    # Run the backward pass.\n    dist_autograd.backward(context_id, [loss])\n\n    # Retrieve the gradients from the context.\n    dist_autograd.get_gradients(context_id)\n\nThe distributed autograd graph with dependencies would be as follows (t5.sum() excluded for simplicity):\n\n.. image:: ../_static/img/distributed_autograd/distributed_dependencies_computed.png\n\nThe `FAST mode algorithm`_ applied to the above example would be as follows:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":292,"to":305}}}}],["1493",{"pageContent":"1. On ``Worker 0`` we start from the roots ``loss`` and ``send1`` to compute\n   dependencies. As a result ``send1`` is marked with a dependency of 1 and ``mul``\n   on ``Worker 0`` is marked with a dependency of 1.\n2. Now, we kickoff the local autograd engine on ``Worker 0``. We first execute\n   the ``mul`` function, accumulate its output in the autograd context as the\n   gradient for ``t4``. Then, we execute ``recv2`` which sends the gradients to\n   ``Worker 1``.\n3. Since this is the first time ``Worker 1`` has heard about this backward pass,\n   it starts dependency computation and marks the dependencies for ``send2``,\n   ``add`` and ``recv1`` appropriately.\n4. Next, we enqueue ``send2`` on the local autograd engine of ``Worker 1``, which\n   in turn executes ``add`` and ``recv1``.\n5. When ``recv1`` is executed it sends the gradients over to ``Worker 0``.\n6. Since ``Worker 0`` has already computed dependencies for this backward pass,\n   it just enqueues and executes ``send1`` locally.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":307,"to":321}}}}],["1494",{"pageContent":"6. Since ``Worker 0`` has already computed dependencies for this backward pass,\n   it just enqueues and executes ``send1`` locally.\n7. Finally, gradients for ``t1``, ``t2`` and ``t4`` are accumulated in the\n   `Distributed Autograd Context`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":321,"to":324}}}}],["1495",{"pageContent":"SMART mode algorithm\n--------------------\nFull details of this algorithm are still in the works, but for the general idea\nyou can refer to **Distributed Autograd Algorithm Smart mode** section in the\n`RFC`_.\n\nDistributed Optimizer\n^^^^^^^^^^^^^^^^^^^^^\n\nThe :class:`~torch.distributed.optim.DistributedOptimizer` operates as follows:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":326,"to":335}}}}],["1496",{"pageContent":"1. Takes a list of remote parameters (:class:`~torch.distributed.rpc.RRef`) to\n   optimize. These could also be local parameters wrapped within a local\n   ``RRef``.\n2. Takes a :class:`~torch.optim.Optimizer` class as the local\n   optimizer to run on all distinct ``RRef`` owners.\n3. The distributed optimizer creates an instance of the local ``Optimizer`` on\n   each of the worker nodes and holds an ``RRef`` to them.\n4. When :meth:`torch.distributed.optim.DistributedOptimizer.step` is invoked,\n   the distributed optimizer uses RPC to remotely execute all the local\n   optimizers on the appropriate remote workers. A distributed autograd\n   ``context_id`` must be provided as input to\n   :meth:`torch.distributed.optim.DistributedOptimizer.step`. This is used\n   by local optimizers to apply gradients stored in the corresponding\n   context.\n5. If multiple concurrent distributed optimizers are updating the same\n   parameters on a worker, these updates are serialized via a lock.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":337,"to":352}}}}],["1497",{"pageContent":"Simple end to end example\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPutting it all together, the following is a simple end to end example using\ndistributed autograd and the distributed optimizer. If the code is placed into a\nfile called \"dist_autograd_simple.py\", it can be run with the command\n:code:`MASTER_ADDR=\"localhost\" MASTER_PORT=29500 python dist_autograd_simple.py`:\n\n.. code::\n\n  import torch\n  import torch.multiprocessing as mp\n  import torch.distributed.autograd as dist_autograd\n  from torch.distributed import rpc\n  from torch import optim\n  from torch.distributed.optim import DistributedOptimizer\n\n  def random_tensor():\n      return torch.rand((3, 3), requires_grad=True)\n\n  def _run_process(rank, dst_rank, world_size):\n      name = \"worker{}\".format(rank)\n      dst_name = \"worker{}\".format(dst_rank)\n\n      # Initialize RPC.\n      rpc.init_rpc(\n          name=name,\n          rank=rank,\n          world_size=world_size\n      )","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":354,"to":383}}}}],["1498",{"pageContent":"# Initialize RPC.\n      rpc.init_rpc(\n          name=name,\n          rank=rank,\n          world_size=world_size\n      )\n\n      # Use a distributed autograd context.\n      with dist_autograd.context() as context_id:\n          # Forward pass (create references on remote nodes).\n          rref1 = rpc.remote(dst_name, random_tensor)\n          rref2 = rpc.remote(dst_name, random_tensor)\n          loss = rref1.to_here() + rref2.to_here()\n\n          # Backward pass (run distributed autograd).\n          dist_autograd.backward(context_id, [loss.sum()])\n\n          # Build DistributedOptimizer.\n          dist_optim = DistributedOptimizer(\n          optim.SGD,\n          [rref1, rref2],\n          lr=0.05,\n          )\n\n          # Run the distributed optimizer step.\n          dist_optim.step(context_id)\n\n  def run_process(rank, world_size):\n      dst_rank = (rank + 1) % world_size\n      _run_process(rank, dst_rank, world_size)\n      rpc.shutdown()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":383,"to":413}}}}],["1499",{"pageContent":"def run_process(rank, world_size):\n      dst_rank = (rank + 1) % world_size\n      _run_process(rank, dst_rank, world_size)\n      rpc.shutdown()\n\n  if __name__ == '__main__':\n    # Run world_size workers\n    world_size = 2\n    mp.spawn(run_process, args=(world_size,), nprocs=world_size)\n\n.. _RFC: https://github.com/pytorch/pytorch/issues/23110","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/distributed_autograd.rst","loc":{"lines":{"from":413,"to":423}}}}],["1500",{"pageContent":":orphan:\n\n.. _remote-reference-protocol:\n\nRemote Reference Protocol\n=========================\n\nThis note describes the design details of Remote Reference protocol and walks\nthrough message flows in different scenarios. Make sure you're familiar with the\n:ref:`distributed-rpc-framework` before proceeding.\n\nBackground\n^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":1,"to":13}}}}],["1501",{"pageContent":"Background\n^^^^^^^^^^\n\nRRef stands for Remote REFerence. It is a reference of an object which is\nlocated on the local or remote worker, and transparently handles reference\ncounting under the hood. Conceptually, it can be considered as a distributed\nshared pointer. Applications can create an RRef by calling\n:meth:`~torch.distributed.rpc.remote`. Each RRef is owned by the callee worker\nof the :meth:`~torch.distributed.rpc.remote` call (i.e., owner) and can be used\nby multiple users. The owner stores the real data and keeps track of the global\nreference count. Every RRef can be uniquely identified by a global ``RRefId``,\nwhich is assigned at the time of creation on the caller of the\n:meth:`~torch.distributed.rpc.remote` call.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":13,"to":25}}}}],["1502",{"pageContent":"On the owner worker, there is only one ``OwnerRRef`` instance, which contains\nthe real data, while on user workers, there can be as many ``UserRRefs`` as\nnecessary, and ``UserRRef`` does not hold the data. All usage on the owner will\nretrieve the unique ``OwnerRRef`` instance using the globally unique ``RRefId``.\nA ``UserRRef`` will be created when it is used as an argument or return value in\n:meth:`~torch.distributed.rpc.rpc_sync`,\n:meth:`~torch.distributed.rpc.rpc_async` or\n:meth:`~torch.distributed.rpc.remote` invocation, and the owner will be notified\naccording to update the reference count. An ``OwnerRRef`` and its data will be\ndeleted when there is no ``UserRRef`` instances globally and there are no\nreference to the ``OwnerRRef`` on the owner as well.\n\n\nAssumptions\n^^^^^^^^^^^\n\nRRef protocol is designed with the following assumptions.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":27,"to":43}}}}],["1503",{"pageContent":"Assumptions\n^^^^^^^^^^^\n\nRRef protocol is designed with the following assumptions.\n\n- **Transient Network Failures**: The RRef design handles transient\n  network failures by retrying messages. It cannot handle node crashes or\n  permanent network partitions. When those incidents occur, the application\n  should take down all workers, revert to the previous checkpoint, and resume\n  training.\n- **Non-idempotent UDFs**: We assume the user functions (UDF) provided to\n  :meth:`~torch.distributed.rpc.rpc_sync`,\n  :meth:`~torch.distributed.rpc.rpc_async` or\n  :meth:`~torch.distributed.rpc.remote` are not idempotent and therefore\n  cannot be retried. However, internal RRef control messages are idempotent and\n  retried upon message failure.\n- **Out of Order Message Delivery**: We do not assume message delivery order\n  between any pair of nodes, because both sender and receiver are using multiple\n  threads. There is no guarantee on which message will be processed first.\n\n\nRRef Lifetime\n^^^^^^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":43,"to":65}}}}],["1504",{"pageContent":"RRef Lifetime\n^^^^^^^^^^^^^\n\nThe goal of the protocol is to delete an ``OwnerRRef`` at an appropriate time.\nThe right time to delete an ``OwnerRRef`` is when there are no living\n``UserRRef`` instances and user code is not holding references to the\n``OwnerRRef`` either. The tricky part is to determine if there are any living\n``UserRRef`` instances.\n\nDesign Reasoning\n----------------\n\nA user can get a ``UserRRef`` in three situations:\n\n1) Receiving a ``UserRRef`` from the owner.\n2) Receiving a ``UserRRef`` from another user.\n3) Creating a new ``UserRRef`` owned by another worker.\n\n\nCase 1 is the simplest where the owner passes its RRef to a user, where the\nowner calls :meth:`~torch.distributed.rpc.rpc_sync`,\n:meth:`~torch.distributed.rpc.rpc_async`, or\n:meth:`~torch.distributed.rpc.remote` and uses its RRef as an argument. In this\ncase a new ``UserRRef`` will be created on the user. As the owner is the caller,\nit can easily update its local reference count on the ``OwnerRRef``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":65,"to":89}}}}],["1505",{"pageContent":"The only requirement is that any\n``UserRRef`` must notify the owner upon destruction. Hence, we need the first\nguarantee:\n\n**G1. The owner will be notified when any UserRRef is deleted.**\n\nAs messages might come delayed or out-of-order, we need one more guarantee to\nmake sure the delete message is not processed too soon. If A sends a message to\nB that involves an RRef, we call the RRef on A (the parent RRef) and the RRef on B\n(the child RRef).\n\n**G2. Parent RRef will NOT be deleted until the child RRef is confirmed by the\nowner.**","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":91,"to":103}}}}],["1506",{"pageContent":"**G2. Parent RRef will NOT be deleted until the child RRef is confirmed by the\nowner.**\n\nIn cases 2 and 3, it is possible that the owner has only partial or no knowledge\nat all about the RRef fork graph. For example, an RRef could be\nconstructed on a user, and before the owner receives any RPC call, the\ncreator user might have already shared the RRef with other users, and those\nusers could further share the RRef. One invariant is that the fork graph of\nany RRef is always a tree, because forking an RRef always\ncreates a new ``UserRRef`` instance on the callee (except if the callee is the\nowner), and hence every RRef has a single parent.\n\nThe owner's view on any ``UserRRef`` in the tree has three stages:\n\n.. code::\n\n  1) unknown -> 2) known -> 3) deleted.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":103,"to":119}}}}],["1507",{"pageContent":"The owner's view on any ``UserRRef`` in the tree has three stages:\n\n.. code::\n\n  1) unknown -> 2) known -> 3) deleted.\n\nThe owner's view of the entire tree keeps changing. The owner deletes its\n``OwnerRRef`` instance when it thinks there are no living ``UserRRef``\ninstances, i.e.,\nwhen ``OwnerRRef`` is deleted, all ``UserRRef`` instances could be either indeed\ndeleted or unknown. The dangerous case is when some forks are unknown and others\nare deleted.\n\n**G2** trivially guarantees that no parent ``UserRRef`` can be deleted before\nthe owner knows all of its children ``UserRRef`` instances. However, it is\npossible that the child ``UserRRef`` may be deleted before the owner knows its\nparent ``UserRRef``.\n\nConsider the following example, where the ``OwnerRRef`` forks to A, then A forks\nto Y, and Y forks to Z:\n\n.. code::\n\n  OwnerRRef -> A -> Y -> Z","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":119,"to":142}}}}],["1508",{"pageContent":"Consider the following example, where the ``OwnerRRef`` forks to A, then A forks\nto Y, and Y forks to Z:\n\n.. code::\n\n  OwnerRRef -> A -> Y -> Z\n\nIf all of Z's messages, including the delete message, are processed by the\nowner before Y's messages. the owner will learn of Z's deletion befores\nknowing Y exists. Nevertheless, this does not cause any problem. Because, at least\none of Y's ancestors will be alive (A) and it will\nprevent the owner from deleting the ``OwnerRRef``. More specifically, if the\nowner does not know Y, A cannot be deleted due to **G2**, and the owner knows A\nsince it is A's parent.\n\nThings get a little trickier if the RRef is created on a user:\n\n\n.. code::\n\n  OwnerRRef\n      ^\n      |\n      A -> Y -> Z","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":142,"to":165}}}}],["1509",{"pageContent":"Things get a little trickier if the RRef is created on a user:\n\n\n.. code::\n\n  OwnerRRef\n      ^\n      |\n      A -> Y -> Z\n\n\nIf Z calls :meth:`~torch.distributed.rpc.RRef.to_here` on the ``UserRRef``, the\nowner at least knows A when Z is deleted, because otherwise,\n:meth:`~torch.distributed.rpc.RRef.to_here` wouldn't finish. If Z does not call\n:meth:`~torch.distributed.rpc.RRef.to_here`, it is possible that the owner\nreceives all messages from Z before any message from A and Y. In this case, as\nthe real data of the ``OwnerRRef`` has not been created yet, there is nothing to\nbe deleted either. It is the same as Z does not exist at all. Hence, it's still\nOK.\n\nImplementation\n--------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":165,"to":186}}}}],["1510",{"pageContent":"Implementation\n--------------\n\n**G1** is implemented by sending out a delete message in ``UserRRef``\ndestructor. To provide **G2**, the parent ``UserRRef`` is put into a context\nwhenever it is forked, indexed by the new ``ForkId``. The parent ``UserRRef`` is\nonly removed from the context when it receives an acknowledgement message (ACK)\nfrom the child, and the child will only send out the ACK when it is confirmed by\nthe owner.\n\n\nProtocol Scenarios\n^^^^^^^^^^^^^^^^^^\n\nLet's now discuss how the above designs translate to the protocol in four\nscenarios.\n\nUser Share RRef with Owner as Return Value\n------------------------------------------\n\n\n.. code::\n\n  import torch\n  import torch.distributed.rpc as rpc\n\n  # on worker A\n  rref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))\n  # say the rref has RRefId 100 and ForkId 1\n  rref.to_here()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":186,"to":215}}}}],["1511",{"pageContent":".. code::\n\n  import torch\n  import torch.distributed.rpc as rpc\n\n  # on worker A\n  rref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))\n  # say the rref has RRefId 100 and ForkId 1\n  rref.to_here()\n\n\nIn this case, the ``UserRRef`` is created on the user worker A, then it is\npassed to the owner worker B together with the remote message, and then B\ncreates the ``OwnerRRef``. The method :meth:`~torch.distributed.rpc.remote`\nreturns immediately, meaning that the ``UserRRef`` can be forked/used before\nthe owner knows about it.\n\nOn the owner, when receiving the :meth:`~torch.distributed.rpc.remote` call, it\nwill create the ``OwnerRRef``, and returns an ACK to acknowledge ``{100, 1}``\n(``RRefId``, ``ForkId``). Only after receiving this ACK, can A delete its\n``UserRRef``. This involves both **G1** and **G2**. **G1** is obvious. For\n**G2**, the ``OwnerRRef`` is a child of the ``UserRRef``, and the ``UserRRef``\nis not deleted until it receives the ACK from the owner.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":215,"to":237}}}}],["1512",{"pageContent":".. image:: https://user-images\\.githubusercontent\\.com/16999635/69164772-98181300-0abe-11ea-93a7-9ad9f757cd94.png\n    :alt: user_to_owner_ret.png\n    :width: 500 px\n\nThe diagram above shows the message flow, where solid arrow contains user\nfunction and dashed arrow are builtin messages. Note that the first two messages\nfrom A to B (:meth:`~torch.distributed.rpc.remote` and\n:meth:`~torch.distributed.rpc.RRef.to_here`) may\narrive at B in any order, but the final delete message will only be sent out\nwhen:\n\n- B acknowledges ``UserRRef {100, 1}`` (G2), and\n- Python GC agrees to delete the local ``UserRRef`` instance. This occurs when\n  the RRef is no longer in scope and is eligible for garbage collection.\n\n\n\nUser Share RRef with Owner as Argument\n--------------------------------------\n\n.. code::\n\n  import torch\n  import torch.distributed.rpc as rpc\n\n  # on worker A and worker B\n  def func(rref):\n    pass","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":239,"to":266}}}}],["1513",{"pageContent":"User Share RRef with Owner as Argument\n--------------------------------------\n\n.. code::\n\n  import torch\n  import torch.distributed.rpc as rpc\n\n  # on worker A and worker B\n  def func(rref):\n    pass\n\n  # on worker A\n  rref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))\n  # say the rref has RRefId 100 and ForkId 1\n  rpc.rpc_async('B', func, args=(rref, ))\n\n\nIn this case, after creating the ``UserRRef`` on A, A uses it as an argument in\na followup RPC call to B. A will keep ``UserRRef {100, 1}`` alive until it\nreceives the acknowledge from B (**G2**, not the return value of the RPC call).\nThis is necessary because A should not send out the delete message until all\nprevious messages are received, otherwise, the ``OwnerRRef`` could be\ndeleted before usage as we do not guarantee message delivery order. This is done\nby creating a child ``ForkId`` of RRef, holding them in a map until receives the\nowner confirms the child ``ForkId``. The figure below shows the message flow.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":266,"to":291}}}}],["1514",{"pageContent":".. image:: https://user-images.githubusercontent.com/16999635/69164845-b67e0e80-0abe-11ea-93fa-d24674e75a2b.png\n    :alt: user_to_owner_arg.png\n    :width: 500 px\n\n\nNote that the ``UserRRef`` could be deleted on B before func finishes or even\nstarts. However this is OK, as at the time B sends out ACK for the child\n``ForkId``, it already acquired the ``OwnerRRef`` instance, which would prevent\nit been deleted too soon.\n\n\nOwner Share RRef with User\n--------------------------\n\nOwner to user is the simplest case, where the owner can update reference\ncounting locally, and does not need any additional control message to notify\nothers. Regarding **G2**, it is same as the parent receives the ACK from the\nowner immediately, as the parent is the owner.\n\n.. code::\n\n  import torch\n  import torch.distributed.rpc as RRef, rpc\n\n  # on worker B and worker C\n  def func(rref):\n    pass","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":293,"to":319}}}}],["1515",{"pageContent":".. code::\n\n  import torch\n  import torch.distributed.rpc as RRef, rpc\n\n  # on worker B and worker C\n  def func(rref):\n    pass\n\n  # on worker B, creating a local RRef\n  rref = RRef(\"data\")\n  # say the rref has RRefId 100\n  dist.rpc_async('C', func, args=(rref, ))\n\n\n.. image:: https://user-images.githubusercontent.com/16999635/69164921-c990de80-0abe-11ea-9250-d32ad00cf4ae.png\n    :alt: owner_to_user.png\n    :width: 500 px\n\nThe figure above shows the message flow. Note that when the ``OwnerRRef`` exits\nscope after the rpc_async call, it will not be deleted, because internally\nthere is a map to hold it alive if there is any known forks, in which case is\n``UserRRef {100, 1}``. (**G2**)\n\n\nUser Share RRef with User\n-------------------------\n\nThis is the most complicated case where caller user (parent ``UserRRef``),\ncallee user (child ``UserRRef``), and the owner all need to get involved.\n\n.. code::\n\n  import torch\n  import torch.distributed.rpc as rpc\n\n  # on worker A and worker C\n  def func(rref):\n    pass","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":319,"to":357}}}}],["1516",{"pageContent":".. code::\n\n  import torch\n  import torch.distributed.rpc as rpc\n\n  # on worker A and worker C\n  def func(rref):\n    pass\n\n  # on worker A\n  rref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))\n  # say the rref has RRefId 100 and ForkId 1\n  rpc.rpc_async('C', func, args=(rref, ))\n\n.. image:: https://user-images.githubusercontent.com/16999635/69164971-d6adcd80-0abe-11ea-971d-6b7af131f0fd.png\n    :alt: user_to_user.png\n    :width: 500 px\n\nWhen C receives the child ``UserRRef`` from A, it sends out a fork request to\nthe owner B. Later, when the B confirms the ``UserRRef`` on C, C will perform\ntwo actions in parallel: 1) send out the child ACK to A ,and 2) run the user\nprovided function. During this time, the parent (A) will hold its\n``UserRRef {100, 1}`` alive to achieve **G2**.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc/rref.rst","loc":{"lines":{"from":357,"to":379}}}}],["1517",{"pageContent":".. _distributed-rpc-framework:\n\nDistributed RPC Framework\n=========================\n\nThe distributed RPC framework provides mechanisms for multi-machine model\ntraining through a set of primitives to allow for remote communication, and a\nhigher-level API to automatically differentiate models split across several\nmachines.\n\n.. warning ::\n     APIs in the RPC package are stable. There are multiple ongoing work items\n     to improve performance and error handling, which will ship in future releases.\n\n.. warning ::\n    CUDA support was introduced in PyTorch 1.9 and is still a **beta** feature.\n    Not all features of the RPC package are yet compatible with CUDA support and\n    thus their use is discouraged. These unsupported features include: RRefs,\n    JIT compatibility, dist autograd and dist optimizer, and profiling. These\n    shortcomings will be addressed in future releases.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":1,"to":20}}}}],["1518",{"pageContent":".. note ::\n    Please refer to `PyTorch Distributed Overview <https://pytorch.org/tutorials/beginner/dist_overview.html>`__\n    for a brief introduction to all features related to distributed training.\n\nBasics\n------\n\nThe distributed RPC framework makes it easy to run functions remotely, supports\nreferencing remote objects without copying the real data around, and provides\nautograd and optimizer APIs to transparently run backward and update parameters\nacross RPC boundaries. These features can be categorized into four sets of APIs.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":22,"to":32}}}}],["1519",{"pageContent":"1) **Remote Procedure Call (RPC)** supports running a function on the specified\n   destination worker with the given arguments and getting the return value back\n   or creating a reference to the return value. There are three main RPC APIs:\n   :meth:`~torch.distributed.rpc.rpc_sync` (synchronous),\n   :meth:`~torch.distributed.rpc.rpc_async` (asynchronous), and\n   :meth:`~torch.distributed.rpc.remote` (asynchronous and returns a reference\n   to the remote return value). Use the synchronous API if the user code cannot\n   proceed without the return value. Otherwise, use the asynchronous API to get\n   a future, and wait on the future when the return value is needed on the\n   caller. The :meth:`~torch.distributed.rpc.remote` API is useful when the\n   requirement is to create something remotely but never need to fetch it to\n   the caller. Imagine the case that a driver process is setting up a parameter\n   server and a trainer. The driver can create an embedding table on the","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":34,"to":46}}}}],["1520",{"pageContent":"the caller. Imagine the case that a driver process is setting up a parameter\n   server and a trainer. The driver can create an embedding table on the\n   parameter server and then share the reference to the embedding table with the\n   trainer, but itself will never use the embedding table locally. In this case,\n   :meth:`~torch.distributed.rpc.rpc_sync` and\n   :meth:`~torch.distributed.rpc.rpc_async` are no longer appropriate, as they\n   always imply that the return value will be returned to the caller\n   immediately or in the future.\n2) **Remote Reference (RRef)** serves as a distributed shared pointer to a local\n   or remote object. It can be shared with other workers and reference counting\n   will be handled transparently. Each RRef only has one owner and the object\n   only lives on that owner. Non-owner workers holding RRefs can get copies of\n   the object from the owner by explicitly requesting it. This is useful when","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":46,"to":58}}}}],["1521",{"pageContent":"only lives on that owner. Non-owner workers holding RRefs can get copies of\n   the object from the owner by explicitly requesting it. This is useful when\n   a worker needs to access some data object, but itself is neither the creator\n   (the caller of :meth:`~torch.distributed.rpc.remote`) or the owner of the\n   object. The distributed optimizer, as we will discuss below, is one example\n   of such use cases.\n3) **Distributed Autograd** stitches together local autograd engines on all the\n   workers involved in the forward pass, and automatically reach out to them\n   during the backward pass to compute gradients. This is especially helpful if\n   the forward pass needs to span multiple machines when conducting, e.g.,\n   distributed model parallel training, parameter-server training, etc. With\n   this feature, user code no longer needs to worry about how to send gradients\n   across RPC boundaries and in which order should the local autograd engines","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":58,"to":70}}}}],["1522",{"pageContent":"this feature, user code no longer needs to worry about how to send gradients\n   across RPC boundaries and in which order should the local autograd engines\n   be launched, which can become quite complicated where there are nested and\n   inter-dependent RPC calls in the forward pass.\n4) **Distributed Optimizer**'s constructor takes a\n   :meth:`~torch.optim.Optimizer` (e.g., :meth:`~torch.optim.SGD`,\n   :meth:`~torch.optim.Adagrad`, etc.) and a list of parameter RRefs, creates an\n   :meth:`~torch.optim.Optimizer` instance on each distinct RRef owner, and\n   updates parameters accordingly when running ``step()``. When you have\n   distributed forward and backward passes, parameters and gradients will be\n   scattered across multiple workers, and hence it requires an optimizer on each\n   of the involved workers. Distributed Optimizer wraps all those local\n   optimizers into one, and provides a concise constructor and ``step()`` API.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":70,"to":82}}}}],["1523",{"pageContent":".. _rpc:\n\nRPC\n---\n\nBefore using RPC and distributed autograd primitives, initialization must take\nplace. To initialize the RPC framework we need to use\n:meth:`~torch.distributed.rpc.init_rpc` which would initialize the RPC\nframework, RRef framework and distributed autograd.\n\n.. automodule:: torch.distributed.rpc\n.. autofunction:: init_rpc\n\nThe following APIs allow users to remotely execute functions as well as create\nreferences (RRefs) to remote data objects. In these APIs, when passing a\n``Tensor`` as an argument or a return value, the destination worker will try to\ncreate a ``Tensor`` with the same meta (i.e., shape, stride, etc.). We\nintentionally disallow transmitting CUDA tensors because it might crash if the\ndevice lists on source and destination workers do not match. In such cases,\napplications can always explicitly move the input tensors to CPU on the caller\nand move it to the desired devices on the callee if necessary.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":85,"to":105}}}}],["1524",{"pageContent":".. warning::\n  TorchScript support in RPC is a prototype feature and subject to change. Since\n  v1.5.0, ``torch.distributed.rpc`` supports calling TorchScript functions as\n  RPC target functions, and this will help improve parallelism on the callee\n  side as executing TorchScript functions does not require GIL.\n\n\n.. autofunction:: rpc_sync\n.. autofunction:: rpc_async\n.. autofunction:: remote\n.. autofunction:: get_worker_info\n.. autofunction:: shutdown\n.. autoclass:: WorkerInfo\n    :members:\n\n\nThe RPC package also provides decorators which allow applications to specify\nhow a given function should be treated on the callee side.\n\n\n.. autofunction:: torch.distributed.rpc.functions.async_execution\n\n\n.. _rpc-backends:\n\nBackends\n^^^^^^^^","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":107,"to":133}}}}],["1525",{"pageContent":".. autofunction:: torch.distributed.rpc.functions.async_execution\n\n\n.. _rpc-backends:\n\nBackends\n^^^^^^^^\n\nThe RPC module can leverage different backends to perform the communication\nbetween the nodes. The backend to be used can be specified in the\n:func:`~torch.distributed.rpc.init_rpc` function, by passing a certain value of\nthe :class:`~torch.distributed.rpc.BackendType` enum. Regardless of what backend\nis used, the rest of the RPC API won't change. Each backend also defines its own\nsubclass of the :class:`~torch.distributed.rpc.RpcBackendOptions` class, an\ninstance of which can also be passed to :func:`~torch.distributed.rpc.init_rpc`\nto configure the backend's behavior.\n\n.. autoclass:: BackendType\n\n.. autoclass:: RpcBackendOptions\n    :members:\n\n\nTensorPipe Backend\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":133,"to":157}}}}],["1526",{"pageContent":".. autoclass:: BackendType\n\n.. autoclass:: RpcBackendOptions\n    :members:\n\n\nTensorPipe Backend\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\nThe TensorPipe agent, which is the default, leverages `the TensorPipe library\n<https://github.com/pytorch/tensorpipe>`_, which provides a natively\npoint-to-point communication primitive specifically suited for machine learning\nthat fundamentally addresses some of the limitations of Gloo. Compared to Gloo,\nit has the advantage of being asynchronous, which allows a large number of\ntransfers to occur simultaneously, each at their own speed, without blocking\neach other. It will only open pipes between pairs of nodes when needed, on\ndemand, and when one node fails only its incident pipes will be closed, while\nall other ones will keep working as normal. In addition, it is able to support\nmultiple different transports (TCP, of course, but also shared memory, NVLink,\nInfiniBand, ...) and can automatically detect their availability and negotiate\nthe best transport to use for each pipe.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":157,"to":177}}}}],["1527",{"pageContent":"The TensorPipe backend has been introduced in PyTorch v1.6 and is being actively\ndeveloped. At the moment, it only supports CPU tensors, with GPU support coming\nsoon. It comes with a TCP-based transport, just like Gloo. It is also able to\nautomatically chunk and multiplex large tensors over multiple sockets and\nthreads in order to achieve very high bandwidths. The agent will be able to pick\nthe best transport on its own, with no intervention required.\n\nExample::\n\n    >>> import os\n    >>> from torch.distributed import rpc\n    >>> os.environ['MASTER_ADDR'] = 'localhost'\n    >>> os.environ['MASTER_PORT'] = '29500'\n    >>>\n    >>> rpc.init_rpc(\n    >>>     \"worker1\",\n    >>>     rank=0,\n    >>>     world_size=2,\n    >>>     rpc_backend_options=rpc.TensorPipeRpcBackendOptions(\n    >>>         num_worker_threads=8,\n    >>>         rpc_timeout=20 # 20 second timeout\n    >>>     )\n    >>> )\n    >>>\n    >>> # omitting init_rpc invocation on worker2","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":179,"to":203}}}}],["1528",{"pageContent":".. autoclass:: TensorPipeRpcBackendOptions\n    :members:\n    :inherited-members:\n\n.. note ::\n  The RPC framework does not automatically retry any\n  :meth:`~torch.distributed.rpc.rpc_sync`,\n  :meth:`~torch.distributed.rpc.rpc_async` and\n  :meth:`~torch.distributed.rpc.remote` calls. The reason being that there is\n  no way the RPC framework can determine whether an operation is idempotent or\n  not and whether it is safe to retry. As a result, it is the application's\n  responsibility to deal with failures and retry if necessary. RPC communication\n  is based on TCP and as a result failures could happen due to network failures\n  or intermittent network connectivity issues. In such scenarios, the application\n  needs to retry appropriately with reasonable backoffs to ensure the network\n  isn't overwhelmed by aggressive retries.\n\n.. _rref:\n\nRRef\n----\n\n.. warning ::\n    RRefs are not currently supported when using CUDA tensors","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":205,"to":228}}}}],["1529",{"pageContent":".. _rref:\n\nRRef\n----\n\n.. warning ::\n    RRefs are not currently supported when using CUDA tensors\n\nAn ``RRef`` (Remote REFerence) is a reference to a value of some type ``T``\n(e.g. ``Tensor``) on a remote worker. This handle keeps the referenced remote\nvalue alive on the owner, but there is no implication that the value will be\ntransferred to the local worker in the future. RRefs can be used in\nmulti-machine training by holding references to `nn.Modules\n<https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ that exist on\nother workers, and calling the appropriate functions to retrieve or modify their\nparameters during training. See :ref:`remote-reference-protocol` for more\ndetails.\n\n.. autoclass:: RRef\n    :members:\n\n\n.. toctree::\n    :caption: More Information about RRef\n\n    rpc/rref\n\n.. _remote_module:\n\nRemoteModule\n------------\n\n.. warning ::\n    RemoteModule is not currently supported when using CUDA tensors","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":228,"to":261}}}}],["1530",{"pageContent":".. toctree::\n    :caption: More Information about RRef\n\n    rpc/rref\n\n.. _remote_module:\n\nRemoteModule\n------------\n\n.. warning ::\n    RemoteModule is not currently supported when using CUDA tensors\n\n``RemoteModule`` is an easy way to create an nn.Module remotely on a different\nprocess. The actual module resides on a remote host, but the local host has a\nhandle to this module and invoke this module similar to a regular nn.Module.\nThe invocation however incurs RPC calls to the remote end and can be performed\nasynchronously if needed via additional APIs supported by RemoteModule.\n\n.. autoclass:: torch.distributed.nn.api.remote_module.RemoteModule\n    :members: remote_parameters, get_module_rref\n\n\nDistributed Autograd Framework\n------------------------------\n\n.. warning ::\n    Distributed autograd is not currently supported when using CUDA tensors","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":261,"to":288}}}}],["1531",{"pageContent":"Distributed Autograd Framework\n------------------------------\n\n.. warning ::\n    Distributed autograd is not currently supported when using CUDA tensors\n\nThis module provides an RPC-based distributed autograd framework that can be\nused for applications such as model parallel training. In short, applications\nmay send and receive gradient recording tensors over RPC. In the forward pass,\nwe record when gradient recording tensors are sent over RPC and during the\nbackward pass we use this information to perform a distributed backward pass\nusing RPC. For more details see :ref:`distributed-autograd-design`.\n\n.. automodule:: torch.distributed.autograd\n    :members: context, backward, get_gradients\n\n.. toctree::\n    :caption: More Information about RPC Autograd\n\n    rpc/distributed_autograd\n\n\nDistributed Optimizer\n---------------------\n\nSee the `torch.distributed.optim <https://pytorch.org/docs/master/distributed.optim.html>`__ page for documentation on distributed optimizers.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":288,"to":313}}}}],["1532",{"pageContent":"Distributed Optimizer\n---------------------\n\nSee the `torch.distributed.optim <https://pytorch.org/docs/master/distributed.optim.html>`__ page for documentation on distributed optimizers.\n\nDesign Notes\n------------\nThe distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training.\n\n-  :ref:`distributed-autograd-design`\n\nThe RRef design note covers the design of the :ref:`rref` (Remote REFerence) protocol used to refer to values on remote workers by the framework.\n\n-  :ref:`remote-reference-protocol`\n\nTutorials\n---------\nThe RPC tutorials introduce users to the RPC framework, provide several example applications\nusing :ref:`torch.distributed.rpc<distributed-rpc-framework>` APIs, and demonstrate how\nto use `the profiler <https://pytorch.org/docs/stable/autograd.html#profiler>`__ to profile RPC-based workloads.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":313,"to":332}}}}],["1533",{"pageContent":"-  `Getting started with Distributed RPC Framework <https://pytorch.org/tutorials/intermediate/rpc_tutorial.html>`__\n-  `Implementing a Parameter Server using Distributed RPC Framework <https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html>`__\n-  `Combining Distributed DataParallel with Distributed RPC Framework <https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html>`__ (covers **RemoteModule** as well)\n-  `Profiling RPC-based Workloads <https://pytorch.org/tutorials/recipes/distributed_rpc_profiling.html>`__\n-  `Implementing batch RPC processing <https://pytorch.org/tutorials/intermediate/rpc_async_execution.html>`__\n-  `Distributed Pipeline Parallel <https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html>`__","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/rpc.rst","loc":{"lines":{"from":334,"to":339}}}}],["1534",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\ntorch.signal\n============\n.. automodule:: torch.signal\n.. currentmodule:: torch.signal\n\nThe `torch.signal` module, modeled after SciPy's `signal <https://docs.scipy.org/doc/scipy/reference/signal.html>`_ module.\n\ntorch.signal.windows\n--------------------\n\n.. automodule:: torch.signal.windows\n.. currentmodule:: torch.signal.windows\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    bartlett\n    blackman\n    cosine\n    exponential\n    gaussian\n    general_cosine\n    general_hamming\n    hamming\n    hann\n    kaiser\n    nuttall","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/signal.rst","loc":{"lines":{"from":1,"to":31}}}}],["1535",{"pageContent":".. automodule:: torch.sparse\n\n.. currentmodule:: torch\n\n.. _sparse-docs:\n\ntorch.sparse\n============\n\n.. warning::\n\n  The PyTorch API of sparse tensors is in beta and may change in the near future.\n  We highly welcome feature requests, bug reports and general suggestions as GitHub issues.\n\nWhy and when to use sparsity\n++++++++++++++++++++++++++++\n\nBy default PyTorch stores :class:`torch.Tensor` stores elements contiguously\nphysical memory. This leads to efficient implementations of various array\nprocessing algorithms that require fast access to elements.\n\nNow, some users might decide to represent data such as graph adjacency\nmatrices, pruned weights or points clouds by Tensors whose *elements are\nmostly zero valued*. We recognize these are important applications and aim\nto provide performance optimizations for these use cases via sparse storage formats.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1,"to":25}}}}],["1536",{"pageContent":"Various sparse storage formats such as COO, CSR/CSC, LIL, etc. have been\ndeveloped over the years. While they differ in exact layouts, they all\ncompress data through efficient representation of zero valued elements.\nWe call the uncompressed values *specified* in contrast to *unspecified*,\ncompressed elements.\n\nBy compressing repeat zeros sparse storage formats aim to save memory\nand computational resources on various CPUs and GPUs. Especially for high\ndegrees of sparsity or highly structured sparsity this can have significant\nperformance implications. As such sparse storage formats can be seen as a\nperformance optimization.\n\nLike many other performance optimization sparse storage formats are not\nalways advantageous. When trying sparse formats for your use case\nyou might find your execution time to increase rather than decrease.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":27,"to":41}}}}],["1537",{"pageContent":"Please feel encouraged to open a GitHub issue if you analytically\nexpected to see a stark increase in performance but measured a\ndegradation instead. This helps us prioritize the implementation\nof efficient kernels and wider performance optimizations.\n\nWe make it easy to try different sparsity layouts, and convert between them,\nwithout being opinionated on what's best for your particular application.\n\nFunctionality overview\n++++++++++++++++++++++\n\nWe want it to be straightforward to construct a sparse Tensor from a\ngiven dense Tensor by providing conversion routines for each layout.\n\nIn the next example we convert a 2D Tensor with default dense (strided)\nlayout to a 2D Tensor backed by the COO memory layout. Only values and\nindices of non-zero elements are stored in this case.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":43,"to":59}}}}],["1538",{"pageContent":"In the next example we convert a 2D Tensor with default dense (strided)\nlayout to a 2D Tensor backed by the COO memory layout. Only values and\nindices of non-zero elements are stored in this case.\n\n    >>> a = torch.tensor([[0, 2.], [3, 0]])\n    >>> a.to_sparse()\n    tensor(indices=tensor([[0, 1],\n                           [1, 0]]),\n           values=tensor([2., 3.]),\n           size=(2, 2), nnz=2, layout=torch.sparse_coo)\n\nPyTorch currently supports :ref:`COO<sparse-coo-docs>`, :ref:`CSR<sparse-csr-docs>`,\n:ref:`CSC<sparse-csc-docs>`, :ref:`BSR<sparse-bsr-docs>`, and :ref:`BSC<sparse-bsc-docs>`.\nPlease see the references for more details.\n\nNote that we provide slight generalizations of these formats.\n\nBatching: Devices such as GPUs require batching for optimal performance and\nthus we support batch dimensions.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":59,"to":77}}}}],["1539",{"pageContent":"Note that we provide slight generalizations of these formats.\n\nBatching: Devices such as GPUs require batching for optimal performance and\nthus we support batch dimensions.\n\nWe currently offer a very simple version of batching where each component of a sparse format\nitself is batched. This also requires the same number of specified elements per batch entry.\nIn this example we construct a 3D (batched) CSR Tensor from a 3D dense Tensor.\n\n    >>> t = torch.tensor([[[1., 0], [2., 3.]], [[4., 0], [5., 6.]]])\n    >>> t.dim()\n    3\n    >>> t.to_sparse_csr()\n    tensor(crow_indices=tensor([[0, 1, 3],\n                                [0, 1, 3]]),\n           col_indices=tensor([[0, 0, 1],\n                               [0, 0, 1]]),\n           values=tensor([[1., 2., 3.],\n                          [4., 5., 6.]]), size=(2, 2, 2), nnz=3,\n           layout=torch.sparse_csr)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":77,"to":96}}}}],["1540",{"pageContent":"Dense dimensions: On the other hand, some data such as Graph embeddings might be\nbetter viewed as sparse collections of vectors instead of scalars.\n\nIn this example we create a 3D Hybrid COO Tensor with 2 sparse and 1 dense dimension\nfrom a 3D strided Tensor. If an entire row in the 3D strided Tensor is zero, it is\nnot stored. If however any of the values in the row are non-zero, they are stored\nentirely. This reduces the number of indices since we need one index one per row instead\nof one per element. But it also increases the amount of storage for the values. Since\nonly rows that are *entirely* zero can be emitted and the presence of any non-zero\nvalued elements cause the entire row to be stored.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":99,"to":108}}}}],["1541",{"pageContent":">>> t = torch.tensor([[[0., 0], [1., 2.]], [[0., 0], [3., 4.]]])\n    >>> t.to_sparse(sparse_dim=2)\n    tensor(indices=tensor([[0, 1],\n                           [1, 1]]),\n           values=tensor([[1., 2.],\n                          [3., 4.]]),\n           size=(2, 2, 2), nnz=2, layout=torch.sparse_coo)\n\n\nOperator overview\n+++++++++++++++++\n\nFundamentally, operations on Tensor with sparse storage formats behave the same as\noperations on Tensor with strided (or other) storage formats. The particularities of\nstorage, that is the physical layout of the data, influences the performance of\nan operation but should not influence the semantics.\n\n\nWe are actively increasing operator coverage for sparse tensors. Users should not\nexpect support same level of support as for dense Tensors yet.\nSee our :ref:`operator<sparse-ops-docs>` documentation for a list.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":110,"to":130}}}}],["1542",{"pageContent":">>> b = torch.tensor([[0, 0, 1, 2, 3, 0], [4, 5, 0, 6, 0, 0]])\n    >>> b_s = b.to_sparse_csr()\n    >>> b_s.cos()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n    RuntimeError: unsupported tensor layout: SparseCsr\n    >>> b_s.sin()\n    tensor(crow_indices=tensor([0, 3, 6]),\n           col_indices=tensor([2, 3, 4, 0, 1, 3]),\n           values=tensor([ 0.8415,  0.9093,  0.1411, -0.7568, -0.9589, -0.2794]),\n           size=(2, 6), nnz=6, layout=torch.sparse_csr)\n\nAs shown in the example above, we don't support non-zero preserving unary\noperators such as cos. The output of a non-zero preserving unary operation\nwill not be able to take advantage of sparse storage formats to the same\nextent as the input and potentially result in a catastrophic increase in memory.\nWe instead rely on the user to explicitly convert to a dense Tensor first and\nthen run the operation.\n\n    >>> b_s.to_dense().cos()\n    tensor([[ 1.0000, -0.4161],\n            [-0.9900,  1.0000]])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":132,"to":153}}}}],["1543",{"pageContent":">>> b_s.to_dense().cos()\n    tensor([[ 1.0000, -0.4161],\n            [-0.9900,  1.0000]])\n\nWe are aware that some users want to ignore compressed zeros for operations such\nas `cos` instead of preserving the exact semantics of the operation. For this we\ncan point to torch.masked and its MaskedTensor, which is in turn also backed and\npowered by sparse storage formats and kernels.\n\nAlso note that, for now, the user doesn't have a choice of the output layout. For example,\nadding a sparse Tensor to a regular strided Tensor results in a strided Tensor. Some\nusers might prefer for this to stay a sparse layout, because they know the result will\nstill be sufficiently sparse.\n\n    >>> a + b.to_sparse()\n    tensor([[0., 3.],\n            [3., 0.]])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":153,"to":169}}}}],["1544",{"pageContent":">>> a + b.to_sparse()\n    tensor([[0., 3.],\n            [3., 0.]])\n\nWe acknowledge that access to kernels that can efficiently produce different output\nlayouts can be very useful. A subsequent operation might significantly benefit from\nreceiving a particular layout. We are working on an API to control the result layout\nand recognize it is an important feature to plan a more optimal path of execution for\nany given model.\n\n\n.. _sparse-coo-docs:\n\nSparse COO tensors\n++++++++++++++++++\n\nPyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular,\n\n  - the indices of specified elements are collected in ``indices``\n    tensor of size ``(ndim, nse)`` and with element type\n    ``torch.int64``,","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":169,"to":192}}}}],["1545",{"pageContent":"- the indices of specified elements are collected in ``indices``\n    tensor of size ``(ndim, nse)`` and with element type\n    ``torch.int64``,\n\n  - the corresponding values are collected in ``values`` tensor of\n    size ``(nse,)`` and with an arbitrary integer or floating point\n    number element type,\n\nwhere ``ndim`` is the dimensionality of the tensor and ``nse`` is the\nnumber of specified elements.\n\n.. note::\n\n   The memory consumption of a sparse COO tensor is at least ``(ndim *\n   8 + <size of element type in bytes>) * nse`` bytes (plus a constant\n   overhead from storing other tensor data).\n\n   The memory consumption of a strided tensor is at least\n   ``product(<tensor shape>) * <size of element type in bytes>``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":192,"to":210}}}}],["1546",{"pageContent":"The memory consumption of a strided tensor is at least\n   ``product(<tensor shape>) * <size of element type in bytes>``.\n\n   For example, the memory consumption of a 10 000 x 10 000 tensor\n   with 100 000 non-zero 32-bit floating point numbers is at least\n   ``(2 * 8 + 4) * 100 000 = 2 000 000`` bytes when using COO tensor\n   layout and ``10 000 * 10 000 * 4 = 400 000 000`` bytes when using\n   the default strided tensor layout. Notice the 200 fold memory\n   saving from using the COO storage format.\n\nConstruction\n------------\n\nA sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\n:func:`torch.sparse_coo_tensor`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":210,"to":226}}}}],["1547",{"pageContent":"Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write:\n\n    >>> i = [[0, 1, 1],\n             [2, 0, 2]]\n    >>> v =  [3, 4, 5]\n    >>> s = torch.sparse_coo_tensor(i, v, (2, 3))\n    >>> s\n    tensor(indices=tensor([[0, 1, 1],\n                           [2, 0, 2]]),\n           values=tensor([3, 4, 5]),\n           size=(2, 3), nnz=3, layout=torch.sparse_coo)\n    >>> s.to_dense()\n    tensor([[0, 0, 3],\n            [4, 0, 5]])\n\nNote that the input ``i`` is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":228,"to":248}}}}],["1548",{"pageContent":"Note that the input ``i`` is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:\n\n    >>> i = [[0, 2], [1, 0], [1, 2]]\n    >>> v =  [3,      4,      5    ]\n    >>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))\n    >>> # Or another equivalent formulation to get s\n    >>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))\n    >>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()\n    tensor([[0, 0, 3],\n            [4, 0, 5]])\n\nAn empty sparse COO tensor can be constructed by specifying its size\nonly:\n\n    >>> torch.sparse_coo_tensor(size=(2, 3))\n    tensor(indices=tensor([], size=(2, 0)),\n           values=tensor([], size=(0,)),\n           size=(2, 3), nnz=0, layout=torch.sparse_coo)\n\n.. _sparse-hybrid-coo-docs:\n\nSparse hybrid COO tensors\n-------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":248,"to":272}}}}],["1549",{"pageContent":".. _sparse-hybrid-coo-docs:\n\nSparse hybrid COO tensors\n-------------------------\n\nPyTorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors.\n\nPyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe ``values`` tensor to be a multi-dimensional tensor so that we\nhave:\n\n  - the indices of specified elements are collected in ``indices``\n    tensor of size ``(sparse_dims, nse)`` and with element type\n    ``torch.int64``,\n\n  - the corresponding (tensor) values are collected in ``values``\n    tensor of size ``(nse, dense_dims)`` and with an arbitrary integer\n    or floating point number element type.\n\n.. note::\n\n   We use (M + K)-dimensional tensor to denote a N-dimensional sparse\n   hybrid tensor, where M and K are the numbers of sparse and dense\n   dimensions, respectively, such that M + K == N holds.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":272,"to":297}}}}],["1550",{"pageContent":"We use (M + K)-dimensional tensor to denote a N-dimensional sparse\n   hybrid tensor, where M and K are the numbers of sparse and dense\n   dimensions, respectively, such that M + K == N holds.\n\nSuppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write\n\n    >>> i = [[0, 1, 1],\n             [2, 0, 2]]\n    >>> v =  [[3, 4], [5, 6], [7, 8]]\n    >>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n    >>> s\n    tensor(indices=tensor([[0, 1, 1],\n                           [2, 0, 2]]),\n           values=tensor([[3, 4],\n                          [5, 6],\n                          [7, 8]]),\n           size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)\n\n    >>> s.to_dense()\n    tensor([[[0, 0],\n             [0, 0],\n             [3, 4]],\n            [[5, 6],\n             [0, 0],\n             [7, 8]]])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":297,"to":323}}}}],["1551",{"pageContent":">>> s.to_dense()\n    tensor([[[0, 0],\n             [0, 0],\n             [3, 4]],\n            [[5, 6],\n             [0, 0],\n             [7, 8]]])\n\nIn general, if ``s`` is a sparse COO tensor and ``M =\ns.sparse_dim()``, ``K = s.dense_dim()``, then we have the following\ninvariants:\n\n  - ``M + K == len(s.shape) == s.ndim`` - dimensionality of a tensor\n    is the sum of the number of sparse and dense dimensions,\n  - ``s.indices().shape == (M, nse)`` - sparse indices are stored\n    explicitly,\n  - ``s.values().shape == (nse,) + s.shape[M : M + K]`` - the values\n    of a hybrid tensor are K-dimensional tensors,\n  - ``s.values().layout == torch.strided`` - values are stored as\n    strided tensors.\n\n.. note::\n\n   Dense dimensions always follow sparse dimensions, that is, mixing\n   of dense and sparse dimensions is not supported.\n\n.. note::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":323,"to":349}}}}],["1552",{"pageContent":".. note::\n\n   Dense dimensions always follow sparse dimensions, that is, mixing\n   of dense and sparse dimensions is not supported.\n\n.. note::\n\n   To be sure that a constructed sparse tensor has consistent indices,\n   values, and size, the invariant checks can be enabled per tensor\n   creation via ``check_invariants=True`` keyword argument, or\n   globally using :class:`torch.sparse.check_sparse_tensor_invariants`\n   context manager instance. By default, the sparse tensor invariants\n   checks are disabled.\n\n.. _sparse-uncoalesced-coo-docs:\n\nUncoalesced sparse COO tensors\n------------------------------\n\nPyTorch sparse COO tensor format permits sparse *uncoalesced* tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n``3`` and ``4``, for the same index ``1``, that leads to an 1-D\nuncoalesced tensor:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":349,"to":373}}}}],["1553",{"pageContent":">>> i = [[1, 1]]\n    >>> v =  [3, 4]\n    >>> s=torch.sparse_coo_tensor(i, v, (3,))\n    >>> s\n    tensor(indices=tensor([[1, 1]]),\n           values=tensor(  [3, 4]),\n           size=(3,), nnz=2, layout=torch.sparse_coo)\n\nwhile the coalescing process will accumulate the multi-valued elements\ninto a single value using summation:\n\n    >>> s.coalesce()\n    tensor(indices=tensor([[1]]),\n           values=tensor([7]),\n           size=(3,), nnz=1, layout=torch.sparse_coo)\n\nIn general, the output of :meth:`torch.Tensor.coalesce` method is a\nsparse tensor with the following properties:\n\n- the indices of specified tensor elements are unique,\n- the indices are sorted in lexicographical order,\n- :meth:`torch.Tensor.is_coalesced()` returns ``True``.\n\n.. note::\n\n  For the most part, you shouldn't have to care whether or not a\n  sparse tensor is coalesced or not, as most operations will work\n  identically given a sparse coalesced or uncoalesced tensor.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":375,"to":402}}}}],["1554",{"pageContent":"For the most part, you shouldn't have to care whether or not a\n  sparse tensor is coalesced or not, as most operations will work\n  identically given a sparse coalesced or uncoalesced tensor.\n\n  However, some operations can be implemented more efficiently on\n  uncoalesced tensors, and some on coalesced tensors.\n\n  For instance, addition of sparse COO tensors is implemented by\n  simply concatenating the indices and values tensors:\n\n    >>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))\n    >>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))\n    >>> a + b\n    tensor(indices=tensor([[0, 0, 1, 1]]),\n           values=tensor([7, 8, 5, 6]),\n           size=(2,), nnz=4, layout=torch.sparse_coo)\n\n  If you repeatedly perform an operation that can produce duplicate\n  entries (e.g., :func:`torch.Tensor.add`), you should occasionally\n  coalesce your sparse tensors to prevent them from growing too large.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":402,"to":421}}}}],["1555",{"pageContent":"On the other hand, the lexicographical ordering of indices can be\n  advantageous for implementing algorithms that involve many element\n  selection operations, such as slicing or matrix products.\n\nWorking with sparse COO tensors\n-------------------------------\n\nLet's consider the following example:\n\n    >>> i = [[0, 1, 1],\n             [2, 0, 2]]\n    >>> v =  [[3, 4], [5, 6], [7, 8]]\n    >>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n\nAs mentioned above, a sparse COO tensor is a :class:`torch.Tensor`\ninstance and to distinguish it from the `Tensor` instances that use\nsome other layout, on can use :attr:`torch.Tensor.is_sparse` or\n:attr:`torch.Tensor.layout` properties:\n\n    >>> isinstance(s, torch.Tensor)\n    True\n    >>> s.is_sparse\n    True\n    >>> s.layout == torch.sparse_coo\n    True\n\nThe number of sparse and dense dimensions can be acquired using\nmethods :meth:`torch.Tensor.sparse_dim` and\n:meth:`torch.Tensor.dense_dim`, respectively. For instance:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":423,"to":451}}}}],["1556",{"pageContent":"The number of sparse and dense dimensions can be acquired using\nmethods :meth:`torch.Tensor.sparse_dim` and\n:meth:`torch.Tensor.dense_dim`, respectively. For instance:\n\n    >>> s.sparse_dim(), s.dense_dim()\n    (2, 1)\n\n\nIf ``s`` is a sparse COO tensor then its COO format data can be\nacquired using methods :meth:`torch.Tensor.indices()` and\n:meth:`torch.Tensor.values()`.\n\n.. note::\n\n  Currently, one can acquire the COO format data only when the tensor\n  instance is coalesced:\n\n    >>> s.indices()\n    RuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first\n\n  For acquiring the COO format data of an uncoalesced tensor, use\n  :func:`torch.Tensor._values()` and :func:`torch.Tensor._indices()`:\n\n    >>> s._indices()\n    tensor([[0, 1, 1],\n            [2, 0, 2]])\n\n  .. See https://github.com/pytorch/pytorch/pull/45695 for a new API.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":451,"to":478}}}}],["1557",{"pageContent":">>> s._indices()\n    tensor([[0, 1, 1],\n            [2, 0, 2]])\n\n  .. See https://github.com/pytorch/pytorch/pull/45695 for a new API.\n\n  .. warning::\n    Calling :meth:`torch.Tensor._values()` will return a *detached* tensor.\n    To track gradients, :meth:`torch.Tensor.coalesce().values()` must be\n    used instead.\n\nConstructing a new sparse COO tensor results a tensor that is not\ncoalesced:\n\n    >>> s.is_coalesced()\n    False\n\nbut one can construct a coalesced copy of a sparse COO tensor using\nthe :meth:`torch.Tensor.coalesce` method:\n\n    >>> s2 = s.coalesce()\n    >>> s2.indices()\n    tensor([[0, 1, 1],\n           [2, 0, 2]])","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":478,"to":501}}}}],["1558",{"pageContent":">>> s2 = s.coalesce()\n    >>> s2.indices()\n    tensor([[0, 1, 1],\n           [2, 0, 2]])\n\nWhen working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on a sparse uncoalesced tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because ``c *\n(a + b) == c * a + c * b`` holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because ``sqrt(a + b) == sqrt(a) + sqrt(b)`` does not\nhold in general.\n\nSlicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":501,"to":519}}}}],["1559",{"pageContent":"Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:\n\n    >>> s[1]\n    tensor(indices=tensor([[0, 2]]),\n           values=tensor([[5, 6],\n                          [7, 8]]),\n           size=(3, 2), nnz=2, layout=torch.sparse_coo)\n    >>> s[1, 0, 1]\n    tensor(6)\n    >>> s[1, 0, 1:]\n    tensor([6])\n\n\nIn PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, :func:`torch.sparse.softmax` computes the softmax with the\nassumption that the fill value is negative infinity.\n\n.. See https://github.com/Quansight-Labs/rfcs/tree/pearu/rfc-fill-value/RFC-0004-sparse-fill-value for a new API\n\n.. _sparse-compressed-docs:\n\nSparse Compressed Tensors\n+++++++++++++++++++++++++","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":519,"to":545}}}}],["1560",{"pageContent":".. See https://github.com/Quansight-Labs/rfcs/tree/pearu/rfc-fill-value/RFC-0004-sparse-fill-value for a new API\n\n.. _sparse-compressed-docs:\n\nSparse Compressed Tensors\n+++++++++++++++++++++++++\n\nSparse Compressed Tensors represents a class of sparse tensors that\nhave a common feature of compressing the indices of a certain dimension\nusing an encoding that enables certain optimizations on linear algebra\nkernels of sparse compressed tensors. This encoding is based on the\n`Compressed Sparse Row (CSR)`__ format that PyTorch sparse compressed\ntensors extend with the support of sparse tensor batches, allowing\nmulti-dimensional tensor values, and storing sparse tensor values in\ndense blocks.\n\n__ https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)\n\n.. note::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":545,"to":563}}}}],["1561",{"pageContent":"__ https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)\n\n.. note::\n\n   We use (B + M + K)-dimensional tensor to denote a N-dimensional\n   sparse compressed hybrid tensor, where B, M, and K are the numbers\n   of batch, sparse, and dense dimensions, respectively, such that\n   ``B + M + K == N`` holds. The number of sparse dimensions for\n   sparse compressed tensors is always two, ``M == 2``.\n\n.. note::\n\n   We say that an indices tensor ``compressed_indices`` uses CSR\n   encoding if the following invariants are satisfied:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":563,"to":576}}}}],["1562",{"pageContent":".. note::\n\n   We say that an indices tensor ``compressed_indices`` uses CSR\n   encoding if the following invariants are satisfied:\n\n   - ``compressed_indices`` is a contiguous strided 32 or 64 bit\n     integer tensor\n   - ``compressed_indices`` shape is ``(*batchsize,\n     compressed_dim_size + 1)`` where ``compressed_dim_size`` is the\n     number of compressed dimensions (e.g. rows or columns)\n   - ``compressed_indices[..., 0] == 0`` where ``...`` denotes batch\n     indices\n   - ``compressed_indices[..., compressed_dim_size] == nse`` where\n     ``nse`` is the number of specified elements\n   - ``0 <= compressed_indices[..., i] - compressed_indices[..., i -\n     1] <= plain_dim_size`` for ``i=1, ..., compressed_dim_size``,\n     where ``plain_dim_size`` is the number of plain dimensions\n     (orthogonal to compressed dimensions, e.g. columns or rows).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":576,"to":593}}}}],["1563",{"pageContent":"To be sure that a constructed sparse tensor has consistent indices,\n   values, and size, the invariant checks can be enabled per tensor\n   creation via ``check_invariants=True`` keyword argument, or\n   globally using :class:`torch.sparse.check_sparse_tensor_invariants`\n   context manager instance. By default, the sparse tensor invariants\n   checks are disabled.\n\n.. note::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":595,"to":602}}}}],["1564",{"pageContent":"The generalization of sparse compressed layouts to N-dimensional\n   tensors can lead to some confusion regarding the count of specified\n   elements. When a sparse compressed tensor contains batch dimensions\n   the number of specified elements will correspond to the number of such\n   elements per-batch. When a sparse compressed tensor has dense dimensions\n   the element considered is now the K-dimensional array. Also for block\n   sparse compressed layouts the 2-D block is considered as the element\n   being specified.  Take as an example a 3-dimensional block sparse\n   tensor, with one batch dimension of length ``b``, and a block\n   shape of ``p, q``. If this tensor has ``n`` specified elements, then\n   in fact we have ``n`` blocks specified per batch. This tensor would\n   have ``values`` with shape ``(b, n, p, q)``. This interpretation of the\n   number of specified elements comes from all sparse compressed layouts\n   being derived from the compression of a 2-dimensional matrix. Batch","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":604,"to":617}}}}],["1565",{"pageContent":"number of specified elements comes from all sparse compressed layouts\n   being derived from the compression of a 2-dimensional matrix. Batch\n   dimensions are treated as stacking of sparse matrices, dense dimensions\n   change the meaning of the element from a simple scalar value to an\n   array with its own dimensions.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":617,"to":621}}}}],["1566",{"pageContent":".. _sparse-csr-docs:\n\nSparse CSR Tensor\n-----------------\n\nThe primary advantage of the CSR format over the COO format is better\nuse of storage and much faster computation operations such as sparse\nmatrix-vector multiplication using MKL and MAGMA backends.\n\nIn the simplest case, a (0 + 2 + 0)-dimensional sparse CSR tensor\nconsists of three 1-D tensors: ``crow_indices``, ``col_indices`` and\n``values``:\n\n  - The ``crow_indices`` tensor consists of compressed row\n    indices. This is a 1-D tensor of size ``nrows + 1`` (the number of\n    rows plus 1). The last element of ``crow_indices`` is the number\n    of specified elements, ``nse``. This tensor encodes the index in\n    ``values`` and ``col_indices`` depending on where the given row\n    starts. Each successive number in the tensor subtracted by the\n    number before it denotes the number of elements in a given row.\n\n  - The ``col_indices`` tensor contains the column indices of each\n    element. This is a 1-D tensor of size ``nse``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":623,"to":645}}}}],["1567",{"pageContent":"- The ``col_indices`` tensor contains the column indices of each\n    element. This is a 1-D tensor of size ``nse``.\n\n  - The ``values`` tensor contains the values of the CSR tensor\n    elements. This is a 1-D tensor of size ``nse``.\n\n.. note::\n\n   The index tensors ``crow_indices`` and ``col_indices`` should have\n   element type either ``torch.int64`` (default) or\n   ``torch.int32``. If you want to use MKL-enabled matrix operations,\n   use ``torch.int32``. This is as a result of the default linking of\n   pytorch being with MKL LP64, which uses 32 bit integer indexing.\n\nIn the general case, the (B + 2 + K)-dimensional sparse CSR tensor\nconsists of two (B + 1)-dimensional index tensors ``crow_indices`` and\n``col_indices``, and of (1 + K)-dimensional ``values`` tensor such\nthat\n\n  - ``crow_indices.shape == (*batchsize, nrows + 1)``\n\n  - ``col_indices.shape == (*batchsize, nse)``\n\n  - ``values.shape == (nse, *densesize)``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":645,"to":668}}}}],["1568",{"pageContent":"- ``crow_indices.shape == (*batchsize, nrows + 1)``\n\n  - ``col_indices.shape == (*batchsize, nse)``\n\n  - ``values.shape == (nse, *densesize)``\n\nwhile the shape of the sparse CSR tensor is ``(*batchsize, nrows,\nncols, *densesize)`` where ``len(batchsize) == B`` and\n``len(densesize) == K``.\n\n.. note::\n\n   The batches of sparse CSR tensors are dependent: the number of\n   specified elements in all batches must be the same. This somewhat\n   artificial constraint allows efficient storage of the indices of\n   different CSR batches.\n\n.. note::\n\n   The number of sparse and dense dimensions can be acquired using\n   :meth:`torch.Tensor.sparse_dim` and :meth:`torch.Tensor.dense_dim`\n   methods. The batch dimensions can be computed from the tensor\n   shape: ``batchsize = tensor.shape[:-tensor.sparse_dim() -\n   tensor.dense_dim()]``.\n\n.. note::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":668,"to":693}}}}],["1569",{"pageContent":".. note::\n\n   The memory consumption of a sparse CSR tensor is at least\n   ``(nrows * 8 + (8 + <size of element type in bytes> *\n   prod(densesize)) * nse) * prod(batchsize)`` bytes (plus a constant\n   overhead from storing other tensor data).\n\n   With the same example data of :ref:`the note in sparse COO format\n   introduction<sparse-coo-docs>`, the memory consumption of a 10 000\n   x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers\n   is at least ``(10000 * 8 + (8 + 4 * 1) * 100 000) * 1 = 1 280 000``\n   bytes when using CSR tensor layout. Notice the 1.6 and 310 fold\n   savings from using CSR storage format compared to using the COO and\n   strided formats, respectively.\n\nConstruction of CSR tensors\n'''''''''''''''''''''''''''","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":693,"to":709}}}}],["1570",{"pageContent":"Construction of CSR tensors\n'''''''''''''''''''''''''''\n\nSparse CSR tensors can be directly constructed by using the\n:func:`torch.sparse_csr_tensor` function. The user must supply the row\nand column indices and values tensors separately where the row indices\nmust be specified using the CSR compression encoding.  The ``size``\nargument is optional and will be deduced from the ``crow_indices`` and\n``col_indices`` if it is not present.\n\n    >>> crow_indices = torch.tensor([0, 2, 4])\n    >>> col_indices = torch.tensor([0, 1, 0, 1])\n    >>> values = torch.tensor([1, 2, 3, 4])\n    >>> csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.float64)\n    >>> csr\n    tensor(crow_indices=tensor([0, 2, 4]),\n           col_indices=tensor([0, 1, 0, 1]),\n           values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n           dtype=torch.float64)\n    >>> csr.to_dense()\n    tensor([[1., 2.],\n            [3., 4.]], dtype=torch.float64)\n\n.. note::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":709,"to":732}}}}],["1571",{"pageContent":".. note::\n\n   The values of sparse dimensions in deduced ``size`` is computed\n   from the size of ``crow_indices`` and the maximal index value in\n   ``col_indices``. If the number of columns needs to be larger than\n   in the deduced ``size`` then the ``size`` argument must be\n   specified explicitly.\n\nThe simplest way of constructing a 2-D sparse CSR tensor from a\nstrided or sparse COO tensor is to use\n:meth:`torch.Tensor.to_sparse_csr` method. Any zeros in the (strided)\ntensor will be interpreted as missing values in the sparse tensor:\n\n    >>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)\n    >>> sp = a.to_sparse_csr()\n    >>> sp\n    tensor(crow_indices=tensor([0, 1, 3, 3]),\n          col_indices=tensor([2, 0, 1]),\n          values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)\n\nCSR Tensor Operations\n'''''''''''''''''''''","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":732,"to":753}}}}],["1572",{"pageContent":"CSR Tensor Operations\n'''''''''''''''''''''\n\nThe sparse matrix-vector multiplication can be performed with the\n:meth:`tensor.matmul` method. This is currently the only math operation\nsupported on CSR tensors.\n\n    >>> vec = torch.randn(4, 1, dtype=torch.float64)\n    >>> sp.matmul(vec)\n    tensor([[0.9078],\n            [1.3180],\n            [0.0000]], dtype=torch.float64)\n\n.. _sparse-csc-docs:\n\nSparse CSC Tensor\n-----------------\n\nThe sparse CSC (Compressed Sparse Column) tensor format implements the\nCSC format for storage of 2 dimensional tensors with an extension to\nsupporting batches of sparse CSC tensors and values being\nmulti-dimensional tensors.\n\n.. note::\n\n   Sparse CSC tensor is essentially a transpose of the sparse CSR\n   tensor when the transposition is about swapping the sparse\n   dimensions.\n\nSimilarly to :ref:`sparse CSR tensors <sparse-csr-docs>`, a sparse CSC\ntensor consists of three tensors: ``ccol_indices``, ``row_indices``\nand ``values``:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":753,"to":784}}}}],["1573",{"pageContent":"Similarly to :ref:`sparse CSR tensors <sparse-csr-docs>`, a sparse CSC\ntensor consists of three tensors: ``ccol_indices``, ``row_indices``\nand ``values``:\n\n  - The ``ccol_indices`` tensor consists of compressed column\n    indices. This is a (B + 1)-D tensor of shape ``(*batchsize, ncols + 1)``.\n    The last element is the number of specified\n    elements, ``nse``. This tensor encodes the index in ``values`` and\n    ``row_indices`` depending on where the given column starts. Each\n    successive number in the tensor subtracted by the number before it\n    denotes the number of elements in a given column.\n\n  - The ``row_indices`` tensor contains the row indices of each\n    element. This is a (B + 1)-D tensor of shape ``(*batchsize, nse)``.\n\n  - The ``values`` tensor contains the values of the CSC tensor\n    elements. This is a (1 + K)-D tensor of shape ``(nse, *densesize)``.\n\nConstruction of CSC tensors\n'''''''''''''''''''''''''''","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":784,"to":803}}}}],["1574",{"pageContent":"- The ``values`` tensor contains the values of the CSC tensor\n    elements. This is a (1 + K)-D tensor of shape ``(nse, *densesize)``.\n\nConstruction of CSC tensors\n'''''''''''''''''''''''''''\n\nSparse CSC tensors can be directly constructed by using the\n:func:`torch.sparse_csc_tensor` function. The user must supply the row\nand column indices and values tensors separately where the column indices\nmust be specified using the CSR compression encoding.  The ``size``\nargument is optional and will be deduced from the ``row_indices`` and\n``ccol_indices`` tensors if it is not present.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":803,"to":814}}}}],["1575",{"pageContent":">>> ccol_indices = torch.tensor([0, 2, 4])\n    >>> row_indices = torch.tensor([0, 1, 0, 1])\n    >>> values = torch.tensor([1, 2, 3, 4])\n    >>> csc = torch.sparse_csc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)\n    >>> csc\n    tensor(ccol_indices=tensor([0, 2, 4]),\n           row_indices=tensor([0, 1, 0, 1]),\n           values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n           dtype=torch.float64, layout=torch.sparse_csc)\n    >>> csc.to_dense()\n    tensor([[1., 3.],\n            [2., 4.]], dtype=torch.float64)\n\n.. note::\n\n   The sparse CSC tensor constructor function has the compressed\n   column indices argument before the row indices argument.\n\nThe (0 + 2 + 0)-dimensional sparse CSC tensors can be constructed from\nany two-dimensional tensor using :meth:`torch.Tensor.to_sparse_csc`\nmethod. Any zeros in the (strided) tensor will be interpreted as\nmissing values in the sparse tensor:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":816,"to":837}}}}],["1576",{"pageContent":">>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)\n    >>> sp = a.to_sparse_csc()\n    >>> sp\n    tensor(ccol_indices=tensor([0, 1, 2, 3, 3]),\n           row_indices=tensor([1, 1, 0]),\n           values=tensor([1., 2., 1.]), size=(3, 4), nnz=3, dtype=torch.float64,\n           layout=torch.sparse_csc)\n\n.. _sparse-bsr-docs:\n\nSparse BSR Tensor\n-----------------\n\nThe sparse BSR (Block compressed Sparse Row) tensor format implements the\nBSR format for storage of two-dimensional tensors with an extension to\nsupporting batches of sparse BSR tensors and values being blocks of\nmulti-dimensional tensors.\n\nA sparse BSR tensor consists of three tensors: ``crow_indices``,\n``col_indices`` and ``values``:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":839,"to":858}}}}],["1577",{"pageContent":"A sparse BSR tensor consists of three tensors: ``crow_indices``,\n``col_indices`` and ``values``:\n\n  - The ``crow_indices`` tensor consists of compressed row\n    indices. This is a (B + 1)-D tensor of shape ``(*batchsize,\n    nrowblocks + 1)``.  The last element is the number of specified blocks,\n    ``nse``. This tensor encodes the index in ``values`` and\n    ``col_indices`` depending on where the given column block\n    starts. Each successive number in the tensor subtracted by the\n    number before it denotes the number of blocks in a given row.\n\n  - The ``col_indices`` tensor contains the column block indices of each\n    element. This is a (B + 1)-D tensor of shape ``(*batchsize,\n    nse)``.\n\n  - The ``values`` tensor contains the values of the sparse BSR tensor\n    elements collected into two-dimensional blocks. This is a (1 + 2 +\n    K)-D tensor of shape ``(nse, nrowblocks, ncolblocks,\n    *densesize)``.\n\nConstruction of BSR tensors\n'''''''''''''''''''''''''''","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":858,"to":879}}}}],["1578",{"pageContent":"Construction of BSR tensors\n'''''''''''''''''''''''''''\n\nSparse BSR tensors can be directly constructed by using the\n:func:`torch.sparse_bsr_tensor` function. The user must supply the row\nand column block indices and values tensors separately where the row block indices\nmust be specified using the CSR compression encoding.\nThe ``size`` argument is optional and will be deduced from the ``crow_indices`` and\n``col_indices`` tensors if it is not present.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":879,"to":887}}}}],["1579",{"pageContent":">>> crow_indices = torch.tensor([0, 2, 4])\n    >>> col_indices = torch.tensor([0, 1, 0, 1])\n    >>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]],\n    ...                        [[3, 4, 5], [9, 10, 11]],\n    ...                        [[12, 13, 14], [18, 19, 20]],\n    ...                        [[15, 16, 17], [21, 22, 23]]])\n    >>> bsr = torch.sparse_bsr_tensor(crow_indices, col_indices, values, dtype=torch.float64)\n    >>> bsr\n    tensor(crow_indices=tensor([0, 2, 4]),\n           col_indices=tensor([0, 1, 0, 1]),\n           values=tensor([[[ 0.,  1.,  2.],\n                           [ 6.,  7.,  8.]],\n                          [[ 3.,  4.,  5.],\n                           [ 9., 10., 11.]],\n                          [[12., 13., 14.],\n                           [18., 19., 20.]],\n                          [[15., 16., 17.],\n                           [21., 22., 23.]]]),\n           size=(4, 6), nnz=4, dtype=torch.float64, layout=torch.sparse_bsr)\n    >>> bsr.to_dense()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":889,"to":908}}}}],["1580",{"pageContent":"[[15., 16., 17.],\n                           [21., 22., 23.]]]),\n           size=(4, 6), nnz=4, dtype=torch.float64, layout=torch.sparse_bsr)\n    >>> bsr.to_dense()\n    tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n            [ 6.,  7.,  8.,  9., 10., 11.],\n            [12., 13., 14., 15., 16., 17.],\n            [18., 19., 20., 21., 22., 23.]], dtype=torch.float64)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":908,"to":915}}}}],["1581",{"pageContent":"The (0 + 2 + 0)-dimensional sparse BSR tensors can be constructed from\nany two-dimensional tensor using :meth:`torch.Tensor.to_sparse_bsr`\nmethod that also requires the specification of the values block size:\n\n    >>> dense = torch.tensor([[0, 1, 2, 3, 4, 5],\n    ...                       [6, 7, 8, 9, 10, 11],\n    ...                       [12, 13, 14, 15, 16, 17],\n    ...                       [18, 19, 20, 21, 22, 23]])\n    >>> bsr = dense.to_sparse_bsr(blocksize=(2, 3))\n    >>> bsr\n    tensor(crow_indices=tensor([0, 2, 4]),\n           col_indices=tensor([0, 1, 0, 1]),\n           values=tensor([[[ 0,  1,  2],\n                           [ 6,  7,  8]],\n                          [[ 3,  4,  5],\n                           [ 9, 10, 11]],\n                          [[12, 13, 14],\n                           [18, 19, 20]],\n                          [[15, 16, 17],\n                           [21, 22, 23]]]), size=(4, 6), nnz=4,\n           layout=torch.sparse_bsr)\n\n.. _sparse-bsc-docs:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":917,"to":939}}}}],["1582",{"pageContent":".. _sparse-bsc-docs:\n\nSparse BSC Tensor\n-----------------\n\nThe sparse BSC (Block compressed Sparse Column) tensor format implements the\nBSC format for storage of two-dimensional tensors with an extension to\nsupporting batches of sparse BSC tensors and values being blocks of\nmulti-dimensional tensors.\n\nA sparse BSC tensor consists of three tensors: ``ccol_indices``,\n``row_indices`` and ``values``:\n\n  - The ``ccol_indices`` tensor consists of compressed column\n    indices. This is a (B + 1)-D tensor of shape ``(*batchsize,\n    ncolblocks + 1)``.  The last element is the number of specified blocks,\n    ``nse``. This tensor encodes the index in ``values`` and\n    ``row_indices`` depending on where the given row block\n    starts. Each successive number in the tensor subtracted by the\n    number before it denotes the number of blocks in a given column.\n\n  - The ``row_indices`` tensor contains the row block indices of each\n    element. This is a (B + 1)-D tensor of shape ``(*batchsize,\n    nse)``.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":939,"to":962}}}}],["1583",{"pageContent":"- The ``row_indices`` tensor contains the row block indices of each\n    element. This is a (B + 1)-D tensor of shape ``(*batchsize,\n    nse)``.\n\n  - The ``values`` tensor contains the values of the sparse BSC tensor\n    elements collected into two-dimensional blocks. This is a (1 + 2 +\n    K)-D tensor of shape ``(nse, nrowblocks, ncolblocks,\n    *densesize)``.\n\nConstruction of BSC tensors\n'''''''''''''''''''''''''''\n\nSparse BSC tensors can be directly constructed by using the\n:func:`torch.sparse_bsc_tensor` function. The user must supply the row\nand column block indices and values tensors separately where the column block indices\nmust be specified using the CSR compression encoding.\nThe ``size`` argument is optional and will be deduced from the ``ccol_indices`` and\n``row_indices`` tensors if it is not present.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":962,"to":979}}}}],["1584",{"pageContent":">>> ccol_indices = torch.tensor([0, 2, 4])\n    >>> row_indices = torch.tensor([0, 1, 0, 1])\n    >>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]],\n    ...                        [[3, 4, 5], [9, 10, 11]],\n    ...                        [[12, 13, 14], [18, 19, 20]],\n    ...                        [[15, 16, 17], [21, 22, 23]]])\n    >>> bsc = torch.sparse_bsc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)\n    >>> bsc\n    tensor(ccol_indices=tensor([0, 2, 4]),\n           row_indices=tensor([0, 1, 0, 1]),\n           values=tensor([[[ 0.,  1.,  2.],\n                           [ 6.,  7.,  8.]],\n                          [[ 3.,  4.,  5.],\n                           [ 9., 10., 11.]],\n                          [[12., 13., 14.],\n                           [18., 19., 20.]],\n                          [[15., 16., 17.],\n                           [21., 22., 23.]]]), size=(4, 6), nnz=4,\n           dtype=torch.float64, layout=torch.sparse_bsc)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":981,"to":999}}}}],["1585",{"pageContent":"Tools for working with sparse compressed tensors\n------------------------------------------------\n\nAll sparse compressed tensors --- CSR, CSC, BSR, and BSC tensors ---\nare conceptionally very similar in that their indices data is split\ninto two parts: so-called compressed indices that use the CSR\nencoding, and so-called plain indices that are orthogonal to the\ncompressed indices. This allows various tools on these tensors to\nshare the same implementations that are parameterized by tensor\nlayout.\n\nConstruction of sparse compressed tensors\n'''''''''''''''''''''''''''''''''''''''''","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1001,"to":1013}}}}],["1586",{"pageContent":"Construction of sparse compressed tensors\n'''''''''''''''''''''''''''''''''''''''''\n\nSparse CSR, CSC, BSR, and CSC tensors can be constructed by using\n:func:`torch.sparse_compressed_tensor` function that have the same\ninterface as the above discussed constructor functions\n:func:`torch.sparse_csr_tensor`, :func:`torch.sparse_csc_tensor`,\n:func:`torch.sparse_bsr_tensor`, and :func:`torch.sparse_bsc_tensor`,\nrespectively, but with an extra required ``layout`` argument. The\nfollowing example illustrates a method of constructing CSR and CSC\ntensors using the same input data by specifying the corresponding\nlayout parameter to the :func:`torch.sparse_compressed_tensor`\nfunction:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1013,"to":1025}}}}],["1587",{"pageContent":">>> compressed_indices = torch.tensor([0, 2, 4])\n    >>> plain_indices = torch.tensor([0, 1, 0, 1])\n    >>> values = torch.tensor([1, 2, 3, 4])\n    >>> csr = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csr)\n    >>> csr\n    tensor(crow_indices=tensor([0, 2, 4]),\n           col_indices=tensor([0, 1, 0, 1]),\n           values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,\n           layout=torch.sparse_csr)\n    >>> csc = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csc)\n    >>> csc\n    tensor(ccol_indices=tensor([0, 2, 4]),\n           row_indices=tensor([0, 1, 0, 1]),\n           values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,\n           layout=torch.sparse_csc)\n    >>> (csr.transpose(0, 1).to_dense() == csc.to_dense()).all()\n    tensor(True)\n\n.. _sparse-ops-docs:\n\nSupported operations\n+++++++++++++++++++++++++++++++++++\n\nLinear Algebra operations\n-------------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1027,"to":1051}}}}],["1588",{"pageContent":".. _sparse-ops-docs:\n\nSupported operations\n+++++++++++++++++++++++++++++++++++\n\nLinear Algebra operations\n-------------------------\n\nThe following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\n``T[layout]`` denotes a tensor with a given layout. Similarly,\n``M[layout]`` denotes a matrix (2-D PyTorch tensor), and ``V[layout]``\ndenotes a vector (1-D PyTorch tensor). In addition, ``f`` denotes a\nscalar (float or 0-D PyTorch tensor), ``*`` is element-wise\nmultiplication, and ``@`` is matrix multiplication.\n\n.. csv-table::\n   :header: \"PyTorch operation\", \"Sparse grad?\", \"Layout signature\"\n   :widths: 20, 5, 60\n   :delim: ;","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1051,"to":1070}}}}],["1589",{"pageContent":":func:`torch.mv`;no; ``M[sparse_coo] @ V[strided] -> V[strided]``\n   :func:`torch.mv`;no; ``M[sparse_csr] @ V[strided] -> V[strided]``\n   :func:`torch.matmul`; no; ``M[sparse_coo] @ M[strided] -> M[strided]``\n   :func:`torch.matmul`; no; ``M[sparse_csr] @ M[strided] -> M[strided]``\n   :func:`torch.mm`; no; ``M[sparse_coo] @ M[strided] -> M[strided]``\n   :func:`torch.sparse.mm`; yes; ``M[sparse_coo] @ M[strided] -> M[strided]``\n   :func:`torch.smm`; no; ``M[sparse_coo] @ M[strided] -> M[sparse_coo]``\n   :func:`torch.hspmm`; no; ``M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo]``\n   :func:`torch.bmm`; no; ``T[sparse_coo] @ T[strided] -> T[strided]``\n   :func:`torch.addmm`; no; ``f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]``\n   :func:`torch.sparse.addmm`; yes; ``f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided]``\n   :func:`torch.sspaddmm`; no; ``f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo]``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1072,"to":1083}}}}],["1590",{"pageContent":":func:`torch.sspaddmm`; no; ``f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo]``\n   :func:`torch.lobpcg`; no; ``GENEIG(M[sparse_coo]) -> M[strided], M[strided]``\n   :func:`torch.pca_lowrank`; yes; ``PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided]``\n   :func:`torch.svd_lowrank`; yes; ``SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1083,"to":1086}}}}],["1591",{"pageContent":"where \"Sparse grad?\" column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept :func:`torch.smm`, support backward with respect to strided\nmatrix arguments.\n\n.. note::\n\n   Currently, PyTorch does not support matrix multiplication with the\n   layout signature ``M[strided] @ M[sparse_coo]``. However,\n   applications can still compute this using the matrix relation ``D @\n   S == (S.t() @ D.t()).t()``.\n\nTensor methods and sparse\n-------------------------\n\nThe following Tensor methods are related to sparse tensors:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Tensor.is_sparse\n    Tensor.is_sparse_csr\n    Tensor.dense_dim\n    Tensor.sparse_dim\n    Tensor.sparse_mask\n    Tensor.to_sparse\n    Tensor.to_sparse_coo\n    Tensor.to_sparse_csr\n    Tensor.to_sparse_csc\n    Tensor.to_sparse_bsr\n    Tensor.to_sparse_bsc\n    Tensor.to_dense\n    Tensor.values\n\nThe following Tensor methods are specific to sparse COO tensors:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1088,"to":1123}}}}],["1592",{"pageContent":"The following Tensor methods are specific to sparse COO tensors:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Tensor.coalesce\n    Tensor.sparse_resize_\n    Tensor.sparse_resize_and_clear_\n    Tensor.is_coalesced\n    Tensor.indices\n\nThe following methods are specific to :ref:`sparse CSR tensors <sparse-csr-docs>` and :ref:`sparse BSR tensors <sparse-bsr-docs>`:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Tensor.crow_indices\n    Tensor.col_indices\n\nThe following methods are specific to :ref:`sparse CSC tensors <sparse-csc-docs>` and :ref:`sparse BSC tensors <sparse-bsc-docs>`:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Tensor.row_indices\n    Tensor.ccol_indices\n\nThe following Tensor methods support sparse COO tensors:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1123,"to":1153}}}}],["1593",{"pageContent":":meth:`~torch.Tensor.add`\n:meth:`~torch.Tensor.add_`\n:meth:`~torch.Tensor.addmm`\n:meth:`~torch.Tensor.addmm_`\n:meth:`~torch.Tensor.any`\n:meth:`~torch.Tensor.asin`\n:meth:`~torch.Tensor.asin_`\n:meth:`~torch.Tensor.arcsin`\n:meth:`~torch.Tensor.arcsin_`\n:meth:`~torch.Tensor.bmm`\n:meth:`~torch.Tensor.clone`\n:meth:`~torch.Tensor.deg2rad`\n:meth:`~torch.Tensor.deg2rad_`\n:meth:`~torch.Tensor.detach`\n:meth:`~torch.Tensor.detach_`\n:meth:`~torch.Tensor.dim`\n:meth:`~torch.Tensor.div`\n:meth:`~torch.Tensor.div_`\n:meth:`~torch.Tensor.floor_divide`\n:meth:`~torch.Tensor.floor_divide_`\n:meth:`~torch.Tensor.get_device`\n:meth:`~torch.Tensor.index_select`\n:meth:`~torch.Tensor.isnan`\n:meth:`~torch.Tensor.log1p`\n:meth:`~torch.Tensor.log1p_`\n:meth:`~torch.Tensor.mm`\n:meth:`~torch.Tensor.mul`\n:meth:`~torch.Tensor.mul_`\n:meth:`~torch.Tensor.mv`\n:meth:`~torch.Tensor.narrow_copy`\n:meth:`~torch.Tensor.neg`\n:meth:`~torch.Tensor.neg_`\n:meth:`~torch.Tensor.negative`\n:meth:`~torch.Tensor.negative_`\n:meth:`~torch.Tensor.numel`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1155,"to":1189}}}}],["1594",{"pageContent":":meth:`~torch.Tensor.mv`\n:meth:`~torch.Tensor.narrow_copy`\n:meth:`~torch.Tensor.neg`\n:meth:`~torch.Tensor.neg_`\n:meth:`~torch.Tensor.negative`\n:meth:`~torch.Tensor.negative_`\n:meth:`~torch.Tensor.numel`\n:meth:`~torch.Tensor.rad2deg`\n:meth:`~torch.Tensor.rad2deg_`\n:meth:`~torch.Tensor.resize_as_`\n:meth:`~torch.Tensor.size`\n:meth:`~torch.Tensor.pow`\n:meth:`~torch.Tensor.sqrt`\n:meth:`~torch.Tensor.square`\n:meth:`~torch.Tensor.smm`\n:meth:`~torch.Tensor.sspaddmm`\n:meth:`~torch.Tensor.sub`\n:meth:`~torch.Tensor.sub_`\n:meth:`~torch.Tensor.t`\n:meth:`~torch.Tensor.t_`\n:meth:`~torch.Tensor.transpose`\n:meth:`~torch.Tensor.transpose_`\n:meth:`~torch.Tensor.zero_`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1189,"to":1211}}}}],["1595",{"pageContent":"Torch functions specific to sparse Tensors\n------------------------------------------\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    sparse_coo_tensor\n    sparse_csr_tensor\n    sparse_csc_tensor\n    sparse_bsr_tensor\n    sparse_bsc_tensor\n    sparse_compressed_tensor\n    sparse.sum\n    sparse.addmm\n    sparse.sampled_addmm\n    sparse.mm\n    sspaddmm\n    hspmm\n    smm\n    sparse.softmax\n    sparse.log_softmax\n    sparse.spdiags\n\nOther functions\n---------------\n\nThe following :mod:`torch` functions support sparse tensors:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1213,"to":1240}}}}],["1596",{"pageContent":"Other functions\n---------------\n\nThe following :mod:`torch` functions support sparse tensors:\n\n:func:`~torch.cat`\n:func:`~torch.dstack`\n:func:`~torch.empty`\n:func:`~torch.empty_like`\n:func:`~torch.hstack`\n:func:`~torch.index_select`\n:func:`~torch.is_complex`\n:func:`~torch.is_floating_point`\n:func:`~torch.is_nonzero`\n:func:`~torch.is_same_size`\n:func:`~torch.is_signed`\n:func:`~torch.is_tensor`\n:func:`~torch.lobpcg`\n:func:`~torch.mm`\n:func:`~torch.native_norm`\n:func:`~torch.pca_lowrank`\n:func:`~torch.select`\n:func:`~torch.stack`\n:func:`~torch.svd_lowrank`\n:func:`~torch.unsqueeze`\n:func:`~torch.vstack`\n:func:`~torch.zeros`\n:func:`~torch.zeros_like`\n\nTo manage checking sparse tensor invariants, see:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    sparse.check_sparse_tensor_invariants\n\nUnary functions\n---------------\n\nWe aim to support all zero-preserving unary functions.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1240,"to":1280}}}}],["1597",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    sparse.check_sparse_tensor_invariants\n\nUnary functions\n---------------\n\nWe aim to support all zero-preserving unary functions.\n\nIf you find that we are missing a zero-preserving unary function\nthat you need, please feel encouraged to open an issue for a feature request.\nAs always please kindly try the search function first before opening an issue.\n\nThe following operators currently support sparse COO/CSR/CSC/BSR/CSR tensor inputs.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1280,"to":1295}}}}],["1598",{"pageContent":"The following operators currently support sparse COO/CSR/CSC/BSR/CSR tensor inputs.\n\n:func:`~torch.abs`\n:func:`~torch.asin`\n:func:`~torch.asinh`\n:func:`~torch.atan`\n:func:`~torch.atanh`\n:func:`~torch.ceil`\n:func:`~torch.conj_physical`\n:func:`~torch.floor`\n:func:`~torch.log1p`\n:func:`~torch.neg`\n:func:`~torch.round`\n:func:`~torch.sin`\n:func:`~torch.sinh`\n:func:`~torch.sign`\n:func:`~torch.sgn`\n:func:`~torch.signbit`\n:func:`~torch.tan`\n:func:`~torch.tanh`\n:func:`~torch.trunc`\n:func:`~torch.expm1`\n:func:`~torch.sqrt`\n:func:`~torch.angle`\n:func:`~torch.isinf`\n:func:`~torch.isposinf`\n:func:`~torch.isneginf`\n:func:`~torch.isnan`\n:func:`~torch.erf`\n:func:`~torch.erfinv`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/sparse.rst","loc":{"lines":{"from":1295,"to":1324}}}}],["1599",{"pageContent":".. role:: hidden\n    :class: hidden-section\n\ntorch.special\n=============\n\nThe torch.special module, modeled after SciPy's `special <https://docs.scipy.org/doc/scipy/reference/special.html>`_ module.\n\n.. automodule:: torch.special\n.. currentmodule:: torch.special\n\nFunctions\n-----------------------","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/special.rst","loc":{"lines":{"from":1,"to":13}}}}],["1600",{"pageContent":".. autofunction:: airy_ai\n.. autofunction:: bessel_j0\n.. autofunction:: bessel_j1\n.. autofunction:: digamma\n.. autofunction:: entr\n.. autofunction:: erf\n.. autofunction:: erfc\n.. autofunction:: erfcx\n.. autofunction:: erfinv\n.. autofunction:: exp2\n.. autofunction:: expit\n.. autofunction:: expm1\n.. autofunction:: gammainc\n.. autofunction:: gammaincc\n.. autofunction:: gammaln\n.. autofunction:: i0\n.. autofunction:: i0e\n.. autofunction:: i1\n.. autofunction:: i1e\n.. autofunction:: log1p\n.. autofunction:: log_ndtr\n.. autofunction:: log_softmax\n.. autofunction:: logit\n.. autofunction:: logsumexp\n.. autofunction:: multigammaln\n.. autofunction:: ndtr\n.. autofunction:: ndtri\n.. autofunction:: polygamma\n.. autofunction:: psi\n.. autofunction:: round\n.. autofunction:: scaled_modified_bessel_k0\n.. autofunction:: scaled_modified_bessel_k1\n.. autofunction:: sinc\n.. autofunction:: softmax\n.. autofunction:: spherical_bessel_j0\n.. autofunction:: xlog1py\n.. autofunction:: xlogy\n.. autofunction:: zeta","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/special.rst","loc":{"lines":{"from":15,"to":52}}}}],["1601",{"pageContent":"torch.Storage\n===================================\n\n:class:`torch.Storage` is an alias for the storage class that corresponds with\nthe default data type (:func:`torch.get_default_dtype()`). For instance, if the\ndefault data type is :attr:`torch.float`, :class:`torch.Storage` resolves to\n:class:`torch.FloatStorage`.\n\nThe :class:`torch.<type>Storage` and :class:`torch.cuda.<type>Storage` classes,\nlike :class:`torch.FloatStorage`, :class:`torch.IntStorage`, etc., are not\nactually ever instantiated. Calling their constructors creates\na :class:`torch.TypedStorage` with the appropriate :class:`torch.dtype` and\n:class:`torch.device`.  :class:`torch.<type>Storage` classes have all of the\nsame class methods that :class:`torch.TypedStorage` has.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/storage.rst","loc":{"lines":{"from":1,"to":14}}}}],["1602",{"pageContent":"A :class:`torch.TypedStorage` is a contiguous, one-dimensional array of\nelements of a particular :class:`torch.dtype`. It can be given any\n:class:`torch.dtype`, and the internal data will be interpreted appropriately.\n:class:`torch.TypedStorage` contains a :class:`torch.UntypedStorage` which\nholds the data as an untyped array of bytes.\n\nEvery strided :class:`torch.Tensor` contains a :class:`torch.TypedStorage`,\nwhich stores all of the data that the :class:`torch.Tensor` views.\n\n.. warning::\n  All storage classes except for :class:`torch.UntypedStorage` will be removed\n  in the future, and :class:`torch.UntypedStorage` will be used in all cases.\n\n.. autoclass:: torch.TypedStorage\n   :members:\n   :undoc-members:\n   :inherited-members:\n\n.. autoclass:: torch.UntypedStorage\n   :members:\n   :undoc-members:\n   :inherited-members:\n\n.. autoclass:: torch.DoubleStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.FloatStorage\n   :members:\n   :undoc-members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/storage.rst","loc":{"lines":{"from":16,"to":45}}}}],["1603",{"pageContent":".. autoclass:: torch.DoubleStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.FloatStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.HalfStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.LongStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.IntStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.ShortStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.CharStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.ByteStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.BoolStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.BFloat16Storage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.ComplexDoubleStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.ComplexFloatStorage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.QUInt8Storage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.QInt8Storage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.QInt32Storage\n   :members:\n   :undoc-members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/storage.rst","loc":{"lines":{"from":45,"to":103}}}}],["1604",{"pageContent":".. autoclass:: torch.QUInt8Storage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.QInt8Storage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.QInt32Storage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.QUInt4x2Storage\n   :members:\n   :undoc-members:\n\n.. autoclass:: torch.QUInt2x4Storage\n   :members:\n   :undoc-members:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/storage.rst","loc":{"lines":{"from":103,"to":121}}}}],["1605",{"pageContent":".. currentmodule:: torch\n\n.. _tensor-attributes-doc:\n\nTensor Attributes\n=================\n\nEach ``torch.Tensor`` has a :class:`torch.dtype`, :class:`torch.device`, and :class:`torch.layout`.\n\n.. _dtype-doc:\n\ntorch.dtype\n-----------\n\n.. class:: dtype\n\nA :class:`torch.dtype` is an object that represents the data type of a\n:class:`torch.Tensor`. PyTorch has twelve different data types:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":1,"to":18}}}}],["1606",{"pageContent":"========================== ===========================================   ===========================\nData type                  dtype                                         Legacy Constructors\n========================== ===========================================   ===========================\n32-bit floating point      ``torch.float32`` or ``torch.float``          ``torch.*.FloatTensor``\n64-bit floating point      ``torch.float64`` or ``torch.double``         ``torch.*.DoubleTensor``\n64-bit complex             ``torch.complex64`` or ``torch.cfloat``\n128-bit complex            ``torch.complex128`` or ``torch.cdouble``\n16-bit floating point [1]_ ``torch.float16`` or ``torch.half``           ``torch.*.HalfTensor``\n16-bit floating point [2]_ ``torch.bfloat16``                            ``torch.*.BFloat16Tensor``\n8-bit integer (unsigned)   ``torch.uint8``                               ``torch.*.ByteTensor``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":20,"to":29}}}}],["1607",{"pageContent":"16-bit floating point [2]_ ``torch.bfloat16``                            ``torch.*.BFloat16Tensor``\n8-bit integer (unsigned)   ``torch.uint8``                               ``torch.*.ByteTensor``\n8-bit integer (signed)     ``torch.int8``                                ``torch.*.CharTensor``\n16-bit integer (signed)    ``torch.int16`` or ``torch.short``            ``torch.*.ShortTensor``\n32-bit integer (signed)    ``torch.int32`` or ``torch.int``              ``torch.*.IntTensor``\n64-bit integer (signed)    ``torch.int64`` or ``torch.long``             ``torch.*.LongTensor``\nBoolean                    ``torch.bool``                                ``torch.*.BoolTensor``\n========================== ===========================================   ===========================","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":29,"to":36}}}}],["1608",{"pageContent":".. [1] Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\n  significand bits. Useful when precision is important.\n\n.. [2] Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7\n  significand bits. Useful when range is important, since it has the same\n  number of exponent bits as ``float32``\n\nTo find out if a :class:`torch.dtype` is a floating point data type, the property :attr:`is_floating_point`\ncan be used, which returns ``True`` if the data type is a floating point data type.\n\nTo find out if a :class:`torch.dtype` is a complex data type, the property :attr:`is_complex`\ncan be used, which returns ``True`` if the data type is a complex data type.\n\n.. _type-promotion-doc:\n\nWhen the dtypes of inputs to an arithmetic operation (`add`, `sub`, `div`, `mul`) differ, we promote\nby finding the minimum dtype that satisfies the following rules:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":38,"to":54}}}}],["1609",{"pageContent":".. _type-promotion-doc:\n\nWhen the dtypes of inputs to an arithmetic operation (`add`, `sub`, `div`, `mul`) differ, we promote\nby finding the minimum dtype that satisfies the following rules:\n\n* If the type of a scalar operand is of a higher category than tensor operands\n  (where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\n  all scalar operands of that category.\n* If a zero-dimension tensor operand has a higher category than dimensioned operands,\n  we promote to a type with sufficient size and category to hold all zero-dim tensor operands of\n  that category.\n* If there are no higher-category zero-dim operands, we promote to a type with sufficient size\n  and category to hold all dimensioned operands.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":54,"to":66}}}}],["1610",{"pageContent":"A floating point scalar operand has dtype `torch.get_default_dtype()` and an integral\nnon-boolean scalar operand has dtype `torch.int64`. Unlike numpy, we do not inspect\nvalues when determining the minimum `dtypes` of an operand.  Quantized and complex types\nare not yet supported.\n\nPromotion Examples::\n\n    >>> float_tensor = torch.ones(1, dtype=torch.float)\n    >>> double_tensor = torch.ones(1, dtype=torch.double)\n    >>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n    >>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n    >>> int_tensor = torch.ones(1, dtype=torch.int)\n    >>> long_tensor = torch.ones(1, dtype=torch.long)\n    >>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n    >>> double_tensor = torch.ones(1, dtype=torch.double)\n    >>> bool_tensor = torch.ones(1, dtype=torch.bool)\n    # zero-dim tensors\n    >>> long_zerodim = torch.tensor(1, dtype=torch.long)\n    >>> int_zerodim = torch.tensor(1, dtype=torch.int)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":68,"to":86}}}}],["1611",{"pageContent":">>> torch.add(5, 5).dtype\n    torch.int64\n    # 5 is an int64, but does not have higher category than int_tensor so is not considered.\n    >>> (int_tensor + 5).dtype\n    torch.int32\n    >>> (int_tensor + long_zerodim).dtype\n    torch.int32\n    >>> (long_tensor + int_tensor).dtype\n    torch.int64\n    >>> (bool_tensor + long_tensor).dtype\n    torch.int64\n    >>> (bool_tensor + uint_tensor).dtype\n    torch.uint8\n    >>> (float_tensor + double_tensor).dtype\n    torch.float64\n    >>> (complex_float_tensor + complex_double_tensor).dtype\n    torch.complex128\n    >>> (bool_tensor + int_tensor).dtype\n    torch.int32\n    # Since long is a different kind than float, result dtype only needs to be large enough\n    # to hold the float.\n    >>> torch.add(long_tensor, float_tensor).dtype\n    torch.float32","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":88,"to":110}}}}],["1612",{"pageContent":"When the output tensor of an arithmetic operation is specified, we allow casting to its `dtype` except that:\n  * An integral output tensor cannot accept a floating point tensor.\n  * A boolean output tensor cannot accept a non-boolean tensor.\n  * A non-complex output tensor cannot accept a complex tensor\n\nCasting Examples::\n\n    # allowed:\n    >>> float_tensor *= float_tensor\n    >>> float_tensor *= int_tensor\n    >>> float_tensor *= uint_tensor\n    >>> float_tensor *= bool_tensor\n    >>> float_tensor *= double_tensor\n    >>> int_tensor *= long_tensor\n    >>> int_tensor *= uint_tensor\n    >>> uint_tensor *= int_tensor\n\n    # disallowed (RuntimeError: result type can't be cast to the desired output type):\n    >>> int_tensor *= float_tensor\n    >>> bool_tensor *= int_tensor\n    >>> bool_tensor *= uint_tensor\n    >>> float_tensor *= complex_float_tensor\n\n\n.. _device-doc:\n\ntorch.device\n------------\n\n.. class:: device","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":112,"to":141}}}}],["1613",{"pageContent":".. _device-doc:\n\ntorch.device\n------------\n\n.. class:: device\n\nA :class:`torch.device` is an object representing the device on which a :class:`torch.Tensor` is\nor will be allocated.\n\nThe :class:`torch.device` contains a device type (``'cpu'``, ``'cuda'`` or ``'mps'``) and optional device\nordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even after :func:`torch.cuda.set_device()` is called; e.g.,\na :class:`torch.Tensor` constructed with device ``'cuda'`` is equivalent to ``'cuda:X'`` where X is\nthe result of :func:`torch.cuda.current_device()`.\n\nA :class:`torch.Tensor`'s device can be accessed via the :attr:`Tensor.device` property.\n\nA :class:`torch.device` can be constructed via a string or via a string and device ordinal\n\nVia a string:\n::\n\n    >>> torch.device('cuda:0')\n    device(type='cuda', index=0)\n\n    >>> torch.device('cpu')\n    device(type='cpu')\n\n    >>> torch.device('mps')\n    device(type='mps')","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":141,"to":171}}}}],["1614",{"pageContent":"Via a string:\n::\n\n    >>> torch.device('cuda:0')\n    device(type='cuda', index=0)\n\n    >>> torch.device('cpu')\n    device(type='cpu')\n\n    >>> torch.device('mps')\n    device(type='mps')\n\n    >>> torch.device('cuda')  # current cuda device\n    device(type='cuda')\n\nVia a string and device ordinal:\n\n::\n\n    >>> torch.device('cuda', 0)\n    device(type='cuda', index=0)\n\n    >>> torch.device('mps', 0)\n    device(type='mps', index=0)\n\n    >>> torch.device('cpu', 0)\n    device(type='cpu', index=0)\n\nThe device object can also be used as a context manager to change the default\ndevice tensors are allocated on:\n\n::\n\n    >>> with torch.device('cuda:1'):\n    ...     r = torch.randn(2, 3)\n    >>> r.device\n    device(type='cuda', index=1)\n\nThis context manager has no effect if a factory function is passed an explicit,\nnon-None device argument.  To globally change the default device, see also\n:func:`torch.set_default_device`.\n\n.. warning::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":171,"to":213}}}}],["1615",{"pageContent":"This context manager has no effect if a factory function is passed an explicit,\nnon-None device argument.  To globally change the default device, see also\n:func:`torch.set_default_device`.\n\n.. warning::\n\n    This function imposes a slight performance cost on every Python\n    call to the torch API (not just factory functions).  If this\n    is causing problems for you, please comment on\n    https://github.com/pytorch/pytorch/issues/92701\n\n.. note::\n   The :class:`torch.device` argument in functions can generally be substituted with a string.\n   This allows for fast prototyping of code.\n\n   >>> # Example of a function that takes in a torch.device\n   >>> cuda1 = torch.device('cuda:1')\n   >>> torch.randn((2,3), device=cuda1)\n\n   >>> # You can substitute the torch.device with a string\n   >>> torch.randn((2,3), device='cuda:1')","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":213,"to":233}}}}],["1616",{"pageContent":">>> # You can substitute the torch.device with a string\n   >>> torch.randn((2,3), device='cuda:1')\n\n.. note::\n   For legacy reasons, a device can be constructed via a single device ordinal, which is treated\n   as a cuda device.  This matches :meth:`Tensor.get_device`, which returns an ordinal for cuda\n   tensors and is not supported for cpu tensors.\n\n   >>> torch.device(1)\n   device(type='cuda', index=1)\n\n.. note::\n   Methods which take a device will generally accept a (properly formatted) string\n   or (legacy) integer device ordinal, i.e. the following are all equivalent:\n\n   >>> torch.randn((2,3), device=torch.device('cuda:1'))\n   >>> torch.randn((2,3), device='cuda:1')\n   >>> torch.randn((2,3), device=1)  # legacy\n\n\n.. _layout-doc:\n\ntorch.layout\n------------\n\n.. class:: layout\n\n.. warning::\n  The ``torch.layout`` class is in beta and subject to change.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":233,"to":261}}}}],["1617",{"pageContent":".. _layout-doc:\n\ntorch.layout\n------------\n\n.. class:: layout\n\n.. warning::\n  The ``torch.layout`` class is in beta and subject to change.\n\nA :class:`torch.layout` is an object that represents the memory layout of a\n:class:`torch.Tensor`. Currently, we support ``torch.strided`` (dense Tensors)\nand have beta support for ``torch.sparse_coo`` (sparse COO Tensors).\n\n``torch.strided`` represents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associated\n:class:`torch.Storage`, which holds its data. These tensors provide\nmulti-dimensional, `strided <https://en.wikipedia.org/wiki/Stride_of_an_array>`_\nview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently.\n\nExample::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":261,"to":284}}}}],["1618",{"pageContent":"Example::\n\n    >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n    >>> x.stride()\n    (5, 1)\n\n    >>> x.t().stride()\n    (1, 5)\n\nFor more information on ``torch.sparse_coo`` tensors, see :ref:`sparse-docs`.\n\ntorch.memory_format\n-------------------\n\n.. class:: memory_format\n\nA :class:`torch.memory_format` is an object representing the memory format on which a :class:`torch.Tensor` is\nor will be allocated.\n\nPossible values are:\n\n- ``torch.contiguous_format``:\n  Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.\n\n- ``torch.channels_last``:\n  Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in\n  ``strides[0] > strides[2] > strides[3] > strides[1] == 1`` aka NHWC order.\n\n- ``torch.channels_last_3d``:\n  Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in\n  ``strides[0] > strides[2] > strides[3] > strides[4] > strides[1] == 1`` aka NDHWC order.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":284,"to":314}}}}],["1619",{"pageContent":"- ``torch.preserve_format``:\n  Used in functions like `clone` to preserve the memory format of the input tensor. If input tensor is\n  allocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\n  Otherwise output strides will follow ``torch.contiguous_format``","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_attributes.rst","loc":{"lines":{"from":316,"to":319}}}}],["1620",{"pageContent":".. currentmodule:: torch\n\n.. _tensor-view-doc:\n\nTensor Views\n=============\n\nPyTorch allows a tensor to be a ``View`` of an existing tensor. View tensor shares the same underlying data\nwith its base tensor. Supporting ``View`` avoids explicit data copy, thus allows us to do fast and memory efficient\nreshaping, slicing and element-wise operations.\n\nFor example, to get a view of an existing tensor ``t``, you can call ``t.view(...)``.\n\n::\n\n    >>> t = torch.rand(4, 4)\n    >>> b = t.view(2, 8)\n    >>> t.storage().data_ptr() == b.storage().data_ptr()  # `t` and `b` share the same underlying data.\n    True\n    # Modifying view tensor changes base tensor as well.\n    >>> b[0][0] = 3.14\n    >>> t[0][0]\n    tensor(3.14)\n\nSince views share underlying data with its base tensor, if you edit the data\nin the view, it will be reflected in the base tensor as well.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_view.rst","loc":{"lines":{"from":1,"to":26}}}}],["1621",{"pageContent":"Since views share underlying data with its base tensor, if you edit the data\nin the view, it will be reflected in the base tensor as well.\n\nTypically a PyTorch op returns a new tensor as output, e.g. :meth:`~torch.Tensor.add`.\nBut in case of view ops, outputs are views of input tensors to avoid unnecessary data copy.\nNo data movement occurs when creating a view, view tensor just changes the way\nit interprets the same data. Taking a view of contiguous tensor could potentially produce a non-contiguous tensor.\nUsers should pay additional attention as contiguity might have implicit performance impact.\n:meth:`~torch.Tensor.transpose` is a common example.\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_view.rst","loc":{"lines":{"from":26,"to":36}}}}],["1622",{"pageContent":"::\n\n    >>> base = torch.tensor([[0, 1],[2, 3]])\n    >>> base.is_contiguous()\n    True\n    >>> t = base.transpose(0, 1)  # `t` is a view of `base`. No data movement happened here.\n    # View tensors might be non-contiguous.\n    >>> t.is_contiguous()\n    False\n    # To get a contiguous tensor, call `.contiguous()` to enforce\n    # copying data when `t` is not contiguous.\n    >>> c = t.contiguous()\n\nFor reference, here’s a full list of view ops in PyTorch:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_view.rst","loc":{"lines":{"from":36,"to":49}}}}],["1623",{"pageContent":"- Basic slicing and indexing op, e.g. ``tensor[0, 2:, 1:7:2]`` returns a view of base ``tensor``, see note below.\n- :meth:`~torch.Tensor.adjoint`\n- :meth:`~torch.Tensor.as_strided`\n- :meth:`~torch.Tensor.detach`\n- :meth:`~torch.Tensor.diagonal`\n- :meth:`~torch.Tensor.expand`\n- :meth:`~torch.Tensor.expand_as`\n- :meth:`~torch.Tensor.movedim`\n- :meth:`~torch.Tensor.narrow`\n- :meth:`~torch.Tensor.permute`\n- :meth:`~torch.Tensor.select`\n- :meth:`~torch.Tensor.squeeze`\n- :meth:`~torch.Tensor.transpose`\n- :meth:`~torch.Tensor.t`\n- :attr:`~torch.Tensor.T`\n- :attr:`~torch.Tensor.H`\n- :attr:`~torch.Tensor.mT`\n- :attr:`~torch.Tensor.mH`\n- :attr:`~torch.Tensor.real`\n- :attr:`~torch.Tensor.imag`\n- :meth:`~torch.Tensor.view_as_real`\n- :meth:`~torch.Tensor.unflatten`\n- :meth:`~torch.Tensor.unfold`\n- :meth:`~torch.Tensor.unsqueeze`\n- :meth:`~torch.Tensor.view`\n- :meth:`~torch.Tensor.view_as`\n- :meth:`~torch.Tensor.unbind`\n- :meth:`~torch.Tensor.split`\n- :meth:`~torch.Tensor.hsplit`\n- :meth:`~torch.Tensor.vsplit`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_view.rst","loc":{"lines":{"from":51,"to":80}}}}],["1624",{"pageContent":"- :meth:`~torch.Tensor.view`\n- :meth:`~torch.Tensor.view_as`\n- :meth:`~torch.Tensor.unbind`\n- :meth:`~torch.Tensor.split`\n- :meth:`~torch.Tensor.hsplit`\n- :meth:`~torch.Tensor.vsplit`\n- :meth:`~torch.Tensor.tensor_split`\n- :meth:`~torch.Tensor.split_with_sizes`\n- :meth:`~torch.Tensor.swapaxes`\n- :meth:`~torch.Tensor.swapdims`\n- :meth:`~torch.Tensor.chunk`\n- :meth:`~torch.Tensor.indices` (sparse tensor only)\n- :meth:`~torch.Tensor.values`  (sparse tensor only)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_view.rst","loc":{"lines":{"from":80,"to":92}}}}],["1625",{"pageContent":".. note::\n   When accessing the contents of a tensor via indexing, PyTorch follows Numpy behaviors\n   that basic indexing returns views, while advanced indexing returns a copy.\n   Assignment via either basic or advanced indexing is in-place. See more examples in\n   `Numpy indexing documentation <https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html>`_.\n\nIt's also worth mentioning a few ops with special behaviors:\n\n- :meth:`~torch.Tensor.reshape`, :meth:`~torch.Tensor.reshape_as` and :meth:`~torch.Tensor.flatten` can return either a view or new tensor, user code shouldn't rely on whether it's view or not.\n- :meth:`~torch.Tensor.contiguous` returns **itself** if input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data.\n\nFor a more detailed walk-through of PyTorch internal implementation,\nplease refer to `ezyang's blogpost about PyTorch Internals <http://blog.ezyang.com/2019/05/pytorch-internals/>`_.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensor_view.rst","loc":{"lines":{"from":94,"to":106}}}}],["1626",{"pageContent":"torch.utils.tensorboard\n===================================\n.. automodule:: torch.utils.tensorboard\n\nBefore going further, more details on TensorBoard can be found at\nhttps://www.tensorflow.org/tensorboard/\n\nOnce you've installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs.\n\nThe SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example:\n\n.. code:: python\n\n\n    import torch\n    import torchvision\n    from torch.utils.tensorboard import SummaryWriter\n    from torchvision import datasets, transforms\n\n    # Writer will output to ./runs/ directory by default\n    writer = SummaryWriter()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensorboard.rst","loc":{"lines":{"from":1,"to":25}}}}],["1627",{"pageContent":"# Writer will output to ./runs/ directory by default\n    writer = SummaryWriter()\n\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n    trainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n    model = torchvision.models.resnet50(False)\n    # Have ResNet model take in grayscale rather than RGB\n    model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n    images, labels = next(iter(trainloader))\n\n    grid = torchvision.utils.make_grid(images)\n    writer.add_image('images', grid, 0)\n    writer.add_graph(model, images)\n    writer.close()\n\nThis can then be visualized with TensorBoard, which should be installable\nand runnable with::\n\n    pip install tensorboard\n    tensorboard --logdir=runs","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensorboard.rst","loc":{"lines":{"from":25,"to":45}}}}],["1628",{"pageContent":"This can then be visualized with TensorBoard, which should be installable\nand runnable with::\n\n    pip install tensorboard\n    tensorboard --logdir=runs\n\n\nLots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \"Loss/train\" and \"Loss/test\" will be grouped\ntogether, while \"Accuracy/train\" and \"Accuracy/test\" will be grouped separately\nin the TensorBoard interface.\n\n.. code:: python\n\n\n    from torch.utils.tensorboard import SummaryWriter\n    import numpy as np\n\n    writer = SummaryWriter()\n\n    for n_iter in range(100):\n        writer.add_scalar('Loss/train', np.random.random(), n_iter)\n        writer.add_scalar('Loss/test', np.random.random(), n_iter)\n        writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n        writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n\n\nExpected result:\n\n.. image:: _static/img/tensorboard/hier_tags.png\n    :scale: 75 %\n\n|\n|","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensorboard.rst","loc":{"lines":{"from":45,"to":79}}}}],["1629",{"pageContent":"Expected result:\n\n.. image:: _static/img/tensorboard/hier_tags.png\n    :scale: 75 %\n\n|\n|\n\n.. currentmodule:: torch.utils.tensorboard.writer\n\n.. autoclass:: SummaryWriter\n\n   .. automethod:: __init__\n   .. automethod:: add_scalar\n   .. automethod:: add_scalars\n   .. automethod:: add_histogram\n   .. automethod:: add_image\n   .. automethod:: add_images\n   .. automethod:: add_figure\n   .. automethod:: add_video\n   .. automethod:: add_audio\n   .. automethod:: add_text\n   .. automethod:: add_graph\n   .. automethod:: add_embedding\n   .. automethod:: add_pr_curve\n   .. automethod:: add_custom_scalars\n   .. automethod:: add_mesh\n   .. automethod:: add_hparams\n   .. automethod:: flush\n   .. automethod:: close","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensorboard.rst","loc":{"lines":{"from":79,"to":108}}}}],["1630",{"pageContent":".. currentmodule:: torch\n\n.. _tensor-doc:\n\ntorch.Tensor\n===================================\n\nA :class:`torch.Tensor` is a multi-dimensional matrix containing elements of\na single data type.\n\n\nData types\n----------\n\nTorch defines 10 tensor types with CPU and GPU variants which are as follows:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":1,"to":15}}}}],["1631",{"pageContent":"======================================= =========================================== ============================= ================================\nData type                               dtype                                       CPU tensor                    GPU tensor\n======================================= =========================================== ============================= ================================\n32-bit floating point                   ``torch.float32`` or ``torch.float``        :class:`torch.FloatTensor`    :class:`torch.cuda.FloatTensor`\n64-bit floating point                   ``torch.float64`` or ``torch.double``       :class:`torch.DoubleTensor`   :class:`torch.cuda.DoubleTensor`\n16-bit floating point [1]_              ``torch.float16`` or ``torch.half``         :class:`torch.HalfTensor`     :class:`torch.cuda.HalfTensor`\n16-bit floating point [2]_              ``torch.bfloat16``                          :class:`torch.BFloat16Tensor` :class:`torch.cuda.BFloat16Tensor`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":17,"to":23}}}}],["1632",{"pageContent":"16-bit floating point [2]_              ``torch.bfloat16``                          :class:`torch.BFloat16Tensor` :class:`torch.cuda.BFloat16Tensor`\n32-bit complex                          ``torch.complex32`` or ``torch.chalf``\n64-bit complex                          ``torch.complex64`` or ``torch.cfloat``\n128-bit complex                         ``torch.complex128`` or ``torch.cdouble``\n8-bit integer (unsigned)                ``torch.uint8``                             :class:`torch.ByteTensor`     :class:`torch.cuda.ByteTensor`\n8-bit integer (signed)                  ``torch.int8``                              :class:`torch.CharTensor`     :class:`torch.cuda.CharTensor`\n16-bit integer (signed)                 ``torch.int16`` or ``torch.short``          :class:`torch.ShortTensor`    :class:`torch.cuda.ShortTensor`\n32-bit integer (signed)                 ``torch.int32`` or ``torch.int``            :class:`torch.IntTensor`      :class:`torch.cuda.IntTensor`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":23,"to":30}}}}],["1633",{"pageContent":"32-bit integer (signed)                 ``torch.int32`` or ``torch.int``            :class:`torch.IntTensor`      :class:`torch.cuda.IntTensor`\n64-bit integer (signed)                 ``torch.int64`` or ``torch.long``           :class:`torch.LongTensor`     :class:`torch.cuda.LongTensor`\nBoolean                                 ``torch.bool``                              :class:`torch.BoolTensor`     :class:`torch.cuda.BoolTensor`\nquantized 8-bit integer (unsigned)      ``torch.quint8``                            :class:`torch.ByteTensor`     /\nquantized 8-bit integer (signed)        ``torch.qint8``                             :class:`torch.CharTensor`     /\nquantized 32-bit integer (signed)       ``torch.qint32``                            :class:`torch.IntTensor`      /\nquantized 4-bit integer (unsigned) [3]_ ``torch.quint4x2``                          :class:`torch.ByteTensor`     /","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":30,"to":36}}}}],["1634",{"pageContent":"quantized 4-bit integer (unsigned) [3]_ ``torch.quint4x2``                          :class:`torch.ByteTensor`     /\n======================================= =========================================== ============================= ================================","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":36,"to":37}}}}],["1635",{"pageContent":".. [1]\n  Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\n  significand bits. Useful when precision is important at the expense of range.\n.. [2]\n  Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\n  significand bits. Useful when range is important, since it has the same\n  number of exponent bits as ``float32``\n.. [3]\n  quantized 4-bit integer is stored as a 8-bit signed integer. Currently it's only supported in EmbeddingBag operator.\n\n:class:`torch.Tensor` is an alias for the default tensor type (:class:`torch.FloatTensor`).\n\nInitializing and basic operations\n---------------------------------\n\nA tensor can be constructed from a Python :class:`list` or sequence using the\n:func:`torch.tensor` constructor:\n\n::\n\n    >>> torch.tensor([[1., -1.], [1., -1.]])\n    tensor([[ 1.0000, -1.0000],\n            [ 1.0000, -1.0000]])\n    >>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n    tensor([[ 1,  2,  3],\n            [ 4,  5,  6]])\n\n.. warning::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":39,"to":66}}}}],["1636",{"pageContent":".. warning::\n\n    :func:`torch.tensor` always copies :attr:`data`. If you have a Tensor\n    :attr:`data` and just want to change its ``requires_grad`` flag, use\n    :meth:`~torch.Tensor.requires_grad_` or\n    :meth:`~torch.Tensor.detach` to avoid a copy.\n    If you have a numpy array and want to avoid a copy, use\n    :func:`torch.as_tensor`.\n\nA tensor of specific data type can be constructed by passing a\n:class:`torch.dtype` and/or a :class:`torch.device` to a\nconstructor or tensor creation op:\n\n::\n\n    >>> torch.zeros([2, 4], dtype=torch.int32)\n    tensor([[ 0,  0,  0,  0],\n            [ 0,  0,  0,  0]], dtype=torch.int32)\n    >>> cuda0 = torch.device('cuda:0')\n    >>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\n    tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n            [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n\nFor more information about building Tensors, see :ref:`tensor-creation-ops`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":66,"to":89}}}}],["1637",{"pageContent":"For more information about building Tensors, see :ref:`tensor-creation-ops`\n\n\nThe contents of a tensor can be accessed and modified using Python's indexing\nand slicing notation:\n\n::\n\n    >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n    >>> print(x[1][2])\n    tensor(6)\n    >>> x[0][1] = 8\n    >>> print(x)\n    tensor([[ 1,  8,  3],\n            [ 4,  5,  6]])\n\nUse :meth:`torch.Tensor.item` to get a Python number from a tensor containing a\nsingle value:\n\n::\n\n    >>> x = torch.tensor([[1]])\n    >>> x\n    tensor([[ 1]])\n    >>> x.item()\n    1\n    >>> x = torch.tensor(2.5)\n    >>> x\n    tensor(2.5000)\n    >>> x.item()\n    2.5\n\nFor more information about indexing, see :ref:`indexing-slicing-joining`\n\nA tensor can be created with :attr:`requires_grad=True` so that\n:mod:`torch.autograd` records operations on them for automatic differentiation.\n\n::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":89,"to":126}}}}],["1638",{"pageContent":"A tensor can be created with :attr:`requires_grad=True` so that\n:mod:`torch.autograd` records operations on them for automatic differentiation.\n\n::\n\n    >>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n    >>> out = x.pow(2).sum()\n    >>> out.backward()\n    >>> x.grad\n    tensor([[ 2.0000, -2.0000],\n            [ 2.0000,  2.0000]])\n\nEach tensor has an associated :class:`torch.Storage`, which holds its data.\nThe tensor class also provides multi-dimensional, `strided <https://en.wikipedia.org/wiki/Stride_of_an_array>`_\nview of a storage and defines numeric operations on it.\n\n.. note::\n   For more information on tensor views, see :ref:`tensor-view-doc`.\n\n.. note::\n   For more information on the :class:`torch.dtype`, :class:`torch.device`, and\n   :class:`torch.layout` attributes of a :class:`torch.Tensor`, see\n   :ref:`tensor-attributes-doc`.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":126,"to":148}}}}],["1639",{"pageContent":".. note::\n   For more information on the :class:`torch.dtype`, :class:`torch.device`, and\n   :class:`torch.layout` attributes of a :class:`torch.Tensor`, see\n   :ref:`tensor-attributes-doc`.\n\n.. note::\n   Methods which mutate a tensor are marked with an underscore suffix.\n   For example, :func:`torch.FloatTensor.abs_` computes the absolute value\n   in-place and returns the modified tensor, while :func:`torch.FloatTensor.abs`\n   computes the result in a new tensor.\n\n.. note::\n    To change an existing tensor's :class:`torch.device` and/or :class:`torch.dtype`, consider using\n    :meth:`~torch.Tensor.to` method on the tensor.\n\n.. warning::\n   Current implementation of :class:`torch.Tensor` introduces memory overhead,\n   thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\n   If this is your case, consider using one large structure.\n\n\nTensor class reference\n----------------------\n\n.. class:: Tensor()","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":148,"to":172}}}}],["1640",{"pageContent":"Tensor class reference\n----------------------\n\n.. class:: Tensor()\n\n   There are a few main ways to create a tensor, depending on your use case.\n\n   - To create a tensor with pre-existing data, use :func:`torch.tensor`.\n   - To create a tensor with specific size, use ``torch.*`` tensor creation\n     ops (see :ref:`tensor-creation-ops`).\n   - To create a tensor with the same size (and similar types) as another tensor,\n     use ``torch.*_like`` tensor creation ops\n     (see :ref:`tensor-creation-ops`).\n   - To create a tensor with similar type but different size as another tensor,\n     use ``tensor.new_*`` creation ops.\n\n.. autoattribute:: Tensor.T\n.. autoattribute:: Tensor.H\n.. autoattribute:: Tensor.mT\n.. autoattribute:: Tensor.mH\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Tensor.new_tensor\n    Tensor.new_full\n    Tensor.new_empty\n    Tensor.new_ones\n    Tensor.new_zeros","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":172,"to":201}}}}],["1641",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Tensor.new_tensor\n    Tensor.new_full\n    Tensor.new_empty\n    Tensor.new_ones\n    Tensor.new_zeros\n\n    Tensor.is_cuda\n    Tensor.is_quantized\n    Tensor.is_meta\n    Tensor.device\n    Tensor.grad\n    Tensor.ndim\n    Tensor.real\n    Tensor.imag\n    Tensor.nbytes\n    Tensor.itemsize","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":201,"to":220}}}}],["1642",{"pageContent":"Tensor.abs\n    Tensor.abs_\n    Tensor.absolute\n    Tensor.absolute_\n    Tensor.acos\n    Tensor.acos_\n    Tensor.arccos\n    Tensor.arccos_\n    Tensor.add\n    Tensor.add_\n    Tensor.addbmm\n    Tensor.addbmm_\n    Tensor.addcdiv\n    Tensor.addcdiv_\n    Tensor.addcmul\n    Tensor.addcmul_\n    Tensor.addmm\n    Tensor.addmm_\n    Tensor.sspaddmm\n    Tensor.addmv\n    Tensor.addmv_\n    Tensor.addr\n    Tensor.addr_\n    Tensor.adjoint\n    Tensor.allclose\n    Tensor.amax\n    Tensor.amin\n    Tensor.aminmax\n    Tensor.angle\n    Tensor.apply_\n    Tensor.argmax\n    Tensor.argmin\n    Tensor.argsort\n    Tensor.argwhere\n    Tensor.asin\n    Tensor.asin_\n    Tensor.arcsin\n    Tensor.arcsin_\n    Tensor.as_strided\n    Tensor.atan\n    Tensor.atan_\n    Tensor.arctan\n    Tensor.arctan_\n    Tensor.atan2\n    Tensor.atan2_\n    Tensor.arctan2\n    Tensor.arctan2_\n    Tensor.all\n    Tensor.any\n    Tensor.backward\n    Tensor.baddbmm\n    Tensor.baddbmm_\n    Tensor.bernoulli\n    Tensor.bernoulli_\n    Tensor.bfloat16\n    Tensor.bincount\n    Tensor.bitwise_not","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":222,"to":278}}}}],["1643",{"pageContent":"Tensor.all\n    Tensor.any\n    Tensor.backward\n    Tensor.baddbmm\n    Tensor.baddbmm_\n    Tensor.bernoulli\n    Tensor.bernoulli_\n    Tensor.bfloat16\n    Tensor.bincount\n    Tensor.bitwise_not\n    Tensor.bitwise_not_\n    Tensor.bitwise_and\n    Tensor.bitwise_and_\n    Tensor.bitwise_or\n    Tensor.bitwise_or_\n    Tensor.bitwise_xor\n    Tensor.bitwise_xor_\n    Tensor.bitwise_left_shift\n    Tensor.bitwise_left_shift_\n    Tensor.bitwise_right_shift\n    Tensor.bitwise_right_shift_\n    Tensor.bmm\n    Tensor.bool\n    Tensor.byte\n    Tensor.broadcast_to\n    Tensor.cauchy_\n    Tensor.ceil\n    Tensor.ceil_\n    Tensor.char\n    Tensor.cholesky\n    Tensor.cholesky_inverse\n    Tensor.cholesky_solve\n    Tensor.chunk\n    Tensor.clamp\n    Tensor.clamp_\n    Tensor.clip\n    Tensor.clip_\n    Tensor.clone\n    Tensor.contiguous\n    Tensor.copy_\n    Tensor.conj\n    Tensor.conj_physical\n    Tensor.conj_physical_\n    Tensor.resolve_conj\n    Tensor.resolve_neg\n    Tensor.copysign\n    Tensor.copysign_\n    Tensor.cos\n    Tensor.cos_\n    Tensor.cosh","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":278,"to":327}}}}],["1644",{"pageContent":"Tensor.conj\n    Tensor.conj_physical\n    Tensor.conj_physical_\n    Tensor.resolve_conj\n    Tensor.resolve_neg\n    Tensor.copysign\n    Tensor.copysign_\n    Tensor.cos\n    Tensor.cos_\n    Tensor.cosh\n    Tensor.cosh_\n    Tensor.corrcoef\n    Tensor.count_nonzero\n    Tensor.cov\n    Tensor.acosh\n    Tensor.acosh_\n    Tensor.arccosh\n    Tensor.arccosh_\n    Tensor.cpu\n    Tensor.cross\n    Tensor.cuda\n    Tensor.logcumsumexp\n    Tensor.cummax\n    Tensor.cummin\n    Tensor.cumprod\n    Tensor.cumprod_\n    Tensor.cumsum\n    Tensor.cumsum_\n    Tensor.chalf\n    Tensor.cfloat\n    Tensor.cdouble\n    Tensor.data_ptr\n    Tensor.deg2rad\n    Tensor.dequantize\n    Tensor.det\n    Tensor.dense_dim\n    Tensor.detach\n    Tensor.detach_\n    Tensor.diag\n    Tensor.diag_embed\n    Tensor.diagflat\n    Tensor.diagonal\n    Tensor.diagonal_scatter\n    Tensor.fill_diagonal_\n    Tensor.fmax\n    Tensor.fmin\n    Tensor.diff\n    Tensor.digamma\n    Tensor.digamma_\n    Tensor.dim\n    Tensor.dist\n    Tensor.div\n    Tensor.div_\n    Tensor.divide\n    Tensor.divide_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":327,"to":381}}}}],["1645",{"pageContent":"Tensor.fill_diagonal_\n    Tensor.fmax\n    Tensor.fmin\n    Tensor.diff\n    Tensor.digamma\n    Tensor.digamma_\n    Tensor.dim\n    Tensor.dist\n    Tensor.div\n    Tensor.div_\n    Tensor.divide\n    Tensor.divide_\n    Tensor.dot\n    Tensor.double\n    Tensor.dsplit\n    Tensor.element_size\n    Tensor.eq\n    Tensor.eq_\n    Tensor.equal\n    Tensor.erf\n    Tensor.erf_\n    Tensor.erfc\n    Tensor.erfc_\n    Tensor.erfinv\n    Tensor.erfinv_\n    Tensor.exp\n    Tensor.exp_\n    Tensor.expm1\n    Tensor.expm1_\n    Tensor.expand\n    Tensor.expand_as\n    Tensor.exponential_\n    Tensor.fix\n    Tensor.fix_\n    Tensor.fill_\n    Tensor.flatten\n    Tensor.flip\n    Tensor.fliplr\n    Tensor.flipud\n    Tensor.float\n    Tensor.float_power\n    Tensor.float_power_\n    Tensor.floor\n    Tensor.floor_\n    Tensor.floor_divide\n    Tensor.floor_divide_\n    Tensor.fmod\n    Tensor.fmod_\n    Tensor.frac\n    Tensor.frac_\n    Tensor.frexp\n    Tensor.gather\n    Tensor.gcd\n    Tensor.gcd_\n    Tensor.ge\n    Tensor.ge_\n    Tensor.greater_equal\n    Tensor.greater_equal_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":381,"to":438}}}}],["1646",{"pageContent":"Tensor.fmod\n    Tensor.fmod_\n    Tensor.frac\n    Tensor.frac_\n    Tensor.frexp\n    Tensor.gather\n    Tensor.gcd\n    Tensor.gcd_\n    Tensor.ge\n    Tensor.ge_\n    Tensor.greater_equal\n    Tensor.greater_equal_\n    Tensor.geometric_\n    Tensor.geqrf\n    Tensor.ger\n    Tensor.get_device\n    Tensor.gt\n    Tensor.gt_\n    Tensor.greater\n    Tensor.greater_\n    Tensor.half\n    Tensor.hardshrink\n    Tensor.heaviside\n    Tensor.histc\n    Tensor.histogram\n    Tensor.hsplit\n    Tensor.hypot\n    Tensor.hypot_\n    Tensor.i0\n    Tensor.i0_\n    Tensor.igamma\n    Tensor.igamma_\n    Tensor.igammac\n    Tensor.igammac_\n    Tensor.index_add_\n    Tensor.index_add\n    Tensor.index_copy_\n    Tensor.index_copy\n    Tensor.index_fill_\n    Tensor.index_fill\n    Tensor.index_put_\n    Tensor.index_put\n    Tensor.index_reduce_\n    Tensor.index_reduce\n    Tensor.index_select\n    Tensor.indices\n    Tensor.inner\n    Tensor.int\n    Tensor.int_repr\n    Tensor.inverse\n    Tensor.isclose\n    Tensor.isfinite\n    Tensor.isinf\n    Tensor.isposinf\n    Tensor.isneginf","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":438,"to":492}}}}],["1647",{"pageContent":"Tensor.index_select\n    Tensor.indices\n    Tensor.inner\n    Tensor.int\n    Tensor.int_repr\n    Tensor.inverse\n    Tensor.isclose\n    Tensor.isfinite\n    Tensor.isinf\n    Tensor.isposinf\n    Tensor.isneginf\n    Tensor.isnan\n    Tensor.is_contiguous\n    Tensor.is_complex\n    Tensor.is_conj\n    Tensor.is_floating_point\n    Tensor.is_inference\n    Tensor.is_leaf\n    Tensor.is_pinned\n    Tensor.is_set_to\n    Tensor.is_shared\n    Tensor.is_signed\n    Tensor.is_sparse\n    Tensor.istft\n    Tensor.isreal\n    Tensor.item\n    Tensor.kthvalue\n    Tensor.lcm\n    Tensor.lcm_\n    Tensor.ldexp\n    Tensor.ldexp_\n    Tensor.le\n    Tensor.le_\n    Tensor.less_equal\n    Tensor.less_equal_\n    Tensor.lerp\n    Tensor.lerp_\n    Tensor.lgamma\n    Tensor.lgamma_\n    Tensor.log\n    Tensor.log_\n    Tensor.logdet\n    Tensor.log10\n    Tensor.log10_\n    Tensor.log1p\n    Tensor.log1p_\n    Tensor.log2\n    Tensor.log2_\n    Tensor.log_normal_\n    Tensor.logaddexp\n    Tensor.logaddexp2\n    Tensor.logsumexp\n    Tensor.logical_and\n    Tensor.logical_and_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":492,"to":545}}}}],["1648",{"pageContent":"Tensor.log1p\n    Tensor.log1p_\n    Tensor.log2\n    Tensor.log2_\n    Tensor.log_normal_\n    Tensor.logaddexp\n    Tensor.logaddexp2\n    Tensor.logsumexp\n    Tensor.logical_and\n    Tensor.logical_and_\n    Tensor.logical_not\n    Tensor.logical_not_\n    Tensor.logical_or\n    Tensor.logical_or_\n    Tensor.logical_xor\n    Tensor.logical_xor_\n    Tensor.logit\n    Tensor.logit_\n    Tensor.long\n    Tensor.lt\n    Tensor.lt_\n    Tensor.less\n    Tensor.less_\n    Tensor.lu\n    Tensor.lu_solve\n    Tensor.as_subclass\n    Tensor.map_\n    Tensor.masked_scatter_\n    Tensor.masked_scatter\n    Tensor.masked_fill_\n    Tensor.masked_fill\n    Tensor.masked_select\n    Tensor.matmul\n    Tensor.matrix_power\n    Tensor.matrix_exp\n    Tensor.max\n    Tensor.maximum\n    Tensor.mean\n    Tensor.nanmean\n    Tensor.median\n    Tensor.nanmedian\n    Tensor.min\n    Tensor.minimum\n    Tensor.mm\n    Tensor.smm\n    Tensor.mode\n    Tensor.movedim\n    Tensor.moveaxis\n    Tensor.msort\n    Tensor.mul\n    Tensor.mul_\n    Tensor.multiply\n    Tensor.multiply_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":545,"to":597}}}}],["1649",{"pageContent":"Tensor.min\n    Tensor.minimum\n    Tensor.mm\n    Tensor.smm\n    Tensor.mode\n    Tensor.movedim\n    Tensor.moveaxis\n    Tensor.msort\n    Tensor.mul\n    Tensor.mul_\n    Tensor.multiply\n    Tensor.multiply_\n    Tensor.multinomial\n    Tensor.mv\n    Tensor.mvlgamma\n    Tensor.mvlgamma_\n    Tensor.nansum\n    Tensor.narrow\n    Tensor.narrow_copy\n    Tensor.ndimension\n    Tensor.nan_to_num\n    Tensor.nan_to_num_\n    Tensor.ne\n    Tensor.ne_\n    Tensor.not_equal\n    Tensor.not_equal_\n    Tensor.neg\n    Tensor.neg_\n    Tensor.negative\n    Tensor.negative_\n    Tensor.nelement\n    Tensor.nextafter\n    Tensor.nextafter_\n    Tensor.nonzero\n    Tensor.norm\n    Tensor.normal_\n    Tensor.numel\n    Tensor.numpy\n    Tensor.orgqr\n    Tensor.ormqr\n    Tensor.outer\n    Tensor.permute\n    Tensor.pin_memory\n    Tensor.pinverse\n    Tensor.polygamma\n    Tensor.polygamma_\n    Tensor.positive\n    Tensor.pow\n    Tensor.pow_\n    Tensor.prod\n    Tensor.put_\n    Tensor.qr\n    Tensor.qscheme\n    Tensor.quantile\n    Tensor.nanquantile\n    Tensor.q_scale","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":597,"to":652}}}}],["1650",{"pageContent":"Tensor.polygamma_\n    Tensor.positive\n    Tensor.pow\n    Tensor.pow_\n    Tensor.prod\n    Tensor.put_\n    Tensor.qr\n    Tensor.qscheme\n    Tensor.quantile\n    Tensor.nanquantile\n    Tensor.q_scale\n    Tensor.q_zero_point\n    Tensor.q_per_channel_scales\n    Tensor.q_per_channel_zero_points\n    Tensor.q_per_channel_axis\n    Tensor.rad2deg\n    Tensor.random_\n    Tensor.ravel\n    Tensor.reciprocal\n    Tensor.reciprocal_\n    Tensor.record_stream\n    Tensor.register_hook\n    Tensor.remainder\n    Tensor.remainder_\n    Tensor.renorm\n    Tensor.renorm_\n    Tensor.repeat\n    Tensor.repeat_interleave\n    Tensor.requires_grad\n    Tensor.requires_grad_\n    Tensor.reshape\n    Tensor.reshape_as\n    Tensor.resize_\n    Tensor.resize_as_\n    Tensor.retain_grad\n    Tensor.retains_grad\n    Tensor.roll\n    Tensor.rot90\n    Tensor.round\n    Tensor.round_\n    Tensor.rsqrt\n    Tensor.rsqrt_\n    Tensor.scatter\n    Tensor.scatter_\n    Tensor.scatter_add_\n    Tensor.scatter_add\n    Tensor.scatter_reduce_\n    Tensor.scatter_reduce\n    Tensor.select","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":652,"to":700}}}}],["1651",{"pageContent":"Tensor.round_\n    Tensor.rsqrt\n    Tensor.rsqrt_\n    Tensor.scatter\n    Tensor.scatter_\n    Tensor.scatter_add_\n    Tensor.scatter_add\n    Tensor.scatter_reduce_\n    Tensor.scatter_reduce\n    Tensor.select\n    Tensor.select_scatter\n    Tensor.set_\n    Tensor.share_memory_\n    Tensor.short\n    Tensor.sigmoid\n    Tensor.sigmoid_\n    Tensor.sign\n    Tensor.sign_\n    Tensor.signbit\n    Tensor.sgn\n    Tensor.sgn_\n    Tensor.sin\n    Tensor.sin_\n    Tensor.sinc\n    Tensor.sinc_\n    Tensor.sinh\n    Tensor.sinh_\n    Tensor.asinh\n    Tensor.asinh_\n    Tensor.arcsinh\n    Tensor.arcsinh_\n    Tensor.size\n    Tensor.slogdet\n    Tensor.slice_scatter\n    Tensor.softmax\n    Tensor.sort\n    Tensor.split\n    Tensor.sparse_mask\n    Tensor.sparse_dim\n    Tensor.sqrt\n    Tensor.sqrt_\n    Tensor.square\n    Tensor.square_\n    Tensor.squeeze\n    Tensor.squeeze_\n    Tensor.std\n    Tensor.stft\n    Tensor.storage\n    Tensor.untyped_storage\n    Tensor.storage_offset\n    Tensor.storage_type\n    Tensor.stride\n    Tensor.sub\n    Tensor.sub_\n    Tensor.subtract","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":700,"to":754}}}}],["1652",{"pageContent":"Tensor.std\n    Tensor.stft\n    Tensor.storage\n    Tensor.untyped_storage\n    Tensor.storage_offset\n    Tensor.storage_type\n    Tensor.stride\n    Tensor.sub\n    Tensor.sub_\n    Tensor.subtract\n    Tensor.subtract_\n    Tensor.sum\n    Tensor.sum_to_size\n    Tensor.svd\n    Tensor.swapaxes\n    Tensor.swapdims\n    Tensor.t\n    Tensor.t_\n    Tensor.tensor_split\n    Tensor.tile\n    Tensor.to\n    Tensor.to_mkldnn\n    Tensor.take\n    Tensor.take_along_dim\n    Tensor.tan\n    Tensor.tan_\n    Tensor.tanh\n    Tensor.tanh_\n    Tensor.atanh\n    Tensor.atanh_\n    Tensor.arctanh\n    Tensor.arctanh_\n    Tensor.tolist\n    Tensor.topk\n    Tensor.to_dense\n    Tensor.to_sparse\n    Tensor.to_sparse_csr\n    Tensor.to_sparse_csc\n    Tensor.to_sparse_bsr\n    Tensor.to_sparse_bsc\n    Tensor.trace\n    Tensor.transpose\n    Tensor.transpose_\n    Tensor.triangular_solve\n    Tensor.tril\n    Tensor.tril_\n    Tensor.triu\n    Tensor.triu_\n    Tensor.true_divide\n    Tensor.true_divide_\n    Tensor.trunc\n    Tensor.trunc_\n    Tensor.type\n    Tensor.type_as","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":754,"to":807}}}}],["1653",{"pageContent":"Tensor.triangular_solve\n    Tensor.tril\n    Tensor.tril_\n    Tensor.triu\n    Tensor.triu_\n    Tensor.true_divide\n    Tensor.true_divide_\n    Tensor.trunc\n    Tensor.trunc_\n    Tensor.type\n    Tensor.type_as\n    Tensor.unbind\n    Tensor.unflatten\n    Tensor.unfold\n    Tensor.uniform_\n    Tensor.unique\n    Tensor.unique_consecutive\n    Tensor.unsqueeze\n    Tensor.unsqueeze_\n    Tensor.values\n    Tensor.var\n    Tensor.vdot\n    Tensor.view\n    Tensor.view_as\n    Tensor.vsplit\n    Tensor.where\n    Tensor.xlogy\n    Tensor.xlogy_\n    Tensor.zero_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/tensors.rst","loc":{"lines":{"from":807,"to":835}}}}],["1654",{"pageContent":"torch.testing\n=============\n\n.. automodule:: torch.testing\n.. currentmodule:: torch.testing\n\n.. autofunction:: assert_close\n.. autofunction:: make_tensor\n.. autofunction:: assert_allclose","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/testing.rst","loc":{"lines":{"from":1,"to":9}}}}],["1655",{"pageContent":".. _torch_ao_ns_numeric_suite:\n\ntorch.ao.ns._numeric_suite\n--------------------------\n\n.. warning ::\n     This module is an early prototype and is subject to change.\n\n.. currentmodule:: torch.ao.ns._numeric_suite\n\n.. automodule:: torch.ao.ns._numeric_suite\n    :members:\n    :member-order: bysource","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.ao.ns._numeric_suite.rst","loc":{"lines":{"from":1,"to":13}}}}],["1656",{"pageContent":".. _torch_ao_ns_numeric_suite_fx:\n\ntorch.ao.ns._numeric_suite_fx\n-----------------------------\n\n.. warning ::\n     This module is an early prototype and is subject to change.\n\n.. currentmodule:: torch.ao.ns._numeric_suite_fx\n\n.. automodule:: torch.ao.ns._numeric_suite_fx\n    :members:\n    :member-order: bysource\n\n\ntorch.ao.ns.fx.utils\n--------------------------------------\n\n.. warning ::\n     This module is an early prototype and is subject to change.\n\n.. currentmodule:: torch.ao.ns.fx.utils\n\n.. autofunction:: torch.ao.ns.fx.utils.compute_sqnr(x, y)\n.. autofunction:: torch.ao.ns.fx.utils.compute_normalized_l2_error(x, y)\n.. autofunction:: torch.ao.ns.fx.utils.compute_cosine_similarity(x, y)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.ao.ns._numeric_suite_fx.rst","loc":{"lines":{"from":1,"to":26}}}}],["1657",{"pageContent":".. currentmodule:: torch.overrides\n\ntorch.overrides\n---------------\n\nThis module exposes various helper functions for the ``__torch_function__``\nprotocol. See :ref:`extending-torch` for more detail on the\n``__torch_function__`` protocol.\n\nFunctions\n~~~~~~~~~\n\n.. autofunction::  get_ignored_functions\n\n.. autofunction::  get_overridable_functions\n\n.. autofunction::  resolve_name\n\n.. autofunction::  get_testing_overrides\n\n.. autofunction::  handle_torch_function\n\n.. autofunction::  has_torch_function\n\n.. autofunction::  is_tensor_like\n\n.. autofunction::  is_tensor_method_or_property\n\n.. autofunction::  wrap_torch_function","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.overrides.rst","loc":{"lines":{"from":1,"to":29}}}}],["1658",{"pageContent":"torch\n=====\n.. automodule:: torch\n.. currentmodule:: torch\n\nTensors\n-------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    is_tensor\n    is_storage\n    is_complex\n    is_conj\n    is_floating_point\n    is_nonzero\n    set_default_dtype\n    get_default_dtype\n    set_default_device\n    set_default_tensor_type\n    numel\n    set_printoptions\n    set_flush_denormal\n\n.. _tensor-creation-ops:\n\nCreation Ops\n~~~~~~~~~~~~\n\n.. note::\n    Random sampling creation ops are listed under :ref:`random-sampling` and\n    include:\n    :func:`torch.rand`\n    :func:`torch.rand_like`\n    :func:`torch.randn`\n    :func:`torch.randn_like`\n    :func:`torch.randint`\n    :func:`torch.randint_like`\n    :func:`torch.randperm`\n    You may also use :func:`torch.empty` with the :ref:`inplace-random-sampling`\n    methods to create :class:`torch.Tensor` s with values sampled from a broader\n    range of distributions.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":1,"to":47}}}}],["1659",{"pageContent":".. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    tensor\n    sparse_coo_tensor\n    sparse_csr_tensor\n    sparse_csc_tensor\n    sparse_bsr_tensor\n    sparse_bsc_tensor\n    asarray\n    as_tensor\n    as_strided\n    from_numpy\n    from_dlpack\n    frombuffer\n    zeros\n    zeros_like\n    ones\n    ones_like\n    arange\n    range\n    linspace\n    logspace\n    eye\n    empty\n    empty_like\n    empty_strided\n    full\n    full_like\n    quantize_per_tensor\n    quantize_per_channel\n    dequantize\n    complex\n    polar\n    heaviside\n\n.. _indexing-slicing-joining:\n\nIndexing, Slicing, Joining, Mutating Ops\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":47,"to":90}}}}],["1660",{"pageContent":".. _indexing-slicing-joining:\n\nIndexing, Slicing, Joining, Mutating Ops\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    adjoint\n    argwhere\n    cat\n    concat\n    concatenate\n    conj\n    chunk\n    dsplit\n    column_stack\n    dstack\n    gather\n    hsplit\n    hstack\n    index_add\n    index_copy\n    index_reduce\n    index_select\n    masked_select\n    movedim\n    moveaxis\n    narrow\n    narrow_copy\n    nonzero\n    permute\n    reshape\n    row_stack\n    select\n    scatter\n    diagonal_scatter\n    select_scatter\n    slice_scatter\n    scatter_add\n    scatter_reduce\n    split\n    squeeze\n    stack\n    swapaxes\n    swapdims\n    t\n    take\n    take_along_dim\n    tensor_split\n    tile\n    transpose\n    unbind\n    unsqueeze\n    vsplit\n    vstack\n    where\n\n.. _generators:\n\nGenerators\n----------------------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Generator\n\n.. _random-sampling:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":90,"to":158}}}}],["1661",{"pageContent":".. _generators:\n\nGenerators\n----------------------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Generator\n\n.. _random-sampling:\n\nRandom sampling\n----------------------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    seed\n    manual_seed\n    initial_seed\n    get_rng_state\n    set_rng_state\n\n.. autoattribute:: torch.default_generator\n   :annotation:  Returns the default CPU torch.Generator\n\n.. The following doesn't actually seem to exist.\n   https://github.com/pytorch/pytorch/issues/27780\n   .. autoattribute:: torch.cuda.default_generators\n      :annotation:  If cuda is available, returns a tuple of default CUDA torch.Generator-s.\n                    The number of CUDA torch.Generator-s returned is equal to the number of\n                    GPUs available in the system.\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":158,"to":193}}}}],["1662",{"pageContent":"bernoulli\n    multinomial\n    normal\n    poisson\n    rand\n    rand_like\n    randint\n    randint_like\n    randn\n    randn_like\n    randperm\n\n.. _inplace-random-sampling:\n\nIn-place random sampling\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nThere are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":195,"to":212}}}}],["1663",{"pageContent":"In-place random sampling\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nThere are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:\n\n- :func:`torch.Tensor.bernoulli_` - in-place version of :func:`torch.bernoulli`\n- :func:`torch.Tensor.cauchy_` - numbers drawn from the Cauchy distribution\n- :func:`torch.Tensor.exponential_` - numbers drawn from the exponential distribution\n- :func:`torch.Tensor.geometric_` - elements drawn from the geometric distribution\n- :func:`torch.Tensor.log_normal_` - samples from the log-normal distribution\n- :func:`torch.Tensor.normal_` - in-place version of :func:`torch.normal`\n- :func:`torch.Tensor.random_` - numbers sampled from the discrete uniform distribution\n- :func:`torch.Tensor.uniform_` - numbers sampled from the continuous uniform distribution\n\nQuasi-random sampling\n~~~~~~~~~~~~~~~~~~~~~\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: sobolengine.rst\n\n    quasirandom.SobolEngine","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":212,"to":233}}}}],["1664",{"pageContent":"Quasi-random sampling\n~~~~~~~~~~~~~~~~~~~~~\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: sobolengine.rst\n\n    quasirandom.SobolEngine\n\nSerialization\n----------------------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    save\n    load\n\nParallelism\n----------------------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    get_num_threads\n    set_num_threads\n    get_num_interop_threads\n    set_num_interop_threads\n\n.. _torch-rst-local-disable-grad:\n\nLocally disabling gradient computation\n--------------------------------------\nThe context managers :func:`torch.no_grad`, :func:`torch.enable_grad`, and\n:func:`torch.set_grad_enabled` are helpful for locally disabling and enabling\ngradient computation. See :ref:`locally-disable-grad` for more details on\ntheir usage.  These context managers are thread local, so they won't\nwork if you send work to another thread using the ``threading`` module, etc.\n\nExamples::","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":233,"to":272}}}}],["1665",{"pageContent":"Examples::\n\n  >>> x = torch.zeros(1, requires_grad=True)\n  >>> with torch.no_grad():\n  ...     y = x * 2\n  >>> y.requires_grad\n  False\n\n  >>> is_train = False\n  >>> with torch.set_grad_enabled(is_train):\n  ...     y = x * 2\n  >>> y.requires_grad\n  False\n\n  >>> torch.set_grad_enabled(True)  # this can also be used as a function\n  >>> y = x * 2\n  >>> y.requires_grad\n  True\n\n  >>> torch.set_grad_enabled(False)\n  >>> y = x * 2\n  >>> y.requires_grad\n  False\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    no_grad\n    enable_grad\n    set_grad_enabled\n    is_grad_enabled\n    inference_mode\n    is_inference_mode_enabled\n\nMath operations\n---------------\n\nPointwise Ops\n~~~~~~~~~~~~~~~~~~~~~~\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":272,"to":315}}}}],["1666",{"pageContent":"abs\n    absolute\n    acos\n    arccos\n    acosh\n    arccosh\n    add\n    addcdiv\n    addcmul\n    angle\n    asin\n    arcsin\n    asinh\n    arcsinh\n    atan\n    arctan\n    atanh\n    arctanh\n    atan2\n    arctan2\n    bitwise_not\n    bitwise_and\n    bitwise_or\n    bitwise_xor\n    bitwise_left_shift\n    bitwise_right_shift\n    ceil\n    clamp\n    clip\n    conj_physical\n    copysign\n    cos\n    cosh\n    deg2rad\n    div\n    divide\n    digamma\n    erf\n    erfc\n    erfinv\n    exp\n    exp2\n    expm1\n    fake_quantize_per_channel_affine\n    fake_quantize_per_tensor_affine\n    fix\n    float_power\n    floor\n    floor_divide\n    fmod\n    frac\n    frexp\n    gradient\n    imag\n    ldexp\n    lerp\n    lgamma\n    log\n    log10\n    log1p\n    log2\n    logaddexp\n    logaddexp2\n    logical_and\n    logical_not\n    logical_or\n    logical_xor\n    logit\n    hypot\n    i0\n    igamma\n    igammac\n    mul\n    multiply\n    mvlgamma\n    nan_to_num\n    neg\n    negative\n    nextafter\n    polygamma\n    positive\n    pow\n    quantized_batch_norm\n    quantized_max_pool1d\n    quantized_max_pool2d\n    rad2deg","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":317,"to":402}}}}],["1667",{"pageContent":"mul\n    multiply\n    mvlgamma\n    nan_to_num\n    neg\n    negative\n    nextafter\n    polygamma\n    positive\n    pow\n    quantized_batch_norm\n    quantized_max_pool1d\n    quantized_max_pool2d\n    rad2deg\n    real\n    reciprocal\n    remainder\n    round\n    rsqrt\n    sigmoid\n    sign\n    sgn\n    signbit\n    sin\n    sinc\n    sinh\n    softmax\n    sqrt\n    square\n    sub\n    subtract\n    tan\n    tanh\n    true_divide\n    trunc\n    xlogy","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":402,"to":437}}}}],["1668",{"pageContent":"Reduction Ops\n~~~~~~~~~~~~~~~~~~~~~~\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    argmax\n    argmin\n    amax\n    amin\n    aminmax\n    all\n    any\n    max\n    min\n    dist\n    logsumexp\n    mean\n    nanmean\n    median\n    nanmedian\n    mode\n    norm\n    nansum\n    prod\n    quantile\n    nanquantile\n    std\n    std_mean\n    sum\n    unique\n    unique_consecutive\n    var\n    var_mean\n    count_nonzero\n\nComparison Ops\n~~~~~~~~~~~~~~~~~~~~~~\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    allclose\n    argsort\n    eq\n    equal\n    ge\n    greater_equal\n    gt\n    greater\n    isclose\n    isfinite\n    isin\n    isinf\n    isposinf\n    isneginf\n    isnan\n    isreal\n    kthvalue\n    le\n    less_equal\n    lt\n    less\n    maximum\n    minimum\n    fmax\n    fmin\n    ne\n    not_equal\n    sort\n    topk\n    msort\n\n\nSpectral Ops\n~~~~~~~~~~~~~~~~~~~~~~\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":439,"to":517}}}}],["1669",{"pageContent":"Spectral Ops\n~~~~~~~~~~~~~~~~~~~~~~\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    stft\n    istft\n    bartlett_window\n    blackman_window\n    hamming_window\n    hann_window\n    kaiser_window\n\n\nOther Operations\n~~~~~~~~~~~~~~~~~~~~~~\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":517,"to":537}}}}],["1670",{"pageContent":"Other Operations\n~~~~~~~~~~~~~~~~~~~~~~\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    atleast_1d\n    atleast_2d\n    atleast_3d\n    bincount\n    block_diag\n    broadcast_tensors\n    broadcast_to\n    broadcast_shapes\n    bucketize\n    cartesian_prod\n    cdist\n    clone\n    combinations\n    corrcoef\n    cov\n    cross\n    cummax\n    cummin\n    cumprod\n    cumsum\n    diag\n    diag_embed\n    diagflat\n    diagonal\n    diff\n    einsum\n    flatten\n    flip\n    fliplr\n    flipud\n    kron\n    rot90\n    gcd\n    histc\n    histogram\n    histogramdd\n    meshgrid\n    lcm\n    logcumsumexp\n    ravel\n    renorm\n    repeat_interleave\n    roll\n    searchsorted\n    tensordot\n    trace\n    tril\n    tril_indices\n    triu\n    triu_indices\n    unflatten\n    vander\n    view_as_real\n    view_as_complex\n    resolve_conj\n    resolve_neg\n\n\nBLAS and LAPACK Operations\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":537,"to":606}}}}],["1671",{"pageContent":"BLAS and LAPACK Operations\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    addbmm\n    addmm\n    addmv\n    addr\n    baddbmm\n    bmm\n    chain_matmul\n    cholesky\n    cholesky_inverse\n    cholesky_solve\n    dot\n    geqrf\n    ger\n    inner\n    inverse\n    det\n    logdet\n    slogdet\n    lu\n    lu_solve\n    lu_unpack\n    matmul\n    matrix_power\n    matrix_exp\n    mm\n    mv\n    orgqr\n    ormqr\n    outer\n    pinverse\n    qr\n    svd\n    svd_lowrank\n    pca_lowrank\n    lobpcg\n    trapz\n    trapezoid\n    cumulative_trapezoid\n    triangular_solve\n    vdot\n\nForeach Operations\n~~~~~~~~~~~~~~~~~~\n\n.. warning::\n    This API is in beta and subject to future changes.\n    Forward-mode AD is not supported.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":606,"to":662}}}}],["1672",{"pageContent":"_foreach_abs\n    _foreach_abs_\n    _foreach_acos\n    _foreach_acos_\n    _foreach_asin\n    _foreach_asin_\n    _foreach_atan\n    _foreach_atan_\n    _foreach_ceil\n    _foreach_ceil_\n    _foreach_cos\n    _foreach_cos_\n    _foreach_cosh\n    _foreach_cosh_\n    _foreach_erf\n    _foreach_erf_\n    _foreach_erfc\n    _foreach_erfc_\n    _foreach_exp\n    _foreach_exp_\n    _foreach_expm1\n    _foreach_expm1_\n    _foreach_floor\n    _foreach_floor_\n    _foreach_log\n    _foreach_log_\n    _foreach_log10\n    _foreach_log10_\n    _foreach_log1p\n    _foreach_log1p_\n    _foreach_log2\n    _foreach_log2_\n    _foreach_neg\n    _foreach_neg_\n    _foreach_tan\n    _foreach_tan_\n    _foreach_sin\n    _foreach_sin_\n    _foreach_sinh\n    _foreach_sinh_\n    _foreach_round\n    _foreach_round_\n    _foreach_sqrt\n    _foreach_sqrt_\n    _foreach_lgamma\n    _foreach_lgamma_\n    _foreach_frac\n    _foreach_frac_\n    _foreach_reciprocal\n    _foreach_reciprocal_\n    _foreach_sigmoid\n    _foreach_sigmoid_\n    _foreach_trunc\n    _foreach_trunc_\n    _foreach_zero_","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":664,"to":718}}}}],["1673",{"pageContent":"Utilities\n----------------------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    compiled_with_cxx11_abi\n    result_type\n    can_cast\n    promote_types\n    use_deterministic_algorithms\n    are_deterministic_algorithms_enabled\n    is_deterministic_algorithms_warn_only_enabled\n    set_deterministic_debug_mode\n    get_deterministic_debug_mode\n    set_float32_matmul_precision\n    get_float32_matmul_precision\n    set_warn_always\n    is_warn_always_enabled\n    vmap\n    _assert\n\nSymbolic Numbers\n----------------\n.. autoclass:: SymInt\n    :members:\n\n.. autoclass:: SymFloat\n    :members:\n\n.. autoclass:: SymBool\n    :members:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    sym_float\n    sym_int\n    sym_max\n    sym_min\n    sym_not\n\nOptimizations\n-------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    compile\n\n`torch.compile documentation <https://pytorch.org/docs/master/compile/index.html>`__","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":720,"to":771}}}}],["1674",{"pageContent":"Optimizations\n-------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    compile\n\n`torch.compile documentation <https://pytorch.org/docs/master/compile/index.html>`__\n\nOperator Tags\n------------------------------------\n.. autoclass:: Tag\n    :members:\n\n.. Empty submodules added only for tracking.\n.. py:module:: torch.contrib\n.. py:module:: torch.utils.backcompat\n\n.. This submodule is split manually without a top level page.\n.. py:module:: torch.utils\n\n.. This module is only used internally for ROCm builds.\n.. py:module:: torch.utils.hipify\n\n.. This module needs to be documented. Adding here in the meantime\n.. for tracking purposes\n.. py:module:: torch.utils.model_dump\n\n.. automodule:: torch.autograd\n.. currentmodule:: torch.autograd\n\nEngine Configuration\n----------------------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    set_multithreading_enabled","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/torch.rst","loc":{"lines":{"from":771,"to":809}}}}],["1675",{"pageContent":".. currentmodule:: torch\n\n.. _type-info-doc:\n\nType Info\n=========\n\nThe numerical properties of a :class:`torch.dtype` can be accessed through either the :class:`torch.finfo` or the :class:`torch.iinfo`.\n\n.. _finfo-doc:\n\ntorch.finfo\n-----------\n\n.. class:: torch.finfo\n\nA :class:`torch.finfo` is an object that represents the numerical properties of a floating point\n:class:`torch.dtype`, (i.e. ``torch.float32``, ``torch.float64``, ``torch.float16``, and ``torch.bfloat16``). This is similar to `numpy.finfo <https://docs.scipy.org/doc/numpy/reference/generated/numpy.finfo.html>`_.\n\nA :class:`torch.finfo` provides the following attributes:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/type_info.rst","loc":{"lines":{"from":1,"to":20}}}}],["1676",{"pageContent":"===============        =====   ==========================================================================\nName                   Type    Description\n===============        =====   ==========================================================================\nbits                   int     The number of bits occupied by the type.\neps                    float   The smallest representable number such that ``1.0 + eps != 1.0``.\nmax                    float   The largest representable number.\nmin                    float   The smallest representable number (typically ``-max``).\ntiny                   float   The smallest positive normal number. Equivalent to ``smallest_normal``.\nsmallest_normal        float   The smallest positive normal number. See notes.\nresolution             float   The approximate decimal resolution of this type, i.e., ``10**-precision``.\n===============        =====   ==========================================================================","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/type_info.rst","loc":{"lines":{"from":22,"to":32}}}}],["1677",{"pageContent":".. note::\n  The constructor of :class:`torch.finfo` can be called without argument, in which case the class is created for the pytorch default dtype (as returned by :func:`torch.get_default_dtype`).\n\n.. note::\n  `smallest_normal` returns the smallest *normal* number, but there are smaller\n  subnormal numbers. See https://en.wikipedia.org/wiki/Denormal_number\n  for more information.\n\n\n.. _iinfo-doc:\n\ntorch.iinfo\n------------\n\n.. class:: torch.iinfo\n\n\nA :class:`torch.iinfo` is an object that represents the numerical properties of a integer\n:class:`torch.dtype` (i.e. ``torch.uint8``, ``torch.int8``, ``torch.int16``, ``torch.int32``, and ``torch.int64``). This is similar to `numpy.iinfo <https://docs.scipy.org/doc/numpy/reference/generated/numpy.iinfo.html>`_.\n\nA :class:`torch.iinfo` provides the following attributes:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/type_info.rst","loc":{"lines":{"from":34,"to":54}}}}],["1678",{"pageContent":"A :class:`torch.iinfo` provides the following attributes:\n\n=========   =====   ========================================\nName        Type    Description\n=========   =====   ========================================\nbits        int     The number of bits occupied by the type.\nmax         int     The largest representable number.\nmin         int     The smallest representable number.\n=========   =====   ========================================","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/type_info.rst","loc":{"lines":{"from":54,"to":62}}}}]]