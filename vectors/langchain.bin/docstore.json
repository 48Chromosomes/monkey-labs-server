[["0",{"pageContent":"# Website\n\nThis website is built using [Docusaurus 2](https://docusaurus.io/), a modern static website generator.\n\n### Installation\n\n```\n$ yarn\n```\n\n### Local Development\n\n```\n$ yarn start\n```\n\nThis command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.\n\n### Build\n\n```\n$ yarn build\n```\n\nThis command generates static content into the `build` directory and can be served using any static contents hosting service.\n\n### Deployment\n\nUsing SSH:\n\n```\n$ USE_SSH=true yarn deploy\n```\n\nNot using SSH:\n\n```\n$ GIT_USER=<Your GitHub username> yarn deploy\n```\n\nIf you are using GitHub pages for hosting, this command is a convenient way to build the website and push to the `gh-pages` branch.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/README.md","loc":{"lines":{"from":1,"to":41}}}}],["1",{"pageContent":"Continuous Integration\n\nSome common defaults for linting/formatting have been set for you. If you integrate your project with an open source Continuous Integration system (e.g. Travis CI, CircleCI), you may check for issues using the following command.\n\n```\n$ yarn ci\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/README.md","loc":{"lines":{"from":43,"to":49}}}}],["2",{"pageContent":"# Databerry\n\nThis page covers how to use the [Databerry](https://databerry.ai) within LangChain.\n\n## What is Databerry?\n\nDataberry is an [open source](https://github.com/gmpetrov/databerry) document retrievial platform that helps to connect your personal data with Large Language Models.\n\n![Databerry](/img/DataberryDashboard.png)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/ecosystem/databerry.md","loc":{"lines":{"from":1,"to":9}}}}],["3",{"pageContent":"Quick start\n\nRetrieving documents stored in Databerry from LangChain is very easy!\n\n```typescript\nimport { DataberryRetriever } from \"langchain/retrievers/databerry\";\n\nconst retriever = new DataberryRetriever({\n  datastoreUrl: \"https://api.databerry.ai/query/clg1xg2h80000l708dymr0fxc\",\n  apiKey: \"DATABERRY_API_KEY\", // optional: needed for private datastores\n  topK: 8, // optional: default value is 3\n});\n\n// Create a chain that uses the OpenAI LLM and Databerry retriever.\nconst chain = RetrievalQAChain.fromLLM(model, retriever);\n\n// Call the chain with a query.\nconst res = await chain.call({\n  query: \"What's Databerry?\",\n});\n\nconsole.log({ res });\n/*\n{\n  res: {\n    text: 'Databerry provides a user-friendly solution to quickly setup a semantic search system over your personal data without any technical knowledge.'\n  }\n}\n*/\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/ecosystem/databerry.md","loc":{"lines":{"from":11,"to":40}}}}],["4",{"pageContent":"# Helicone\n\nThis page covers how to use the [Helicone](https://helicone.ai) within LangChain.\n\n## What is Helicone?\n\nHelicone is an [open source](https://github.com/Helicone/helicone) observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.\n\n![Helicone](/img/HeliconeDashboard.png)\n\n## Quick start\n\nWith your LangChain environment you can just add the following parameter.\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\nNow head over to [helicone.ai](https://helicone.ai/onboarding?step=2) to create your account, and add your OpenAI API key within our dashboard to view your logs.\n\n![Helicone](/img/HeliconeKeys.png)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/ecosystem/helicone.md","loc":{"lines":{"from":1,"to":27}}}}],["5",{"pageContent":"How to enable Helicone caching\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n    baseOptions: {\n      headers: {\n        \"Helicone-Cache-Enabled\": \"true\",\n      },\n    },\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\n[Helicone caching docs](https://docs.helicone.ai/advanced-usage/caching)\n\n## How to use Helicone custom properties\n\n```typescript\nconst model = new OpenAI(\n  {},\n  {\n    basePath: \"https://oai.hconeai.com/v1\",\n    baseOptions: {\n      headers: {\n        \"Helicone-Property-Session\": \"24\",\n        \"Helicone-Property-Conversation\": \"support_issue_2\",\n        \"Helicone-Property-App\": \"mobile\",\n      },\n    },\n  }\n);\nconst res = await model.call(\"What is a helicone?\");\n```\n\n[Helicone property docs](https://docs.helicone.ai/advanced-usage/custom-properties)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/ecosystem/helicone.md","loc":{"lines":{"from":29,"to":67}}}}],["6",{"pageContent":"# Unstructured\n\nThis page covers how to use [Unstructured](https://unstructured.io) within LangChain.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/ecosystem/unstructured.md","loc":{"lines":{"from":1,"to":3}}}}],["7",{"pageContent":"What is Unstructured?\n\nUnstructured is an [open source](https://github.com/Unstructured-IO/unstructured) Python package\nfor extracting text from raw documents for use in machine learning applications. Currently,\nUnstructured supports partitioning Word documents (in `.doc` or `.docx` format),\nPowerPoints (in `.ppt` or `.pptx` format), PDFs, HTML files, images,\nemails (in `.eml` or `.msg` format), epubs, markdown, and plain text files.\n`unstructured` is a Python package and cannot be used directly with TS/JS, Unstructured\nalso maintains a [REST API](https://github.com/Unstructured-IO/unstructured-api) to support\npre-processing pipelines written in other programming languages. The endpoint for the\nhosted Unstructured API is `https://api.unstructured.io/general/v0/general`, or you can run\nthe service locally using the instructions found\n[here](https://github.com/Unstructured-IO/unstructured-api#dizzy-instructions-for-using-the-docker-image).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/ecosystem/unstructured.md","loc":{"lines":{"from":5,"to":17}}}}],["8",{"pageContent":"Quick start\n\nYou can use Unstructured in`langchainjs` with the following code.\nReplace the filename with the file you would like to process.\nIf you are running the container locally, switch the url to\n`https://api.unstructured.io/general/v0/general`.\n\n```typescript\nimport { UnstructuredLoader } from \"langchain/document_loaders/fs/unstructured\";\n\nconst loader = new UnstructuredLoader(\n  \"https://api.unstructured.io/general/v0/general\",\n  \"langchain/src/document_loaders/tests/example_data/example.txt\"\n);\nconst docs = await loader.load();\n```\n\nStayed tuned for future updates, including functionality equivalent to\n`UnstructuredDirectoryLoader` in `langchain`!.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/ecosystem/unstructured.md","loc":{"lines":{"from":19,"to":37}}}}],["9",{"pageContent":"---\nsidebar_position: 1\n---\n\n# Setup and Installation\n\n:::info\nUpdating from <0.0.52? See [this section](#updating-from-0052) for instructions.\n:::\n\n## Quickstart\n\nIf you want to get started quickly on using LangChain in Node.js, [clone this repository](https://github.com/domeccleston/langchain-ts-starter) and follow the README instructions for a boilerplate project with those dependencies set up.\n\nIf you prefer to set things up yourself, or you want to run LangChain in other environments, read on for instructions.\n\n## Installation\n\nTo get started, install LangChain with the following command:\n\n```bash npm2yarn\nnpm install -S langchain\n```\n\n### TypeScript\n\nLangChain is written in TypeScript and provides type definitions for all of its public APIs.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/getting-started/install.md","loc":{"lines":{"from":1,"to":27}}}}],["10",{"pageContent":"Loading the library\n\n### ESM\n\nLangChain provides an ESM build targeting Node.js environments. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\nIf you are using TypeScript in an ESM project we suggest updating your `tsconfig.json` to include the following:\n\n```json title=\"tsconfig.json\"\n{\n  \"compilerOptions\": {\n    ...\n    \"target\": \"ES2020\", // or higher\n    \"module\": \"nodenext\",\n  }\n}\n```\n\n### CommonJS\n\nLangChain provides a CommonJS build targeting Node.js environments. You can import it using the following syntax:\n\n```typescript\nconst { OpenAI } = require(\"langchain/llms/openai\");\n```\n\n### Cloudflare Workers\n\nLangChain can be used in Cloudflare Workers. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/getting-started/install.md","loc":{"lines":{"from":29,"to":65}}}}],["11",{"pageContent":"Cloudflare Workers\n\nLangChain can be used in Cloudflare Workers. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\n### Vercel / Next.js\n\nLangChain can be used in Vercel / Next.js. We support using LangChain in frontend components, in Serverless functions and in Edge functions. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\nIf you want to use LangChain in frontend `pages`, you need to add the following to your `next.config.js` to enable support for WebAssembly modules (which is required by the tokenizer library `@dqbd/tiktoken`):\n\n```js title=\"next.config.js\"\nconst nextConfig = {\n  webpack(config) {\n    config.experiments = {\n      asyncWebAssembly: true,\n      layers: true,\n    };\n\n    return config;\n  },\n};\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/getting-started/install.md","loc":{"lines":{"from":65,"to":94}}}}],["12",{"pageContent":"Deno / Supabase Edge Functions\n\nLangChain can be used in Deno / Supabase Edge Functions. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"https://esm.sh/langchain/llms/openai\";\n```\n\nWe recommend looking at our [Supabase Template](https://github.com/langchain-ai/langchain-template-supabase) for an example of how to use LangChain in Supabase Edge Functions.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/getting-started/install.md","loc":{"lines":{"from":96,"to":104}}}}],["13",{"pageContent":"Browser\n\nLangChain can be used in the browser. In our CI we test bundling LangChain with Webpack and Vite, but other bundlers should work too. You can import it using the following syntax:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\n#### Create React App\n\nIf you're using `create-react-app` by default it doesn't support WebAssembly modules, so the tokenizer library `@dqbd/tiktoken` will not work in the browser. You can follow the instructions [here](https://github.com/dqbd/tiktoken/tree/main/js#create-react-app) to enable support for WebAssembly modules.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/getting-started/install.md","loc":{"lines":{"from":106,"to":116}}}}],["14",{"pageContent":"Vite\n\nIf you're using Vite, you need to add the following to your `vite.config.js` to enable support for WebAssembly modules (which is required by the tokenizer library `@dqbd/tiktoken`):\n\n```bash npm2yarn\nnpm install -D vite-plugin-wasm vite-plugin-top-level-await\n```\n\n```js title=\"vite.config.js\"\nimport wasm from \"vite-plugin-wasm\";\nimport topLevelAwait from \"vite-plugin-top-level-await\";\nimport { defineConfig } from \"vite\";\n\nexport default defineConfig({\n  plugins: [wasm(), topLevelAwait()],\n});\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/getting-started/install.md","loc":{"lines":{"from":118,"to":134}}}}],["15",{"pageContent":"Updating from <0.0.52\n\nIf you are updating from a version of LangChain prior to 0.0.52, you will need to update your imports to use the new path structure.\n\nFor example, if you were previously doing\n\n```typescript\nimport { OpenAI } from \"langchain/llms\";\n```\n\nyou will now need to do\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/getting-started/install.md","loc":{"lines":{"from":136,"to":149}}}}],["16",{"pageContent":"This applies to all imports from the following 6 modules, which have been split into submodules for each integration. The combined modules are deprecated, do not work outside of Node.js, and will be removed in a future version.\n\n- If you were using `langchain/llms`, see [LLMs](../modules/models/llms/integrations) for updated import paths.\n- If you were using `langchain/chat_models`, see [Chat Models](../modules/models/chat/integrations) for updated import paths.\n- If you were using `langchain/embeddings`, see [Embeddings](../modules/models/embeddings/integrations) for updated import paths.\n- If you were using `langchain/vectorstores`, see [Vector Stores](../modules/indexes/vector_stores/integrations/) for updated import paths.\n- If you were using `langchain/document_loaders`, see [Document Loaders](../modules/indexes/document_loaders/examples/) for updated import paths.\n- If you were using `langchain/retrievers`, see [Retrievers](../modules/indexes/retrievers/) for updated import paths.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/getting-started/install.md","loc":{"lines":{"from":152,"to":159}}}}],["17",{"pageContent":"Other modules are not affected by this change, and you can continue to import them from the same path.\n\nAdditionally, there are some breaking changes that were needed to support new environments:\n\n- `import { Calculator } from \"langchain/tools\";` now moved to\n  - `import { Calculator } from \"langchain/tools/calculator\";`\n- `import { loadLLM } from \"langchain/llms\";` now moved to\n  - `import { loadLLM } from \"langchain/llms/load\";`\n- `import { loadAgent } from \"langchain/agents\";` now moved to\n  - `import { loadAgent } from \"langchain/agents/load\";`\n- `import { loadPrompt } from \"langchain/prompts\";` now moved to\n  - `import { loadPrompt } from \"langchain/prompts/load\";`\n- `import { loadChain } from \"langchain/chains\";` now moved to\n  - `import { loadChain } from \"langchain/chains/load\";`","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/getting-started/install.md","loc":{"lines":{"from":161,"to":174}}}}],["18",{"pageContent":"Unsupported: Node.js 16\n\nWe do not support Node.js 16, but if you still want to run LangChain on Node.js 16, you will need to follow the instructions in this section. We do not guarantee that these instructions will continue to work in the future.\n\nYou will have to make `fetch` available globally, either:\n\n- run your application with `NODE_OPTIONS='--experimental-fetch' node ...`, or\n- install `node-fetch` and follow the instructions [here](https://github.com/node-fetch/node-fetch#providing-global-access)\n\nAdditionally you'll have to polyfill `unstructuredClone`, eg. by installing `core-js` and following the instructions [here](https://github.com/zloirock/core-js).\n\nIf you are running this on Node.js 18 or 19, you do not need to do anything.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/getting-started/install.md","loc":{"lines":{"from":176,"to":187}}}}],["19",{"pageContent":"# Welcome to LangChain\n\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also:\n\n- _Be data-aware_: connect a language model to other sources of data\n- _Be agentic_: allow a language model to interact with its environment\n\nThe LangChain framework is designed with the above principles in mind.\n\n## Getting Started\n\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\n\n- [Getting Started Documentation](./getting-started/guide-llm.mdx)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/index.md","loc":{"lines":{"from":1,"to":14}}}}],["20",{"pageContent":"Components\n\nThere are several main modules that LangChain provides support for. For each module we provide some examples to get started and get familiar with some of the concepts. These modules are, in increasing order of complexity:\n\n- Prompts: This includes prompt management, prompt optimization, and prompt serialization.\n\n- LLMs: This includes a generic interface for all LLMs, and common utilities for working with LLMs.\n\n- Indexes: This includes patterns and functionality for structuring your own text data so it can interact with language models (including embeddings, vectorstores, text splitters, retrievers, etc).\n\n- Memory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/index.md","loc":{"lines":{"from":16,"to":26}}}}],["21",{"pageContent":"- Chains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\n\n- Agents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/index.md","loc":{"lines":{"from":28,"to":30}}}}],["22",{"pageContent":"Reference Docs\n\n---\n\nAll of LangChain's reference documentation, in one place. Full documentation on all methods and classes.\n\n## Production\n\n---\n\nAs you move from prototyping into production, we're developing resources to help you do so.\nThese including:\n\n- Deployment: resources on how to deploy your end application.\n- Tracing: resouces on how to use tracing to log and debug your applications.\n\n## Additional Resources\n\n---\n\nAdditional collection of resources we think may be useful as you develop your application!\n\n- [LangChainHub](https://github.com/hwchase17/langchain-hub): The LangChainHub is a place to share and explore other prompts, chains, and agents.\n\n- [Discord](https://discord.gg/6adMQxSpJS): Join us on our Discord to discuss all things LangChain!\n\n- [Production Support](https://forms.gle/57d8AmXBYp8PP8tZA): As you move your LangChains into production, we'd love to offer more comprehensive support. Please fill out this form and we'll set up a dedicated support Slack channel.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/index.md","loc":{"lines":{"from":32,"to":58}}}}],["23",{"pageContent":"---\nsidebar_label: Getting Started\nhide_table_of_contents: true\n---\n\n# Getting Started: Agent Executors\n\nAgents use an LLM to determine which actions to take and in what order. An action can either be using a tool and observing its output, or returning to the user.\n\nWhen used correctly agents can be extremely powerful. In this tutorial, we show you how to easily use agents through the simplest, highest level API.\n\nIn order to load agents, you should understand the following concepts:\n\n- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, code REPL, other chains. The interface for a tool is currently a function that is expected to have a string as an input, with a string as an output.\n- LLM: The language model powering the agent.\n- Agent: The agent to use. This should be a string that references a support agent class. Because this notebook focuses on the simplest, highest level API, this only covers using the standard supported agents.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/executor/getting-started.md","loc":{"lines":{"from":1,"to":16}}}}],["24",{"pageContent":"For this example, you'll need to set the SerpAPI environment variables in the `.env` file.\n\n```bash\nSERPAPI_API_KEY=\"...\"","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/executor/getting-started.md","loc":{"lines":{"from":18,"to":21}}}}],["25",{"pageContent":"Now we can get started!\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input =\n  \"Who is Olivia Wilde's boyfriend?\" +\n  \" What is his current age raised to the 0.23 power?\";\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/executor/getting-started.md","loc":{"lines":{"from":24,"to":54}}}}],["26",{"pageContent":"```shell\nlangchain-examples:start: Executing with input \"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\"...\nlangchain-examples:start: Got output Olivia Wilde's boyfriend is Jason Sudeikis, and his current age raised to the 0.23 power is 2.4242784855673896.\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/executor/getting-started.md","loc":{"lines":{"from":57,"to":60}}}}],["27",{"pageContent":"# JSON Agent Toolkit\n\nThis example shows how to load and use an agent with a JSON toolkit.\n\n```typescript\nimport * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { JsonToolkit, createJsonAgent } from \"langchain/agents\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const toolkit = new JsonToolkit(new JsonSpec(data));\n  const model = new OpenAI({ temperature: 0 });\n  const executor = createJsonAgent(model, toolkit);\n\n  const input = `What are the required parameters in the request body to the /completions endpoint?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/toolkits/examples/json.md","loc":{"lines":{"from":1,"to":33}}}}],["28",{"pageContent":"console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/toolkits/examples/json.md","loc":{"lines":{"from":33,"to":47}}}}],["29",{"pageContent":"# OpenAPI Agent Toolkit\n\nThis example shows how to load and use an agent with a OpenAPI toolkit.\n\n```typescript\nimport * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { createOpenApiAgent, OpenApiToolkit } from \"langchain/agents\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const headers = {\n    \"Content-Type\": \"application/json\",\n    Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,\n  };\n  const model = new OpenAI({ temperature: 0 });\n  const toolkit = new OpenApiToolkit(new JsonSpec(data), model, headers);\n  const executor = createOpenApiAgent(model, toolkit);","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/toolkits/examples/openapi.md","loc":{"lines":{"from":1,"to":31}}}}],["30",{"pageContent":"const input = `Make a POST request to openai /completions. The prompt should be 'tell me a joke.'`;\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/toolkits/examples/openapi.md","loc":{"lines":{"from":33,"to":47}}}}],["31",{"pageContent":"# VectorStore Agent Toolkit\n\nThis example shows how to load and use an agent with a vectorstore toolkit.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";\nimport {\n  VectorStoreToolkit,\n  createVectorStoreAgent,\n  VectorStoreInfo,\n} from \"langchain/agents\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  /* Load in the file we want to do question answering over */\n  const text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n  /* Split the text into chunks */\n  const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\n  const docs = await textSplitter.createDocuments([text]);\n  /* Create the vectorstore */\n  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/toolkits/examples/vectorstore.md","loc":{"lines":{"from":1,"to":25}}}}],["32",{"pageContent":"/* Create the agent */\n  const vectorStoreInfo: VectorStoreInfo = {\n    name: \"state_of_union_address\",\n    description: \"the most recent state of the Union address\",\n    vectorStore,\n  };\n\n  const toolkit = new VectorStoreToolkit(vectorStoreInfo, model);\n  const agent = createVectorStoreAgent(model, toolkit);\n\n  const input =\n    \"What did biden say about Ketanji Brown Jackson is the state of the union address?\";\n  console.log(`Executing: ${input}`);\n  const result = await agent.call({ input });\n  console.log(`Got output ${result.output}`);\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/toolkits/examples/vectorstore.md","loc":{"lines":{"from":27,"to":50}}}}],["33",{"pageContent":"# Agents with Vector Stores\n\nThis notebook covers how to combine agents and vector stores. The use case for this is that you’ve ingested your data into a vector store and want to interact with it in an agentic manner.\n\nThe recommended method for doing so is to create a VectorDBQAChain and then use that as a tool in the overall agent. Let’s take a look at doing this below. You can do this with multiple different vector databases, and use the agent as a way to choose between them. There are two different ways of doing this - you can either let the agent use the vector stores as normal tools, or you can set `returnDirect: true` to just use the agent as a router.\n\nFirst, you'll want to import the relevant modules:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":1,"to":7}}}}],["34",{"pageContent":"First, you'll want to import the relevant modules:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI, ChainTool } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { HNSWLib } from \"langchain/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport * as fs from \"fs\";","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":7,"to":18}}}}],["35",{"pageContent":"Next, you'll want to create the vector store with your data, and then the QA chain to interact with that vector store.\n\n```typescript\nconst model = new OpenAI({ temperature: 0 });\n/* Load in the file we want to do question answering over */\nconst text = fs.readFileSync(\"state_of_the_union.txt\", \"utf8\");\n/* Split the text into chunks */\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n/* Create the vectorstore */\nconst vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());\n/* Create the chain */\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore);","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":21,"to":33}}}}],["36",{"pageContent":"Now that you have that chain, you can create a tool to use that chain. Note that you should update the name and description to be specific to your QA chain.\n\n```typescript\nconst qaTool = new ChainTool({\n  name: \"state-of-union-qa\",\n  description:\n    \"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address.\",\n  chain: chain,\n});","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":36,"to":44}}}}],["37",{"pageContent":"Now you can construct and using the tool just as you would any other!\n\n```typescript\nconst tools = [\n  new SerpAPI(process.env.SERPAPI_API_KEY, {\n    location: \"Austin,Texas,United States\",\n    hl: \"en\",\n    gl: \"us\",\n  }),\n  new Calculator(),\n  qaTool,\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\nconsole.log(\"Loaded agent.\");\n\nconst input = `What did biden say about ketanji brown jackson is the state of the union address?`;\n\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.call({ input });\n\nconsole.log(`Got output ${result.output}`);","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":47,"to":71}}}}],["38",{"pageContent":"You can also set `returnDirect: true` if you intend to use the agent as a router and just want to directly return the result of the VectorDBQAChain.\n\n```typescript\nconst qaTool = new ChainTool({\n  name: \"state-of-union-qa\",\n  description:\n    \"State of the Union QA - useful for when you need to ask questions about the most recent state of the union address.\",\n  chain: chain,\n  returnDirect: true,\n});\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/agents_with_vectorstores.md","loc":{"lines":{"from":74,"to":84}}}}],["39",{"pageContent":"---\nsidebar_label: Agent with AWS Lambda\nhide_table_of_contents: true\n---\n\n# Agent with AWS Lambda Integration\n\nFull docs here: https://docs.aws.amazon.com/lambda/index.html\n\n**AWS Lambda** is a serverless computing service provided by Amazon Web Services (AWS), designed to allow developers to build and run applications and services without the need for provisioning or managing servers. This serverless architecture enables you to focus on writing and deploying code, while AWS automatically takes care of scaling, patching, and managing the infrastructure required to run your applications.\n\nBy including a AWSLambda in the list of tools provided to an Agent, you can grant your Agent the ability to invoke code running in your AWS Cloud for whatever purposes you need.\n\nWhen an Agent uses the AWSLambda tool, it will provide an argument of type `string` which will in turn be passed into the Lambda function via the `event` parameter.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/lambda_agent.md","loc":{"lines":{"from":1,"to":14}}}}],["40",{"pageContent":"When an Agent uses the AWSLambda tool, it will provide an argument of type `string` which will in turn be passed into the Lambda function via the `event` parameter.\n\nThis quick start will demonstrate how an Agent could use a Lambda function to send an email via [Amazon Simple Email Service](https://aws.amazon.com/ses/). The lambda code which sends the email is not provided, but if you'd like to learn how this could be done, see [here](https://repost.aws/knowledge-center/lambda-send-email-ses). Keep in mind this is an intentionally simple example; Lambda can used to execute code for a near infinite number of other purposes (including executing more Langchains)!","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/lambda_agent.md","loc":{"lines":{"from":14,"to":16}}}}],["41",{"pageContent":"Note about credentials:\n\n- If you have not run [`aws configure`](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) via the AWS CLI, the `region`, `accessKeyId`, and `secretAccessKey` must be provided to the AWSLambda constructor.\n- The IAM role corresponding to those credentials must have permission to invoke the lambda function.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { AWSLambda } from \"langchain/tools/aws_lambda\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/lambda_agent.md","loc":{"lines":{"from":18,"to":27}}}}],["42",{"pageContent":"const model = new OpenAI({ temperature: 0 });\nconst emailSenderTool = new AWSLambda({\n  name: \"email-sender\",\n  // tell the Agent precisely what the tool does\n  description:\n    \"Sends an email with the specified content to testing123@gmail.com\",\n  region: \"us-east-1\", // optional: AWS region in which the function is deployed\n  accessKeyId: \"abc123\", // optional: access key id for a IAM user with invoke permissions\n  secretAccessKey: \"xyz456\", // optional: secret access key for that IAM user\n  functionName: \"SendEmailViaSES\", // the function name as seen in AWS Console\n});\nconst tools = [emailSenderTool, new SerpAPI(\"api_key_goes_here\")];\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\n\nconst input = `Find out the capital of Croatia. Once you have it, email the answer to testing123@gmail.com.`;\nconst result = await executor.call({ input });\nconsole.log(result);\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/lambda_agent.md","loc":{"lines":{"from":29,"to":48}}}}],["43",{"pageContent":"# Agent with Zapier NLA Integration\n\nFull docs here: https://nla.zapier.com/api/v1/dynamic/docs\n\n**Zapier Natural Language Actions** gives you access to the 5k+ apps and 20k+ actions on Zapier's platform through a natural language API interface.\n\nNLA supports apps like Gmail, Salesforce, Trello, Slack, Asana, HubSpot, Google Sheets, Microsoft Teams, and thousands more apps: https://zapier.com/apps\n\nZapier NLA handles ALL the underlying API auth and translation from natural language --> underlying API call --> return simplified output for LLMs. The key idea is you, or your users, expose a set of actions via an oauth-like setup window, which you can then query and execute via a REST API.\n\nNLA offers both API Key and OAuth for signing NLA API requests.\n\nServer-side (API Key): for quickly getting started, testing, and production scenarios where LangChain will only use actions exposed in the developer's Zapier account (and will use the developer's connected accounts on Zapier.com)","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/zapier_agent.md","loc":{"lines":{"from":1,"to":13}}}}],["44",{"pageContent":"User-facing (Oauth): for production scenarios where you are deploying an end-user facing application and LangChain needs access to end-user's exposed actions and connected accounts on Zapier.com\n\nThis quick start will focus on the server-side use case for brevity. Review full docs or reach out to nla@zapier.com for user-facing oauth developer support.\n\nThe example below demonstrates how to use the Zapier integration as an Agent:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport {\n  initializeAgentExecutorWithOptions,\n  ZapierToolKit,\n} from \"langchain/agents\";\nimport { ZapierNLAWrapper } from \"langchain/tools\";\n\nexport const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const zapier = new ZapierNLAWrapper();\n  const toolkit = await ZapierToolKit.fromZapierNLAWrapper(zapier);","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/zapier_agent.md","loc":{"lines":{"from":15,"to":32}}}}],["45",{"pageContent":"export const run = async () => {\n  const model = new OpenAI({ temperature: 0 });\n  const zapier = new ZapierNLAWrapper();\n  const toolkit = await ZapierToolKit.fromZapierNLAWrapper(zapier);\n\n  const executor = await initializeAgentExecutorWithOptions(\n    toolkit.tools,\n    model,\n    {\n      agentType: \"zero-shot-react-description\",\n      verbose: true,\n    }\n  );\n  console.log(\"Loaded agent.\");\n\n  const input = `Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier Slack channel.`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/agents/tools/zapier_agent.md","loc":{"lines":{"from":32,"to":55}}}}],["46",{"pageContent":"# CSV files\n\nThis example goes over how to load data from CSV files. The second argument is the `column` name to extract from the CSV file. One document will be created for each row in the CSV file. When `column` is not specified, each row is converted into a key/value pair with each key/value pair outputted to a new line in the document's `pageContent`. When `column` is specified, one document is created for each row, and the value of the specified column is used as the document's pageContent.\n\n## Setup\n\n```bash npm2yarn\nnpm install d3-dsv@2\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/csv.md","loc":{"lines":{"from":1,"to":9}}}}],["47",{"pageContent":"Setup\n\n```bash npm2yarn\nnpm install d3-dsv@2\n```\n\n## Usage, extracting all columns\n\nExample CSV file:\n\n```csv\nid,text\n1,This is a sentence.\n2,This is another sentence.\n```\n\nExample code:\n\n```typescript\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\nconst loader = new CSVLoader(\"src/document_loaders/example_data/example.csv\");\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"line\": 1,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"id: 1\ntext: This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"line\": 2,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"id: 2\ntext: This is another sentence.\",\n  },\n]\n*/\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/csv.md","loc":{"lines":{"from":9,"to":53}}}}],["48",{"pageContent":"Usage, extracting a single column\n\nExample CSV file:\n\n```csv\nid,text\n1,This is a sentence.\n2,This is another sentence.\n```\n\nExample code:\n\n```typescript\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\nconst loader = new CSVLoader(\n  \"src/document_loaders/example_data/example.csv\",\n  \"text\"\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"line\": 1,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"line\": 2,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/csv.md","loc":{"lines":{"from":55,"to":94}}}}],["49",{"pageContent":"---\nsidebar_position: 1\nhide_table_of_contents: true\n---\n\n# Folders with multiple files\n\nThis example goes over how to load data from folders with multiple files. The second argument is a map of file extensions to loader factories. Each file will be passed to the matching loader, and the resulting documents will be concatenated together.\n\nExample folder:\n\n```text\nsrc/document_loaders/example_data/example/\n├── example.json\n├── example.jsonl\n├── example.txt\n└── example.csv","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/directory.md","loc":{"lines":{"from":1,"to":17}}}}],["50",{"pageContent":"Example code:\n\n```typescript\nimport { DirectoryLoader } from \"langchain/document_loaders/fs/directory\";\nimport {\n  JSONLoader,\n  JSONLinesLoader,\n} from \"langchain/document_loaders/fs/json\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\nconst loader = new DirectoryLoader(\n  \"src/document_loaders/example_data/example\",\n  {\n    \".json\": (path) => new JSONLoader(path, \"/texts\"),\n    \".jsonl\": (path) => new JSONLinesLoader(path, \"/html\"),\n    \".txt\": (path) => new TextLoader(path),\n    \".csv\": (path) => new CSVLoader(path, \"text\"),\n  }\n);\nconst docs = await loader.load();\nconsole.log({ docs });\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/directory.md","loc":{"lines":{"from":20,"to":42}}}}],["51",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Docx files\n\nThis example goes over how to load data from docx files.\n\n# Setup\n\n```bash npm2yarn\nnpm install mammoth\n```\n\n# Usage\n\n```typescript\nimport { DocxLoader } from \"langchain/document_loaders/fs/docx\";\n\nconst loader = new DocxLoader(\n  \"src/document_loaders/tests/example_data/attention.docx\"\n);\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/docx.md","loc":{"lines":{"from":1,"to":25}}}}],["52",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# EPUB files\n\nThis example goes over how to load data from EPUB files. By default, one document will be created for each chapter in the EPUB file, you can change this behavior by setting the `splitChapters` option to `false`.\n\n# Setup\n\n```bash npm2yarn\nnpm install epub2 html-to-text\n```\n\n# Usage, one document per chapter\n\n```typescript\nimport { EPubLoader } from \"langchain/document_loaders/fs/epub\";\n\nconst loader = new EPubLoader(\"src/document_loaders/example_data/example.epub\");\n\nconst docs = await loader.load();\n```\n\n# Usage, one document per file\n\n```typescript\nimport { EPubLoader } from \"langchain/document_loaders/fs/epub\";\n\nconst loader = new EPubLoader(\n  \"src/document_loaders/example_data/example.epub\",\n  {\n    splitChapters: false,\n  }\n);\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/epub.md","loc":{"lines":{"from":1,"to":38}}}}],["53",{"pageContent":"# JSON files\n\nThe JSON loader use [JSON pointer](https://github.com/janl/node-jsonpointer) to target keys in your JSON files you want to target.\n\n### No JSON pointer example\n\nThe most simple way of using it, is to specify no JSON pointer.\nThe loader will load all strings it finds in the JSON object.\n\nExample JSON file:\n\n```json\n{\n  \"texts\": [\"This is a sentence.\", \"This is another sentence.\"]\n}\n```\n\nExample code:\n\n```typescript\nimport { JSONLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLoader(\"src/document_loaders/example_data/example.json\");\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/json.md","loc":{"lines":{"from":1,"to":46}}}}],["54",{"pageContent":"Using JSON pointer example\n\nYou can do a more advanced scenario by choosing which keys in your JSON object you want to extract string from.\n\nIn this example, we want to only extract information from \"from\" and \"surname\" entries.\n\n```json\n{\n  \"1\": {\n    \"body\": \"BD 2023 SUMMER\",\n    \"from\": \"LinkedIn Job\",\n    \"labels\": [\"IMPORTANT\", \"CATEGORY_UPDATES\", \"INBOX\"]\n  },\n  \"2\": {\n    \"body\": \"Intern, Treasury and other roles are available\",\n    \"from\": \"LinkedIn Job2\",\n    \"labels\": [\"IMPORTANT\"],\n    \"other\": {\n      \"name\": \"plop\",\n      \"surname\": \"bob\"\n    }\n  }\n}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/json.md","loc":{"lines":{"from":48,"to":70}}}}],["55",{"pageContent":"Example code:\n\n```typescript\nimport { JSONLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLoader(\n  \"src/document_loaders/example_data/example.json\",\n  [\"/from\", \"/surname\"]\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"BD 2023 SUMMER\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"LinkedIn Job\",\n  },\n  ...\n]\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/json.md","loc":{"lines":{"from":73,"to":104}}}}],["56",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# JSONLines files\n\nThis example goes over how to load data from JSONLines or JSONL files. The second argument is a JSONPointer to the property to extract from each JSON object in the file. One document will be created for each JSON object in the file.\n\nExample JSONLines file:\n\n```json\n{\"html\": \"This is a sentence.\"}\n{\"html\": \"This is another sentence.\"}\n```\n\nExample code:\n\n```typescript\nimport { JSONLinesLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLinesLoader(\n  \"src/document_loaders/example_data/example.jsonl\",\n  \"/html\"\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/jsonl+json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/jsonl+json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/jsonlines.md","loc":{"lines":{"from":1,"to":47}}}}],["57",{"pageContent":"# PDF files\n\nThis example goes over how to load data from PDF files. By default, one document will be created for each page in the PDF file, you can change this behavior by setting the `splitPages` option to `false`.\n\n## Setup\n\n```bash npm2yarn\nnpm install pdfjs-dist\n```\n\n## Usage, one document per page\n\n```typescript\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\");\n\nconst docs = await loader.load();\n```\n\n## Usage, one document per file\n\n```typescript\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  splitPages: false,\n});\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/pdf.md","loc":{"lines":{"from":1,"to":31}}}}],["58",{"pageContent":"Usage, legacy environments\n\nIn legacy environments, you can use the `pdfjs` option to provide a function that returns a promise that resolves to the `PDFJS` object. This is useful if you want to use a custom build of `pdfjs-dist` or if you want to use a different version of `pdfjs-dist`.\n\nHere we use the legacy build of `pdfjs-dist`, which includes several polyfills not included in the default build.\n\n```typescript\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  // you may need to add `.then(m => m.default)` to the end of the import\n  pdfjs: () => import(\"pdfjs-dist/legacy/build/pdf.js\"),\n});\n```\n\nAlternatively, if the legacy build of `pdfjs-dist` doesn't work for you, you can use an older version bundled with `pdf-parse`:\n\n```bash npm2yarn\nnpm rm pdfjs-dist # if you had installed it before\nnpm install pdf-parse","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/pdf.md","loc":{"lines":{"from":33,"to":52}}}}],["59",{"pageContent":"```typescript\nimport { PDFLoader } from \"langchain/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  // you may need to add `.then(m => m.default)` to the end of the import\n  pdfjs: () => import(\"pdf-parse/lib/pdf.js/v1.10.100/build/pdf.js\"),\n});\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/pdf.md","loc":{"lines":{"from":55,"to":62}}}}],["60",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Subtitles\n\nThis example goes over how to load data from subtitle files. One document will be created for each subtitles file.\n\n## Setup\n\n```bash npm2yarn\nnpm install srt-parser-2\n```\n\n## Usage\n\n```typescript\nimport { SRTLoader } from \"langchain/document_loaders/fs/srt\";\n\nconst loader = new SRTLoader(\n  \"src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt\"\n);\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/subtitles.md","loc":{"lines":{"from":1,"to":25}}}}],["61",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Text files\n\nThis example goes over how to load data from text files.\n\n```typescript\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/file_loaders/text.md","loc":{"lines":{"from":1,"to":15}}}}],["62",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# College Confidential\n\nThis example goes over how to load data from the college confidential website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { CollegeConfidentialLoader } from \"langchain/document_loaders/web/college_confidential\";\n\nconst loader = new CollegeConfidentialLoader(\n  \"https://www.collegeconfidential.com/colleges/brown-university/\"\n);\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/college_confidential.md","loc":{"lines":{"from":1,"to":25}}}}],["63",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# GitBook\n\nThis example goes over how to load data from any GitBook, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Load from single GitBook page\n\n```typescript\nimport { GitbookLoader } from \"langchain/document_loaders/web/gitbook\";\n\nconst loader = new GitbookLoader(\n  \"https://docs.gitbook.com/product-tour/navigation\"\n);\n\nconst docs = await loader.load();\n```\n\n## Load from all paths in a given GitBook\n\nFor this to work, the GitbookLoader needs to be initialized with the root path (https://docs.gitbook.com in this example) and have `shouldLoadAllPaths` set to `true`.\n\n```typescript\nimport { GitbookLoader } from \"langchain/document_loaders/web/gitbook\";\n\nconst loader = new GitbookLoader(\"https://docs.gitbook.com\", {\n  shouldLoadAllPaths: true,\n});\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/gitbook.md","loc":{"lines":{"from":1,"to":39}}}}],["64",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# GitHub\n\nThis example goes over how to load data from a GitHub repository.\nYou can set the `GITHUB_ACCESS_TOKEN` environment variable to a GitHub access token to increase the rate limit and access private repositories.\n\n```typescript\nimport { GithubRepoLoader } from \"langchain/document_loaders/web/github\";\n\nconst loader = new GithubRepoLoader(\n  \"https://github.com/hwchase17/langchainjs\",\n  { branch: \"main\", recursive: false, unknown: \"warn\" }\n);\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/github.md","loc":{"lines":{"from":1,"to":18}}}}],["65",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Hacker News\n\nThis example goes over how to load data from the hacker news website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { HNLoader } from \"langchain/document_loaders/web/hn\";\n\nconst loader = new HNLoader(\"https://news.ycombinator.com/item?id=34817881\");\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/hn.md","loc":{"lines":{"from":1,"to":23}}}}],["66",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# IMSDB\n\nThis example goes over how to load data from the internet movie script database website, using Cheerio. One document will be created for each page.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { IMSDBLoader } from \"langchain/document_loaders/web/imsdb\";\n\nconst loader = new IMSDBLoader(\"https://imsdb.com/scripts/BlacKkKlansman.html\");\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/imsdb.md","loc":{"lines":{"from":1,"to":23}}}}],["67",{"pageContent":"---\nsidebar_position: 1\nsidebar_label: Cheerio\nhide_table_of_contents: true\n---\n\n# Webpages, with Cheerio\n\nThis example goes over how to load data from webpages using Cheerio. One document will be created for each webpage.\n\nCheerio is a fast and lightweight library that allows you to parse and traverse HTML documents using a jQuery-like syntax. You can use Cheerio to extract data from web pages, without having to render them in a browser.\n\nHowever, Cheerio does not simulate a web browser, so it cannot execute JavaScript code on the page. This means that it cannot extract data from dynamic web pages that require JavaScript to render. To do that, you can use the [PlaywrightWebBaseLoader](./web_playwright.md) or [PuppeteerWebBaseLoader](./web_puppeteer.md) instead.\n\n## Setup\n\n```bash npm2yarn\nnpm install cheerio\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_cheerio.md","loc":{"lines":{"from":1,"to":19}}}}],["68",{"pageContent":"Setup\n\n```bash npm2yarn\nnpm install cheerio\n```\n\n## Usage\n\n```typescript\nimport { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://news.ycombinator.com/item?id=34817881\"\n);\n\nconst docs = await loader.load();\n```\n\n## Usage, with a custom selector\n\n```typescript\nimport { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://news.ycombinator.com/item?id=34817881\",\n  {\n    selector: \"p.athing\",\n  }\n);\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_cheerio.md","loc":{"lines":{"from":19,"to":50}}}}],["69",{"pageContent":"---\nsidebar_position: 3\nhide_table_of_contents: true\nsidebar_class_name: node-only\nsidebar_label: Playwright\n---\n\n# Webpages, with Playwright\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nThis example goes over how to load data from webpages using Playwright. One document will be created for each webpage.\n\nPlaywright is a Node.js library that provides a high-level API for controlling multiple browser engines, including Chromium, Firefox, and WebKit. You can use Playwright to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.\n\nIf you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the [CheerioWebBaseLoader](./web_cheerio.md) instead.\n\n## Setup\n\n```bash npm2yarn\nnpm install playwright\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright.md","loc":{"lines":{"from":1,"to":24}}}}],["70",{"pageContent":"Setup\n\n```bash npm2yarn\nnpm install playwright\n```\n\n## Usage\n\n```typescript\nimport { PlaywrightWebBaseLoader } from \"langchain/document_loaders/web/playwright\";\n\n/**\n * Loader uses `page.content()`\n * as default evaluate function\n **/\nconst loader = new PlaywrightWebBaseLoader(\"https://www.tabnews.com.br/\");\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright.md","loc":{"lines":{"from":24,"to":42}}}}],["71",{"pageContent":"Options\n\nHere's an explanation of the parameters you can pass to the PlaywrightWebBaseLoader constructor using the PlaywrightWebBaseLoaderOptions interface:\n\n```typescript\ntype PlaywrightWebBaseLoaderOptions = {\n  launchOptions?: LaunchOptions;\n  gotoOptions?: PlaywrightGotoOptions;\n  evaluate?: PlaywrightEvaluate;\n};","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright.md","loc":{"lines":{"from":44,"to":53}}}}],["72",{"pageContent":"1. `launchOptions`: an optional object that specifies additional options to pass to the playwright.chromium.launch() method. This can include options such as the headless flag to launch the browser in headless mode.\n\n2. `gotoOptions`: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.\n\n3. `evaluate`: an optional function that can be used to evaluate JavaScript code on the page using a custom evaluation function. This can be useful for extracting data from the page or interacting with page elements. The function should return a Promise that resolves to a string containing the result of the evaluation.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright.md","loc":{"lines":{"from":56,"to":60}}}}],["73",{"pageContent":"By passing these options to the `PlaywrightWebBaseLoader` constructor, you can customize the behavior of the loader and use Playwright's powerful features to scrape and interact with web pages.\n\nHere is a basic example to do it:\n\n```typescript\nimport { PlaywrightWebBaseLoader } from \"langchain/document_loaders/web/playwright\";\n\nconst loader = new PlaywrightWebBaseLoader(\"https://www.tabnews.com.br/\", {\n  launchOptions: {\n    headless: true,\n  },\n  gotoOptions: {\n    waitUntil: \"domcontentloaded\",\n  },\n  /** Pass custom evaluate, in this case you get page and browser instances */\n  async evaluate(page: Page, browser: Browser) {\n    await page.waitForResponse(\"https://www.tabnews.com.br/va/view\");\n\n    const result = await page.evaluate(() => document.body.innerHTML);\n    return result;\n  },\n});\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_playwright.md","loc":{"lines":{"from":62,"to":86}}}}],["74",{"pageContent":"---\nsidebar_position: 2\nsidebar_label: Puppeteer\nhide_table_of_contents: true\nsidebar_class_name: node-only\n---\n\n# Webpages, with Puppeteer\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\nThis example goes over how to load data from webpages using Puppeteer. One document will be created for each webpage.\n\nPuppeteer is a Node.js library that provides a high-level API for controlling headless Chrome or Chromium. You can use Puppeteer to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.\n\nIf you want a lighterweight solution, and the webpages you want to load do not require JavaScript to render, you can use the [CheerioWebBaseLoader](./web_cheerio.md) instead.\n\n## Setup\n\n```bash npm2yarn\nnpm install puppeteer\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer.md","loc":{"lines":{"from":1,"to":24}}}}],["75",{"pageContent":"Setup\n\n```bash npm2yarn\nnpm install puppeteer\n```\n\n## Usage\n\n```typescript\nimport { PuppeteerWebBaseLoader } from \"langchain/document_loaders/web/puppeteer\";\n\n/**\n * Loader uses `page.evaluate(() => document.body.innerHTML)`\n * as default evaluate function\n **/\nconst loader = new PuppeteerWebBaseLoader(\"https://www.tabnews.com.br/\");\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer.md","loc":{"lines":{"from":24,"to":42}}}}],["76",{"pageContent":"Options\n\nHere's an explanation of the parameters you can pass to the PuppeteerWebBaseLoader constructor using the PuppeteerWebBaseLoaderOptions interface:\n\n```typescript\ntype PuppeteerWebBaseLoaderOptions = {\n  launchOptions?: PuppeteerLaunchOptions;\n  gotoOptions?: PuppeteerGotoOptions;\n  evaluate?: (page: Page, browser: Browser) => Promise<string>;\n};","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer.md","loc":{"lines":{"from":44,"to":53}}}}],["77",{"pageContent":"1. `launchOptions`: an optional object that specifies additional options to pass to the puppeteer.launch() method. This can include options such as the headless flag to launch the browser in headless mode, or the slowMo option to slow down Puppeteer's actions to make them easier to follow.\n\n2. `gotoOptions`: an optional object that specifies additional options to pass to the page.goto() method. This can include options such as the timeout option to specify the maximum navigation time in milliseconds, or the waitUntil option to specify when to consider the navigation as successful.\n\n3. `evaluate`: an optional function that can be used to evaluate JavaScript code on the page using the page.evaluate() method. This can be useful for extracting data from the page or interacting with page elements. The function should return a Promise that resolves to a string containing the result of the evaluation.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer.md","loc":{"lines":{"from":56,"to":60}}}}],["78",{"pageContent":"By passing these options to the `PuppeteerWebBaseLoader` constructor, you can customize the behavior of the loader and use Puppeteer's powerful features to scrape and interact with web pages.\n\nHere is a basic example to do it:\n\n```typescript\nimport { PuppeteerWebBaseLoader } from \"langchain/document_loaders/web/puppeteer\";\n\nconst loader = new PuppeteerWebBaseLoader(\"https://www.tabnews.com.br/\", {\n  launchOptions: {\n    headless: true,\n  },\n  gotoOptions: {\n    waitUntil: \"domcontentloaded\",\n  },\n  /** Pass custom evaluate, in this case you get page and browser instances */\n  async evaluate(page: Page, browser: Browser) {\n    await page.waitForResponse(\"https://www.tabnews.com.br/va/view\");\n\n    const result = await page.evaluate(() => document.body.innerHTML);\n    return result;\n  },\n});\n\nconst docs = await loader.load();\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/document_loaders/examples/web_loaders/web_puppeteer.md","loc":{"lines":{"from":62,"to":86}}}}],["79",{"pageContent":"---\nhide_table_of_contents: true\n---\n\n# Vector Store\n\nOnce you've created a [Vector Store](../vector_stores/), the way to use it as a Retriever is very simple:\n\n```typescript\nvectorStore = ...\nretriever = vectorStore.asRetriever()\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/retrievers/vectorstore.md","loc":{"lines":{"from":1,"to":12}}}}],["80",{"pageContent":"# Chroma\n\nChroma is an open-source Apache 2.0 embedding database.\n\nUse [chroma](https://github.com/chroma-core/chroma) with langchainjs.\n\n## Setup\n\n1. Run chroma inside of docker on your computer [docs](https://docs.trychroma.com/api-reference)\n2. Install the chroma js client.\n\n```bash npm2yarn\nnpm install -S chromadb\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/chroma.md","loc":{"lines":{"from":1,"to":14}}}}],["81",{"pageContent":"Index and query docs\n\n```typescript\nimport { Chroma } from \"langchain/vectorstores/chroma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\n// text sample from Godel, Escher, Bach\nconst vectorStore = await Chroma.fromTexts(\n  [\n    \"Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\\\n        Harmonic Labyrinth of the dreaded Majotaur?\",\n    \"Achilles: Yiikes! What is that?\",\n    \"Tortoise: They say-although I person never believed it myself-that an I\\\n        Majotaur has created a tiny labyrinth sits in a pit in the middle of\\\n        it, waiting innocent victims to get lost in its fears complexity.\\\n        Then, when they wander and dazed into the center, he laughs and\\\n        laughs at them-so hard, that he laughs them to death!\",\n    \"Achilles: Oh, no!\",\n    \"Tortoise: But it's only a myth. Courage, Achilles.\",\n  ],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel-escher-bach\",\n  }\n);","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/chroma.md","loc":{"lines":{"from":16,"to":41}}}}],["82",{"pageContent":"// or alternatively from docs\nconst vectorStore = await Chroma.fromDocuments(docs, new OpenAIEmbeddings(), {\n  collectionName: \"goldel-escher-bach\",\n});\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/chroma.md","loc":{"lines":{"from":43,"to":49}}}}],["83",{"pageContent":"Query docs from existing collection\n\n```typescript\nimport { Chroma } from \"langchain/vectorstores/chroma\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await Chroma.fromExistingCollection(\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel-escher-bach\",\n  }\n);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/chroma.md","loc":{"lines":{"from":51,"to":65}}}}],["84",{"pageContent":"---\nsidebar_class_name: node-only\n---\n\n# Milvus\n\n[Milvus](https://milvus.io/) is a vector database built for embeddings similarity search and AI applications.\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\n## Setup\n\n1. Run Milvus instance with Docker on your computer [docs](https://milvus.io/docs/v2.1.x/install_standalone-docker.md)\n2. Install the Milvus Node.js SDK.\n\n```bash npm2yarn\nnpm install -S @zilliz/milvus2-sdk-node\n```\n\n3. Setup Env variables for Milvus before running the code\n\n```bash\nexport OPENAI_API_KEY=YOUR_OPEN_API_HERE\nexport MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":1,"to":27}}}}],["85",{"pageContent":"Index and query docs\n\n```typescript\nimport { Milvus } from \"langchain/vectorstores/milvus\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":29,"to":33}}}}],["86",{"pageContent":"// text sample from Godel, Escher, Bach\nconst vectorStore = await Milvus.fromTexts(\n  [\n    \"Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\\\n            Harmonic Labyrinth of the dreaded Majotaur?\",\n    \"Achilles: Yiikes! What is that?\",\n    \"Tortoise: They say-although I person never believed it myself-that an I\\\n            Majotaur has created a tiny labyrinth sits in a pit in the middle of\\\n            it, waiting innocent victims to get lost in its fears complexity.\\\n            Then, when they wander and dazed into the center, he laughs and\\\n            laughs at them-so hard, that he laughs them to death!\",\n    \"Achilles: Oh, no!\",\n    \"Tortoise: But it's only a myth. Courage, Achilles.\",\n  ],\n  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel_escher_bach\",\n  }\n);","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":35,"to":54}}}}],["87",{"pageContent":"// or alternatively from docs\nconst vectorStore = await Milvus.fromDocuments(docs, new OpenAIEmbeddings(), {\n  collectionName: \"goldel_escher_bach\",\n});\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":56,"to":62}}}}],["88",{"pageContent":"Query docs from existing collection\n\n```typescript\nimport { Milvus } from \"langchain/vectorstores/milvus\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await Milvus.fromExistingCollection(\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel_escher_bach\",\n  }\n);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/milvus.md","loc":{"lines":{"from":64,"to":78}}}}],["89",{"pageContent":"---\nsidebar_class_name: node-only\n---\n\n# OpenSearch\n\n:::tip Compatibility\nOnly available on Node.js.\n:::\n\n[OpenSearch](https://opensearch.org/) is a fork of [Elasticsearch](https://www.elastic.co/elasticsearch/) that is fully compatible with the Elasticsearch API. Read more about their support for Approximate Nearest Neighbors [here](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/).\n\nLangchain.js accepts [@opensearch-project/opensearch](https://opensearch.org/docs/latest/clients/javascript/index/) as the client for OpenSearch vectorstore.\n\n## Setup\n\n```bash npm2yarn\nnpm install -S @opensearch-project/opensearch\n```\n\nYou'll also need to have an OpenSearch instance running. You can use the [official Docker image](https://opensearch.org/docs/latest/opensearch/install/docker/) to get started. You can also find an example docker-compose file [here](https://github.com/hwchase17/langchainjs/blob/main/examples/src/indexes/vector_stores/opensearch/docker-compose.yml).","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/opensearch.md","loc":{"lines":{"from":1,"to":21}}}}],["90",{"pageContent":"Index docs\n\n```typescript\nimport { Client } from \"@opensearch-project/opensearch\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\nimport { OpenSearchVectorStore } from \"langchain/vectorstores\";\n\nconst client = new Client({\n  nodes: [process.env.OPENSEARCH_URL ?? \"http://127.0.0.1:9200\"],\n});\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"opensearch is also a vector db\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent:\n      \"OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications\",\n  }),\n];","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/opensearch.md","loc":{"lines":{"from":23,"to":53}}}}],["91",{"pageContent":"await OpenSearchVectorStore.fromDocuments(docs, new OpenAIEmbeddings(), {\n  client,\n  indexName: process.env.OPENSEARCH_INDEX, // Will default to `documents`\n});\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/opensearch.md","loc":{"lines":{"from":55,"to":59}}}}],["92",{"pageContent":"Query docs\n\n```typescript\nimport { Client } from \"@opensearch-project/opensearch\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings\";\nimport { OpenAI } from \"langchain/llms\";\nimport { OpenSearchVectorStore } from \"langchain/vectorstores\";\n\nconst client = new Client({\n  nodes: [process.env.OPENSEARCH_URL ?? \"http://127.0.0.1:9200\"],\n});\n\nconst vectorStore = new OpenSearchVectorStore(new OpenAIEmbeddings(), {\n  client,\n});\n\n/* Search the vector DB independently with meta filters */\nconst results = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(JSON.stringify(results, null, 2));\n/* [\n    {\n      \"pageContent\": \"Hello world\",\n      \"metadata\": {\n        \"id\": 2\n      }\n    }\n  ] */","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/opensearch.md","loc":{"lines":{"from":61,"to":88}}}}],["93",{"pageContent":"/* Use as part of a chain (currently no metadata filters) */\nconst model = new OpenAI();\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore, {\n  k: 1,\n  returnSourceDocuments: true,\n});\nconst response = await chain.call({ query: \"What is opensearch?\" });\n\nconsole.log(JSON.stringify(response, null, 2));\n/* \n  {\n    \"text\": \" Opensearch is a collection of technologies that allow search engines to publish search results in a standard format, making it easier for users to search across multiple sites.\",\n    \"sourceDocuments\": [\n      {\n        \"pageContent\": \"What's this?\",\n        \"metadata\": {\n          \"id\": 3\n        }\n      }\n    ]\n  } \n  */\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/opensearch.md","loc":{"lines":{"from":90,"to":112}}}}],["94",{"pageContent":"# Pinecone\n\nLangchain.js accepts [@pinecone-database/pinecone](https://docs.pinecone.io/docs/node-client) as the client for Pinecone vectorstore. Install the library with\n\n```bash npm2yarn\nnpm install -S dotenv langchain @pinecone-database/pinecone\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/pinecone.md","loc":{"lines":{"from":1,"to":7}}}}],["95",{"pageContent":"Index docs\n\n```typescript\nimport { PineconeClient } from \"@pinecone-database/pinecone\";\nimport * as dotenv from \"dotenv\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { PineconeStore } from \"langchain/vectorstores/pinecone\";\n\ndotenv.config();\n\nconst client = new PineconeClient();\nawait client.init({\n  apiKey: process.env.PINECONE_API_KEY,\n  environment: process.env.PINECONE_ENVIRONMENT,\n});\nconst pineconeIndex = client.Index(process.env.PINECONE_INDEX);\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"pinecone is a vector db\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"pinecones are the woody fruiting body and of a pine tree\",\n  }),\n];","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/pinecone.md","loc":{"lines":{"from":9,"to":44}}}}],["96",{"pageContent":"await PineconeStore.fromDocuments(docs, new OpenAIEmbeddings(), {\n  pineconeIndex,\n});\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/pinecone.md","loc":{"lines":{"from":46,"to":49}}}}],["97",{"pageContent":"Query docs\n\n```typescript\nimport { PineconeClient } from \"@pinecone-database/pinecone\";\nimport * as dotenv from \"dotenv\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { PineconeStore } from \"langchain/vectorstores/pinecone\";\n\ndotenv.config();\n\nconst client = new PineconeClient();\nawait client.init({\n  apiKey: process.env.PINECONE_API_KEY,\n  environment: process.env.PINECONE_ENVIRONMENT,\n});\nconst pineconeIndex = client.Index(process.env.PINECONE_INDEX);\n\nconst vectorStore = await PineconeStore.fromExistingIndex(\n  new OpenAIEmbeddings(),\n  { pineconeIndex }\n);\n\n/* Search the vector DB independently with meta filters */\nconst results = await vectorStore.similaritySearch(\"pinecone\", 1, {\n  foo: \"bar\",\n});\nconsole.log(results);\n/*\n[\n  Document {\n    pageContent: 'pinecone is a vector db',\n    metadata: { foo: 'bar' }\n  }\n]\n*/","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/pinecone.md","loc":{"lines":{"from":51,"to":87}}}}],["98",{"pageContent":"/* Use as part of a chain (currently no metadata filters) */\nconst model = new OpenAI();\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore, {\n  k: 1,\n  returnSourceDocuments: true,\n});\nconst response = await chain.call({ query: \"What is pinecone?\" });\nconsole.log(response);\n/*\n{\n  text: ' A pinecone is the woody fruiting body of a pine tree.',\n  sourceDocuments: [\n    Document {\n      pageContent: 'pinecones are the woody fruiting body and of a pine tree',\n      metadata: [Object]\n    }\n  ]\n}\n*/\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/indexes/vector_stores/integrations/pinecone.md","loc":{"lines":{"from":89,"to":108}}}}],["99",{"pageContent":"# Buffer Memory\n\nBufferMemory is the simplest type of memory - it just remembers previous conversational back and forths directly.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst memory = new BufferMemory();\nconst chain = new ConversationChain({ llm: model, memory: memory });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n```\n\n```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/memory/examples/buffer_memory.md","loc":{"lines":{"from":1,"to":27}}}}],["100",{"pageContent":"```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```\n\nYou can also load messages into a `BufferMemory` instance by creating and passing in a `ChatHistory` object.\nThis lets you easily pick up state from past conversations:\n\n```typescript\nimport { ChatMessageHistory } from \"langchain/memory\";\nimport { HumanChatMessage, AIChatMessage } from \"langchain/schema\";\n\nconst pastMessages = [\n  new HumanChatMessage(\"My name's Jonas\"),\n  new AIChatMessage(\"Nice to meet you, Jonas!\"),\n];\n\nconst memory = new BufferMemory({\n  chatHistory: new ChatMessageHistory(pastMessages),\n});\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/memory/examples/buffer_memory.md","loc":{"lines":{"from":27,"to":46}}}}],["101",{"pageContent":"# Buffer Window Memory\n\nBufferWindowMemory keeps track of the back-and-forths in conversation, and then uses a window of size `k` to surface the last `k` back-and-forths to use as memory.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { BufferWindowMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\n\nconst model = new OpenAI({});\nconst memory = new BufferWindowMemory({ k: 1 });\nconst chain = new ConversationChain({ llm: model, memory: memory });\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n```\n\n```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/memory/examples/buffer_window_memory.md","loc":{"lines":{"from":1,"to":28}}}}],["102",{"pageContent":"# Motörhead Memory\n\n[Motörhead](https://github.com/getmetal/motorhead) is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.\n\n## Setup\n\nSee instructions at [Motörhead](https://github.com/getmetal/motorhead) for running the server locally.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/memory/examples/motorhead_memory.md","loc":{"lines":{"from":1,"to":7}}}}],["103",{"pageContent":"Usage\n\n```typescript\nimport { ConversationChain } from \"langchain/chains\";\nimport { ChatOpenAI } from \"langchain/chat_models\";\nimport { MotorheadMemory } from \"langchain/memory\";\n\nconst model = new ChatOpenAI({});\nconst memory = new MotorheadMemory({\n  sessionId: \"user-id\",\n  motorheadUrl: \"localhost:8080\",\n});\n\nawait memory.init(); // loads previous state from Motörhead 🤘\nconst context = memory.context\n  ? `\nHere's previous context: ${memory.context}`\n  : \"\";\n\nconst chatPrompt = ChatPromptTemplate.fromPromptMessages([\n  SystemMessagePromptTemplate.fromTemplate(\n    `The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.${context}`\n  ),\n  new MessagesPlaceholder(\"history\"),\n  HumanMessagePromptTemplate.fromTemplate(\"{input}\"),\n]);\n\nconst chain = new ConversationChain({\n  memory,\n  prompt: chatPrompt,\n  llm: chat,\n});","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/memory/examples/motorhead_memory.md","loc":{"lines":{"from":9,"to":40}}}}],["104",{"pageContent":"const chain = new ConversationChain({\n  memory,\n  prompt: chatPrompt,\n  llm: chat,\n});\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/memory/examples/motorhead_memory.md","loc":{"lines":{"from":40,"to":47}}}}],["105",{"pageContent":"```shell\n{response: \" Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?\"}\n```\n\n```typescript\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n```\n\n```shell\n{response: ' You said your name is Jim. Is there anything else you would like to talk about?'}\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/memory/examples/motorhead_memory.md","loc":{"lines":{"from":50,"to":61}}}}],["106",{"pageContent":"---\nhide_table_of_contents: true\nsidebar_position: 1\n---\n\n# Chat Messages\n\nThe primary interface through which end users interact with LLMs is a chat interface. For this reason, some model providers have started providing access to the underlying API in a way that expects chat messages. These messages have a content field (which is usually text) and are associated with a user (or role). Right now the supported users are System, Human, and AI.\n\n## SystemChatMessage\n\nA chat message representing information that should be instructions to the AI system.\n\n```typescript\nimport { SystemChatMessage } from \"langchain/schema\";\n\nnew SystemChatMessage(\"You are a nice assistant\");\n```\n\n## HumanChatMessage\n\nA chat message representing information coming from a human interacting with the AI system.\n\n```typescript\nimport { HumanChatMessage } from \"langchain/schema\";\n\nnew HumanChatMessage(\"Hello, how are you?\");\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/schema/chat-messages.md","loc":{"lines":{"from":1,"to":28}}}}],["107",{"pageContent":"AIChatMessage\n\nA chat message representing information coming from the AI system.\n\n```typescript\nimport { AIChatMessage } from \"langchain/schema\";\n\nnew AIChatMessage(\"I am doing well, thank you!\");\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/schema/chat-messages.md","loc":{"lines":{"from":30,"to":38}}}}],["108",{"pageContent":"# Document\n\nLanguage models only know information about what they were trained on. In order to get them answer questions or summarize other information you have to pass it to the language model. Therefore, it is very important to have a concept of a document.\n\nA document at its core is fairly simple. It consists of a piece of text and optional metadata. The piece of text is what we interact with the language model, while the optional metadata is useful for keeping track of metadata about the document (such as the source).\n\n```typescript\ninterface Document {\n  pageContent: string;\n  metadata: Record<string, any>;\n}\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/schema/document.md","loc":{"lines":{"from":1,"to":12}}}}],["109",{"pageContent":"Creating a Document\n\nYou can create a document object rather easily in LangChain with:\n\n```typescript\nimport { Document } from \"langchain/document\";\n\nconst doc = new Document({ pageContent: \"foo\" });\n```\n\nYou can create one with metadata with:\n\n```typescript\nimport { Document } from \"langchain/document\";\n\nconst doc = new Document({ pageContent: \"foo\", metadata: { source: \"1\" } });\n```\n\nAlso check out [Document Loaders](../indexes/document_loaders/) for a way to load documents from a variety of sources.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/schema/document.md","loc":{"lines":{"from":14,"to":32}}}}],["110",{"pageContent":"---\n---\n\n# Examples\n\nExamples are input/output pairs that represent inputs to a function and then expected output. They can be used in both training and evaluation of models.\n\n```typescript\ntype Example = Record<string, string>;\n```\n\n## Creating an Example\n\nYou can create an Example like this:\n\n```typescript\nconst example = {\n  input: \"foo\",\n  output: \"bar\",\n};\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/modules/schema/example.md","loc":{"lines":{"from":1,"to":21}}}}],["111",{"pageContent":"# Deployment\n\nYou've built your LangChain app and now you're looking to deploy it to production? You've come to the right place. This guide will walk you through the options you have for deploying your app, and the considerations you should make when doing so.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/production/deployment.md","loc":{"lines":{"from":1,"to":3}}}}],["112",{"pageContent":"Overview\n\nLangChain is a library for building applications that use language models. It is not a web framework, and does not provide any built-in functionality for serving your app over the web. Instead, it provides a set of tools that you can integrate in your API or backend server.\n\nThere are a couple of high-level options for deploying your app:\n\n- Deploying to a VM or container\n  - Persistent filesystem means you can save and load files from disk\n  - Always-running process means you can cache some things in memory\n  - You can support long-running requests, such as WebSockets\n- Deploying to a serverless environment\n  - No persistent filesystem means you can load files from disk, but not save them for later\n  - Cold start means you can't cache things in memory and expect them to be cached between requests\n  - Function timeouts mean you can't support long-running requests, such as WebSockets\n\nSome other considerations include:","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/production/deployment.md","loc":{"lines":{"from":5,"to":20}}}}],["113",{"pageContent":"Some other considerations include:\n\n- Do you deploy your backend and frontend together, or separately?\n- Do you deploy your backend co-located with your database, or separately?\n\nAs you move your LangChains into production, we'd love to offer more comprehensive support. Please fill out [this form](https://forms.gle/57d8AmXBYp8PP8tZA) and we'll set up a dedicated support Slack channel.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/production/deployment.md","loc":{"lines":{"from":20,"to":25}}}}],["114",{"pageContent":"Deployment Options\n\nSee below for a list of deployment options for your LangChain app. If you don't see your preferred option, please get in touch and we can add it to this list.\n\n### Deploying to Fly.io\n\n[Fly.io](https://fly.io) is a platform for deploying apps to the cloud. It's a great option for deploying your app to a container environment.\n\nSee [our Fly.io template](https://github.com/hwchase17/langchain-template-node-fly) for an example of how to deploy your app to Fly.io.","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/production/deployment.md","loc":{"lines":{"from":27,"to":35}}}}],["115",{"pageContent":"# Tracing\n\nSimilar to the Python `langchain` package, JS `langchain` also supports tracing.\n\nYou can view an overview of tracing [here.](https://langchain.readthedocs.io/en/latest/tracing.html)\nTo spin up the tracing backend, run `docker compose up` (or `docker-compose up` if on using an older version of `docker`) in the `langchain` directory.\nYou can also use the `langchain-server` command if you have the python `langchain` package installed.\n\nHere's an example of how to use tracing in `langchain.js`. All that needs to be done is setting the `LANGCHAIN_HANDLER` environment variable to `langchain`.\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport process from \"process\";","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/production/tracing.md","loc":{"lines":{"from":1,"to":16}}}}],["116",{"pageContent":"export const run = async () => {\n  process.env.LANGCHAIN_HANDLER = \"langchain\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];\n\n  const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.call({ input });\n\n  console.log(`Got output ${result.output}`);\n};","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/production/tracing.md","loc":{"lines":{"from":18,"to":43}}}}],["117",{"pageContent":"We are actively working on improving tracing to work better with concurrency. For now, the best way to use tracing with concurrency is to follow the below example:\n\n```typescript\nimport { OpenAI } from \"langchain/llms/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { Calculator } from \"langchain/tools/calculator\";\nimport process from \"process\";\nimport {\n  CallbackManager,\n  LangChainTracer,\n  ConsoleCallbackHandler,\n} from \"langchain/callbacks\";\n\nexport const run = async () => {\n  process.env.LANGCHAIN_HANDLER = \"langchain\";\n  const model = new OpenAI({ temperature: 0 });\n  const tools = [\n    new SerpAPI(process.env.SERPAPI_API_KEY, {\n      location: \"Austin,Texas,United States\",\n      hl: \"en\",\n      gl: \"us\",\n    }),\n    new Calculator(),\n  ];","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/production/tracing.md","loc":{"lines":{"from":46,"to":70}}}}],["118",{"pageContent":"const executor = await initializeAgentExecutorWithOptions(tools, model, {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  });\n  console.log(\"Loaded agent.\");\n\n  const input = `Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  // This will result in a lot of errors, because the shared Tracer is not concurrency-safe.\n  const [resultA, resultB, resultC] = await Promise.all([\n    executor.call({ input }),\n    executor.call({ input }),\n    executor.call({ input }),\n  ]);\n\n  console.log(`Got output ${resultA.output}`);\n  console.log(`Got output ${resultB.output}`);\n  console.log(`Got output ${resultC.output}`);\n\n  // This will work, because each executor has its own Tracer, avoiding concurrency issues.\n  console.log(\"---Now with concurrency-safe tracing---\");","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/production/tracing.md","loc":{"lines":{"from":72,"to":94}}}}],["119",{"pageContent":"// This will work, because each executor has its own Tracer, avoiding concurrency issues.\n  console.log(\"---Now with concurrency-safe tracing---\");\n\n  const executors = [];\n  for (let i = 0; i < 3; i += 1) {\n    const callbackManager = new CallbackManager();\n    callbackManager.addHandler(new ConsoleCallbackHandler());\n    callbackManager.addHandler(new LangChainTracer());\n\n    const model = new OpenAI({ temperature: 0, callbackManager });\n    const tools = [\n      new SerpAPI(process.env.SERPAPI_API_KEY, {\n        location: \"Austin,Texas,United States\",\n        hl: \"en\",\n        gl: \"us\",\n      }),\n      new Calculator(),\n    ];\n    for (const tool of tools) {\n      tool.callbackManager = callbackManager;\n    }\n    const executor = await initializeAgentExecutorWithOptions(tools, model, {\n      agentType: \"zero-shot-react-description\",\n      verbose: true,\n      callbackManager,\n    });\n    executor.agent.llmChain.callbackManager = callbackManager;\n    executors.push(executor);\n  }","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/production/tracing.md","loc":{"lines":{"from":94,"to":122}}}}],["120",{"pageContent":"const results = await Promise.all(\n    executors.map((executor) => executor.call({ input }))\n  );\n  for (const result of results) {\n    console.log(`Got output ${result.output}`);\n  }\n};\n```","metadata":{"source":"/Users/marcus/personal/monkeylabs/docs/docs/production/tracing.md","loc":{"lines":{"from":124,"to":131}}}}]]